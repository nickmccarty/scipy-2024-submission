,Title,URL,Transcript
0,Understanding Missing Data & How to Deal with It,https://www.youtube.com/watch?v=xySfPLTgTEg,library i found is missing no for for just generally visualizing missingness um i guess i may not have mentioned this at the beginning for a lot of the tools for dealing with missing data they're not that well involved in python so there's no i don't particularly advocate any library over any other right now um but i think as far as displaying missing data this one is probably the the most mature okay so um this is taken from uh this is i actually got this from kaggle the data set from kaggle it's uh has to do with housing in london uh it's known to have some fields missing so as again as part of any uh exploratory data analysis one of the first things you should always check for is what's missing in this data set and we're going to go ahead and assume here that if it's blank it comes in as an n a it's missing and you can see here this gives you sort of your general statistics the bar graph method of missing no will basically give you these statistics but in a nice bar graph format and so a lot of these other options and you can look this up in the missingno documentation can help you put in a prettier display if i actually just did the uh i've already kind of pre-programmed these with quote-unquote nice values because i think to be honest if i were to use just the defaults it comes out a little bit ugly and unattractive so um but the the thing about this is it'll give you at a glance um actually i believe this is the opposite of missing this tells you is a bar graph of um yeah this is a bar graph of actually what's there so all the ones that are all the way filled up to the top means nothing's missing or very little is missing and the ones that are further down mean more missing and in this case they sort it by by the uh the degree of missingness so the stuff on the far right has the most missing and those on the left have either none or the fewest also uh the tutorial i mentioned up here actually i might as well just bring it up so that you can see you'll see that it's very much the same type of thing i did not include uh he uses two data sets one was a starbucks store location data set as well i wasn't quite sure what the distribution rates were for that data set so i didn't want to include it um so if you're not getting enough out of this um tutorial i would encourage you to go to the other one the link is in the in the notebook okay another way to look at missing data is using this uh matrix format this basically it's kind of a [Music] this will help you at least visually look at co-missing data and this kind of goes back the gentleman in the back asked about you know identifying patterns this is this is potentially a tool to help you identify you can kind of see that number of houses in area size probably a cool missing you kind of you know any straight line across that's white is a value that's missing um it has this feature here on the side called sparkline i don't really i'm not sure if i find it particularly useful but it's sort of a histogram of what's missing in the other direction it is a visual aid but i think the heat map personally i think is a little more useful for that you know identifying patterns so this is basically it's almost like a correlation heat map so area size and number of houses tend to be missing at the same time well they're missing at the same time all the time when they do so this so number of jobs in population size those two values are missing concurrently the most um okay and probably the the biggest thing that might help you like especially if you're trying to identify a monotonic relationship is this last feature i personally don't really know that much about dendrograms but uh what this shows is when you have clusters of missing data which are more associated and later on with the imputation techniques you'll probably want to start with the way they a lot of them work is that you start with the columns with the least data missing and so these days these features are useful for predicting this one stuff that's predicting missing values in this column and goes on so forth so life satisfaction is probably the one that's going to be you're going to want to use everything to predict um at least that's my take on what how this could be useful so this this is sort of a very short module um partly because in my experience i don't use it that much but it is not a bad way to kind of get a good glance at a data set i mean if uh for instance this matrix looks like swiss cheese you might be in bad shape so that's uh whereas if a lot of it's in straight lines it might uh indicate there's a predictable mechanism and so with that it's easier to address so that's uh it's kind of a short module and that really is all i have to say about it uh aside from you know just seeing how bad the damage is it can help you in predicting um or guiding your imputation tools which is what the next section will be uh we'll introduce um so any questions on this or anything that we've talked about so far yeah yeah i believe this the matrix plot does it um in row order i'm not 100 sure but i think that's what it does uh otherwise you know you stack it and you'd see you know big swaths of uh white so my guess is here it's it's the numbering on the left is probably the row number okay so uh before we keep moving on did you guys have any questions on sort of the theoretical stuff we talked about before the break now they've had some time to let it sink in or okay all right so to be honest we get i personally i think it gets a little more fun going forward less all right so here's how we start um okay so i want you guys to load the first few cells just to make sure that the data is um is loading for you so if you if you haven't been if you're not able to load the first two cells let me know get somebody to help you if you're all running out of binders should probably not be a problem should all be there okay so um i know a few of you have mentioned that uh the way you quite frequently deal with data sets is you simply uh you you just simply drop the data well this is a this is referred to in sort of the missingness world as deletion and it works perfect great for m car missingness and there's two types deletion list and pair this is what you would think of and that is if that observation has a missing data just chuck it you don't need it um and the most comm and it's really simple to do in pandas you just simply do a drop in a and all the columns with missing data are gone and so just as a comparison this i believe is our m oh this is our uh mar example this kind of demonstrates why for mar it's bad because you've actually in this case uh by doing so you have a 10 and 12 percent difference in mean so if this were m car i think we did load the m car data set you can see here in this case it is more justified just simply dropping it the values uh the percentage difference between the mean median and standard deviation is relatively small it's under under a percent under half a percent and that's not bad considering that uh 20 of the data is missing and we haven't changed the statistics really significantly the other reason is the other cause is what they call a way to treat is what they call repair deletion and um that so and that's referred to because a lot of operations do things in pairs like covariance and correlation so pair deletion simply is uh let me go explain this this is i believe yeah this is the formula without all the special notation it's the formula for uh computing the covariance the difference here is that if x sub i or x of j k is missing we just delete that particular term so we only sum over the k's where those values persist so so if they're missing we drop the whole pair so we're not doing dropping the entire row so let's say that you have uh four columns a b c and d and uh if you're computing the correlation between c and d there's no reason to drop that row but if you uh let me back up an a is missing um but if you're computing the correlation between a and b a is missing so you drop that one for that particular row but you don't need to do it for c and d because they're not missing so that's what uh pair deletion is uh there wasn't really a simple example to to duke right in the code but when you do you may have noticed that when i did uh say m car df and i do the correlation right it doesn't complain that i have missing values all right so probably one of the question that should have been asked is well why didn't it say well you had missing values uh built into the correlation functions uh and the covariance function is uh pairwise deletion so if the column is actually missing uh the two columns that are actually computing the correlation for have no missing values it retains it but if it is missing a value it just drops that particular row in doing the calculation so in a lot of ways when you use these pandas functions pairwise deletions already built in so you don't really have to worry about it but it is something to be mindful about when you're actually executing this and i think the other one let's say mar df dot com i mean also mean and median as well but uh i wouldn't call it pairwise deletion because you only have one value so it's it's just simply a deletion but anyway so those are two demonstrations okay um this next topic is a little tangential to where the rest of this is going but i think it's worth mentioning just for completeness sake if your data is time series or there's a spatial the other thing is or spatial connection or geometric connection you can interpolate as opposed to impute and that basically means if your data is continuous in some way you know you can fill you can fill in any missing values by extrapolating from what the existing ones are the neighboring ones for instance so this is a simple example that's in the data set again let me know if you can't have trouble seeing it this one actually has quite a lot of it missing um see so okay i think actually i in my notes originally i had the let me just do the defaults it's ugly but oh it did not include that you'll need to import that if you want to follow along [Music] so this gives you i just misspelled it yeah yeah so you can see that the white is what's missing so you can see that actually a lot of it's missing and in fact if we do the typical thing data frame is null so each of those has uh y and z have 40 missing and i think that's a out of 100 maybe camera if it's out of 100 or 60. well anyway um okay and just uh this next one is just to uh i'm coloring creating a missingness uh oh this is just because i want to color some things in here later so this that's just to set up a color map and also identify which y values are missing so that we can color them differently that's all that is okay so uh the most basic um interpolation technique is uh backfill or forward fill which is just taking the last uh your last observation and just duplicating it forward or taking the next one and copying it backwards i've seen this listed as an imputation technique which i really find there's no justification for if you don't have any correspondence between the rows if there's no connection between the rows how can you take the one before it to fill in so uh so that's why i listed here as an interpolation technique so if we do that you can see that the the red dots of the original one and i think this is a forward fill the blue dots are the corresponding forward fill so you can see that you have the red dot and then they all didn't quite correspond to what i expected but uh i i think actually some of the red dots are covered okay anyway i'm i'm not a huge fan of doing the fills these forward and backfills anyway even for interpolation if you know it's inter interpolatable you've got better techniques anyway um this is just set up as a generic interpolator cell and really what it does is it interpolates at uh y and z and then we'll plot the uh y and z on uh it'll do a scatter plot of the y and z and color the missingness so this is the basic one this is linear interpolation so the data looks like this it uh turns out the there's two others that are uh supported by uh oh i should back up i'm using uh scipy's interpolate module to do the interpolation um i'm sure many of the packages have this that was just the first one i found when i uh did a google search you can run this for quadratic as you can tell actually my original data set i just took sine and cosine and collaborate a bunch of stuff so that really is just uh the um the data should come out to be a circle when i all done with it and then probably the one if you're if you guys ever do any like uh photoshop cubic is probably the best generalized interpolation so when you're taking your photoshop image and you're either in your well for the most part changing the resolution of the image that you you can select the interpolation technique and um i think cubic or spline which is roughly the same thing but the two most popular ones and almost almost looks like a circle although actually my plot is also a little rectangular so you're not going to get a circle anyway okay all right so now we're leaving the world of um kind of data with kind of these known structures now we're going to go into more i have no idea what the data is but i want to deal with the missing data situation so this is sort of what going from here forward is where i would if you know nothing about the data and you need to deal with it and you don't want to spend a lot of time trying to see if there really was an underlying structure the assumption i would make is that it's probably mar and i would use an imputation technique one of the following ones and that's really where we're going with the rest of this uh topic is just lots of different ways explaining these amputation techniques and um giving a lot of a lot of examples on um different ways to do it some of them are computationally much more expensive some of them are faster but of course the trade-off is that you probably have characteristics that you don't you know that are less desirable so easiest way for amputation is to use some form univariate imputation which basically just means you know you're just going to take if you have a column that has missing data you're only going to use data in that column to fill in the rest so some common ways to do this there's uh one that i would not ever recommend unless you have some really strong justifications is the one i would not recommend is just filling it with something because you don't know what that's going to do and you definitely like it especially filling with zero you're likely to skew the mean you're likely to even more so than just dropping it so that is but i bring it up there because i know that that people sometimes do that um mean and median are very common way to do it for numeric values the mode or frequent is often a way to do it uh if you have categorical so we haven't really talked much about categorical data to this point um just because i think it actually opens up a huge can of worms as far as you also have to deal with encoding and so forth we will touch on that a little bit uh in some of the more advanced imputation techniques because you know you probably can't get away with not having to deal with it but i'm not going to go into a lot of details because you get we can get mired in and talking about how you encode the data as well so but one way they do it is you know you just take the if you have a missing value you fill it in with the one that appears the most often okay and then you can do a random fill now this doesn't have to be like i'm going to pick anything out of the air you may do something a little smarter like okay of my remaining data i'm going to calculate the mean and the standard deviation and i'm going to fill it randomly based on a normal distribution based on that mean and standard deviation and that has actually some advantage over the mean if you fill in strictly with the mean you're actually going to compress the uh the stan the standard deviation because everything that's missing is actually a lie right on the mean that's going to actually skew the standard deviation uh smaller so if you actually use this random technique you're actually going to not you know you're going to still preserve sort of the mean and you still it'll do a better job of preserving the standard deviation these all work fine for m car but uh you're still going to get the bias issues from mar okay so i think the rest of the way we won't necessarily go do a lot in terms of um analysis on the different results of the amputation i'm going to show you a lot of techniques to do them um and uh you know if you want me to slow up so that you can evaluate them i can do that as well all right so uh so mean and uh so the first example here is you know median will be identical to this as well so uh mean imputation you know the advantages of these univariate imputations are very fast it takes almost no time to to compute them but as you can see here from mar you still end up with that problem of um let's see what else today okay uh oh yeah the only thing you can do the same thing um you can do the same thing with median but you can see in both cases there's still a significant skew for mar if i were to change this to the m car data set is that m car huh okay something's not right there okay i guess one of the problems with trying to do this on the fly um not sure why the mean and the standard deviation are off by so much but anyhow was it oh that's why thank you yeah it's filling in with the mean with the other data set so all right if you did that yeah okay that's a little bit better did i miss one on the oh yeah it's actually what i was saying the um filling in it with the mean or the median compresses the standard deviation as you can see there so even though it preserves the mean and the median the standard deviation itself has been affected by this technique so even for m car it can have it still has some negative effects um okay at this point uh let me digress for a second talk about the libraries available unfortunately python i don't think has a lot of really mature libraries for imputation the one that seems to progress the furthest as far as i can tell is a package called auto compute auto impute um but it is a little bit buggy and um i still think it probably needs some work uh the most mature imputation packages actually exist in r so if you really need something reliable and you don't want to write it yourself um i would suggest you might want to learn reticulate which is a way of writing our code inside python um but uh you can't for some simpler reputations you can get by with with these packages that just kind of letting you know ahead of time that's also probably that's also one of the reasons i want to explain how these work so that you know worst case scenario you can write your own it's actually not that complicated but you know how um if you were to write a package you kind of have to uh make it uh generalizable to a lot of people but if you're doing it for your own purposes you can focus in on what you need specifically so definitely python has all the building blocks to do a good imputation it's just that depending on what you need it may or may not be there so anyhow uh since it is helpful to have a uh a consistent uh imputation library i'm going to focus mostly on using uh either i'm going to use existing libraries to build up an imputation model or i'll i'll probably show you the counterpart in autoimpute just because if you're going to use one if you know if you force me to use a library i'd pick that one right now scikit-learn has some imputation but it's somewhat incomplete there's only a few methods in there and i think there's also another one called fancy impute so uh i'm not really playing uh trying to play any favors as far as libraries because it's still bennett's all of them need a little ways to go before they're like full service okay so with that digression out of the way uh here's how you would use auto and fue to do the same thing with mean or median so this might actually be a simpler way um do any of you use uh scikit-learn's uh pipeline stuff uh i only bring that up because auto impute sort of fits that model you can uh each of their models there's a fit method there's a transform method and i forget what else there is or in the fit transformation both at the same time this allows you to create a auto computer or auto computer and put it into a scikit-learn pipeline so if you use that tool this is compatible with that okay so i did this imputed and i can do the comparison look at feature a which is what has stuff missing and again uh this was on the mar data set and you can see again the um the bias involved okay uh we did not actually do this in the uh creating data set portion but uh this is a data set where i created a one category one categorical column and i will print that show you what that looks like i'll explain this one so i constructed in a similar way to the other stuff before so uh i created uh three numeric columns in a categorical column and i clovered the categorical one on the basis of i'll figure out which column but one of the columns here so it's a it's an mar mechanism but the values are missing and so the kind of the last fill imputation technique is well no i guess there's another one here [Music] so this one basically does the same thing but it fills the missing one based on see what it was the second one so cat c is apparently the most frequent category that appears so basically i fill it in with the fill in with c and uh depending on your level of knowledge of pandas and i mine is sort of intermediate uh what this does is it computes the the mode but the mode it turns out is actually a list so this extracts the uh the top value so if you actually ran the mode it would actually um give you a sorted list i think in reverse um frequency order and so the most frequent being at the top the least frequent at the bottom so that's just a fancy way of getting the most frequent value [Music] okay and finally i will this is the um this is what i was talking about that instead of just filling in the mean we're going to do something a little more clever which is we're going to fill our missing value based on the picking the value out of a normal distribution based on statistics of the rest of the data set of that column again in this case you still have this is the this is the wrong data frame yeah you still have the skew uh the bias because this is the mar there's no uh univariate imputation technique that will resolve the mar basically because the reason it's missing is dependent on the other columns so if you're not using those and filling it in it's in you're never going to get a good result uh but if it's mcar see if i make sure i got them all here let's see our change them all to mcar you'll see in this case unlike the mean that i did earlier now the standard deviation is also kind of down in that same realm so um you know how i was saying how if you just fill it in with the mean it'll tend to compress the standard deviation a little bit in this case uh by doing this random pick it sort of fills in the standard uh fills in the uh fills out that standard deviation so by actually introducing a little more randomness in your fill you're going to get something that reproduces the standard deviation a little better okay so so we basically covered sort of uh what people most commonly do in the imputation techniques and for those of you who uh actually deal with have dealt with missing data to sound like some of the stuff that you've actually done okay and you can also see from these simple examples why for certain types of missing data it's not necessarily a good thing okay so before we get to the next section i just wanted to kind of uh one question that i don't actually address here but might come into mind is um let's say that you're trying to evaluate a method of imputing it's like well how can you tell if it's any good or not and that's actually kind of a tricky question because when i was i got involved with all this imputation stuff because uh well it's kind of um that's not that long a story but um my boss enlisted who's a adjunct professor and listened one of his grad students to investigate this method of imputation with neural networks uh i saw in that paper there was actually a way the technique can be improved a lot so i ended up publishing a paper on the improved technique and i'll mention that later but in doing so that's when i did the whole background section on m car m r because that was supposed to set the stage for the paper and i did the exactly the things that you weren't supposed to do that i told you not to do like i generated the mnr or the quote mnr in the way that i showed and it turned out it wasn't mnr and you know i fell through all the same pitfalls but in doing so really what boiled down to when we were working with neural networks everything was focused on being square error but is that really the best measure of how good the amputation technique is and the more i dug into it i realized it might not be because if you have stuff that misses the original value on both sides frequently maybe in the worst possible way it could actually do uh more detriment to the statistics than what the mean square error would indicate so the bottom line actually in imputing that evaluating what kind of amputation technique is good for your particular application is probably to benchmark it against the particular type of model you're going to build if you're going to do um a random forest regression model with the data eventually you may want to test random force regressions on you know full data sets and then clobber them and impute them just to evaluate in that way because no real special metric to say this one is good this one is bad uh because nearness to the original value is not necessarily uh the whole story let's just put it that way if you're going to build a neural network model that might be completely different because in theory neural networks are supposed to bring in higher order relationships that you know might get missed in normal statistics so the imputation technique that's suitable for that might be something different so um so without all that all that said when i show you some of these multivariate and advanced imputation techniques you know you're just going to have to kind of keep in your back back your mind you may want to kind of have a a general feel for all of them because whichever one works best for your particular problem may be different okay so sort of the sermon on that over uh the next section is actually pretty lengthy so i want to see if you guys had any questions on stuff so far because um you'll probably make you know i don't even know if any of that will you'll retain any of that when you get into the next section because uh okay i guess the plan is we let's go through this next section if it goes long we could take a break in the middle but uh i'm thinking to try to get through this one and then we can take another break and you'll probably need it after this okay i think this is by far the longest section i have not actually debated breaking it up into a few okay start out with the same data sets and you'll be you're going to be intimately familiar with these the mar and m car data sets basically got the original data set so we can compare some statistics against it we have an amnar collaborates i mean an m carb clobbered set and a mark clobbered set and in some cases uh doubly m car and double seminar so uh i'm probably going to start saying them without uh you know referring back to it so hopefully you'll pick up that really we're going to primarily deal with five data sets here to look at okay so multivariate in imputation falls into two categories the first one is regression imputation and some of you might have encountered that the other is hot deck imputation uh which might be a little less familiar and i bring up the third one here um neural network auto encoders these are sort of a little more cutting edge we're actually not getting into that in this section but we will talk about it in the next section a little bit this one is both going to focus mostly on the conventional multivariate techniques okay so uh this is sort of the obligatory kind of overview of all the different methods um regression imputation actually i got a white board actually make use of this uh slide so let's say you got column a column b and column c we've got data here data here and data here but let's say that this column has a few things missing okay so there are generalizations for this technique when you have stuff missing in different columns but for the most part we're going to deal with one column missing you'll see at the end of the section will be techniques where you extend this to other columns so one way of explaining these cop this column these columns to figure out what's here is just simply a regression and that's what really all regression imputation is is we're going to take the columns that we have full data for and use those to figure out to guess missing values here um if a is categorical and i'll digress back to uh you know or go off and talk a little bit about categoricals that uh well the counterpart when you're dealing with categorical values it to regression is classification so if you wanted to fill in based on a categorical column based on these you might do a classification uh system so uh and any real and it turns out any really can work or could be used i shouldn't say can work but for regression um typically like as in this outline here um you have linear regression and there's also a stochastic linear regression which is actually the um uh kind of the counterpart of the random versus mean for the univariate what stochastic linear regression does is actually throws in a little bit of noise so that the covariances and the variances don't get compressed and then uh i threw these out there because i don't see it actually done much or even explored but hey if you're going to do regression to fill in these values why can't you use uh or regressions and classifications why can't you use any any old regression so i threw in like you know random forest regression um and for classification you use knn or or decision trees um anybody who's curious you know you might want to do a little research who knows you know you might discover one that works even better and publish it uh i just think find it kind of curious if that's not in any of the uh at least not in any of the libraries and it's actually pretty easy to do so uh and i think somewhere in this notebook i threw in one just for fun because why not you can do it okay um so for the uh so we'll go with the linear regression first uh the advantage is it works from mar but the one of the downsides of that is that uh if you do a linear regression you could actually end up with illegal values as an example let's say that you're using this to fill in weight and you might actually get somebody with you could with linear regression get somebody with a negative weight if they're like at the very low end of the rest of the statistics it's possible that instead of imputing 10 pounds it might impute minus 2 pounds which is not correct so that's there's no guard against illegal values with linear regression so to do this i think this one is from if you look here at my at the top of this i'm kind of all over the place as far as libraries so um this kind of goes to sort of a state of imputation there are pieces to do it from all kinds of places but nobody really has it that well established in one place okay so all right so what i'm going to do this is a just my way of pulling things apart and piecing them back together it may not be the most elegant way to do things but i will go ahead and explain it so remember when we did our mar construction feature a is the one that's clobbered so this rest is the list of all the other columns because basically we use all the other columns and do a regression to predict what the missing values for feature a is so uh so this next data frame is basically all the ones that we can actually use for our fitting because it has the entire data set including feature a so we use that we we fit based on these columns feature a and then we predict it by we predict the entire data set okay and i have a little note here it's this is actually more when uh i was having some people uh review the notebooks before uh presenting it i use this i was told this can be kind of confusing so i thought i'd uh bring in this pattern the reason for doing it this way number one uh i can keep it into one of those method chains which i guess is sort of the favorite way of coding in pandas but what this does is basically says assign to feature a this thing okay and then what this thing is feature a you're going to if it's missing um you're going to fill it with the predicted value otherwise you leave it alone i have another pattern that looks very similar to this uh when we talk about some of the more advanced imputation techniques but this is what it is and basically it's it's this except i can kind of one-line it and then potentially change something else to it so um i don't want you to get bogged down in code that looks kind of intimidating that's all it really does is it fills in anything that's missing with the predictive value okay so that's what i've done here and then we do our usual stat comparison you see now from mar this is actually pretty good this is the kind of basic percentage we're seeing with the uh m car for the univariate so so for the most part this is good um and let's see what's the next thing oh and the this is uh oh this is that uh displayer method i had shown you earlier in the helpers the highlighted cells are the ones that were actually imputed probably should actually put the original value that see how it compares but anyhow that that shows the results of the amputation we'll probably not focus a whole lot on you know comparing the statistics and so forth otherwise this thing could be in another three hours so i'm going to go mostly on just explaining the the methods um you know i guess we'd leave it as an exercise to the reader uh if you want to delve into any of these and pick up hardest statistics and his properties a little bit later okay now the statistic stochastic regression basically uh i hope i'm using that term correctly for any of the statisticians here but the residuals are the difference between the predicted value from the regression and the actual value right so what we do is we take the residuals we form we we're going to assume a normal distribution in this case and we're going to then generate noise based on that normal distribution and then add it back into a regression so what so what it does is it does a little bit better job of simulating the variances uh but it also still has the same pitfall of creating out of bound values so first step here is we're going to take calculate the residuals as well as the mean and standard deviation i guess if you want to see the mean i didn't need to cluster them together so this one is pretty close to zero mean and then what we're going to do is we're going to add this noise to our predicted value so this predicted value is the one that we generated from our linear regression before and we can do the usual state [Music] i don't know what it's called let's call it bf oh look in the column all right again pretty good i think the standard deviation is even better here if we were to go through and do the whole covariance thing you'll probably find that that actually matches better to the original than even the linear regression version um i don't know if any if you guys want us be to slow down so you can play with these anymore or just want me to move forward originally when i was developing the notebook i'd run all kinds of statistics on each of these but you know they can be somewhat of a rabbit hole and it turns out you know this is as i said it's a pretty long notebook to go through okay um they actually call the linear regression and autoimpute least squares so uh got the the least squares imputations and also the counterpart to the stochastic is this one so then we can analyze the results and i generally won't go any further than this in the rest of the analyses just again and then we can look at this this actually might be better we can get a better idea of how the stochastic compares yeah i think that the main significance of this is that you see that standard deviation in the predictions is better if you add in a little bit of that noise from the residuals okay so the last thing is sort of this regression section when i was coming up with this i just said you know if all this really is a regression we're taking the existing columns we know about and we're predicting values based on what we do know about a the relationship between a and these and then just filling in the rest of the data so i said you know in the in the class on machine learning i teach the two regressors that we use in the curriculum are linear regression and random force regression so it's like i thought well why not let's just do it for the hell of it and see how it goes so this is basically the same pattern we're taking the data that we know that we have full knowledge of we're using that to make to fit a model and then we're going to predict the rest of the data set from the missing [Music] to fill in the missing values so i guess this one takes a little longer random force regression and okay so now that we've got the predicted values we're only going to fill in based on the prediction of the uh only fill in the missing values we're not going to replace values we already know and let's do the comparison again and you know off the bat it's actually not bad for standard deviation even so who knows i mean and i'm sure if you go through um you know scikit-learn package or any of the others there's tons of regression methods and to be honest i don't know why people haven't tried others [Music] uh you know and who knows maybe that's a good survey paper to come up with is just try different uh different regression techniques for imputation and see how they work uh so i couldn't find any papers i'm not even when i was in grad school i was not very good at doing literature searches so i could have missed a whole bunch of stuff but you know like you know in the at least in the few hours i spent looking i couldn't find anything other than linear regression for imputation so you know coming kind of from the world of playing with artificial neural networks it was like you know why not even try an artificial neural network to get that will you get better results who knows never tried so um so if you want something to play with you know copy this pattern find try look up all the different regression techniques in scikit learn come up with a summary of which generally does better okay so so far the uh categorical stuff has been sort of the step child of this uh talk so we'll at least talk a little bit about that um so the counterpart for catechol categorical variables to regression is classification and i always found it kind of funny but you know sort of the the rudimentary basic method of uh or you know the one they teach first anyway of classification is called logistic regression right so i always found that kind of interesting the misnomer that a regression classification algorithm is actually called a regression but with that said that's what we're doing okay so uh load this categorical variable oh i guess this this piece of code is buried in there but uh this imputation displayer what that does is it figures out what's missing and then sets up um a style sheet in your panda data frame so that when we display it using the display any data frame with this uh with the displayer method it'll highlight the values that were actually imputed so that's just a it's a it's a useful little tool to see what you know work has been done okay so logistic regression uh the only thing that is a little different here is that um or do i use it here maybe i don't oh i don't uh this line is extraneous um because actually the i believe the scikit-learn this logistic regression module handles categorical values without actually having to encode them so i believe that lines extraneous so this is just basically the same pattern we uh calculate uh we we do the prediction we you know bad variable naming but uh we predict it and then we fill in the missing ones and so this is actually you know gives me i seem to remember that we imputed with the most frequent value and that was category c and in this case you know here it puts cat b cat d there and i'm sure other places it'll put in e as well as c in a so again there's probably a huge universe of classification algorithms um as far as i can tell no reason you can't try them but so but this is the tried and true one that they use in these amputation packages autoimpute also has that in there i just did not um incorporate it because i i its handling of categorical variables is a little bit dicey so but again all the stuff can be done with if you understand the underlying mechanism you can do it by hand and it's not that hard okay the other major way of uh multivariate imputation is called hot deck imputation um so basically [Music] well actually let me draw it so all right so basically what you do is let's say we're going to impute this value here based on some kind of metric of these values we're going to pick a set of what they call donors so let's say um say it's these three things okay uh and we're gonna do that for every missing value so this algorithm is a little more computationally intensive because you're gonna be doing it it's kind of like doing it one at a time but of these three values we're going to pick based on you know it can be straight uniformly random or could be some other random based on how close it fits the metric whatever you're going to pick one of those three and then you're going to let's say it's this one and you're going to fill that value in then you're going to start the whole process over select three more donors could be the same one based on whatever your criteria is and randomly pick one of those to fill this value and so forth now the advantage of doing hot deck imputation is that you never get an illegal value because you're picking from already known legal values so in the case of you know you're not going to get somebody with the negative height or um you know uh for that matter prop uh you know someone with a zero blood pressure okay so the easiest way to demonstrate this is probably with euclidean distance so i took the this is sort of the data set that uh i think this is actually our mar data set but i only took the top 10 because i didn't want to uh we're only going to show sort of one iteration of this okay so um since we're going to do the euclidean distance i'm going to apply this method here called distance and now our demo df looks like this okay so the distance here so what we're going to want to do is select our donors based on this euclidean distance so this is basically so what i mean by euclidean distance is okay from this point based on the remaining features what are the closest points so actually just by eyeballing it number eight is the closest to number seven as far as euclidean distance 0.103 okay so that's what i mean by you know based on distance okay so the remainder of this okay so this is kind of described in this paper by van buren link is here also in the reference section okay so method one is the simplest one is just pick the one that's closest and so like i was saying number eight is the closest because 0.013 so 2.74 so for uh so what we'll do is we'll just take this one and fill it in there that's uh kind of the quote-unquote method one the second method is pick all the donors under a particular distance threshold so we'll do this okay so we've set two as our distance threshold so our donors will be selected from one of these five and this is going to be a some kind of random selection so if you run this again you may get a different value okay uh a lot of these amputation techniques have a stochastic nature to it so uh if you run it again you're gonna get a different imputation and that's actually a useful fact which we'll mention uh later when we talk about multiple imputations okay so here we go the third way is just pick the three and closest values in this case three is actually if you look at some of the software or packages out there that actually implement this three is actually they use this method and the default is three so in this case again eight six and five are the closest so we'll pick one of our fill in one of the values from those three the fourth method that's mentioned is you're going to consider alt this one is a little different you're going to consider all the donors as possible values but you're going to select the one inversely proportional to the distance so that's the one that's uh so it's possible to get any of the values but you're going to get a different one and since this process is different every time and it's integrated with that the donor selection and the random choice are integrated together i don't have you know the fun little table so in this case this value got picked which is we'll probably go back to the table and look to see which one that is uh two five it looks like you know it happened to pick actually did it it's either picked one or five picked one so it just happened to pick uh that one if i run this again this cell again is going to pick a different one by random chance okay that's the overall principle behind hot deck amputation um in principle they do not use euclidean distances as the metric the most common hot deck amputation technique and fortunately this is about as deep as we're going to get into this category but it's important to mention this because this is what is actually used is this method called predictive mean matching i'm not quite sure how this name correlates with the um with the technique i don't know where the mean comes from where the matching comes in but um but that's what that's how it's termed okay again we'll load this demo data set all right this one is a little more complicated uh i think i'll run the cells first until i get a picture all right so the metric is you do a linear regression to pred you do the linear regression prediction and you get the regressed value and then you're going to select the donors we're going to use the n equals 3 version and we're going to pick the donors that have a regression value closest to the predicted regression value so that seems a little uh convoluted but that's what the algorithm is if you actually look at the if your statistician you might be interested in some of the other options there are all kinds of things about prior probabilities and stuff like that which i don't really understand and so for the most part i leave those on default values but the underlying mechanism is this so the donors will be from those those are three closest distances so our donors will be selected from these three values and again this one is actually seems to be very computationally intensive so i truncated the autoimpu version autoimpute does the full-blown version of this which probably incorporates other uh options which i have not accounted for in my demonstration so if you run this one it actually is even at 100 i truncated it down to 100 rows it still takes a fair amount of time which you'll see and make sure i'll go and run the next cell and you can see what is okay well i mean you can leave it running uh yeah i i did notice this one takes quite a while to run does a lot of stuff be i'm not sure why uh it may just be because the example i gave is a very special purpose focused solely on the objective whereas these probably have to accommodate other parameters which is why it may take longer but this is actually the from what i understand for sort of a lot of the computers this is the the default algorithm they use and uh one of the reasons why i did not have time to really investigate it on these data sets but appears that if you use if you have more than one column missing with data missing linear regression at least doesn't really work that well and uh predictive mean matching does and i'm not even even for m car so um i'm not sure if that's actually completely true but uh when i tried uh try to doubt it's that seemed to be the case yeah this was going to take a little while um okay well while this is going on do you guys have any questions on what we've talked what i've discussed so far was it uh music comprehensible you guys are have been kind of quiet so i'm just kind of wondering if you're all still with me or not it's um so this is one of the reasons why uh okay so it's done it took a couple minutes actually some of the stuff subsequent to this will actually also take a little bit of time uh so you can kind of see what it's doing with the imputation uh if you want you can do the usual comparisons that we've been doing all along um to be honest i mean i'm sure for you guys as well it gets if we keep doing that every time it's it's going to get really dry but if there's any particular technique you know you want to dig into i mean you certainly can get these you have these notebooks so you can try them out on your own okay so that sort of vote that's actually one long section do you guys want to take a short break or you want to keep and then break after that break okay yeah because i know after a long thing like that you might need a chance to get let your mind settle so go for another 10 minutes we're actually on a good pace so the next technique this is actually more you know we kind of just talked about univariate versus multivariate this is a technique that builds on top of it this is the sort of has been the gold standard for imputation for ages um only recently have there been implementations in python autoimpute has one it's a technique called mice and it deals with case so far we kind of made things easy by saying that there's only one one column with stuff missing but realistically right you're going to have a b c d e and then you'll have stuff here stuff here stuff here and if you're not lucky you're gonna have some stuff missing here missing there maybe one missing there a couple missing there so that's probably a more realistic situation um well [Music] you're probably gonna have a few columns that don't have anything missing but this is more real world so far you know we've just kind of confined ourselves into one and that's really good for explaining the concept but now we're getting into sort of well all that's nice but this is what i get so what mice does and sort of the optimal version of this is that you sort the columns by how much is missing first so let's say that column e has the fewest actually the first thing you do is that you impute each of these columns using some form of univariate imputation um you don't really need to get fancy with the stochastic imputation quite commonly mean or median works fine so you fill them all in with sort of your first cut at an imputation which um um you know mean well basically it's mean for numeric data and um mode for categorical data okay then you take the uh so what you do is you take uh i got it backwards you take the column that has the most data missing and then you you're going to delete the values that you just imputed and you use the rest of this to do some kind of imputation to fill this in so your first pass will revise this imputation based on everything else now let's say column b is the next has the next most missing stuff then you take a c d and e based on these imputed values and you fill in that and you just keep doing this until you get through all the columns that have missing values okay so that's sort of your first iteration now you just repeat the process you go back to a and you refine the imputation of this based on the new values you impute and because this is sort of that kind of iterative process you stop when you basically are satisfied with where you've gotten and that may just simply be that these values don't change much there is a danger that sometimes these go cyclic and you don't actually stop because uh you know it may go to say point one then point two then down to zero and then point one point two just kind of so there's always that kind of risk so at some point you're gonna have to you may have to look into it and say you know that's enough um most mice computers have like thresholds and stuff that'll keep it from going on forever and you can also set thresholds for saying you know when it starts changing by this much i'm saying it's done so uh mice is sort of this iterative process which also means that when you compute each column based on the other you can choose your technique i think by default mice uses uh predictive mean matching which we just kind of finished talking about and um for and if you tell it's categorical i think it does a multinomial logistic regression for categorical values um again we're kind of for the rest of the for these demonstrations we're going to sweep categorical data under the rug but uh and you know if at any point you want to get into categorical data i will post some contact information at the end and you know we can talk about that but just categorical data also involves encoding as well and so you get into a huge can of worms there i kind of mentioned some of them at a very high level but you don't but we purposely don't go into you know a lot of these data sets purposely are numerical just because i think it would tend to confuse what i'm trying to demonstrate so here's kind of what i was saying with this uh the reason i use the uh m car set here is just because uh for my iterations i'm going to use linear regression and uh some reason i think linear regression when there's missing data doesn't really work as well on mar missingness predictive mean matching is what you actually want to use in that case okay so step one as i said i used a univariate imputation and just to be different i used a different one if you recall our double missingness variables only clobber call feature a and feature b so we don't need to go feature b a b c d e because d and uh well c d and uncorrelated don't have anything missing so in this case just to be different i did column a with mean imputation to start column b with median uh no particular reason just wanted to show that the you know you could use either of those um because this is already this is an iterative process the starting point isn't super important you don't want to pick zero because it's just too far away from where it should be to start with but um you know any of these techniques you don't need to go through the elaborate mechanism to do like a normal or random imputation you can just use mean and median okay so that's what happens in the first iteration then we impute feature a based on the other columns so this shows what values got imputed and then the next step is to just repeat the same process now we're going to impute column b with everything and note that the oh okay so this shows all the imputed values but you can note here that column b also used the more recently imputed value for column a as well as c d uncorrelated actually as a side note because we actually know what the uncorrelated column is uncorrelated it really is of no use in helping to predict the other columns so if you actually know it's uncorrelated you don't need to incorporate that in your model and that may simplify things uh i was just lazy i didn't want to drop it but uh it doesn't really do any work because it's not it doesn't provide any information about feature a or b okay and now i put both of the now i'm going to do a and b again um but i'm going to kind of put them in the same step we get this result and then we'll do our usual stat comparison and you know for m for this double m car it's pretty good again i'm doing a lot of eyeball testing you know it's like under four four you know for eight thousand missing values out of uh 20 000 rows having a um a bias under a percent pretty good right you got uh close to 40 40 of your rows are affected and you you're only skewing your data by less than a percent view that is pretty good okay so just for completeness if you want to use that automp package it does have a mice computer that pretty much does this this is the case where having the package actually is helpful rather than building yourself you saw how cumbersome it was for me to do that i could easily put that into a loop and write it for x number of iterations to get the result but this does it all for me again i'm doing the least squares just because um the predictive mean matching kind of takes too long for this so uh i'm not sure if i got the exact it probably didn't get the same results as before because this thing ran through as many iterations as it needed until the default criteria termination criteria was met because i didn't specify any of the parameters for you know how long to run what the thresholds are i just kind of left it up to the computer but again you know like with using any software package always look at the uh the parameters and the options sometimes sometimes you'll look at the defaults and say yeah that makes sense to me i'm going to keep those other times they may not um has a ton of options and if it could spend an hour going through them all but if you actually use this you know read it on your own so i mean i'm probably not giving mice enough uh it's do uh doing justice some mice because it actually is sort of this technique is sort of the gold standard by which you should impute data um but uh and it has and quite often people just use the default settings and it seems to work work fine i believe like if you look at publications on imputation techniques almost all of them will make in comparison to mice because it's it's just been the thing that's been there forever and it's really what the yardstick by which other imputation techniques should be compared to um and if you really if you're actually doing a research paper and you're comparing an imputation technique to mice you might even want to run the r implementation because that's probably what most people are referring to but it can be done in python and this is the way you do it um oh yeah and actually one thing i kind of glossed over if you were actually paying attention to the code uh when i did the mice computer in there's this kind of strange little uh [Music] indexing i was doing here uh that i kind of glossed over turns out mice is usually uh implemented using um what they call a multiple imputation so what this actually resulted in was um actually an array of imputations um and so in order just by setting these parameters here i'm actually just saying do it only once and so i'm just getting the value for the one imputation i did do but uh in the net the other advanced technique that closes out this section it's called multiple imputation and this is why i mentioned it here so before i move on any questions on mice because i think you know you could probably get by not knowing anything else if you figure out how to use mice um it may not be the best overall because there people are coming up with newer techniques but it's the one that's been around the longest the most established and you know with all that probably for any research you need to apply it to was probably the most unassailable technique so anybody questions on mice all right and actually there are also even some good i actually did find some good blogs explaining the the basic technique all right so multiple imputation is really um in some of our amputation techniques i mentioned above there's actually a stochastic element to it right the hot deck imputation we're going to uh select owners but we're going to randomly pick from those donors uh stochastic linear regression we're adding some noise to it some random noise so every time we run the imputation we're going to get something a little different on on some of these techniques so what multiple amputation essentially is is you're going to run that same imputation multiple times and so instead of getting a single value for anything missing you're gonna get a set of values now why that's significant and this is probably more applicable to the statisticians is that if you actually get it it's more like rather than filling in this value you're filling it in with almost like a probability distribution of what that missing value is and you can carry that forward into other statistical analysis personally my background is much more my mathematical background was much more theoretical so i liked everything crisp and not uncertain so this always kind of bothered me a little bit but um but if you carry it forward you can do things like you know carry you can incorporate this kind of fuzziness in the value into your error calculations or your confidence intervals down the road into your model so you can actually incorporate it so it provides a little bit more of a statistical result for the imputation rather than you know a single value so that way um you know in your final result the fact that you don't know this value but you have a good idea of what it might be can carry forward into maybe increasing your you know error margin or your confidence interval in your final result uh and so for those of you that actually deal with that in statistics that's uh that should be useful so to do this autoimpute has that as actually a feature as well but even if you don't use auto and use some other computer if it has a stochastic element you just run it five times or seven times or however many times you want okay so uh so this is sort of what the raw output looks like from auto impute and so this is what this will explain why i had that weird uh array notation in the previous section okay so it produces in this case we have five imputations so it produces a tuple where the first element is basically indexed to count which one so this is the first imputation second imputation third fourth fifth second element in the tuple is a data frame so this is the entire data frame with the values imputed um and you know you've probably seen if you actually print a panda's data frame rather than having um jupiter render it it looks kind of ugly like this but so that's that's what the return values look like okay but i did note up here or here that the least squares um option you know doing a least squares fit for uh regression is actually deterministic you do it twice you're going to get the same answer you do it five times you're going to get the same answer okay so that's the imputed value so we can see like like number one feature a row one is imputed so if we look at what that looks like when i did this it's linear you know linear least squares regression get the same answer every time so in effect it's kind of useless to do it on something you know it's completely deterministic because you've got no statistical variation there so uh so really the rule is uh for multiple imputation if you're going to do a multiple imputation you need to do some form of imputation that has a stochastic element to it okay so we'll do the same ex same thing but now we're going to use that stochastic linear regression so this is basically the same as the linear regression except we're going to add random noise based on the residuals of the linear regression okay and if we look at this now for that imputed value we get something missing i mean some not missing um we get some variation now based on even these five values you can compute a mean and a standard deviation for that missing value and that presumably you can carry through onto your model and incorporate the fact that's missing but you have an estimate for it so this is why you would use multiple imputation uh in any kind of statistical modeling that you you build because it actually enables you to um kind of carry into your statistics in your you know final result any contribution based on the fact you actually don't know what the value is but have an idea of what it might be okay so to summarize this okay so any questions on multiple imputation i don't know but yeah i think it has it's probably highly dependent on the model you're using uh how how it carries through and as i said you know i'm not that much of a statistician so i don't know exactly how it carries forward but um you know from sort of my um minimal statistical background i can see how you know right there i got error bars right and so i can use that to sort of carry forward and any calculations i do subsequent to that i can incorporate the error margins in there um but i'm sure you know if you've got a deeper statistical background you can probably find more ways to incorporate that so it can incorporate the uncertainty from here okay yeah so certainly if you have models that account for uncertainty this would be very helpful right so like you're saying that mom yeah so those you you could actually see how those possibly how those are tied together so that's a good good mention okay uh and really that's the key point of the certainly the multiple imputation is much more useful to those people that have have to be much more concerned with the statistics of the of the model all right so um so basically multi univariate imputation if you know the data is m car it's the fastest quickest easiest way to deal with it multivariate uh really you get the regression classification which is sort of the simplest way and then hot deck imputation is actually um has the advantage that you're not going to get illegal values and then based on those building blocks you can do mice which is really the uh it's actually great for dealing with missingness patterns that are beyond just a single column and probably the one thing that if not if you you um don't take anything else from this talk or workshop away um just remembering that mice is really what you want to probably use if you don't have a clue about how you want to deal with the missing data that's probably the safest bet methods like dropping data are filling in in you know they're fast but they have a lot of problems mice tends to have the fewest um and then multiple input imputation is a way of actually incorporating the uncertainty of your or representing the uncertainty in your imputation into your model itself all right so getting down to the home stretch here all right do any of you guys here work with artificial neural networks because this is because this is sort of now i mean i like to play with artificial neural networks i don't really need to do it for my job but i find it quite interesting i yeah actually when i was in graduate school long ago when this was all really new stuff you know that's when i was first exposed to it but uh you know it's kind of made full circles now the hot thing again so uh not to date myself but i was doing this in the 90s so i i'm kind of i still have fun playing with it so this is sort of like um and part of the reason to see how many of you out there is before i start insulting you all because there is certain uh um casualness with some of the data handling when it comes to the neural network people especially with regard to statistics um in fact um i'll get to a little anecdote i've uh because of coba uh i have a friend up in the bay area that goes to these uh frontiers of ai you know meetup type thing and because of covet i don't live in the bay area he invited me and i went to a couple days and during sort of the networking part before that someone was asking and uh you know how do you deal with unclean data and there was a guy i think he worked at google and he said don't use unclean data just go get clean data that's and that's sort of been kind of the attitude i get from some of the people that are really into neural networks and we'll just you know find a nice curated clean data set well that's great for those of you in the ivory tower but those of us in the real world we have to deal with unclean data so um and in neural networks there's a lot of like speculation on how things actually work internally but nobody really knows so and i'll be describing some of that theory behind some of this uh in this section so first thing is we'll stay on the imputation line for a minute or a few minutes anyway um you might this is also where some of the stuff will run slower because i actually have some small but i guess really no such thing as quick uh neural network model training but the first category is um the use of you can actually use neural networks to impute data so this is um this is actually the paper that uh my boss had a graduate student investigate for us uses something called a denoising auto encoder now auto encoder for those who might not be familiar with the term is basically a model that predicts whether the inputs predict uh the output that is predicted is the same as the input the theory is and this is sort of where that hand waving neural network thing comes in uh the belief is that the neural network learns the structure and the missing values are just kind of noise so uh it will eventually learn to what the true structure is and ignore the noise whether it really does that you know but that's sort of the mindset behind auto encoders in general and so the denoising ones are they believe that that eliminates noise and they treat the missing values or the deviation of the uh because neural networks you have to fill in a value so they start out by imputing with the mean they treat the deviation from the mean as noise and hope that the autoencoder figures it out okay so and actually it had been a while since i've done this so when i was doing this notebook all the predictions were really bad and i realized the reason is that this the data needs to be scaled to work properly uh so i'm using scikit-learn standard scalar here and this restore df function is really just the inverse of that scalar so that when i'm done scaling i can get back to my original values okay oh yeah this is one of the few places i started actually putting in diagrams but it wasn't until the sixth slide i figured out the way i wanted to to do it okay so this is a for those of you well if you're not familiar with neural networks uh you know let me know i'll slow down but they basically have light layers of they call them neurons and um the basic architecture of this auto encoder you start out with n which is a number of uh features you have and then the next hidden layer has n plus theta and theta is a hyper parameter and in the paper here that's cited uh theta is seven y seven they just said that works best so i stick with that um then the next one has increases the width by another theta and then finally the middle layer has basically an extra three theta version then it kind of gets smaller until it's back down to n so it's kind of grows in the middle and shrinks back down uh until it's back down to the same so basically the way you train it is you feed the the predicted value uh well you you feed the you the ground truth is the same as the input if that makes sense to you so so basically okay so so i guess if it doesn't make sense let's see if a little bit goes through it and if that still doesn't make sense and uh if you have a question we can kind of slow it down so the first thing we do is we will impute it the missing data with a univariate imputation in this case median and it's the same in his paper he cites mean or median or most frequent data in the case of categorical uh and again just deducting coding issue we just we're just gonna stick strictly to uh uh numeric theta is seven all right here we split the data into uh um a test and training set okay and then here's our keras model uh using tensorflow because they do neural networks you use tensorflow or pi torch tensorflow okay so we build this model and apparently this library has some i think the error messages you come here is trying to use some graphic accelerators to speed things up which aren't available yeah and yep and actually the docker image i have supplied it actually intentionally doesn't sell okay uh the thing with the way these tensorflow models are once you kind of declare its structure you compile it use a particular optimizer these days i think adam and i forget what it stands for exactly adaptive something is kind of the most common one um that we won't get into any like the nuances of the neural network itself i'd just pick those of you who dealt with neural networks and know that there are tons of hyper parameters i just picked a bunch that seemed to make sense uh also for those that are familiar uh they say that the hyperbolic tangent is actually a better activation function than the rectified linear which is actually the one that's used most now um so that may not mean much to the rest of you but okay this next cell will take a while to run it and i'm only running 50 epochs if you want to actually see the progress you can get rid of the verbose is false and just let it run and it'll print a nice uh progress graph um i suppressed it because later on in the enhanced version um they also ch and then oh and we're going to use mean square error as our criteria because we're talk um we're doing strictly numeric values there's actually another where another potential can of worms can open up is when you start using um okay it's done when you start using categorical values then you're gonna you may want a different metrics for the loss function and so for those of you who are familiar with neural networks this is sort of the progression of the loss function as progress goes we only ran at 50 epics it might go down a little further if you go farther out okay so that in a nutshell is is this auto-encoder technique [Music] oh yeah uh i guess the final thing is we fill it in and did i do a stats compute that's that's comparison and i forgot what i call the data set it's df i found that there's actually a pandas method that will fill in if something is actually missing actually it's called combine first it's kind of a fun it's actually a useful function if you're going to build your own imputation this does is in this data frame sd mar df uh if this has a nan in a spot it's going to fill it in with this corresponding value from this data frame so that's actually uh especially for these neural network ones where you get entire data frames back instead of just a column it's actually a convenient function i kind of wish i knew about that when i was actually doing some of these auto encoders a couple years ago all right so uh and that's imputed is the name of the data frame and since this is all this is a couple columns but we'll look at feature eight it's not great but it's it's um but you know i did not tweak all the type of parameters i just threw down some that made sense uh i just wanted to demonstrate the technique uh you can do some other uh display yeah i can also do this to just show you where the you can look at the entire data frame oops okay oh how many rows do i want to show yeah so that's what it did to fill in okay um uh that's probably more to the fact that panda's nan is actually always a float so if you have an integer column it's going to coerce your column to be a float and then you may have to convert it back to an end if you want to enforce that yeah there's a bit of type uh that's one problem with i think it's actually a numpy problem at the end that there's no n-a-n that's an integer and so if you're actually going to try to use yeah so and that you'll you'll see i'm not super careful with that through here but yeah okay so those are good points though um but in in this particular context the um the combined first actually works well because all the neural networks always generate floats and um the predicted the second one has no nands in it okay not going to go into this one in a huge amount of detail but um i figured i might as well cite my own work as well as i did about two years ago the link is here and it's follows the same uh kind of methodology except what i do is um see what's the best way to describe this so what i do is i have my neural network and i train so it's not an encoder so i trained it on x but after a certain number of iterations you get an um an x with imputed values so let's call it x1 instead of just saying it i do it far less than i do for the autoencoder in the first case but then instead of stopping i actually then use x1 and i train it again and then after actually after one or two iterations only or epics i go through and i take revised imputation and do the whole thing again so in a way it's it's very mice like in that you revise the amputation and kind of go through this process because i found what happened with the autoencoder is that if you compute with the mean this method will tend to still try to pull your imputed value towards the mean so if the mean was actually a bad guess for your value it's going to still try to influence it back towards the mean whereas this one will allow the value to drift a little more so that if the mean was actually a bad initial guess it would come closer so uh so this sort of was saying in this cell here this is sort of the initial fit we only go through the previous one i went through 50 epics i'm only going through 10 here okay and then uh subsequent that to that i'm just going to do a bunch of two epic states but i'm going to revise the imputed value that i feed into the auto encoder now it turns out in my paper i do another thing where i actually modify the loss function so that uh if the value is actually missing i don't use it in the loss calculation but that's actually rather complicated to do in tensorflow and i didn't want to digress down that path but if you actually look at the paper uh that's explained so what i've done here is just to show you one iteration in this cell finally in this cell i do the other how many times 19 iterations so all in all i'm doing 50 uh 50 epics as well with this technique it's just i'm allowing it on after 10 because you can see the original learning curve the error rate the loss function drops sharply and by the time you get down to 10 iterations is practically you know you're fine-tuning at that point so uh this basically you can think of it as a very coarse training and then a bunch of fine tuning revising the reputation in between okay yeah so we're heading down to our last half hour here right this is over at 12 right yeah okay um i'll let this run and i'll uh okay so you can kind of see here sort of this is the uh i don't know if this is really very telling but you know the the basic thing started here and then at this point i'm fine tuning uh turns out uh at least from the examples i tried in my paper it actually was actually surprisingly better at preserving covariant relationships as well than the uh the auto encoder that was in the first paper so we'll go through this and then you can display since we are running a little bit shorter on time i'll just you know leave it up to you as an exercise if you want to to to you know analyze the relative merits of these techniques okay uh the final thing i think let me just make sure that's the final oh yeah um okay so uh sort of this is sort of the general process behind machine learning right you train the data and they test that you train the model then you test the model the part that sometimes is overlooked is that okay you got your model built but beyond that you're actually going to use it so there may be data coming in that's subsequent to your initial training and test set so there's really three stages to a machine learning model which is train test and then once the model is established you're going to still use it so how do you incorporate imputation into this pipeline uh it's pretty much simple as long as your computer you know all these imputation mechanisms are models as long as you can save them as you save your pipeline you can just stick it into your pipeline so this is sort of what i demonstrated here and so this is the uh this is a a wine quality test set that i got from the uci data repository it's just one of the few that had a lot of numeric values not a lot of categorical okay so this is uh this is a typical scikit-learn pipeline just doing a really doing a random forced regression on it so basic thing is that we're going to one-hot encode our type column which is actually uh since when we were running at it i didn't display the whole thing but basically there's all the features are numeric except for the type which is the categorical and it's uh the type is red or white okay so we'll do the one honda code on the on the type and then we're going to scale everything and then put pass through the random force regressor that's your standard way of building a model uh you made you know yours your pipeline will be something similar you might have additional cleaning steps and so we'll run it we'll get our result um that's building the pipeline then we can test it oh sorry button so it's uh it that's score of 53 percent not great but anyway this is just more about demonstrating what you do why am i clicking the plus and so this is sort of the uh you know what you might do to predict at the end so all you do you can incorporate and autoimpute provides the computer uh one note here there's a panda hack because uh some of the auto some of the scikit-learn pipeline stuff generates numpy arrays and not pandas data frames and uh autoimpute requires panda data frames so these panda hack transformers all they do is they label the columns so they can make them into a data frame okay so this is really just showing i'm inserting a computer into the same pipeline i might use for training uh these data sets actually have um data that's been clobbered the wine test and then after the data is trained you might still get data to fit into you know when you're using your model you might get data that also still has data missing right now it's not like you're going to train on imperfect data and they get perfect data after the fact so if you incorporate the computer into your pipeline uh you can make predictions so this is my so this source simulator i got my model all built now i want to predict these new values that are rolling in later so that's what's going on there um okay the last topic in this section really is uh and i apologize i am kind of speeding up the pace because we're getting shorter on time but uh be happy to talk to any of you about this if i've glossed over something uh okay so now you got kind of two different worlds here uh you especially with neural networks uh okay let's say that i've got uh missing data now i want to actually um use multiple can i use multiple imputation with artificial neural networks right because it almost seems like the you know the concepts don't connect well one approach is to simply you can kind of fussify the imputed values by just augmenting your data set by just stacking them together so i can take the data set and impute it five times and then that's my data set it's going to be five times the size it was originally but for the missing values they're not going to appear the same every time in those five versions and i'll use that as my training set so that's one way of doing it so here's the code here so basically all i've done here is um i've stacked the i've stacked the data frames all on top of each other they're concatenated so i've fought or i'm going to here so i got five copies of that data frame uh i whipped up a quick tensorflow model to do the classification turns out the wine cat wine quality data set the quality column is really just a it's a numeric value but it's considered categorical because it's how do you rate this wine from one to ten okay so i'll run that and i'll get my quality uh i can fit them up i'm fitting the model i'm this is done really short because only 10 epics because i don't think this is actually at least the model i selected isn't because it's so small it doesn't do a good job of a classificat great job of classification yeah so it does okay so now we do that then it's like well how do you test it you can do it testing in a couple ways uh your test set even if it has amputation you don't necessarily have to like do the stack you can just do one copy you could actually test it by doing all of them i'll skip over here same thing when you get a new a new value future value coming in you if you do a multiple imputation on that using their imputation models um you could either just take one of them and you know just do one imputation and ignore the rest or you could do some kind of uh combining like uh do a majority vote in this case um so i will skip over running these because i don't know if they well actually i guess i can run them these don't take long so actually it turns out in either cases you see in the first case i only picked the first imputed value and the second case i just did i do uh or combine them together uh in both cases it came out with this for this um same value it came up with seven so it didn't didn't matter for that particular line okay another way is to just train multiple models independently one based on each imputation you do um i should add that i had not seen any of these things really applied to neural networks this kind of these ideas actually came from other types of machine learning algorithms they do for random forests and stuff like that the there are a few papers that do this to incorporate multiple imputations um so what this does is i basically am running five models and each one will produce a different result at the end how do you put combine them uh you can use one of many uh you know ensemble techniques uh i'm sure uh if you're familiar with machine learning especially in the uh random forest and boosted tree world there's a lot of ensembles for combining models the results models you can use one of those techniques and again we can do our aggregation again you know for that particular value i get seven so all these techniques have been equivalent in my limited case uh the other thing that's just worth mentioning because i've seen this this method in the literature is bagging which basically what they do is um i will run this code but you you know you can certainly run this uh really at your convenience you choose to um also worth mentioning uh you can also define the actual missingness as a feature so you know back in the world of neural networks where you just kind of throw everything into a bucket and hope it figures it out throwing the fact that a particular column on these features is actually missing or is imputed gives it more information and so maybe it can incorporate that missingness into the model uh this definitely works with they do this a lot definitely in decision trees uh i haven't seen it in neural networks but just another thing to throw in there maybe you decide you know this is just too heavy-handed for uh imputation i'm going to let my neural network sorted out i'm just going to do a simple imputation and then i'm going to mark it and feed the whole thing into the neural network if you want to do that take that approach that might work okay i know this section went a little bit faster than uh then i probably was expecting so uh let's just take some time here maybe a couple minutes if you guys have any questions on this especially the the last part um it is somewhat of a survey of techniques and um as far as i know a lot of this has not been combined with neural networks so those of you that sort of have some somewhat researchy type of interests you may want to explore this if you can get a paper out of it and okay the final one it's this final one is actually short and can actually be just described at a high level if you want um okay so we talked about imputation we've talked about um you know missing this but it turns out if you actually use these clobbering mechanisms that code that i gave at the beginning to play with well that actually has another useful application data so this technique um it's actually pretty simple the thing about the non-writing data is like the legal minimum that you can apply but sometimes uh certainly with my organization we have a small non-profit we have in some cases you know self-assuring your data we're all extra but it still so you want to do something you may want to do something additionally about experimenting with your data but still keep it useful so this is a possible way of doing that this is something it and then i'm going to do another i'm just going to repeat this process until there's not that much of the original data left if the invitation is good then the data will be still good for my purpose to be anonymized because yes the person with the weight of exactly 106.3 pounds doesn't actually exist but this is the data set is close enough to do what i i need to do um and the remainder of the example i just used wine quality for this suppose it was health care data so because for this purpose i didn't want to deal with the categorical um so really all i did here was uh i took and since now i'm controlling the way the data is missing i and i know m car is easier to deal with i purposely collaborate with mcar this is one of the luxury i actually have the luxury of being able to know the type of data type of missingness mechanism so i clobber it yeah here's an iteration then i impute it so i figure so let's say number four acidity for i collaborate and imputed and then so that's sort of like one pass and i did i think 10 percent so i repeat this 10 times and so most my data gets clobbered this is actually something i have not i know that my organization has been looking at it i haven't seen anybody else again with my poor literature search capabilities do this just something to be mindful of if you actually are have a need for anonymizing data and yet still preserving a lot of the statistical qualities of that data set and so that wraps it up i'm glad you guys stuck with me in case you know any follow-up and this is actually it's in slides.pdf is in that github repo so if you as long as you keep that github repo you can get all the resources i may correct a few of the things i notice that were wrong here um at some point i'm going to morph this workshop into the blog we have at west health i said that about the last workshop i've conducted and there's only two out of seven written up for that one that was a year ago so yeah bear with me on that but uh there there are certainly references at the end uh they probably go into more statistical detail the not definitely more statistical detail than i covered um if you're interested in that stuff a lot of good deeper articles on this i'm just trying to cover you at the you know sort of the give you the working knowledge rather than the deep statistical knowledge and if you have anything that's sort of a general question i'd encourage people to just post issues on github because then i can address it there and anyone else with the same question can find it but my email addresses are here in case you uh want to contact me directly for some reason or another and um that's uh pretty oh i guess the other thing about github is that actually that kind of guarantees that i get the message around any spam filters i might have i'm not very diligent about keeping track of those so um so that's it well thank you for being a patient group here and i hope you learned something from us [Applause]
1,Sharing scientific tools from script to desktop,https://www.youtube.com/watch?v=2dd4BduDkG8,all right so we're going to start slowly here welcome everybody my name is jonathan we're going to all introduce ourselves and we're even going to get to learn to know you guys but before we start really i just want to make sure that everybody has cloned the repo and then if you even if you cloned the repo more than uh an hour ago please do a good pull because we were still improving and tweaking things improving slides and everything we built all this material just for you guys so so um you're gonna be the first ones to go through it and uh and yeah we're we're continuing we're continuing to improve it and we've been continuing to improve it for the uh for the last few weeks all right so um welcome everyone um my name again is jonathan roche i work at kbi biopharma i drive software engineering and digital innovation there and we're super happy to have you guys and we can go around so we'll be four in the in this team to present this tutorial and that's going to give actually a lot of a lot of space a lot of manpower to help every one of you we have lots of exercises throughout this tutorial so so please don't hesitate raise your hands and we're going to all be able to help you hi everybody i'm jason chambliss i actually work with jonathan at kbi uh as a software developer i'm the only one of the four up here that doesn't or hasn't ever worked at in thought but um i'll be up in the side here if i'll be watching for hands if anybody needs any help um just raise your hand i'll come over hey everyone uh i'm siddharth i work at anthod right now as a scientific software developer um where i write code for i build applications that are used by geoscientists i've been at n thought for a couple of years now and before that i did my phd at ud in computational science great to be here and be presenting this tutorial hi everyone i'm prabhu ramachandran sorry i just finished another tutorial at lunch and came in so my voices are breaking up a bit so i've been using and thought tool suite for many years now i worked briefly with nthot for a couple of years in 2013 but i've been using the unthought tool suite since 2005 roughly and i've built several applications using some of the stuff that we're going to be teaching today one of which perhaps the longest lived has been mayavi which is a 3d visualization package which is built with the ats which is part of the ets as well so that's that's where i come from but i in my day job i'm a faculty member at iit bombay and i'm an aerospace engineer and my research is around computational fluid mechanics yeah so glad to be here and it's really nice to have worked together to build this material i think it's exciting material because it covers a lot of ground some of the stuff even i don't know fully but hopefully at the end of this you'll see um you may have to be a little patient initially there are lots of concepts lots of things but at the end of it we at least we believe that there's uh that you'll you will go away with a set of tools and set of skills that will be very valuable so thank you for being here all right so in this tutorial we're going to use an excuse for teaching you guys ets and thought tool suites to build a desktop application and so we're actually going to build this uh this tool here uh in just uh just under three and a half hours so i think that speaks to the power of the framework that we're going to we're going to be uh using and trying to put into your hands and so with without further ado let's just give you guys a quick overview of the of the actually before we go into the outline i i want to do a little a few questions with show of hands to get to know you guys better so let me ask a few questions how many people have been using python for less than a year one two to one two to five years okay and more than five years everybody else wow that's awesome uh great um how many people um build tools uh for academia okay versus the private sector it's the rest okay so that's that's about even uh that's great um and then um how many of you work in very small teams i'd say less than than five people dedicated to building a single tool okay and so more than five people one or two uh okay yeah so so you most of you guys are building tools in small teams uh that's been my case until very recently also at kbi uh before kbi actually i was at n thought as well so that's how i got to know the entire tool suite i've been using it since 2010 so so we we have a a a couple of decades of experience with nthot tool suite here it's going to be it's going to be great to share this and so um yeah kbi at first i was uh you know developer number one or or close to it uh the only one doing full-time development of tools and uh and the power of ets is is to be able to empower very small teams i want to speak more to this but small teams or fairly small teams to build tools very quickly so we're going to try to build this in in three hours we've already you know built it for you guys but we we're going to teach you to do it yourself any questions before we get started actually another show of hand has everybody who has not cloned the the ets tutorial repo you have you have not uh so i'm gonna quickly i'm gonna bring it back here is the link for the tutorial um sorry it's uh it's in inside my own space so yeah you got it so please clone the repo real quick and then in the readme there are instructions for creating let's create we should all have a dedicated python environment that we can you know mess around with for this tutorial so we we have instructions in the readme for you guys to make sure that you have a python environment for this okay and then so while uh i'm going to speak to the outline general outline and then i'll let you probably jump into the introduction uh so just a quick outline of the of the four hours three and a half hours that we have together uh we're going to give you a quick introduction to the in thought tool suite um you know what it is why why why use ets there are some good reasons and and even more specifically when to use ets they are they are situations where ets is going to shine it's going to be great but ets is not for every digital solution out there there are lots of problem sets that don't fall into the the space that ets uh solves okay so we're going to introduce uh kind of some of the criteria um uh to to you know recognize what type of tools uh uh is ets good for uh versus not and and then we're gonna slowly build our application and we're going to uh add we are going to mainly present three packages to you guys three packages within ets that are going to allow us to build this this application so the first one is going to be traits it's at the core of ets and it's going to allow you to have reactive programming if you want to build a tool it needs to react to events so that's what trace does it it allows you to do reactive programming then we're going to build uh uh saddam's going to lead the that second section and and build uh views uh uis for uh for your uh your data using traits ui all right so we're gonna build some panels there and then we're gonna use pi face uh at the as the third step uh to put it all together and build an application uh if i go back to if i go back to the to the title of this of this tutorial actually i'm not sure if this title was in the uh in the the the agenda or if it was something slightly different but i feel like the most important word in this title is scalable where we're going to try and put into your hands tools that allow you to build something scalable there are so many tools out there to build a tool that cannot grow what we're presenting to to you guys today is something that can scale we i've built many products for many companies using ets products that are worth lots lots of money i've done that at nthot i've done that at kbi these are tools that can grow that can be maintained over time that can scale to to large and valuable scopes okay so we're gonna we're gonna put it all together with this uh with this pi face package uh that has a that has a framework in it uh to allow you to think of your applications in certain terms and start kind of offloading your um your responsibilities all right so um as we're going to go along and then we'll talk a little bit about packaging at the end and give you guys some pointers um to to wrap it all up and as we're going to do this if you've looked at the repository that we've given you guys um you you'll see that there are lots of folders stage stage one stage two stage three and so on and so forth so we're basically all of these stages or all of the uh kind of you can think of them as a as git commits right you're we're building an application and we're just showing you the the life of the application starting from a bare script on stage one all the way to a fairly fancy application at stage seven which we will probably i doubt that we'll have time to to cover stage seven but we still gave it to you if we can that would be awesome so okay with that i'm gonna leave it to prabhu to introduce ets absolutely [Music] actually do you want me to do a git pool okay yeah we're good no problem oh awesome so i don't need this you know thank you thanks again um shall i full screen yeah can i make it a bit smaller why doesn't it go in full screen i did oh sorry too much okay i guess that's okay it's okay okay all right so let's jump right in um so all of this content that we have there's a there's a in the repository there's a directory called slides which has you've committed the yeah so it has both the ipython notebooks and markdown files we edit it usually in the markdown and generate ipython notebooks um so you you feel you can feel free to in fact you're recommended that you follow along using um jupiter notebooks so you should if you're not familiar with this already cd into that directory and do jupyter space notebook on a console or whatever your favorite way of starting jupyter notebooks is in that directory and then select we've numbered the notebooks so zero one is just the intro it's just blah blah and the second one is really where you're going to type you're going to type along and follow do some exercises and stuff like that um and after that we probably will switch to using a mixture of the console terminal as well as using perhaps your favorite editor for the exercises all right so um first steps does everyone have the repository and all the installation instructions is is there anyone here who does not have it and needs some help can you raise your hands please awesome fantastic i love this crowd okay all right so then i can skip this and we can jump right in so why do we need a ui this is one of the first questions you know many python programmers might think about do you really need an application especially if you're used to programming you're used to you know terminals um so it turns out certain tasks are obviously well done with just scripting but there are lots of tasks that are much easier with the ui especially those that have a lot of visual component a lot of those where you have a you know a whole bunch of options and you can't remember all the commands it's a lot easier to drive these applications with a ui the other major factor about uis being useful is they're much easier for non-programmers sorry um and this you know so it it lowers the barriers for those who are not too comfortable doing the programming and it also makes the application much easier to share so i think most of you may have examples in your own lives where you have specific applications that are very well tuned and well designed to do specific tasks and they will typically have a very carefully designed user interface um so user interface has to be done with some care right so you want to optimize a certain workflow that you're working with so uis are like great for things like this and this is you know this is where we want to facilitate building uis easier for scientists and engineers that's kind of our emphasis so first question again is so okay fine we built a ui so why do you want to do it with ets why not just write it with qt the first thing is of course traits and ets are not new toys and the new kids in the block they've been around for a while i think traits may have started 2004 2003 something like that they're all open source they're fairly mature and as jonathan just said uh we've all used traits to build a whole variety of applications so it's not like you know it's a new shiny little library that we've just built um and also that it's easy to start so if you want to just build a small tiny little ui just for some throwaway code it's very easy to do with ets in fact sometimes when i want to show off to a student saying why you should do python i like start up a ipython console and in four lines of code i can show him a little ui and they're like oh wow you can do this with python so there is that of course but the more important thing is you can go from that point to building a very scalable application as well so this is the nice thing that trades rights ui and this set of packages brings for you and to do that ets actually in some ways forces you to rethink how you write your applications can't fully force you of course being python at all but it basically promotes the use of a model view controller kind of design which we will probably touch upon more by practice rather than by theory but the emphasis is basically to build reusable models and then associate a view with those in a very declarative way that's kind of how you tend to build applications with ets and this kind of forces you to think about your application in a very different way it forces you to think about you know your what some people call business logic or what some people call you know your core models and that promotes writing very scalable and reusable testable code so a lot of this comes from the you know fact that a lot of these applications are enterprise kind of applications that need to live longer than just an assignment or a throwaway piece of explanatory code the other nice thing that the ets tools afford is the fact that they are sort of back-end agnostic to some extent so initially ets and trades were using the wx python toolkit i don't know how many of you how many of you have heard of wx python not many okay so there was a time before qt where wx python was one of the front runners in terms of ui ui toolkits but then since qt has come along wx python has sort of seen a decline um even in within ets i think qt support is much better now than we expected but ets will work with either with some pros and cons in each case um all right the other thing is it interfaces very well with a set of other sister packages in the ets ets universe which facilitate plotting both in two dimensions in three dimensions so this makes it possible to do scientific kind of application development makes that possible with eds yes and again reduced development cost a single programming language that's fine um yeah so okay so it's all all of this is going to be desktop site none of this is going to be client server none of that stuff so this the emphasis today is about building desktop applications not for client server applications all right and so now the downsides the downsides are one of the biggest downside perhaps today especially in this context with web-based tools being very popular is that we currently in ets are limited by toolkits that are primarily desktop oriented so qte wx python there is no clear-cut solution that we have for a web-based application in principle it's possible but we don't yet have the infrastructure or the requirement internally to build something like this we do have some libraries which allow you to take a trades ui and give it a html based user interface um something that's currently i wrote this with some colleagues many years ago but it's kind of in disuse still not maintained very well because it's kind of a one person project weekend project so um a few years ago no this i'm talking about before which is yeah okay which is a which basically presents a traits ui as a javascript object model and so using tornado you can make an application which has a that kind of predates that but the other project he's talking about is to build a ipi widget's backend for trades ui which started but it kind of petered out because ipad widgets is not as featureful as qt or wx it's not a full-fledged toolkit um so the natural downside of this is that all the applications you tend to build with ets tend to be desktop applications and that's not the solution for all kinds of problems right so you may want to have something that from the get go has to have a web presence in which case this may not be the right tool set having said that i think if there were enough resources it's something that's definitely doable it's not not doable but i don't think nthotis or anyone using ets has actually had the resources or the requirements to build something like that okay so what is ets so ets is an open source set of tools you can find this from the documentation here i mean it has a whole bunch of packages the first among these is traits because straits is kind of the bedrock it's kind of like numpy being the bedrock of all scientific computing in python ecosystem in ets traits is that bedrock so all of the packages will use traits and use rates heavily and they're written in a particular fashion that relies on the fact that you have traits and as a side note what many people are not aware of is there's a library called traitlets that is used in jupyter notebooks and that library actually has its origins in prints so it's a sort of simpler version of traits if you will which doesn't have a c backend for performance reasons traits has a c back end as well for performance uh but tracel traitlets doesn't have that and it doesn't have a bunch of additional features uh but traitlets is based on traits the ideas came from trains traits ui allows us to do easy ui building which is basically declarative kind of ui building so normally if you build a qt application you create the widgets and then you start wiring things and connecting things together you don't have to do that with something you barely ever need to write a qt specific code then on top of this is a library called envisage which allows someone to build a plugin based application framework so if you want a framework where you want people to be able to contribute plugins and things like that you would use something like end message but that's a bit advanced more on the scientific side if you're looking to do plots there's a very powerful interactive and performant 2d plotting library called chako which is in thought which is again part of ets it's again open source unfortunately it's not as well used outside of nthot and that environment i don't know why but i guess because matplotlib is so good at producing publication quality plots but charcoal does have a lot of powerful features and it's really well suited for doing interactive plots so if you notice if you look at matplotlib let's say you do a scatter plot or not a scatter plot if you do a histogram and you want to update the data on the fly it's a lot harder okay it's not set up to be useful or efficient when you want to do updates on the fly charcoal on the other hand is built from the ground up on top of traits and it's fully reactive so if you want to do like quick updates on plots you'll do something like jaco and then there's mayavi which is does 3d plotting and again it sits on top of traits so again you can embed 3d interactive visualization in your application using ets so this is a big selling point the fact that you can do 2d plots and 3d plots a lot of nth applications tend to have this in them the other thing that's pretty common when you want to do um application development is you may have tasks that are long running and you don't want to interfere on with your ui so there's a trades futures package i don't know if you have time to talk about it today if we do i think there's some content inside the slide set that we have about it but what it allows you to do is it allows you to use a thread pool executor or a variety of executors that the concurrent futures package of offers and it allows us to run computations on the background without blocking the application from running so that is also part of the ets tool suite so that you can you know build interactive applications as well which while doing some long-running computation in addition there are some others but these are probably the most popular used packages the ets tool suite and the way that this is set up is through a layered approach so basically we have python which is most of the time we are talking about desktop applications and we are talking about a c python layer and we may have applications which have extensions that are built in c or c plus for example qt is a great application great library implemented in c plus plus but then has python wrappers and then on top of this is like pandas and traits numpy and then on the ui side we have you know pi qt or pi side qt or wx and wx python and then on top of that there's pi phase it's straight ui and then the sister packages like maya v and charcoal dvdk enable a whole bunch of these packages and the goal is to get you comfortable enough that you you kind of see the core pieces here and the core building block ideas so that you can use any of these mix and match and build your application sitting on top of this framework and that's the goal of this tutorial okay so um just to showcase and give you some examples jonathan already showed you the screenshot of the application we'll be building today but you can also do other kind of applications so i've put together just a few simple ones so here's a little dialogue which has a 3d visualization with a little some sliders here and some text fields where you can edit and put stuff you can write all of this in pure python without a single line of qt or double expression that's the nice thing about this and this application is about 130 lines of pure python that's it so you can basically create something as rich as this with all of the features that are available with very little code another case is so this is a case that is okay i'm going to make this a bit smaller okay i guess you can see to some extent what this is so this is an application that's very this is something that i use for my research i do particle-based computations and we generate a bunch of files and you want a visualizer for this right you're generating output you want to quickly be able to visualize and then explore the data that's being generated so i wrote this application again pure python using ets in my rv and you can put together a fairly sophisticated application with a whole fancy ui without you know spending a lot of effort that's the kind of power that you get with ets okay so um okay so we'll start with a simple python script that's where we're going to start the details of this python script is not important so some of you may not be familiar or may not come from an image processing background so don't worry about the details that's the reason we give you that basic scaffolding code the goal is to see okay given that you have this code if you want to make it into a nice application a scalable application so we're going to start with the script which detects faces and it also extracts some metadata from the images okay so let's say we found pil or matplotlib and we know hey i can make this plot i can extract the exif information from these images and now we want to say ok i want to build a image processing not an image or image viewing library which is customized to what we want to do okay so that's the kind of that's the application we have in mind and this entire tutorial is built around building this application so there are many things that we could have done we could have shown you a bunch of other things but the way we've tailored this tutorial is to make sure that we are sort of giving you all the basic pieces in order to build that final application okay that's that's what we're trying to do here um and we want it to be an easy to use ui we want explicitly a user interface and we want to write it in a way that it's scalable we want from the get go not to be a throwaway application we want something to be scalable and finally we'd like to share this application so i don't think we have we wanted to make an installer at the end of it but i don't know if that piece works yet uh but what we will do is we will give you a scaffolding so that you know how to set up a full-fledged project with a setup.pi so that you could either put it up on pi pi or ship it as a tar ball or something like that or build a wheel from it and then ship it and share it with friends so those things are possible okay so i'm skipping this as you've already seen the resulting application that's going to be shown so this is the schedule of what we're going to be doing we're going to start with a simple python script now we're not going to explain the gory details of that script i think it just uses pil and matplotlib to do something i don't know what it does for the face detection sk image it does uh uses the image for skimming for that yeah so we use psychic image to detect some faces and that's it so that you can treat that as a black box you can look at the script you can feel free to modify it as well so what we're going to do is we'll start there from a throwaway script to literally throw a throwaway script clean it up using traits okay that's going to be our starting point and to do this we're going to have to spend some time learning traits so we'll throw in some toy examples that you can play with and some toy exercises so that you're familiar with the traits model and as you've seen traits is like the bedrock of all of this so it's good to understand that and once we do that we'll build a basic user interface for this little thing that we simple image viewer that we have we'll build that and put in matplotlib as well so we'll show you how you can use matplotlib to make a little plot into this simple user interface so that's going to be the first phase and we'll do that using traits ui so you do it in a declarative fashion no qt no wx 5 and then we step into the process of taking this still at a throwaway script level with some cleanline with a little bit of cleanliness we're going to make it uh suitable to be written out as an application a full-fledged application and we'll introduce pi phase at that point and then start building more features menus and things like that to it and then if there's time we'll do optional features like trades futures or packaging so that's the schedule so the basic script is already there in the repository uh if you go to stage stage one uh there'll be a single script there you can run it what it does is it detects faces in a given image some single terminal so i can increase the font size yeah sure so i'm currently in the ets tutorial repository i'm just going to do oops sorry yeah this is a script as you see there's not even functions it's just all it's like throwaway code i am going to just run it okay so this is what it does this is pretty much oops so if you remember the screenshot that jonathan showed he had exactly the same matplotlib figure but embedded in a larger application so all this has is just that mac loaded plot okay so the application we're going to build is going to have lot more than just that so you can you'll probably have a file browser you can click on different images so it will do all of that for us but this is the starting point oh okay um yeah this is sometimes so this is what's happening because it's a relative path yeah so the paths in that script are relative so you need to be in the right directory okay okay so it's yeah okay more people all right so i guess everyone saw the image detection however good or bad it was it doesn't matter so the idea is even though you're using sk image now you could obviously use something more sophisticated later on because that's now just a detail what we're looking for is you know building that application which kind of tells you why this is important to design from the get go in a clean way that the detail of the computation is not the focus at this point so if you have you know ideas or ways to improve that face detection go for it that's fine um so anyway so this is a joke but yeah that was a dream of reviving this application yeah okay so um so the next step is from starting from here we're going to learn about traits and this is going to be sort of a detour in some sense and some of the things you're going to be taught in that part is maybe going to why are we doing this now but just be a little patient because the things that we introduced there will be used later on when we build the application and the goal that we're going to go for not in the next immediate slide is to build a clean model using traits so the first step is going to be to extract what we think as a good model from this little script build that out with traits and then start you know building a ui on top of that okay oh it already opened it okay so you can go ahead and open the zero two trades dot ipinb on your jupyter notebooks um and you can just follow along as we do this okay okay do i need to wait sit down do i need to wait or okay uh okay um um okay jonathan just let me know when i should start okay all right so um let's get started with traits so you can think of traits as kind of python attributes on steroids that's what i used to call it in the past so you're all familiar with attributes on a python object right you can think of traits as being attributes but which have a bunch of additional features the first is that these attributes can be typed so we are used to python objects having any type right you have an attribute attribute can be anything it could be a string it could be a float it could be anything but with traits you can actually constrain it and make it a typed attribute so you can say this is an integer and if it is declared as such if you assign it a string it's going to complain so traits does allows you to have typed attributes it is reactive in that if you change that attribute you can listen to changes to an attribute you can explicitly listen or you could define what are called static handlers where when you change that attribute it will call back a function okay and it's very easy to do this they can also be observed which means it's part of the same reactivity that you can observe and a trait on a specific object and react to that okay we'll show you how this can be done and this sort of leads to much cleaner code and from the get go because you have typed attributes a lot of mistakes there are a bunch of mistakes that reduce because obviously these attributes can't be anything and because we have declared the attributes with specific types it also becomes easier to generate a ui so there's machinery in traits ui that uses this information about what every attribute is and can therefore automatically make a user interface for large part and you can configure this using declarative approaches which we'll look at so um there's a pretty solid amount of documentation for trades especially for trades trades ui may be not as much as trades but there are lots of there is reasonable documentation there's also a tutorial that you can follow which goes building an application using trade center hui so there is documentation you can check it out okay all right so why all of this why do we need to do trades why can't we just stick with python objects well there's you you win something by doing this and it's a lot it's not a lot of work it's a it's a change in how you think uh but it's it's worthwhile because it gives you a lot of a lot of big benefits you get at the end of it so so just bear with us for a little while um all right i'm going to skip that we've already done this so basically a recap of that you can do initialization so when you normally create a python class you'd probably define in dunder init dunder and then you'll give it a bunch of keyword arguments and those become your defaults right whereas with traits it's much easier to do this you'll start seeing examples very soon so you can initialize things with default values very easily with traits validation is fully possible because you have every attribute can be typed you can do deferral and delegation so you can say this attribute on this object is going to be deferred to another object this is something that can be done with traits very easily you can do notification again this reactive programming and there's also a visualization that comes for free so let's get started so this is our i would say a canonical example of traits okay so pardon the example some of them are very simplistic and very silly but they're trying to just illustrate a specific feature of traits um so we start with the following all imports typically in an ets package will have this will have the structure of from package name dot api very rarely will we have from package name dot something else dot something else import something right usually it's going to be from trades dot api import this from trade cy dot api import something so trade start api import a few of these there's delegate each of these is a trait okay except this perhaps has strict rates is the base class from which we will enter it so is everyone here familiar with basics of object-oriented programming with python is anyone not familiar with object-oriented programming with python okay great okay that i hadn't thought about that but that is scary if someone didn't know this okay um so each of these instance in str um are traits we'll talk about this in a little while and this observe is used to you know attach uh to observe a specific trait and react to that that's where it's used okay so the simplest class that you can think of well you can think of a simpler one with a single attribute here is class parent okay the parent has a last name and the last name is a string as you can see so you can read this literally and it kind of reads quite easily parent has strict rates and there's a last name trait whose default is empty it's just the empty string okay and then we have a child which also has strict states which has an age which is an integer now you can question me why should it be an integer why not a floating point number well we can do that too but for now it's an integer then the child has a father in this case which is an instance of a parent again you can read it off and it makes sense right and then the last name is delegated to the father so the last name of the child is dedicated to the father very patriarchal i'm sorry but it's just an example the notification here again is illustrated by a very simple use this is something that you'll see in a lot of existing trades code some code doesn't use this approach but this is what's called a static handler so this is a method and the name here has significance the underscore age underscore changed is like this magic method if you define this it's a method so self is the first argument you can have an old and a new okay and this is just printing that a change from old to new so what this does is whenever the age attribute the age rate changes it'll call this function automatically okay so this is an indication of how you can do notification so i'm going to just run these two and i urge you to do the same on your notebooks hopefully it runs and now i'm going to instantiate this and show you various features about this you have a question oh yes it's zero two traits dot nb inside the slides folder yeah so all of this is in the slides folder just jupyter notebook inside there and then you should be able to run all of these yeah be a bad instruction anyone having issues with that so this isn't okay it's in slides oh yeah you had bad instructors oh i did something i shouldn't have okay does has does everyone have this running okay so now what we do is we just instantiate these and notice we didn't have to write an under dunder in it dunder at all so traits basically takes care of that for us so um we're just instantiating joe as his parent joe dot last name is johnson but i could have also done this i could say so i could have done this as well i'm sorry i'm struggling with this keyboard so what we've done here is we've created a parent whose last name is jason and then we change this back to johnson i'm just showing you there are different ways that you could have done this and then we have a child called mo and mo's father is said to joe so we could have also done this i could have said you could also do this so notice that we didn't have to write an init method explicitly but okay so now the first thing we're going to show you is delegation perhaps not in the same order that we described them notice that moore's last name inherits directly from the parent so if i change the parent's last name it would basically just reflect automatically i don't have to do anything about that the other thing is notification so if i change moh's age to 10 it will tell me that the 8 changed from 0 to 10 and all it's doing is calling that method automatically for us and here's an example of the validation so if i now set mo's edge to a string it's going to complain okay so don't you have some weirdness in your if i hit escape it goes into some edit mode and why doesn't it scroll i think this is uh oh okay i have configuration options okay it's fine because without the slides okay now it's on mine i've set it up so i'll scroll down sorry okay so i guess you must have seen this error message i couldn't scroll on the other one um so you can see it gives me a message saying age trait of a child instance must be an integer but a string was passed so the validation is kind of helpful when you run into problems and the final feature is like really nice which is i get this ui for free i did nothing i just did configure trades and it automatically made a qt based ui so this is really nice but what makes this more fun is the fact that if i turn on qt interactive workflow on a jupyter notebook if you do this what jupyter notebook does is it sets up a qt interactive loop so that i can create qt widgets and interact with them live okay and now if i did this yeah how do i bring it over and now you can bring it hang on i'm going to shut off the header shut off the toolbar okay okay it's right here okay great okay so now i'm going to show you something interesting i'm going to change the age of mo here and notice that the ui changed live automatically i didn't have to do any wiring because the traits ui automatically hooks into any change right changes on the object and it'll update the ui so this makes it very easy to create a user interface which has a lot of these very cool features built in you don't have to do much more you don't have to wire those things yourself so this is like a major selling point for me because for most of us because we don't have to worry about updating something that's built on a traits ui we just change the object and everything reflects automatically so that's a very convenient feature right so with that basic start the next thing is okay what if so what if well you want to still do something in dunder in it the way we would suggest that you do that is you define a dunder in it and make sure that you call super and the way that the pattern that is used for all traits is that you always are given a keyword argument list of all possible traits that someone may have sent you and you just pass that along to uh has strict rates is in it and that's that's it and then you can do whatever you want inside the unit so if you really want to create an init you may do so but the beauty of the traits approach is that you don't have to create them so the instant you define those traits you're done so basically your your classes look much easier they look much nicer and they're easier to write so there are a bunch of predefined trade types there is like a whole host this is just a small subset of them there's like booleans complex numbers integers floats strings tuples lists dictionaries sets range so range is like constrained kind of numbers you can have constrained names constraining floats you can do regular expressions you can have things that are read-only which means people can't change that a whole bunch of others okay so and we'll talk about some of these we'll show you simple throwaway examples for these that you can use to learn trades a little bit oops this should be has traits has strict traits versus hashtags oh sorry all right so you will see a lot of code out there that uses what's called has traits and not has strict traits there's a there's an important difference between the two and we feel that you should use hashtag traits because if you just define a class that uses has traits you can add any additional pure python attributes willingly as in you can just say okay object.some crazy attribute equal to something else that no longer has the nice properties of traits and sometimes this leads to errors because you're typing you made a typo and you thought you're doing something to one of the traits that you have but that it's a typo right so it catches those because the instant you're doing one of those you'll get an either an attribute error or if it's a trait that you're trying to assign you will actually get a writer okay so it catches errors and it will not allow you to set any attribute you have not declared so which means you're kind of being honest to your own object model so you've created an object which has certain traits you're not going to introduce anything new inside a method for example so it kind of makes it forces better documentation if you will all right so let's move on and let's look at the first very powerful feature that traits offers which is observable the fact that you can observe any trade change so here's the same example that we had before we had parent last name we have child age is an end and father is instance of parent we have this static handler here where you don't need to apply any decorator like this observe but let's say you have another method where you say look if the father's last name changes i want to be notified so you can do that by creating a method here and the observe has this interesting syntax where you give it either a trait and this is called an extended trait name syntax so here you're saying if the father traits last name changes notify and call this attribute call this function okay so and this function name has no there's no requirement of it to have a specific name it can be any method name it's unlike age change which has to be underscore age underscore changed this can be anything so this will always be past and even so whenever you use observe the pattern is you will be passed an event and there are different kinds of events depending on what kind of a trait was changed and we'll talk about this in a short while so in this case you will get what's called a trait change event okay and that will have a bunch of methods sorry a bunch of traits that you can look look at we'll look at that in a short way so i'm just going to run this and here is our dad last name is zubizareta brownie points for anyone who knows who that is and child is okay the father is dead and now notice that it says the father's name changed to zubiz that was because i passed su bizarre it has the as the name now i changed it to valderrama and the father's name changed so notice that even though i changed the parent object's last name i got notified so this is like a quick example of how you would use the observe the observe function the decorator in order to be notified um so again a recap there are these two things here first is doing this is the static trait change handlers which is straight name underscore changed and the other is a decorator based approach which could work with any function any method that you have and this has this extended trait name syntax and if you want to learn more about the names possible yes yes please yes great question great question i have an example a little later that does that but the idea here is you just say h for example so this will listen to both age on the object as well as father's last name does it support the bracket i can give it a list of strings that used to be there with ah not for observe okay so my problem is i've carried along knowledge from 2005 so i'm i'm a bit messed up with some of the new syntax but that's why i keep looking there to make sure i'm not saying something wrong but yes that's easy to do um you can do a lot more fancy things there's something else i'll talk about very soon so for example if you have a list and you have an item changing inside the list you can actually get notified of that thanks for a great question yes yes yes all right okay uh we have the first exercise uh i think we can skip this exercise and move on we're running short on time but i'll just go through this so that we want to modify the first example to produce this one it's already done so it's okay we want to add a boolean tray to specify no i changed this and i didn't change this i'm sorry add a bowl trade and add an enum so i'm going to just forget about this description here i'm just going to walk over the solution and explain what this does okay so we have a parent whose last name same old parent we have a child same in agent father as instance of parent we've added a first name to the child that's a string we have a i guess a running joke here does the child like queso and it's a boolean so bull traits can have a default value of true or false so this case the child likes queso and we've handedness is the child left handed or right hand okay so we have an enum so an enum can have in any la any number of possible options left handed has no hands whatever it is okay so basically you can have any number of possible string options in this case and yes setting a value to a handedness to something else that's not specified here is an error so you will get an exception if that happens um yeah so that's the example so we've just introduced two new traits here bool and enum nothing much else here yeah so again even though we've introduced new traits the nice thing is the dunder in it we don't have to do anything no extra work for that you can pass optionally any kind of traits that you've defined defined on the as as part of the constructor okay all right [Music] okay so now let's get more into events so i said that whenever you have this method a method that's being observed sorry or rather you have a trait that's being observed and there's a handler defined as a method you're always passed an event right so let's look at that so in this particular case the event that you will get back is a trade change event okay it's a an instance of a trade change event so here's an example that walks you through that everything is the same as before i've just changed the dad name updated to show you um the different attributes that the event has so it has the following trait change event instances have an object they have a name they have the old and the new values so the object is the object on which the trait was changed okay so this might be useful in some cases you want to know which object was changed the second is the name of the trade so in this case it will be last name um the event.old will be the old value event.new will be the new value okay so this makes it uh convenient so let's do this yeah so notice it says 8 change from 0 to 21 main.child object that's the object father trait was changed from none to the main parent object okay and then the dad name was changed to the yeah that's the that's the next print statement okay so now i change the dad's last name it says main.parent object last name ray ahmed okay so basically it tells you that these are all attributes of the trait change event that you can use and the many cases where you don't really need the event but you have to pass it anywhere mandatorily but there are lots of cases when you say i want to know the old value if the old value is this do that you can write your handler however you want okay but this is just a pattern that is worthwhile knowing about okay so any questions at this point or should i just roll along okay um can you guess what would be the widget that would show up for these for these two things that's it what did i do sorry oh that's the parent so i need to edit the child yeah that's a check box and that's the drop down okay moving on so the next thing we're looking at is container trades and these are very powerful you can do a lot of neat things with them so i'm going to talk mostly about trait lists at this point and the rest is an exercise for you to look at so here's an example like again a silly toy example we have a bowl of fruits okay so the bowl has strict rates and it has fruits which is a list of str so we're introducing the list so notice that you can constrain not just the fact that fruits is a list but you can also constrain what it contains in this case just strings if you don't give it anything you just say list it'll contain it can contain anything right you're not constraining the type of what what goes in and now there is something interesting with this is the fact that you can change it in two ways you can either set the entire list so you can say fruits object dot fruits is equal to some list of strings that's one way the other way is to say look append or remove because it's a list right you can you can mutate that list so there are two ways to check for notifications of this kind they are very different kinds of notifications and so if you're just looking for changes if someone just changed the list as a whole you just do the same observe fruits okay or you use the static handler underscore fruits underscore changed and then you'll get a trade change event which just has the information about what was the old list what is the newest is that okay so this is this is just a trade change event okay on the other hand if you want to know if the specific item inside that list was changed you can do the fruits dot items and what this will do is it will give you a trait list event so that's what this is going to illustrate so let's just create this i'm going to create a empty bowl and now the first thing it's going to do is going to do create clobber that list with a new list within just an apple in which case it will call this the second one it should call this let's see what it does what did i do wrong no no it's the fact that when you do fruits dot items i think i should do fruits colon items is it yes and if the items changes okay so this is a this was seen earlier as well i didn't talk about it then because earlier when we defined father dot last name if you notice it printed two lines one where the child's father changed and the next whether styles father's last name changed i didn't talk about it at that point but okay now we have to bite that bullet so when you do fruits dot items it will listen for both changes to the fruits as well as fruits items when you do fruits colon items it's just going to look for the items change so let's try this now yes and that works so first if the list was updated that's this and the second was fruits the items alone is updated so notice that here you can actually find out what was changed and where it was changed so it'll tell you what was added what was removed and at which location in that list it was changed so it's kind of powerful way to look for changes inside a specific list there are other state change events there's a dict change events so i urge you to write your own little example with dictionaries as well as sets as well but we're not going to look at that there are some more useful traits and these specifically will apply to our little exercise our application picasa because we have a directory of images right so we have files and directories so it turns out there are two traits for this as well on the face of it they look very simple because it doesn't really do any validation to check if the given directory name is valid or not that is on you but what it does help us is with the visualization so if you create something which has a file and a folder the when you create the automatic visualization the ui element that's used for that will be an appropriate element okay so right now doesn't do anything fancy but i'm just telling you that these exist so here's a tiny example which shows you how to use dictionaries directories and files again straightforward so we have root is a directory and we have a list of files in this and we have a sizes which is a dictionary in this case okay so let's do a walkthrough of this the dictionary can have any keys or values in this case okay there's no constraints so dictionary can have anything just like a python dictionary but if you want to apply constraints you can also use the keyword arguments key trait and the value trait and notice that the value trait can take a nested dictionary which itself has constraints of strings and integers so what i have here is a simple object called a folder which has a root directory and it has a collection of files inside that and it has a sizes which is a dictionary uh for every file it has so the key is a sorry for every directory for every file there's a key and there's a dictionary corresponding to that having a set of strings and integers so in this case it's sizes so it's just going to have size as the key and integers i just showed this as a simple example of just like four lines you can have a fairly fairly interesting set of constrained types that you can enforce on your data okay so for example if you look at this and so now if i were to say ah f dot sizes i'm really struggling with this keyboard no no it's fine it's my problem so so notice is actually going to complain even though i've i've passed it to dictionary and the key is string that's okay but the value is not a dictionary okay and even if i were to make this a dictionary and say one something it's going to complain because now this is not the right dictionary so if instead i've made this now that's okay so you see the power of this that you can actually constrain the types it makes it easy for you to debug issues you know if your dictionaries aren't set right or whatever ok so now we have a little exercise i think we should do this exercise which is modify the above example so that when you set the root directory it finds all the files in that directory and then the file sizes for each of those and then sets the appropriate rates internet it seems complicated but the hint is use os list to list the directory you can assume that the directory given is a reasonable directory and use os.path.getsize if you give it a file name it will actually give you the size of that file so use this to just build the so i had a method so that whenever you change the route it'll automatically fill you know fill up the files and the sizes so five minutes for this okay so we'll come around if you need help and maybe after three minutes i will start typing i if you press escape and then you can do jk or i don't use jk even when i use it do you want to turn it up yeah please appreciate it should be free edit keyboard shortcuts this is probably extensions that's fine then leave it it's okay i i've now started getting the hang of it the other problem is i have a full keyboard so i'm always in a mess when i go to another keyboard i'm typing xs instead of cds so oh should i start the solution i would like to suggest that you guys use the observed sentence because that's when you were probably but the the observed is the newer way and the one with the underscore name change uh so how many people are already done oh yeah maybe we should oh i'm sorry oh hmm it is oh it's all named right okay yes yes okay oh i didn't import directory okay so i'm going to quickly try a solution here i don't know i think there's an option to os dot lister to give me the full path but being a bit pedantic oh okay i don't even need this yeah it's okay i'll just leave it sizes is i think that should do so if i say fs folder oops it worked oh yes ah okay didn't work so i did something wrong hmm yes so i made them so it tells me exactly what i goofed up here so notice that sizeish should be [Music] file name and then a dictionary with a string and a i just put a dictionary with just an integer here so this should really be size column this so i'll just walk you through the error it's saying each value of the sizes straight of a folder instance must be a dictionary with keys which are string and with values which are an integer so i just set sizes to a dictionary which is just keys strings and values are integers that's not what i want and then that worked so let me yes oh sorry very sorry is that big enough okay okay so it has all of these and sizes all of this so again this is just some but i guess you get the idea right that you can the only thing we've done that is trades ui specific here is to use this method and the new is the new value of the objective slash dmp or whatever it is and the nice thing here is if i were to now say f dot root is flash home now immediately it will automatically set up the rest of the trades so this is the nice thing about the folder object now that i've created this object whenever i change the route it is going to make sure that the rest of the trades are up to date so this is kind of you know reactive programming so if you have an object that listens to changes and does what you expect it to do right then it's very easy to start composing things together and have them interact in a very specific way and notice that here there's no ui this is just all just simple python object stuff there's no ui coming in there's no you're just focusing on an object model that does something specific is any of this kind of unclear if you want me to go through something i am happy to explain it i am just using list comprehensions yes what would be the advantage of observing root versus the starting point when you have something like just printing whenever it's the static handler i think the static handlers get wired first and then the observers are added on top but that's an implementation detail that you shouldn't rely on but in principle there's no real difference it's just that the syntax is slightly different the observed syntax is actually a bit cleaner because it gives you an event object whereas here there's a bit of magic going on in that there are many ways you can define the underscore root change so if you do root chains with no arguments it will work you won't get any arguments if you get a single give it a single argument like new in this case you'll get the latest value if you do old command u you'll get older mu so the problem is there's this implicit i remember this because i've done it so many times but it's a pain for someone who's new so suppose you give it three arguments will it give you a name old new or is it name new old so that's a pain point whereas if you use the even observer it's like you get an even and even thought you even talk so it's a little easier if you're writing it for the first time so this is just my old habits showing but usually you want the latest value in which case you know just writing a static handler just works and i personally like the static handlers because i don't have to think about what to call this name the method name if it is root the name of the method is root under underscore root underscore changed otherwise i have to think okay root change i'll put some name there'll be no consistency here i'm forced that consistency is forced on it so i like that a bit but i think the modern way of using it is to use observe and do whatever you feel okay so the next important thing that we're going to need even though you may think it's you know maybe not that important but we're going to use this later supposing you have a quantity that is computed and let's say it's a computation that takes a while so so there's a toy example we have here which is truly a toy it's not doing anything important um sometimes you so for example the let's look at this example and make it easier so we have a circle and the circle has a the only property that we saw the circle has is the radius the area of the of the circle is not an independent quantity right it depends directly on the radius now it doesn't make sense to say okay circle has a radius but then it has another area which is also a floating point number because it doesn't make sense for a user to think about changing the area independent of the radius the area has no meaning outside of the radius this so if that's your object model or you can always argue that maybe the area is the more important quantity the radius is secondary but let's say the situation is that you only care about the radius so in this case area should really be a property it should never be explicitly stored you can just compute it so you could just do the following if i just remove that i say get underscore get area and i declare area as a property this would become a property which you can just so it will never actually store the area it will always compute it and return the result so let's just run this once i've also introduced a new trait here this is again useful this is a range trade first argument i'm relying on positional arguments here the first argument you could also specify keyword arguments is low low value the next argument is the high value and the third argument optionally is the actual value of that the initial value okay so you can use the keyword arguments for better readability but the range is again very useful especially in scientific apps where you want a slider so the typical ui element you'll get for this is a slider so it's useful and here i have declared this area as a property and the property is also having a tight constraint it's going to be your floating point and in this case it only has a get it doesn't have a set you can also do a underscore set area and that will be a setter for that property so this is the same as python properties but i think traits properties predate the python properties but anyway so let's run this and now if i were to do this it will compute the area if i run it again it's going to compute area every time so the problem here is even though it's nice to have this property it's going to keep computing it every time you change now even if you don't change the radius right it's computing so if you really want if the computation matters and it takes a lot of time you can use this cased property decorator and now let's see what happens so if i were to do this it's going to do that and now notice that it didn't recompute the it just gave me the same value so the case property basically does the caching and the magic happens here the area when you say observe equal to radius again this has the same syntax sorry the same syntax as we observe decorate so you could have a list of traits there or a comma separated set of traits and it will honor those so in any of those change it will automatically update the area for you so let's try that so if i were to now say now notice it recomputed it whereas if i were to try it again it will not be computed so this is very useful especially for long running computation so if you have something that's a property you don't want to recompute it you can just use case property and again the traits takes care of you know how to cash it what to do all of that stuff it's taken care of all right so what's the next one so the next one is the array rate so now we're getting to more sciency things like ranges and arrays uh so this can handle arrays so if you want to declare something as an upper array uh you can use the array trait uh it allows you to specify a d type it also allows you to specify a shape if you don't specify those it's unconstrained you can do whatever but if you specify those it will require that you have numpy arrays of this the one thing to be careful about with numpy arrays is unlike lists where you can listen to changes inside a list you cannot listen to changes inside a numpy array because it does not make sense numpy does not support that and it would in fact be very inefficient if you had to listen to everything changing inside so that's one thing you need to keep in mind but otherwise it's a very useful trick so here's a simple example which demonstrates the phenomenon of beats so you have two frequencies as just the np sign of this frequency plus that frequency so you can plot it if you want so once you learn how to build a ui in principle you could just take this and add a little view and make a matplotlib plot so when you change the frequencies you can actually see it updating that's an exercise for home but this simple toy example here basically walks you through how to set this up so here is a i'm showing you range in different incantations low high otherwise you use the explicit keyword arguments okay just building on the previous range that i used and here the array here i'm saying is i'm constraining it to be a float array and i'm saying that the dimensions here by saying none comma this tuple saying it's a one dimensional number okay it's not it will not allow you to specify two dimensional and that's it and now notice the syntax for observing two trait changes so you say f1 or f2 neither of these two changes it will update this self dot signal um and yeah that's it so it works so again if you notice uh with ten lines of code we have fairly complex not really very complex but we have an object model which is which which has a kind of very nice behavior you change f you know it'll update you change f2 it'll update and now and we can listen to changes so and everything is nicely cleanly specified and as you read this code it makes it's not very hard to read right you kind of know everything that's explicitly specified you know all the types of all of the things so it's it's nice um okay how much more do we have okay so one thing that sometimes may trip you up is let's say you have an object and you have a trait and you want the traits default value to not be hard coded but you want it to be computed so for example if you have the date you want the date default to be today's date okay not hardcoded to the time the programmer wrote this right you want to default so that when somebody instantiates the object it gives me that particular time so if you have something like this so date is also a trait so this is again going to constrain you to take time objects so now what you can do is you can say if you define this magic method called underscore date underscore default whenever this trait is accessed it's not going to be called by default so whenever someone asks for a date this method will be called and then it will be set to this default so let's try this maybe so i'm going to just do this now oh girl what did i do sorry um just to prove the point that what is this okay all right so this is right notice that this print default was never called because i didn't access it but now if i were to say okay give me the edge it will be computed sorry date now it will be computed and it will give me a what sorry why am i doing this tired okay i've noticed that okay it was computed and the next time you call it it's not going to be changed it's this only the default value when you ask for it for the first time it'll call this so this kind of allows you to do delayed initialization of an object and allows you to do it in a lazy fashion as in you it will only do it when you actually ask for it okay so there are these nice features as well as as you can see it's not a lot of code it's and when you read it it's kind of self-explanatory that's the default okay i think this is the last major concept that you're going to see why on earth is this guy doing this but we're going to need this later so this is what's called an event right so sometimes let's say you have some object model and you want to say this thing changed and you want to you want to make this an event that other people can listen to the point of this is that it's not a value that has any value importance it is just something that's an event right which is so the value has no meaning it's just the fact that it changed that's that's the important thing so that's an event trick so for example here we have a data file okay and the data changed is an event so which means if i have a data file whose data has changed i want to fire this and this may happen for various reasons right the file may exist but you may have edited it for some reason and then let's say you have a data reader so what you want is whenever the data changed in that data file you want to fire uh you know the reader should do something so what you can do here is this data reader has an instance of this file which is a data file instance and you can observe file dot data changed so whenever the data changes for that file it will do something some computation so this is a nice way to it's a nice trait when you explicitly want to declare something as an event so it's not like you have to define something as an integer and you know look for some specific values or anything like that so the way you do it is you define an even trait and you can listen to that change for that event does that make sense any questions yeah it just gets clicked and something happens so for example here ah keep doing this and now when i fire this it'll say file data changed okay so that it basically notifies this and it's just purely an even the actual value doesn't matter okay so that brings us to the end of traits now you have an exercise the next few lines uh are essentially defining the detector the face detector so the details here doesn't matter don't matter so all that matters here is that you know we have someone did all the hard work for us and at the end of the day we have access to something that can detect faces uh after that we are looping um through the list of image paths that we defined earlier we are using a pil [Music] to get hold of an image object so this is image an image object that is a pil image object um after that we're extracting some metadata from the uh the image object so this is just the exif data from the image so if you haven't seen this already this is these are things like you know details of the camera that was used uh take the picture uh what was the focal length aperture so details like that sometimes it even has gps coordinates then we're using that detector object to detect faces and again the detail the api here doesn't really matter all that matters is that there is a detect multi-scale method on the detector which will find faces and the first argument to that method is an array representation of the image and then there are some keyword arguments some other keyword arguments that control how the face detection algorithm works again not super relevant um so okay so that returns um a list of faces that method returns a list of faces what we're doing here is updating uh the metadata dictionary with just the number of faces we found in that image printing them so that is that was sort of our algorithm for finding faces after that we're trying to visualize what we found we are using matplotlib um plt.i am show simply plots the image in a matplotlib plot and then the rest of the code is simply drawing some rectangles where faces were found again details are not really important all that matters here here is that you know this is a way to [Music] just slap rectangles on wherever faces were detected so okay to summarize we have a image paths we have we're trying to extract the an array representation of the image in that path we are computing we're trying to detect where in that image faces are located and then we're storing some image metadata so does anyone have suggestions on if we were to build an image file model uh what what attributes could it have what what are sort of things in here that we could um throw in that image file uh model so if we had something like this any ideas um so think about uh stuff we're just direct we're directly using in this script so the size of the image yes it is important but in this application what we're just interested in it interested in is detecting faces and to do that you know we need to get hold of an array um where do we find that array how do we do that um yeah exactly um anything else what could we do once we have the file path remember we need to bridge the gap to detect faces what was that yeah and then extract uh so one thing we could do is just directly extract the array from that image so this would be for example the rgb data in the array oops not here i started doing syntax highlighting off but um okay and so one more thing that we're interested in this application are these are some kind of metadata and initially it could just contain the xf tags of the image so one more thing that we'd like in here is the metadata what behavior should this class have can you think of a method that this class could have that'll allow us to get to what we're really looking for we are trying to detect faces something like that okay so that's uh let me write that scattered in here i have already written that scattered in here so i try to come up with a traits model so something that in inherits from has strict traits and um we've already talked about this uh this uh hashtag has straits model should have a file path attribute uh metadata attribute a metadata trade um data containing the rgb um array oh uh one more thing we could have in there is the list of faces found for an image and then this model should have a detect faces method that returns a list of detected faces so what we want you to do is choose appropriate trade types for each of these attributes so as a hint file paths could be a file trade um something else to think about is um these the model should be reactive so if you change if anyone goes in and changes the file path the entire model should remain in sync it should act as a collective hole so metadata should not be out of sync with the file path the metadata should always correspond to the file path of the image file so you can either just work off of face detect.pi itself that's totally fine too what we do not want you to do is try to do any plotting in the model so just think of this model as a collection of attributes and behavior that's all we're trying to do we really don't want to really visualize anything um right now um some hints um i think you've already seen this this is how to extract rgb data from [Music] an image if you know its file path and some sample images are available here are there any questions about the exercise maybe 10 minutes and then start and we'll code collectively that's something democratic programming and as usual we're happy to help you out if you get stuck uh just feel free to ask for help so um he does not know where to start so pretty would it help if we had some traits examples up um stuff that we talked about already is if anybody doesn't know how to change that environment huh hey um i'm not sure why it's not like taking the string or really anything when i do uppercase here um okay it represents file i wish i had we start discussing a solution okay um so one thing we could do here is sort of uh try to go this up um live so okay i can see um so the file pass trade like we talked about represents the path um of the image file on disk so what better trade to capture that other than the the file trade which once again represents a file on disk um the other attribute we wanted in there was a metadata dictionary so initially let's just let it be a dict trade but we might want to come back and refine that when we try to make our model reactive [Music] so i am calling the the rgb data inside the image file uh the data trade and this is going to be a numpy array and finally we had the list of faces so that is going to be a list trade oh is it not yeah okay um and then we wanted this class to have um the ability to detect faces so here what i'm just going to do is simply copy code from our face detect script and just based um okay uh i think better ux would be um if i did this um okay that's better yes um so here i'm simply copying from just populating this method from the script these are actually our faces so better variable name here is faces okay so now um [Music] if you oh thank you this should not error okay yeah so but this isn't ready yet because this isn't reactive the the file path trade well first of all we have not uh figured out how to compute um metadata and data and even if we did uh there is no reactivity here um these traits could easily uh go out of sync yes so this can be self dot data perfect so um remember the the notion of a property that we talked about earlier so this is something that is computed from another trade so here we think the data trade which is the rgb array and the metadata trade both could just be computed from knowing the file path so let's make these uh property trades and these depend on filebots so remember whenever we define a property we have to go in and define how to access it or how to get it so the getter for metadata would be called underscore get metadata um yep and once again i'm just going to okay and then similarly for the rgb data array we can define another getter yep thank you um so yeah so and so a good sanity check um both of these are properties they depend on so good sanity check if just to make sure you've done everything correctly is to go back and look at your property and the things it depends on um or the things you're the trait you're observing and make sure that is the only uh sort of dependence on self uh that shows up in the getter so here in the getter we're only depending on self.filepath and that is indeed how we defined both of these traits um so we probably do not want to be going into disk and reopening files so it'll be a good idea to cache these properties just so that we get that performance boost of not only recomputing these uh trades if the file path changes and we can import that from here pil is weird because i think you need to do this um i am going to cheat for tags let's see what it does oh um [Music] what was that jonathan there is um okay um we're pretty much there i think the only thing missing is uh populating the list of faces and we can just do that here whenever we detect phases okay let's try this out okay so um it seems to be working let's see how many it did not find any oh because um there you go so i think there are five it detected five faces in there and these are just this is the sort of representation that this method returns so this is a representation of a phase returned by this method um [Music] there is a lot of improvement we could do here we don't have to go in um and you know use appeal to this pil image object every time we can also check if this is an actual this is the extension the file extension of the image but those are um those are details i think these are these broad brush strokes should um suffice for an initial model there any questions who has questions oh yeah for sure you pushed this solution yes so uh a full-fledged solution is in stage 2.1 slash traded face detect so just a question slash comment is it perhaps it makes sense to have faces also as a property yeah i agree but then things would have been really boring um yes it is sort of the nature of the object here because it literally represents a file everything just ends up ends up depending on a file and yeah i actually agree but that has little pedagogical value so any questions any trouble finding the solution i have it up on the right hand side so it's right here okay so that model represents um [Music] uh an image file so another thing another model we need later on is um in our application is that of a collection of image files so think of a directory containing several image files so what i'm going to show you next is a model for an image folder so once again this is a traits model it represents um a directory containing several image files and so because this thing is a directory uh the first trait in here is just that um this is a directory trade which represents a directory trade type which represents the directories on disk um here i'm just setting its default value to the home directory um because this directory is expected to contain a list of image files the next trade is just that it's a list and each element of that list is an element of the image file model that we just came up with together and then the final trait in here is is a pandas data frame and the idea here is to collect the metadata from each image file and then just throw it together into a data frame um just so that we have a collection of all the just so that we can look at the metadata for each image in one place uh another thing i'd like to point out here is um simple validation that we are performing um on the directory that is specified when the when this object is instantiated so um what we are checking here is whether this is a valid uh directory or not does it exist on disk um and if it does not we raise a value error and sort of just drop our hands and say we cannot go any further if we do succeed we just end up setting calling a helper method to create this metadata dictionary metadata data frame using this helper method and once again we want our model uh to be reactive so whenever the directory is changed uh we want the list of images to correspond to that so here um we're using the observe decorator and listening to the directory trade and whenever directory is changed we just go in you know look inside that directory um and create or and populate our list of images um to be image file objects uh again this is somewhere this is a place where cache properties would have worked but this is just i want to try something different um finally so if in the images trades in the um yeah in the images trade if so that remember that is a list of image file objects so if either the images trade as a whole is changed or if any one of its elements is swapped out with a different image file we want the data frame to stay in sync with those those list of images we want that collection of metadata to to remain true to what's actually inside images so anytime that happens we go in and update metadata so this is just another model um to help you think about and reason about these things um about traits um a little better um and we'll be using these uh to build our applications in pretty soon i think next um so first of all were there any questions about everything we talked about um if not i what we'll do next is learn how to build [Music] graphical user interfaces with traits ui okay so now the fun begins actually siddhant will do the more fun things i'll probably set up the basics okay um so i'm gonna get a hard press for time so i'm gonna be doing this fast so maybe you can slow me down if i'm going too fast um okay we've said enough about traits ui so we'll just jump right in so the approach that traits ui takes is not for you to have to write the ui code directly but for you to just say what you want to be done and it will do the rest for you so it's called declarative approach to building user interfaces and this also facilitates this model view controller design pattern that we're going to be using and the idea here is the following so you notice what he did in his little image file and image folder example while it may not like seem like he's done much it's like just putting things together into a class it's actually in terms of the model of what is going on it makes it much easier right you can think of it as okay here's a image folder and here's an image file and the image file is like this object now that has this rich set of behavior you set the file path and it immediately updates a bunch of things and so now you can build these rich objects with a with code that sort of mimics a specific behavior that you're looking at that's hopefully easy to read so this is called a model so and the nice thing with this is so if you look at any of the code that was just discussed it's actually easy to write a test case for any of these because it's just an object right you can just create an image file put a sample image file create a test and then quickly write this and do that so it's a model based that basically manages data and state and has a bunch of traits that are convenient for a user so that's you so you focus on building those set of models and the idea is once you have that you already have a the basics of a nice application and now you can add a view to say okay for each of these objects i'd like to specify how it should be visualized and that's where traceu steps in i'm skipping the controller part it's not relevant at this point but there is a way to actually put in a controller as well but we're not going to cover it here um yeah so basically model is has state objects has strict traits objects the views are all traits ui view classes instances of that we're not going to talk about the controller at this point all right so let's start with our little silly examples again so we have traits.api a bunch of traits and we have a person with name age handedness and whether the person drinks or not okay so um we have a little ui and we have our favorite star trek personal uh person not my favorite but okay somebody from star trek and there we have a little ui so now this is the default user interface that traits ui gives us okay so when you do edit trades it inspects the object looks at all the metadata that's available in the trades and then decides for you a ui and makes it up for you and pops it up now obviously you may want to configure this you know you may want to change things so that's where we get to specifying a view so the first entry point into this is to learn how to specify a view so the first line here is simply an import which says from traits ui.api import two things item and view so an item basically represents a single trait that you want visualized on the ui okay so here we're saying view one is view and it has a bunch of positional arguments saying item item item item and each of these items has a rich syntax so we'll walk through this the simplest is just item name equal to h this name refers to a trait on the object for which this is a view okay so our object has the following traits it has name age handedness and drinks okay um and which is exactly all of these names so these correspond to trade names uh the second slightly more complicated example example here is item name there's a style keyword argument so usually the most common usage of this is style is custom is something that you're going to see often but i've just illustrated read only so what this is saying is i want to see this element but it's a read-only element it should not be something the user can change okay on the ui so that's a rule custom is used when you want a customized editor you don't want just the default you want some special purpose editor there are different kinds of editors there's a simple editor and there's a custom editor the default is i think the simple editor if you say custom it'll give you a slightly better editor you can do further customizations by specifying what's called an editor we'll come to that so um item the hybrid less is also straightforward no additional keyword arguments but this one's a cool one so when you want to say a person drinks or not well it's irrelevant if the person has an age that's less than 18. i don't know what's the drinking age lee oh i'm sorry we'll change this to 21. in india i think it's it's 18. okay 21. fine i'll come to india and drink so okay so um so now you notice uh this has a specific ui and then the ordering of this matters so the name is first page is second handedness then and then drinks comes at the bottom so you can control that and now notice that if i made this 20 oh yeah oh sorry i'm still in india okay 22. okay notice that the drinks pop up so this is the this is a really nice feature uh which is you can make things visible when disabled or enabled when these are things that you can do so there are a bunch of these nice attributes so you can look at the documentation for the item and you can learn more about this but i just want to quickly show you that so this is what we mean by a declarative ui i have not really said anything about what widget to use whether there's no qt code i'm just saying i want this view and i want these elements that's a thing so you're specifying behavior what i want and the system figures it out and makes something so there are many other attributes there's something called label so if you notice the previous example so this one here is the label and notice that it takes the name of the trait capitalizes it and puts it there now if i had a trait which has underscores it will put a space instead of showing you here so it does some default mangling of the trait name to make it look pretty on the interface but if you want a custom name you don't want the same name that the trait has you want to put something else you can say label and it will do that some cases you don't want to show a label you just want to show the editor for whatever reason uh we'll find out very soon so if you have very specialized editors like you want to you want to plot you have a plot and you want to show a plot you don't want to say plot or whatever that thing is right you want to see the actual plot so in those cases you can say show label false and it will not show you the label you can provide a tooltip or a help attribute and you can specify something there and when you hover over that widget it will show you help you can actually specify an explicit editor so let's say you have a matplotlib plotting widget which we're going to show you very soon it has a specific editor so you can create a trait as an instance of a matplotlib plot or patches or whatever it is or a figure or whatever and then say the editor is going to be an mpl matlab editor and it will use that for you [Music] a style usually is simple custom text or read-only so if you say text you'll just get a text box and then enabled when visible when defined then i've shown you a simple example you can also make these items resizable so that if you resize the window whether the widget should resize itself or not now that you have items you may want to now once you start putting these items together you may want to start grouping these items you want to say okay this is a group that manages say name of a person and you know personal details this may have some employment details so you can put together these items in groups and you can do a bunch of additional attributes to those you can configure them using columns so if you want to layout these things in two columns three columns you can use columns you can specify a label for a group so that when it places it on the ui it'll actually show you those labels you can lay it out in different ways horizontal vertical stuff like that you can show a border for a group and again you can define this whole group itself can be a visible when some trait is you know greater than 15 or something like that okay all right um so now in the previous example what i did was the following i had a view that was removed from the person class okay so i had a separate view and i just bound those two together by saying edit traits with that um i'm lazy and sometimes i prefer to just put everything in one place so one common pattern you'll see with traits ui code is people will write a class which has stick traits and they'll stick a traits view right here which just has the view defined so a default view comes along with the class some purists will not like this because they would like to purely separate out the model from the view but given that this is really a declarative way of saying this is what the view should look like and you can customize this anyways a lot of people will tend to do this so if you want to you can't do this so now if i just did this i would get the this ui that's defined here so when you define a view like this trait ui will not go and find and come up with its own it will use this particular view so if we define a view like you define any of the other tricks that is the view that will be used for that object okay so the view itself has a bunch of additional keyword arguments it can be docked which means if you have like multiple multiple views you can actually drag them drop them and place them in certain contexts you can so supposing you have multiple multiple views you have multiple objects you can place them together in a tabbed view for example using this doc tabbed option you can specify a height you can give it an icon you can specify whether the view is allowed to resize or not allowed you can give it a title and the title will basically give you a window title for that and you can also do this buttons thing which is um you can specify whether you want an ok button or a cancel button or a revert button there are some standard set of buttons that are available i'll just show you a quick demo demonstration of that you can also do key bindings and this is something we're not going to talk about but you can read the documentation for more so here's a simple example that demonstrates the ok button and the cancel button so everything else is the same this is not a person that likes queso it's not about whether they drink or not um so oh okay this is fine and now the buttons is basically going to use the ok button and the cancel button from tracy so when you do that so let's run this and in contrast let me run the previous one notice that this has no buttons there's no okay no cancel whereas if we run this you will have okay and a cancel and it actually does it's not just a silly button sitting there if you change some traits and then you hit cancel it will actually undo those changes if you say okay it'll retain those changes so there's something that comes for free when you just turn on those buttons um so i'm not going to demonstrate that but you can try it out yourself but yeah so this basically shows you that you can now create models and now just this declarative way you can put together a user a very simple user interface um okay so i think you want to go now right i think maybe you can just take over from here right okay so now saddam is going to talk about more fancy editors and that's where the real fun begins so he's going to do plots he's going to do data frame editors and stuff like that sorry but any questions so is the general approach clear basically have a model and then it's just declarative things that describe how we want to see yes oh yes yes so you have to create your own editors uh you can't do it he's not going to talk about that he's going to talk about fancier editors that are available he's not going to talk about how to create one of your own no because that now will go so that's where the qpe or the wx 510 comes in so the instant you want to go there you need to basically expose lower level uh stuff um so we are going to talk about you can find out that the example that he has now matlock editor for example if you read the code you'll kind of see how we do okay so uh the plan is to talk about uh some interesting editors so editor is just something no problem that lets you interact with uh one of these um crates here so it'll show you a widget or something that the user can play with um to interact with that trade so uh one powerful edit editor is an instance editor which as the name suggests just lets you edit uh the instance um assigned to a particular trait so here again we have a person with the usual name age and handedness but they have a bff who is another person and note how we defined um how we how we expressed that bff is another person we haven't defined this class yet so we can't just go in and you know type person without code so this is an indirect way of saying we want the same thing that we are defining here okay so because this is an instance trade it this says that bff is an instance of a person uh when you um create an item for it in this view oops do not even do that um this view will uh use the um instance editor by default um and let me show you what that does okay so we'll create some a lot of the rings characters and so this the the real power of this um editor is that so obviously we have um the main person uh which this object is describing and that is um that is sam yes uh and sam's bff is a frodo and you'll see that a view for frodo is nested inside the view for sam so the real power of this editor is that it lets you create these um nested views without with just the tiniest amount of code any questions about this perfect okay so um i think um we don't really need this so maybe yeah yeah okay so it is also possible um to configure the view used by the the instance editor so here we are defining explicitly what the the view for the bff should look like and you know we added a label here um what else the idea here is you don't want to see the bff's bff and then that bff so they can sit and embed all of those so you just want to say okay i don't want to see that you can customize it in this case so the syntax here is just yeah so here you explicitly create an instance editor and and pass it the view that you want to use for that nested instance so we'll just go through that example one more time yeah and okay um so we as scientists use uh do a lot of data science at times um and the well the most commonly used objects used to con hold data or a data container is a data frame so traits ui also has an editor that lets you interact with pandas data frames it's very simply called data frame editor although it needs to be imported not from the api but from a special location in a special module and that has to do with pandas being an optional dependency we don't want rates ui to depend on pandas so it's in a special place um anyways um so here we have a traits model um which has once again a data trade which is an instance of a data frame and this is just a default um value for that data frame [Music] oh so what i wanted to do here was we'll come to model views in just a second okay so here um we're creating a view for uh this data frame uh instance and we're saying hey i want to use the data frame editor to be able to to be able to interact with this data frame and um let's just look at what it let's just try it without editable first and see what happens okay so we have let me make it resizable okay yeah so right now it just it's just showing you the data frame as is um so you can view it you can click around but you can't do much other than that so let's try this with the editable equals true keyword argument and see what happens so now i should be able to go in and change these that is a bug um okay i was able to change that um unfortunately this data this editor is pretty new and i as i found out pretty late last night a lot of the keyword arguments aren't synced up correctly so it's a little buggy but it gets the job done okay finally probably the most uh a really another really powerful editor is um the mpl figure editor and this allows you to interact with matplotlib dot figure dot figure instances um so this isn't a part of trades ui yet uh we've sort of bundled this with the eds tutorial um the repository uh but the plan is for to contribute this to trades ui pretty soon um okay yeah so to illustrate the macbook live figure editor we have a very simple class called image viewer actually this will look pretty similar to the image file model that we just developed and that's on purpose so the data for this the first trade in this class is um called data and it's it's it holds a numpy array uh next we have um a trade called figure which is an instance of a matplotlib figure so this is the thing that we want to show our users and for them to be able to interact with it and the view for this object is simply um is for figure and the editor that we'll use um to interact with it is the mpl figure editor um and once again we don't we just want to see we just want to see the figure we don't want to see the its label um so we are asking trades ui to not show us the label for that item um the data the numpy array by default um contains a picture of chelsea the cat and this is a default value for the figure trade so we are instantiating a figure object we are creating some axes in it calling axes or i am show the usual uh matplotlib um behavior that you should be um i guess it's really which is really popular and then it's the usual um edit traits uh so there is chelsea the cat um yeah and um this editor actually uh allows you to interact with this uh with the figure object just as if you had um called plt.show or in fact it's exactly the same as the behavior that we had with scripts with that initial script without rates ui so we can zoom it's as if we were just interacting with playing matplotlib okay so as prabhu alluded to it's usually a good idea to keep your models and views separate um and that has uh so we'll keep on harping about this because this is important it aids testing um you know if we don't have a ui in there we don't have to set up and you don't have to do complicated things just to test just to get that instance up and running um so the complication arises from event loops um and testing and event loops that it's it's not a road we want to go down and it generally promotes better reuse of your code so if uh you just have a model without a ui you can you know interact with that model in different ways maybe you can wrap it in a web api maybe you can [Music] have two different uis for the same piece of model which you show to different kinds of users maybe an advanced scientist and then someone who just has a working knowledge of science so um with that in mind so that suggests we you know we have this imageviewer class with some data in it and because we want to keep our science logic separate from ui concerns that suggests maybe we should just rip out this um this visualization or gui related code from outside of this class um but where would that go is sort of the next natural question um so although we want to split our models and views um [Music] we also want sort of some interactivity between the two we want our view to be able to respond to changes to our model and our model to be able to sort of or be able to tell what's going on on in our views if not in the model and an object that lets you do that in trades ui is a model view and so that's let me show you what that looks like so here um we have a simple image class which does not have any um any gui related code so i've ripped everything out um instead i've created this new image view class this is a wrapper around our model hence the first trait in here is is says that our model is an instance of image and then it has all the gui related concerns in here so this sort of allows us to do um kill two birds with one stone um it allowed us to rip out all the gui code from outside of our model so that we can reuse it and then at the same time we can have this reactivity between our models and our views so as an example here we are saying hey you know listen to changes to uh the model trade um or the the data attribute of model and if either one of these changes go back in and rebuild my figure because that makes sense if the if the data has changed inside the model then we need to um update our figure so something to notice about model view classes is that um the model trade name is special in that it always points to the thing that it's a view for so always make sure um this is you follow that convention because this model trade is hooked up to other things inside trades ui um so this is also once again a very simple example model views do a lot of other stuff [Music] they listen to for example changes to the first time a window is created whether the user is clicking on the ok or cancel button but those are sort of out of scope for this tutorial but in any non-trivial ui you might have to hook into that functionality and model views on an entry point to do that and the way you we would use model views are pretty simple we first instantiate our image model then we go back in and say create a view for me as an instance of the image view class and the model in here is is this object we just created and now i'm going to edit traits on the image view objects so that that is chelsea the cat again and remember we had this observer hooked in which listens to changes to the data trade of the model so if i go in and change the data trade of the image to be the sample astronaut image hopefully yeah so that all of that happened automatically and was taken care of by this functionality right here any questions concerns even though it seems like this is a very complicated way of making a matter of difference it is a much more powerful way of showing in my budget club because you can show any number of widgets around it then you have now you have a already building a scientific application um okay so let me walk you through so we came up with the image file and image folder uh models earlier so let me walk you through what um model views for the for those two objects would look like and if you want to follow along this is in stage 3.1 this should be the only file in there um yes so there are two files in there three files in there so the first one is traded face detect so we have the close everything else so we had the image file model whose trades are file path metadata data which contains the rgb array and then a list of faces the model view for that looks something like this so remember the model trade is special in that it points to what this is a view for so this is a view this is a model view for an image file so that is specified here and then we have some gui related code here so we are saying we will present to the user so we will use the matplotlib figure and present it to the user so the figure trade is just an instance of matplotlib figure and in the view the first thing we are presenting is the file path of the image file and then the second thing in here is uh the figure and for that we're using the mpl figure editor just like we uh talked about and then there are some buttons in here um and then the title of the our application is picasa here we are hooking up an observer which listens to changes to model or file path and this just builds this figure instance out so what it'll do is if file path is bad in some way it'll just return otherwise it'll create a new instance figure add axes show the rgb array and then so this is interesting here it's calling detect faces on the image file which compiles the list of faces inside the image and then it's adding rectangles corresponding to each of those faces to the um to the to the axis yeah so hence the figure and then it updates self.figure to be figure so if we want to see this in action in terms of behavior this will look feel very similar to the original script but we've done something very powerful we've pulled out models um we've teased apart the models and the views from that script that we had and that'll allow us to extend it even further and embed them sort of craft a full-fledged application around them oh okay so what we're going to do here is select um so this is model.filepath it's it was empty initially so now we have an opportunity to set it um so if you want to let's compare this to the view that we defined so on the top we have model.filepath and this is something that we were actually able to interact with and modify and then we have the rest of the figure with the faces detected and i'm sure we can do this for we can do this any number of times [Music] i doubt if this has oh okay um i think this is a screenshot yeah um that is all i wanted to say for this we define another um class so maybe it'll help to open them up side by side this one is pretty simple so the image folder remember was a collection of images in a directory and the first trait was the directory trade um so in the image folder view we start off again by saying hey this is the view for an image folder so we set the model trade to be image folder the first ui element in here is the directory on the image folder uh but here we're using a read-only style um so we won't be able to interact with it and then the second item in here is model.data which recall was a data frame containing all the containing metadata for all the images in that directory so one interesting thing we've done here is uh we're saying hey only display this data frame if the size of the data frame is non-zero um if size of the data frame is exactly zero then show this helpful error message saying no images have been found um no need to concern i think so what spring does is it just it's just stretchy space it it'll just fill out it's just a great create some padding between the top and the bottom okay so it is quite small but this is the view for um this directory um and it's it's showing us a data frame representation of the metadata for all the images in that directory and then had i changed this to um something that does not have any images oh it does have an image anyways i think we're running behind so i'm going to skip demonstrating the no images found um you'll see that soon enough when this is in an application hopefully okay any questions we're going to start slowly what do you think so um um yes all right let's get started um so we've we've gone so far uh in the in in the first uh in these two first sections we've gone so far to stage three of this application and uh and we have we have eight stages so thank god it's an eight-hour tutorial i guess otherwise would be tough so you know so serious in in all seriousness uh uh we're not gonna be able to to get all the way to the end but the the end is is uh is provided to you guys in the in the github repository and and not only that but we're giving you many stages along the way so we're going to go a little bit further i really want to introduce pi face uh in particular because um uh so prabhu mentioned the the the ets documentation for traits is is really good for traits ui is really good but for pi face it's not as good and so i uh it's i feel like fiveface is one of these packages that that's out there that uh really actually i should take this off that really could benefit from being understood a little bit better being presented a little bit and what i'm showing you guys here is uh is a stage six of the application release seven and eight is uh are our details as uh six is already really fully fully featured uh and and even before that and so if you guys if you guys look at stage six you see a few things that we haven't introduced yet we haven't taught you guys how to do a few things but even though it doesn't look all that impressive um it is uh it is a it is a decent application uh already you you have a you have a browsing pane on the side you can double click on an image uh you can click on a button and it's going to detect the faces and then it's going to draw those faces after the fact what else do i have in here that we have introduced i i have little messages at the bottom this uh this bottom piece here it's called a status bar right so it adds a little bit to it it has some toolbars at the top we just put a couple just to illustrate the concept right we're not trying to build something that's actually useful to anybody we're just trying to teach you guys and then uh so this is a mac right but so the menu is up there but you you can also see that there are some menu items right so so this last phase of the tutorial we want to introduce and we're just going to dip our toes into it we want to introduce pie face and and and kind of the the the framework that it provides to compose all these views that you guys are are now feeling hopefully feeling capable of building right we know it's a lot of information but you are capable of building views you can put a define a view you can put some items and and so now pyface is just providing this framework to embed those views and and make it make the application a little more uh scalable really because a traits ui view um uh you know you can add more and more things but you know it's not going to scale great yeah we've introduced the instance editor so you could you could do some composition right you could define multiple views and put them together in a traits ui view but that would only take you so far and and based on my experience and prabhu's experience in sudan's experience if you guys want to build something of a little bit larger scale uh be getting comfortable with pi face would be super valuable okay so that's why we're going to dip our toes into uh for the last few uh for the remain remaining 25 minutes um okay so uh so apologies um we at the at the end of the preparation for this tutorial we were working in parallel so we we have a deck that is in in markdown that was in the slides folder uh and then the pi face deck i was building it in google slides uh and so you have it you have these slides as well as a pdf inside the docs folder of the repository so you have all of these slides and actually many more um and we're going to continue to work on on merging things so that it's a little bit more unified but it's not going to prevent us from having a good conversation all right so i want to uh i want to come back to a point that we made at the very beginning of this tutorial about what is really my favorite thing about ets the in thought tool suite the reason i like it so much is because you can learn traits and build models that react to themselves and you already infinitely more powerful than than before learning traits and then you can learn traits ui a little bit of traits ui and all of a sudden you can pop up a gui in three lines of code which is gonna uh you know blow a lot of people's mind that are not used to knowing how to do this that simply okay and you can continue this this is a long journey uh that allows you to uh to to build applications that are smarter and smarter and more and more complete and worse we're going to show you guys in this in this last piece the next layer of the onion so if you wanted to build a very small application or a fairly small application i'll let you guys define what that means then traits ui is good enough you can you can put a little widgets you can put some sliders you can put a matplotlib plot or anything else a data frame now you guys know how to display a data frame like there are a million you know an infinite number of ways to build data frames load them from something so you already have tools to build an application of fairly simple scope if you were to build something a little bit larger then the the recommendation is to is to use pie face we're diving into that and then if you wanted to build something even larger something that had lots of different components that should be defined potentially different code bases and so on then the largest piece of the onion is called envisage uh it allows you to define plugins and and uh and continue to compose and and again what i want to insist on on is the fact that you can start with just traits and then one day you have time to build a traits ui panel for it and then you are you are the king of the hill and then one day you don't have to throw anything away for that right and then the pie face layer is the same you can build traits ui panels and then you put them you don't have to throw them away to put them into a pie phase framework and all of a sudden you have an application that's even larger and more scalable and then um if you if you did that you define those tasks we're going to do that together and then and then one day your application continues to grow then you're like okay i'm just going to reuse all of these tasks and things i'm just going to put them in envisage and now i'm i have a larger and even more scalable application so that's what i some frameworks you have this very steep starting point or you have this very slow starting point but then at some point you have to throw everything away and do it again right and what i love about ets is that it gives you the ability not to have to face that you start and you stop wherever you need okay so uh so these are the three frameworks sorry i just spoke but this is summarizing this slide that you we ets is going to provide to you these three frameworks you've you've learned about traits ui you've learned about pi face and then you can know or you will learn about pi face a little bit and then you can know that there is yet another framework if you need it to continue to grow all right so what is pi face um pi faces is multiple things um it it it is this layer this this traded layer between between traits ui and and qt right so it contains uh some of the some of the guts uh that allows traits ui to work uh some of the you know secret sauce it's not it's open source and so it's not anything is uh nothing is secret there but um so it is this it is this layer but what i want to focus on so it's a few things it's even a few things that are now deprecated uh like the workbench it provides native dialogues and so that's another thing you could go and explore i'm going to skip that slide and i'm going to focus on this item number two here which is this task framework so allow me to focus on that so here i have a slide about the little pi face dialogues they look nice they look nicer than some of the traits ui dialogues but because they directly use the operating systems dialogue instead of instead of painting things with with qt so they look nice you you should you may want to take a look uh look into those but what i want to present is the task application framework uh because now you're gonna recognize uh some of these components that that that we wanted to get to with the final application right so in the final application i have this i have these trees uh i have this tree on the side uh but one one thing i didn't show one thing i like about this uh this uh this application and you saw the little splash screen too that's uh that's in stage six uh but is that these are these are paints that can move around so it's it's not only a gui but it's a customizable gui uh so so that's one of the things uh that that this task framework is providing for us it's defining this central piece and then you can define any number of panels around you can even close them and bring them back you can undock them so so i i like this when i when i need to build a product uh i like to provide people with with uh with a framework where they can customize their views because the scientists i work with they have complicated lives they have lots of information that they want to see and maybe lots of information that they want to be able to hide okay so i uh that's one that's the that's one thing that this this task application framework is offering so um so so here it is so so this framework is providing four different levels of uh um of responsibilities okay so i'm going to start i guess five with the model so i'm gonna i'm gonna start with the two layers that you guys are already familiar with right uh the models uh and we won't say enough how important it was the beginning of this tutorial where you guys learn to define your models and make them respond responsive and make them uh never be out of sync so the model is holding the truth right so i'm at the um yeah i'm at the innermost circle uh in this in this picture and then around it we have these model views now you guys have uh seen an example of building a view for your model uh so you you give it the model and then you you start drawing it up and you you if you build an application uh a realistic application you'll have multiple models you'll have a view for each of them and then these are for now disjointed views so around that the task application the task framework uh is providing uh one concept to embed those views together uh if you need to and it's called a pain and what i just dragged around that was a pain okay so a pain can can can be dockable or undocked uh can be hidden or brought back and then it can contain multiple views if need be okay so that's the pain the here for example i have two side pains they are called dog pains and then i have the central pain here the central area so i have three panes in here and then um there is uh the level of responsibility uh above the pane in this framework is called a task and a task is supporting this entire window so a window is made of these three panes in this example this is an actual product i built at kbi by the way and um and so this this task is responsible for supporting the entire window and then an application might launch multiple windows so i have all these levels levels of responsibilities you can you can keep them very simple i'm gonna show you guys in a hello world in that in that framework but but you have the ability that again i'm going to come back to that word scalable right you if you are building something a little bit more complex than just a few sliders it will be powerful it will be useful for you to have these different layers that have different responsibilities so let me repeat that the application is responsible for just launching the tool and generating windows a task is supporting one window and defining the panes for it and then a pane is holding the you the the model views and just displaying them and allowing you to dock them and and so on any questions so far you're good all right so let's do a hello world um we're going to start by the innermost level that's the one you you know the best okay so i'm defining a hello world uh as uh as a hat traits and this is my model so my model is gonna be ridiculously simple and it's just going to hold a string that says hello world okay and then what we've seen before obviously nobody is in would be in their own mind building all this for just a hello world right so you have to imagine your own problems and your own models in in all of these right so if it's complicated enough you want to separate the view from it so i created another class and it's a model view and so the model is an instance of my of my hello world and then i'm defining a view for it in this model view class i'm gonna i'm gonna take a a short second here to mention that the view so far we've been defining views uh directly inside the inside the model view class uh but another way that allows potentially for more complexity or more fanciness is to define the view in a in a dedicated method called traits view all right so i'm doing exactly the same as we were doing before i'm just returning it from a method instead of just defining it directly on the class these views can get complicated you could imagine situations where depending on a situation you want to build one view or another okay so if you are inside a method you could start having some if statements you could start having more complex things so it's a good thing for you to know and then at the application layer at the application level so there are there are two levels in between the pain and the task i'll come back to that in a second but at the application level the the simplest thing uh that the application needs to do is to be created and and call and the the run method on it is going to create the this gui event loop the the whole graphical user interface system to support your views right so that's the application that's going to do that so i'm creating an application here called hello world app and this is going to be a subclass of pi faces uh application okay so hello world app and the only thing the minimum thing i need to do is i need to tell the application what windows it is capable of building or what task it's capable of building okay so um you you guys have learned in the trade section that any object any traits class you can initialize any of its attribute with an underscore the name of the trait underscore defaults so that's what i'm doing here by default if you create an application directly a pi face application there are no task factories so it doesn't know how to make any tasks of course it's just the base class right so your you know what window you want and so you're going to say i in this case for example the simplest possible thing is to say i just want a hello world task and now we're going to define what it does okay so i'm saying underscore task underscore factories underscore defaults and for task factories is an attribute that's defined in the parent class and i'm overriding how to initialize it by giving it my own task all right so let's build that task so to build a task i'm going to inherit from i'm going to make a hello world task and i'm going to inherit from the the capital t task from pi face and i'm going to say you're going to be this the world's simplest task you're only going to have a central pain okay a task needs to have pains uh at least one and so you have to say uh you have to define what that central pain is if you only have one one pain it has to be the central the central pain and then you're just going to return one type of pain so i'm returning that pain and obviously now i need to define what that is okay sorry the code is a little bit jumbled here all right so what is my pain going to do it's just going to show my model view and now we've linked the two sides of the of the code okay i had a model view that was just displaying the model's message hello or hello world and my pain just like a model view my pain is going to define its own view so a pain's view might be again like i said earlier it might be composed of multiple model views in this case again i'm trying to do the simplest thing possible so my pane only has one element and it's showing that model view questions when you define the hello world app in the previous slide you're returning a list yes yeah you can imagine an application that's capable of opening so in this in this example i have multiple windows and they all look the same because i only have one type of window but you could imagine an application that's depending on the situation bring you a dashboard for one thing or a dashboard for something else so it's capable of knowing how to build how to factor make be a factory for multiple types of windows great question any other question there is there is one question uh there is one one question you guys might ask here is like this little parenthesis inside the instant straight um is just crea it's just forcing this model if i don't specify a model it will just make a default model that's what the open close parenthesis does it's a it's a lazy way of saying just return a just a hello world okay all right and so once i've built an application with all of these different layers to run your application you just make an app and you call run on it and then everything else is taken care of okay so let's look at this live you guys all have that hello world example in uh in your in your code but i'm just gonna run it in here so let me close this one first so so this hello world example we're giving it to you as uh as stage four stage stage 4.0 actually and um because we're now building an application we've we have uh splattered uh these different uh classes into their own files so that they can grow so actually before i before i run it uh let me let me say one thing about refactoring code so so far we've been trying to keep everything simple by just putting everything in one file but you can imagine that now if we have to create an application object a task object a pane object some models we already have a couple of models right we have an image file and we have an image folder there's a couple models already and then the views for each of them so we're starting to have quite a few players and so to not get lost with these players the best course of action is to is to put each of these classes into their own modules into their own files so that they can just live and grow happily the other thing that's that's recommended when you get to that point of of scalability um is uh is to even put things into sub packages right we said we should separate models from views so we can do that by even making subfolders so that's what we're uh we're describing here and we're recommending again once you get to that point is let's let's make a proper package at this point so we're going to have an application called picasa so let's make a package called picasso so that's the top level the package name and then within that package we're going to have so some different types of responsibilities so we're breaking it up for you already that way and we're providing it to you broken down so that it can provide a a starting point for you guys in the future to build your own tools okay it's not to make you suffer it's it's to help so okay so i have an application the application should just be responsible for uh the the app sub module subfolder should just be responsible for creating an entry point into our application and uh defining an an application object so in app we're gonna we're gonna define the app and we're gonna define the the launcher for that app do you guys remember what code should be in the launcher it's two two lines of code right this is the this is the launching code so so our main is going to be two lines of code and then the application itself is four lines of code okay and that's gonna grow all right then we have models we've talked about models enough we already had a couple earlier we're going to cut we're going to bring them back at stage 4.1 uh well we would if we had the time so uh that's okay uh and then the ui piece sorry time flies the ui piece is where all of our model views are gonna live io we don't have any i o yet but typically in an application if you were to save your your work or load your work you would have some functions to learn how to do that you would typically put it in i o and then you might have some tools some computations that i typically like to put in tools utilities you can offload them outside anyway so so here is an example so i have that pi casa application inside the picasa application i have the app so i said the app better be pretty darn simple and so that's what it is in the in the slide i was a little bit uh i was a little more tight so i didn't take the time to to be any uh any do anything fancy but if i uh if i if i was to define a name for example it would automatically give the that name to the to the windows that i create but anyway i'm just creating some task factories and then and then the main is going to be that simple i promised it would be two lines of code a little tiny bit more we're importing that application and we're calling run on it okay and then if i go into oops sorry if i go into ui we're gonna have a hello world view same as in the slide etc okay so to be able to now i have multiple files i want to be able to import them so i need uh i need a setup.pi file i uh i don't know if you guys oops i need to go back up um hopefully you are familiar with setup.pi files actually this one should could have been simplified even but this is regular python code and again it can be it can be very simple it could be just a couple lines of code or not not a couple but very few and uh you know this is just hello hello world and then whatever it might it might have a version or it might not have a version and uh we're just defining this is this is optional as well sorry we'll i'll clean it up i it could it should have been that simple all right so the setup.pi is there to allow me to define a package so once i have my python environment activated i can run setup.pi develop and now my picasa package can be imported from anywhere and that allows me to run my my main which is going to import my app my app is going to import my task my task is going to import my pane and so on and so forth all right so if i now run so which file should i run to be able to launch the tool and we'll finish on that can you help me main where is it it was in the app folder very good so that's starting the event loop uh and uh launching the tool this is not hello world uh i i forgot to call uh anyway we're out of time i want to be respectful of you of your time so anyway something is something is doing so something is being funny here um but anyway we're we're gonna we're gonna end on this um there is uh there is a lot more slides to explore the hello world um the hello world example can be morphed into the the the the first picasa application and that's stage 4.1 and then stage 5 and stage 6 are going to add are going to plug in all of the models and the model views that we created in the first phases and then so that's on stage five in stage five we also add the buttons to launch the detection of the faces but all of that has been coded in those models okay so now it's just a matter of of of connecting those things and and and displaying it within the application and stage six is introducing how to create those menus and those toolbars all right any final question thank you guys uh for surviving the whole four hours any questions we're good all right have a good evening oh there is a there is a get-together at the end thought headquarters on cesar chavez in one hour
2,GPU development with Python 101,https://www.youtube.com/watch?v=G6eSPTDM8JU,so hi everybody uh my name is jacob tomlinson i'm going to be teaching you all about gpus in python which should be should be fun so before i kind of get into kind of what's going on let's talk about environments can you put your hand up if you have an nvidia gpu in the machine in front of you that's pretty much what i was expecting so i've set up like a cloud environment so that you can all go and grab one and we can run through all of this together so the way that i've done this is we're going to use binder but this is like a custom deployment of binder that i set up on our gcp just to give you all a gpu participant so the way you're going to get there is if you head to this git repo which is in the slack channel and run on the board behind me and if you have a look at the the website link over in the corner this is something like crazy random ip address on the internet this is a deployment that i've just set up so if you click that it should take you into something that looks and feels and smells like binder but the only difference is you'll get a gpu as part of your session on the other side so go ahead and click that you'll get the spinny wheels this may or may not take a very long time the environment that it's going to pull is very large that's mainly just because i didn't want to trim the environment down so you get a big fat environment if it crashes if it says it failed to start the server or something i i may have got the timeouts wrong i apologize binder is slightly challenging to operate um so just refresh your page if you get stuck if you can't get to a point where you're into jupiter lab put your hand up i've got some awesome helpers down the front here and that's it you're kind of i've lumped you in as well um but put your hand up they'll come around and talk you through this but eventually you should get to something that looks like this this is the first notebook that we're going to talk through in the tutorial we're going to go through everything in jupyter lab this is all just python um i'm running this on like a workstation machine at my house just just in case i like messed up the binder i can still keep going i can also show you like a couple of differences um i apologize i've been a little bit cheap i only gave you one gpu each but there's a couple of things i want to show that use multi gpus so my machine has got two in i can kind of demo that up here just so you can kind of see what the rough difference is um so what i want you to try and do is get to a point where you're in jupiter lab like this you can click the plus and open up a terminal and in that terminal i want you to run nvidia smile and this will print out your gpu configuration on this machine you can see on mine that says there's two quadratic x8000s in there yours should say a p100 i think is the option that i chose on on gcp if you can get to this point where you can run nvidia smi and it tells you you've got a p100 that is enough for things to work and we can go all the way through the tutorial and that's wonderful and i'm really happy so um what i've got down the front here i might have to distribute these is we're going to do the post-it note on the laptop thing so i know when you've got to different stages so if you can once you get to this point that you've got nvidia smi and you're happy stick a post-it note on the back of your laptop just to let me know that you've got got a happy environment and we can move forward please do put your hand up if you're stuck if you need any help we'll get you we'll get you set up here because there's no point doing a gpu tutorial without any gpus uh maybe 10 minutes yeah it's not fast this is because i just launched a whole load of vms on gcp they don't have the docker image yet whoever hits that machine first will cause the image to be pulled everybody else will get put in a queue that's why i'm going to talk to you for like half an hour about stuff and hopefully in that time binder will catch up and we'll get there by the time we're actually going to need it to do some things yeah another question uh no as long as there's a gpu you're all good this is it sure i mean this environment is like a docker image that has everything like pre-configured you should be all good you could um but oh and are you going for the local setup if you could do the local setup charles would be very excited to help you with the with the local setup um i assume there would be like one percent of folks that are going to want to do a local setup it's great if you have a gpu the only thing i would say is i'd be interested to know how much ram your gpu has in a laptop it's probably really small i might try and allocate more ram than you have in this tutorial the p100 you'll get will have 16 gig of ram just on the gpu my my rtx 8000's have got 40 gig of ram each four gigs yeah you'll probably be fine you might just need to tweak some numbers if i try and make a big coupon array that's like 12 gig or something you're just getting out of that mirror but you'll be able to see what i'm doing doing anyway you should be all good you'll have yeah charles would also be really excited to talk to you about wsr um yeah i mean you can do that but also like the wsl stuff um rapids works all of the das gpu stuff python gpu stuff works with wsl it's like reasonably recent and we're kind of keen to like see it in the wild and get people going with it it shouldn't be too difficult like the windows and video drivers should get you like set up and fine but there are there is in the in the zero zero notebook there are instructions on setting up manually if you want to um and there is a wsl blog post on there have a look at that see how well you do with that we'll see how you get on so while you're all doing binder and while folks are going around helping you out let me kind of set the scene introduce the session give you a bit of an overview of what we're going to be doing so i've been at nvidia since 2019 i am a task maintainer if you came to the das tutorial yesterday uh we did a bunch of desk stuff my main like focus is just making the gpu experience with das really nice talking about like multi-gpu multi-node good question we'll come onto das like way later but it's like multi-processing on steroids um that's my main focus is distributed computing it is and just trying to make the nvidia experience pleasant right that's kind of why i've been hired so i don't get my hands messy with like actual gpu code stuff and i am sad about that because gpu code stuff is cool and i'm in a really nice position where i have awesome colleagues that i can like inhale knowledge from and so i put this tutorial together as kind of a way to force myself to learn some cool stuff by coming to teach it to you i can also then kind of be like an insider novice to you all because i've been asking the questions that you're probably going to be asking through this tutorial there is no such thing as a stupid question this is a weird subject it's a super interesting one but i'm kind of hoping that i can be this little bridge because it's very easy to speak to nvidia folks who are like super experts on gpu stuff and they'll start talking to you about very in-depth c plus code um but the like on-ramp stuff is kind of there's there's a lot of space in on-ramping so this is the intention of this tutorial so please do ask questions please interrupt me please put your hands up um please ask questions in the slack channel uh my nice helpers will be kind of keeping an eye on the slack channel maybe bubbling up questions to me if they're you know relevant to the whole group so the main goal of this tutorial is i want all of you to write some python code that is just python code that executes on a gpu rather than a cpu if we can get to that point which should be the first notebook but if we can get to that point i'll be really happy that i've kind of taken you on that step because that was kind of that was the point that i really was interested in when i when i joined was like the gpu is this mysterious thing right it's this cool piece of hardware that i've historically bought to play video games that i know is like there's cool machine learning stuff going on and like whatever else people are doing with gpus but like i don't know how to touch that i know if i install a game and run that game it will use the gpu i know if i use the gpu version of tensorflow or pytorch instead of the cpu version it will touch the gpu but it's kind of just like magic stuff that somebody else is like abstracted away from me and i'm kind of interested in like well how do i touch that gpu how do i interact with this thing and that's kind of the goal of this once we've gone through that of like writing some low-level code talking about gpus talking about how they work and what they are and what they do we're then going to start moving up that abstraction layer to have a look at some libraries which make your life easier fit into hopefully your workflows so who here has uh who here has written any number a numpy code before cool good and who here has written any pandas code before cool okay so all of you will see stuff that you recognize through this talk we're going to talk about gpu accelerated versions of both of those uh what about hands for psychic learn do you do any like machine learning stuff that's like it learn we will also touch on machine learning stuff on the gpu with the scikit-learn style interface as well so as we kind of bump away up that and then like the very last thing i'm going to touch on is like how do i get more gpus how do i make them you know making things faster the gpu isn't is great but how do i go bigger and massive and whatever and we'll touch on that a little bit it's a cool topic it's kind of where i'm actively working at the moment we're running like scale tests on htc's where we're grabbing like a thousand gpus and getting them to do a single kind of uh data frame operation i think we've got like a merge benchmark for data frames that we're running on like thousand gpu clusters to see where it breaks and falls over and stuff so that's kind of where we're at in terms of scale but let's like let's dig right down to like what is this this weird bit of hardware i've tried to like these four hour sessions are long this is your fourth one probably at this point i'm probably not going to expand until the whole four hours if we can give you some time back at the end i'm sure you won't be too sad but i've i've chunked this up into eight chapters each one is going to take maybe 30 minutes um some of them have got exercises some of them is just me kind of talking lecture style some of them might be shorter than 30 minutes right and if we get some time back that's that's great i'm also happy to hang around to the end of the session if folks want to like dive off in any particular direction and we can do off-the-cuff stuff as well so i haven't seen a huge number of post-it notes yet which makes me worried about my binder environment um it's still spinning fine that's good i've still got like a whole load of talking to do but spinning is good that's probably one of the timeouts hitting yeah that will happen i will i can cut things and see how it's doing very quickly okay i have nodes they're doing stuff that's fine we'll just be patient this is uh uh i've run this tutorial this is the third time or maybe the fourth time i've run this tutorial now this has worked every time so far um let's start where's that tremor again okay so while all your binders are spinning and the floor is shaking we'll start working through this this first notebook hmm yeah it's the heat coming out of the gpus yeah so let's talk about gpus and what they actually are in like from like a hardware perspective so when you write code your cpu executes that code 99 times out behind it right when you run these things and it's a device that you don't really give much thought to right you just you're using your computer and your cpu is like the core of that so when you write code stuff happens sometimes you think about threads sometimes you think about processes sometimes you think about these other things but they they are like operating system level things that are mapping onto the processor but you know these are kind of abstract concepts even though we might think of them as quite deep down complex things to think about they are kind of slightly more attractive so we're going to like dig a little bit down into into gpu land and just think about what that is so the way i like to describe this is a gpu is basically a co-processor anybody remembers co-processors i don't but a gpu is basically a co-processor right it's another thing in your machine that you can give algorithms to computations to code two that it's just it's just much better at doing certain things right it's very good at rendering pixels onto a screen right it's very good at doing ray tracing it's very good at doing it's very customized to specific things and it's is this like completely separate thing and so we need to know how to interact with it but we also need to know when is it going to be useful so i grabbed this picture from nvidia documentation on the left hand side you've got the cpu right this is a four core cpu standard kind of thing called might be like low these days i've got 12 in my laptop in front of me i've got like 96 in my fancy workstation or servers and things right like that is a kind it's the kind of architecture where you think about running your code as like a single threaded thing a lot of the time right we're talking about python often we're thinking about single threaded things maybe you're thinking about async io and doing some concurrency maybe you might start reaching into multi-processing and doing stuff with multiple processes but it's at its heart you've got these cpu cores that the operating system is just giving code to to run through um and threads are kind of being allocated each of those cores can do a different thing it can do it very well they've got certain levels of caches and stuff to kind of store things as they're going in and out but the gpu this big thing on the right is very different in that it's got many many more cores right like like modern high-end gpu has got thousands of cuda cores inside it but they're not as separate as they are in a cpu right they don't have the same kind of separated controls they don't have the same kind of separated cache layers they do have some shared caches and things but you basically have to treat the entire device for the most part this is kind of getting less true now but you have to treat the entire device as like one thing that you're going to get to do one task for you your cpu is really good at doing lots of different things at the same time right but at the moment it's easier to think about gpu is like this gpu is going to do one thing for me now and then it's gonna you know probably gonna do a series of things for me but the things that it can do are extremely fast because it can do them in parallel if you kind of set things up in a way where you give it like a big chunk of stuff and say please do this big chunk of stuff for me in parallel and when you're done give me the answer back and then we'll move on to talking about the next big chunk of stuff right it's i find that like a useful conceptual model we do have like newer technologies where you can start slicing up gpus and all kinds of stuff but let's not stress too much about that so i'm going to play a youtube video i don't think i'll get any sound so i'm just going to kind of narrate it over the top here but this uh is like a nice illustration of how this works this is off their nvidia youtube channel if you want to go and find it um but they managed to get the mythbusters guys to like build a physical demo of the difference between these two pieces of hardware so the thing oh don't buffer for me okay the thing that he's showing at the moment this is a cpu this this nice little robot here it's like a little robot arm with some tracks and it's got a paintball gun on the top and if you ask it to paint a picture it's going to paint one paintball at a time it's going to do it in like a very kind of flexible way right this robot is quite a cool little thing you could probably change the arm for something else you could get it to drive somewhere else and do something else and now they're unwrapping the gpu right this is still a paintball gun it's a slightly less flexible paintball gun it's a very much more opinionated paintball gun but it's still a paintball gun robot thing and every single one of these cylinders here these are our gpu cores and they can get those gpu cores to do kind of different stuff but not really like they're all going to do the same thing they're all going to shoot a paintball gun at the same time the difference is they can choose what goes in the barrel right they can pick different paintballs and put those into the barrel of this paintball gun and so if he hits his big button he can paint the mona lisa right it's still a paintball gun it's just a more opinionated paintball gun and this is kind of it's a cool video it's a really cool video so this is kind of what we're going to be doing in python code right instead of writing functions that execute one after another we're going to be writing functions that execute all at the same time and do roughly the same thing but this is maybe like one of the concepts that took me a little while to wrap my head around is like how different can these things be right if i run all of these cores doing this operation like does that have to be really really similar like paintballs in a paintball tube or can this be like very different and the answer actually that surprised me is they can be really really different right you can have an if statement that just completely forks right and goes off down way different branches and each core can be doing a different thing the limitation is they all have to be done before you can move on so if you have one branch that takes an order of magnitude longer than the rest of the branches your gpu is going to be very underutilized you can have a lot of threads sat there waiting for the straggler that's doing something else so it's very much in your benefit to make sure that all of those threads are doing roughly the same thing and when i say roughly the same thing i'm saying take roughly the same amount of time so if we've got this nice device how do we actually speak to this this thing and how do we get it to do what we want it to do now again the conceptual model that i've i've come to personally is to think of this device as like a separate machine altogether but one that i'm slightly less in control of i can't quite touch as easily i can't quite get access to as easily and and the other kind of machine that makes me think those things is like a server or a remote machine that i want to run something on if i have a nice server in the cloud or you know in a data center how do i run code on that machine well i can write some code on my laptop and it lives on my laptop i can have some data somewhere and then i need to get to that other machine right so i would ssh to it i would use remote desktop or vnc or something to like get a remote connection to that machine i would then have to get my code to that machine maybe i'd check it into version control and check it out on the other side i'd create a distribution and pip install it like i would somehow get my code from me to that other machine i'd have to get my data from me to that other machine and then i would have to actually say do the thing do the code right and again that would be ssh that would be clicking a button in a gui app it would be whatever it is but i'm actually i'm reaching to this other thing moving stuff there telling it to go and then at the end i have to get the stuff back again right because this stuff is remote to me and all of that is basically the same for the gpu we have to think about all of these same things but instead of having all these protocols that we are familiar with like ssh remote desktop and sap and ftp and git whatever we have cuda and cuda is like a heavily overloaded thing you'll hear it come up a bunch when i'm talking about it in the context of this talk we're talking about doing those things speaking to the gpu giving it code giving it data telling it to go getting results back asking how it's doing today by in terms of cpu utilization and memory utilization um so if when i said the word cuda you went oh no you said no c plus plus i do mean no c plus plus right i know uh cuda does have like a c plus language extension that is like the historic way of doing stuff it's a very performant way of doing things but you don't have to do it that way anymore this ecosystem is moving forwards and cuda in the context of this talk is just how do i speak to this thing and we can do all of that in python so the main way that we're going to do that is there's a really awesome python package called number you might have heard of it for like other reasons in terms of making code faster but what number is is it's a just in time compiler for your python code you can write python functions you can decorate it with numbers jit function decorator and then it means that next time you run that function it will do some fancy compiling stuff it will take your python byte code and put it through like the llvm and give you some compiled output and it will cache that and then every time you call that function it'll go fast and it's it's really good if you're trying to optimize your your python code but number is this kind of can i go from python to compiled stuff engine and it has a cuda extension so it can compile your python code down into the like nvidia binary compiled code stuff that we we have our own compilot with mvcc which is like a 4k um but it's number will do these things for you and it also provides you all of these nice handles for doing the other stuff right moving data there getting data back speaking to the gpu it's kind of wrapped up all of this stuff to make your life easier and so like the big first chunk of this tutorial we're going to be doing is basically a number tutorial we're going to be focusing on the cuda target number working through some some examples we also have other ways of of doing that right we have a lot of people at nvidia that like writing cpu plus all day and so they've built some really cool tools uh that fit into this ecosystem and then they've exposed all of that through python apis and python binding so all of that falls under the rapids project which is written on my t-shirt which is where i and charles and other folks around sci-fi work and a lot of these libraries are c-plus plus implementations of popular python tools with python wrappers to make them look like those popular tools so you have coupe that looks like numpy you have qdf that looks like pandas you have qml that looks like scikit-learn you have coup graph that looks like um network x and then we have other stuff like who signal and uh spatial and like these useful things that kind of exposing uh like low level c plus plus functionality up to you in python and we'll touch on that as we kind of go through to show you that like higher abstractions but i really want to focus on this like lower down let's write some python functions and run them on the gpu because that's kind of that's what i see is the fun bit so how are we doing in terms of binder environments still spinning tim three it is an error that i see often um sorry i'm just kubernetes in for a minute just to see what's going on right i've done a thing if you all re refresh the page just to give it like a chance that might binder has this nice way of having these like placeholders that look like your containers you're going to ask for and when you come in you should like push those out of the way into like unschedulable land and it will give you some results but maybe that's not working so let's try that and see if that's not working might also yeah i might click the link as well because i've got 100 seats on this or like i've got 100 gpus on this cluster that's like the maximum allocation i could get there so so you're all seeing a page like this at the moment right and you click the show and it is like probably saying launching server thinking about some stuff okay i'll leave this running and see if i get errors that i can do this big chunk here is all the views there's a lot of terminating going on there maybe they'll get out the way and it'll give you some space what i'm going to do i apologize it is just a bit of a wild card setting things up this way we i do have some like fallbacks uh if we're like still having pain in a little while um but let's keep going and give it a bit of a chance to see if it'll come around so i'm gonna start talking through oh yeah yeah sure so nvidia hardware you have to buy nvidia drivers are free but closed and proprietary although the like kernel modules are open now which is an exciting surprise for those of us out there in video um on top of that you've then got cuda which is like this proprietary bit of the driver and then on top of that everything upwards from there is open source mostly apache licensed stuff so number that we're going to be looking at here is open source it's community project it's not one of that you know it's not one of ours it's one that we just contribute to qdf coupe i qml all of those are open source and openly licensed the c plus plus code and the python code you'll find on our github let me go and find that quickly it's all on rapid stop that's one of my backup plans um i'm sure it will i just haven't got instructions which might mean like hashing some stuff together but i might ask you to come help me do that um yeah so if you go to rapids on on github this is kind of our place that we put all of our stuff related to like data science pi data kind of ecosystem stuff you'll find the source of everything there so i would say yes this is open source right this is all open source stuff it's just that it is nvidia only uh on all of these most of these tools i'd say all of these if you look at number there is like there is an amd rock on target um but their investment into that bit of code is not the same as has been into the cuda side of things yeah so let me start digging through the first bit of this notebook because this is going to take a little while of talking as well because there's a few concepts that i need to like set up ahead of time that i'm still not 100 confident i've wrapped my head around i think i'm like mostly there but i want to try and get you at least a part of the way that as we go through so to dive into number and how we're going to write gpu code in number uh we need to know a little bit about what does code look like when we're talking about gpus and you'll hear as we go through this i'm going to use the word kernel quite a lot you'll see it through all the documentation you see it a lot when we talk about gpu computing so i just want to set up a definition of the word kernel here that's relevant to this talk i was kind of intimidated by that word for a long time it's like what does that mean i don't really like how do i make a can or how do i do a thing is that the linux kernel or is this something else like what are we talking about so when i'm talking about a cuda kernel here a kernel that we're gonna run on the gpu i'm basically just talking about function right we're writing a thing which has like a signature it takes inputs and has outputs and we can run it right it's a function but there's a couple of caveats to running functions on a gpu which is why we have to have this kind of extra term because a function that runs on the gpu cannot return it can't give you anything back from what it's doing because this is going to be launched many thousands or millions of times in parallel and just coordinating all of the outputs of all of those calls and putting them somewhere is like a non-trivial task you can generate a lot of memory very quickly doing that so that button has to go on to the user right so instead of returning you have to modify memory in place so when we call a kernel usually we give it some input data and we give it some output place that's often like an empty array or something that it can populate with that data but you have to be a bit thoughtful about like how does my data go into the function how does my data come out of the function the other thing we have to think about with the kernel is when we call this kernel we have to decide how many times we're going to call it right this is a parallel processor it is going to execute many times in parallel well my binder just woke up so maybe some of yours is doing things yeah i'm seeing some thumbs ups and post-it notes and things good you can stand down thank you um yeah so the other thing we have to think about is how do we how do we tell this gpu how many times to do things and this is where we have to like have a little bit of understanding about the actual hardware and what's going on so the way that we talk about the way you would normally talk about i'm going to do things lots of times is you would say for iron range 10 million billion whatever right or you would use numpy or you do something where you are setting up some like parallel calculation but you're deciding how many iterations of this thing there are going to be usually whereas instead of thinking about like an iteration we're thinking about like a parallel execution and we call these parallel executions threads this is kind of a slightly overloaded term compared to other like many other things but we're going to be talking a lot about like cuda threads here a cuda thread is just an execution of the kernel one execution of the kernel is one thread and so we have to decide how many threads do i want how many times do i want this function to be called and that is like that's related to what the problem is right if i have an array of a million elements maybe i want my function to be called a million times because i need to do an operation on every element maybe i'm collapsing or doing a rolling window right whatever i'm doing i need to figure out upfront how many calculations am i going to have on to my my data metadata structure but we have like a slight constraint in that the gpu is a parallel processor but it has a finite number of threads it can only do a certain number of things at a time and also each gpu is different as we make new ones they have more cores in them as we make different skews with different you know you might buy a more expensive one that have more cores in basically and that kind of maps down onto like inside the chip there are these things called streaming multi-processors that are like chugging their way through these threads and doing all these executions but the way that we chunk this up in cuda world is we say i'm going to have a grid of computations right grid is one of these first scary words um this grid of computation is like what is the shape and size of my calculation right i've got my million element array the shape and size of my calculation is going to be a million executions of my thread right so we have this grid but then we have to like chunk that into like some round number that we can tell the gpu about so that it doesn't matter how many cores it's got it can kind of do it in batches so that it can get all the way through a million it won't have a million cores but i can ask it to do a million things it might have two thousand cores and it will just do a bunch of batches of my threads to get all the way through to the end so when we define our grid we have to say how many batches are we going to do and batches in this context called blocks right so we have threads blocks and grids the grid is the overall calculation the block is has a size of a number of threads and we have to work out how many blocks we need to get all the way through to the end of our algorithm and these are silly words that i get muddled up all over the place i apologize if i say the wrong one at the wrong time i uh you most likely i'm certain it won't be clear what these things mean as we start running code we'll kind of maybe get a bit more familiar with this but this is like a big hurdle that took me a while to get around mentally so please don't stress if you're feeling similar it's a hand up yes it's it's basically out of the scope i think i maybe made a joke about what's in the title um but let's not worry too much about this this is kind of it's tough to do with uh threads can have shared cache within the gpu and it's like a term used in describing or these threads have got hype from the cache that they can it's like communication it's like yeah it's it's in that category of typing but i'm not going to touch it in this in this talk mostly because i don't fully understand it um i'm going to use the word warp in a second though which is which is great um as soon as i said i'm not going to use it but when we decide our calculation we have to choose these numbers right we have to tell it what these numbers are going to be and so let's come up with a bit of a strategy of deciding what these numbers are going to be so for our grid size that's kind of the easy one that we look at the problem and say okay how many times do i how many times are we going to go around my for loop how many times do i want my my function to run that is our grid size we can decide that ourselves but then well how do i split this up into blocks and and threads to like make up that overall number and if you look at the number documentation it says the number of threads in a block should be a multiple of the warp which is 32 but let's not worry about that it just says it has to be a multiple of 32. and it should be somewhere between 128 and 512 on newer gpus it can be 1024 and it basically says this number will depend on problem to problem but just choose 128 and don't worry too much about it and then as you're kind of getting into the swing of things then you can start doing some profiling and benchmarking and seeing whether tweaking that number in different directions gets you different performance and maybe you want to start thinking about warps and caches and like lower level stuff but for the scope of this tutorial and for the scope of a lot of the stuff we do choosing 128 as the block size is just like a totally safe option to do and so we're going to do that in everything here and 128 is going to come up a lot so you should be able to like start stepping through this notebook with me if you're into the notebook environment which is great if you're still having trouble with getting into the environment or doing anything like that please put your um please say something in the slack actually because i'm answering i'm quite happy to be interrupted and answer questions so maybe you put something in the slack and one of these guys will help you out um but let's actually write some python codes i've been talking at you for like 40 minutes and the whole point of this is to see some python code running on the gpu so i said at the beginning we need to write some functions for the gpu but those functions have constraints so what we're going to do is let's start in normal python function land and we'll like iterate towards being a kernel that we can run on the gpu we'll do this in a few steps so for me conceptually one of the ways i have found it useful to think about running kernels is list comprehensions it's kind of like a nice step towards this kind of parallelism where you're kind of giving this like parallel expression right you're saying you know for i you know foo i for i in data right in this first hour right you're given this kind of comprehension but you're saying isn't it please iterate all the way through it and give me the output array of that you kind of got it in interval out it's kind of this this array while we're living it so i've written a really simple uh boring example here where we've got our function foo and it's going to return whatever it's given it's going to pass it straight through i've got some input data which is just the range 10 and i can do a list comprehension which would just do through i for i and data and just give me an author nine that's my output right i'm kind of passing a list of stuff in i'm getting a list of stuff out but i said we can't return right this function is returning we can't do returns we should not do that because kind of can't do that so let's take one step in that direction and instead of returning i let's just put i into a list and then that list will be the output of our function so ahead of time we say here's my my input data and here's my output array that's going to store my answers and my kernel or what will be my kernel is going to take in a number and it's just going to append that into the array and then i'm going to do my same list comprehension a lot of things have stayed the same and then i'm going to look at output all right so again it should be the same array we get the same thing that we got last time but we're starting to get the structure of what it's like to write cuda code now you kind of set up your data structures at the beginning you define your kernel that's going to run on them you tell it to run on them and then you look at the output data that came out of that and do something with that so there is uh like an uh yeah it it doesn't and it can't and it shouldn't and i i i was i'm just being lazy in these first couple of cells i guess um it can't be a global so but we'll come on to that in a couple of cells so one of the constraints as well as not being a global is that actually the gpu wants to know what the length of this array is ahead of time i can't just create an empty array it's the same problem as if these things could return right i need somewhere to put this stuff i can't just have like an infinitely length array in gpu memory we're not we don't have an operating system to make our life easier here we are like dealing with this device at a low level so this is where numpy can kind of come to our rescue and we can say well my input data is going to be a numpy range numpy array of these ten elements and my output is going to be a numpy array of zeros there's the same shape and size as these inputs right so instead of just having some some python stuff we've got some numpy stuff but all the same we're gonna do exactly the same instead of appending to my list i'm going to update the index of that list with just the value right this i'm overloading i a little bit here but i'm just going to take a number in i'm going to update a specific point in the index and then i'll get that answer back out here if i run this uh i didn't import numpy should merge those cells i'll do that every time i present this um so that's it right we are at this point now where foo here is a kernel in the definition of kernel that i gave before right it's taking in something it's updating some memory and i can use it in parallel right this is now like a kernel so i can now just decorate it with with number and turn it into an actual gpu kernel that's going to run on the gpu now as you said uh with the global stuff we can't pass uh we can't just grab stuff from the global scope because we're we're running this on the gpu the gpu doesn't know anything about our cpu memory or our global scope or anything like that so i've changed through here to take our input array and take our output array and you might be wondering like what has happened to i in this with each call of this function needs to know what it's doing you don't you know we're going to have this input array of elements and this output array of elements and each call of the kernel is going to need to operate on a different element in this in this context so each kernel needs to know what to do and this is this again was like another step forward in like knowledge for me was the fact that um each thread as executing on the gpu can speak to the gpu and say which thread am i within my big grid of a million threads who am i and it will give it an index within that grid so that is how we can allow our threads to go off on different paths and branch and do whatever is because they each have a discrete index that is assigned by the gpu as things are running and the way that we do that through the numpy uh the number api is we use this kuda.grid method so basically just saying here what is my one-dimensional index within the cuda grid so we're saying one here i kind of wish you could just omit this and it would give you the one-dimensional one because i don't we'll introduce that probably in a bit um but we're kind of saying what is my index within the grid and because i'm deciding i have an input array of 10 i have an output array of 10 and i'm going to run this thing 10 times those like indexes map along i'm just going to say take index 0 from here and put it to index 0 there oh i'm number 4 i'm going to take 4 and i'm going to put it to 4 over there right it's just going to do whatever it's index says it's going to do so this now is actually a gpu cuda kernel this is just a decorated version of our kernel before it takes our input array and output array as arguments and it asks the gpu what its index is as it's executing and so it can do the appropriate assignment so the last thing we need to do is actually say to the gpu please run this thing for us and because we need to decide what our grid size is ahead of time you know we said it was going to be 10 right in this example our 10 element thing we're going to have a grid size of 10. the way that we can figure this in number is on our function that is decorated with cuda.jet we use square brackets right to give it the block size and number of blocks so here we have block size first no we have number of blocks first we have one block with ten threads in there just doing the length of the data so we're kind of configuring our kernel at this point we have this kernel that can do stuff but we're gonna say to it right you need to run this number of times you could you could assign this the square brackety bit to another variable if you wanted to keep track of a specific shape of that kernel but often it's common to do start my function use some square brackets to tell it how many times to run this is my you know for loop basically and then we give it the arguments like a normal function call so we'll see a bunch of weird function calls like this as we go you'll get used to them over time and if i run this now i haven't imported cuda really should match those uh i get a couple of warnings and then i get an output array and that is this is like the earth shattering exciting moment because this function executed on the gpu rather than on the cpu these lines of code in here were gpu instructions that happened on the gpu and it's really nice to be able to look at python and see that being able to run on the gpu kind of in a low level and for me this was like oh yeah i've done it i've written some gpu code i kind of understand but not really now there's a couple of warnings here that's kind of basically saying like that was a very small problem the gpu is not really for you and that's that's fine because these are just toy examples um the other warning is about data which we'll talk about in a second now let's scale this up a little bit instead of working on 10 elements let's do 30 million elements it'll probably still tell me this problem is too small but let's make a big numpy random array with 30 million random elements in it let's make ourselves a big output array full of zeros for our results to go into and we're just gonna double everything in our input array so we've got a kernel here this is our doubling kernel it's going to in python sorry i'm starting python and i'm going to move to the gpu so if i was going to double everything in my numpy array in the most inefficient unpleasant pure python way possible this is how i would do it right i would have python loop over my array and update each element with the double of the input if i run this i think on my machine it will take 10 seconds or so let's let python chug its way through this this big array and do its thing okay 12 seconds basically but that's kind of how i do things in like a really naughty way now let's move over to the gpu and think about how we can parallelize that algorithm because this is you know this is like an embarrassingly parallel thing we're just taking an item doubling it putting it somewhere else we can parallelize this in a really like trivial way so we've still got our input array so i'll leave that alone i'm just kind of resetting the output array here back to zeros i'm doing a little bit of math to work out my blocks and threads instead of me deciding i'm going to have one block of 10 threads i'm just going to say well my block size is 128 and i'm just going to calculate how many blocks i need to get all the way through my my number so to get through my 30 million items this shape here i'm just going to divide that by the number of threads and calculate the ceiling or whatever and i'll get my number of blocks to work my way through we'll do this calculation a lot right it's just work out how many blocks do i need to get all the way through i then have a cuda kernel here which is going to take our input array and our output array it's going to get its index to say which item in the array do i work on and then it's going to update the output with a double of the input and then i'm going to do my strange gpu call where i give it the number of blocks and this number of threads in each block i'm going to give it my input and output and that's going to run on the gpu in 191 milliseconds compared to the 12 seconds that that took in pure pipe uh this has stopped telling me that this is a small problem which is helpful it's still telling me you're not doing very clever data stuff um but that's fine it's just preempting my next bit but like at the beginning of this talk i asked uh you to put your hands up if you use numpy before so i'm sure you're all screaming in your head it's like why are you doing a for loop in python like numpy is amazing and wonderful why don't you just use numpy and of course we can just use numpy right we can just do our array times two and numpy will do this uh on the cpu and num like the workstation i'm on has got 96 cpu chords so numpy is very nice on this machine if i time how long this takes in numpy i'm surprised shouldn't have done that this normally like is there we go 50 yeah okay maybe that was just an internet like right it's come back in 56 milliseconds right so that's come back in a third of the time nearly a quarter of the time that my naughty gpu calculation took so badly written gpu code is still not necessarily better than well-written cpu code right we have to really think about these problems and dig down um but one of the reasons why we're paying so much pain here on the gpu one of the reasons why i would consider this to be badly written enough that it's giving me a warning is when i called my kernel i gave it random array and output these are numpy arrays these are in memory on the cpu these the gpu knows nothing about these and this is where number is really nice but also can trip you up a little bit is i've given it a cpu array and it's gone oh you don't mean that i'm gonna copy that onto the gpu then i'll do the thing and then i'll copy it back for you and then you know you don't have to worry too much about that but that you pay a penalty for that right like it takes time to move data from the cpu to the gpu in the way it takes time to like scp data across to a server or something right there's a transfer time that you have to pay and the kernel that we're running here is a very very small very very simple kernel and so the amount of data we're moving compared to the the performance of the function is just not stacking up so we're actually paying a lot of pain but we don't have to let number do this for us we can be more explicit about our memory so let's be explicit let's take our numpy arrays and we'll use the calls cuda.2 device to say cuda can you move that number can you move these to the gpu for me now at this point so if i run these we'll get these gpu random array and gpu output if i just have a look at one of those you can see this is no longer a numpy array this is now a weird cuda device array that's sitting on my device now this array has moved into the ram of the gpu and i've just got this pointer to it now but i can give that pointer to my kernel and let the kernel operate so number is not going to do that transfer and it's not going to do that transfer back so if i now time this running on there it's now taken 12 milliseconds right so this is faster than the cpu operation if we remove the data transfer from the equation but the data transfer is in the equation you have to pay it at the point that we moved it we also need to realize that our answer is on the gpu still the output is still at arm's reach and we now need to get that back so we also need to use uh copy to host to say can i have my array back and that pops out as an empire ad yeah get all these words all over the place that pops out as a numpy array back on this end but that cost time that was kind of an expensive operation and the time where these are like that you would use this that would be really useful is when you're going to move some data to the gpu and then you're going to do stuff to it and then you're going to do something else to it and you can do something else to it you can make lots of python calls and you're going to keep operating on that data and then finally at the end you're going to get it back hopefully in most of the situations thinking about if you're doing some i don't know whether data stuff you're going to move your data to the gpu you're going to do a whole bunch of stuff you can do a whole bunch of stuff and then hopefully the thing that comes back is a plot that you can look at right you don't want anything else back you just want the kind of final result which should be smaller of course you know that's not always the case but this kind of uh illustrates here if i just used numpy arrays here and call and had three kernels foobar and baz if i called each one of my numpy arrays number would copy the data down on the kernel and pass it back and then it would for the next one copy the data down and pass it back and it would just keep thrashing that data back and forth in like an inefficient way so we would just set this up with okay here's my data put it on the gpu i'm going to call each of my kernels and then i'm going to get my answer back at the end this is where kind of manual memory management is is is going to be beneficial so that's the end of this first notebook we've been going for a good long time so thank you for bearing with me especially with the binder startup and we've got a lab a number lab here um what i'm kind of aiming to do is i'm going to talk at you for a bit not as long as this because i've done like the intro plus some binder stuff plus the first notebook but we're going to do like a notebook and then a lab and then an or or a notebook and then a break or something like that the last ones i've not got not got labs but let's work through this i'm going to ask you to kind of apply the things that we've just talked in that about that notebook maybe take a 15-minute break at this point but if you take all of your post-it notes off because we're going to start the exercise and then start working your way through this and then when you're done or bored or want to move on stick your post-it note back on um also feel free to take a rest break and go get a drink or whatever from outside but i'll aim to resume at about 22. forwards work through the the lab solution and then we'll move on to the the next networks so in this lab it was just asking you to do fizzbuzz basically right so standard computer science thing um but we're going to take all the numbers in the range of 50 million and we're just going to calculate which ones are fizz buzz so we're going to start off by importing our numpy and uh number our cue different number and we're gonna make our array we'll make this with numpy so we'll make our one through fifty million with numpy with where they range and we need to create our output as well our data going to so let's do zeros like that so we've got our big our big zero zero dot array and then we need to calculate the size of our grid right this is these are always like the steps you take it's like what does my input data look like what does my output data look like and what does my calculation look like you need to decide those three things so we've got 50 million elements and we want to do an operation on each one of those so our output is going to be 50 million and our calculation is going to be 50 million so we just need to calculate that basically we're going to do the same thing we did before we'll choose our arbitrary thread size 128 because that's a safe number to choose and we'll just divide the shape of our array by that size and take the ceiling to work out how many blocks we're going to need and you might notice in a lot of these situations i've been calculating the number of blocks that i need but i've been calling ceiling on that so the number of blocks that i have will be a multiple of 128 but that might extend over the end of our array right we might go off the end of our 50 million that's not ideal and we will kind of counter that later but it's also not the end of the world those threads may just operate on indexes that don't exist um they will actually overflow they will just read random bits of ram and write random bits of ram and that is not good so we should handle that later but given that we're only going to go off the end by a little bit and we're the only ones doing anything on this gpu it's not it's not like a killer situation but we should guard against that later now here's our our fizzbuzz algorithm uh now we're using numpy arrays here so we can't actually put fizz and bars and fizz buzz in here so we're just going to substitute the numbers one two and three um fizz buzz fizz buzz and we're going to leave the rest of zeros right so just have a big array of zeros with some ones twos and threes kind of littered through to represent where we found the fizz buzzer fizzbuzz it's going to be basically a mask so when we call our function we're going to say who am i what is my thread index which element do i need to operate on in this large array um i realized i was like part way through typing a answer back sorry till i completely jump over somebody noticed a mistake in one of these where i wasn't updating the right thing in the right place i appreciate like spotting typos in this i can go through and fix those up so if you do spot any more mistakes in this stuff please shout um so i'm gonna take my number out of my input array just to just to make it a bit easier to read the code and i'm going to say does it is it a fizz is it a buzz or is it a fizz buzz and if it is i'm going to update its location in the output array to basically make this mask right and then i'm going to time how long that takes to run on our 50 million elements you just have a look at the first 12 that took half a second to go through that array and update it and you can see i've got zero zero fears buzz zero phase et cetera et cetera um there was also like uh extra credit to just say what was the highest number of fizz we are still in numpy land so you could just do this in numpy you could just do mp.where to work out oh the highest fizz in here was for 49 million 995 998 so let's move on away from kind of abstract strange things um and talk about like slightly more tangible data structures rather than just playing around with the race of numbers we're still going to play with a raise of numbers but we're just going to have some meaning to those so let's take an array of numbers that we deal with all the time an image a bitmap and do some stuff with that so we're going to use matplotlib because matlab will very nicely read in an image and give us an umpi array of that structure and then we're going to play around with that numpy array on the gpu so let me just import my plotlib and a few other bits and pieces set them up like that and because we're talking about number we're going to read in the number logo which is stored in here if we have a look at the type of this that matplotlib has read in this is a numpy pirate it's just a numpy array it's nothing kind of fancy if we do uh m show matplotlib and if we get our axes and things but you can just see this is the number logo stored in an array and in order to operate on this let's move this image onto the gpu so that we can start messing with it in place so we're going to cuda dot 2 device to push our image onto the device and we're also going to create uh some zeros the same shape and size as our image and we'll push that to the gpu i could actually also just ask number can you make some zeros directly on the gpu the shape and size well we could you know it's slightly more efficient than doing this but i like this conceptually of saying here's our output gpu now this array is not the one-dimensional index that we've been playing with uh you know the one-dimensional array we've been playing with this whole time this has got like a more interesting shape to it we have a look at the shape of our numpy array it is 116 high 434 wide and 4 deep if we think about it as an input right it has an x and a y of pixels and the depth here is red green blue alpha and it's just a value of of red green blue and alpha basically within a grid so when we work on our image we kind of want our calculation to be that same shape and size right we want to be working on you know depending on what we're going to do we want it to be the same shape and size and what we're going to do in this notebook but i think in this notebook yeah in this notebook what we're going to do is we're going to blur the image blurring images is like a nice little thing we're going to do that in order to calculate a blur we want to take each pixel look at all of its neighbors and calculate like that the mean of all of those pixels and then update the output with the mean of its neighbors basically right so we're just going to do a mean of all the neighbors for every single value and we only want to be blurring kind of across we don't want to be blurring across the color channels we just want to be like blurring the image itself so we're going to be operating on red independently to blue independently to green we weren't worried about alpha so in order to do that let's think about our calculation also as a three-dimensional structure right we have our x y and channel and we're going to think about our calculation as having x y and channel so we have to decide what our number of threads per block are up front like we had to do before but we're now working in this in this fancy three-dimensional indexing world and so we need to specify our threads per block in three dimensions so what numbers do we choose well like i said the documentation says choose 128 because 128 is a safe number so we're definitely going to choose 128. but i'm doing this in three dimensions so i need to choose three numbers that multiply together to make 128 so here i've chosen 2 16 and 4 and the very arbitrary reasons for why i chose those numbers is because there's four color channels so i chose 4 as the last number our grid is going to be kind of one block deep on the on the array right i then chose 2 and 16 because that's roughly the aspect ratio of the image it doesn't hugely matter that could be a square that could be whatever but i've just kind of chose those numbers arbitrarily and made sure that they multiplied up to 128. so we have this kind of magic number put in here this threads per block and now we need to calculate well how many of those blocks would i need to pass over my image in order to modify every single pixel so i'm going to calculate my blocks for each of those those axes so there's a bit of you know unpleasantness here but we end up with the block spread and let me i should bring this out here okay so it's going to take 58 blocks on the y dimension 28 blocks on the x dimension and one block deep we don't really need to care about these numbers now we've calculated them this is going to make sure we've got full coverage of our image we can make sure we can we can see what the overall size of this calculation is going to be what the size of this grid is if we just multiply all these things back together again we can see that our calculation is going to be 116 by 448 by four which 116 lines up it was 448 so it's slightly over on the y dimension and then it's the same on on the dimension so we've got this kind of stencil that's going to fit over our image and we could there's like a whole bunch of like stenciling stuff in them in number to make your life much easier doing this kind of thing but i wanted to do this from like the low level to show you what we're doing but you can see we've kind of made this big calculation thing that we're going to sit on top of our image and it's going to do some operation to it so let's write our kernel now so for each uh thread we're going to work out where we are in the image and the gpu supports multi-dimensional indexing we've set ourselves up with three-dimensional shapes for our calculation so we can ask for our index in three dimensions as well so instead of doing cuda dot grade of one we can do kudo dog grid of three and say can you give me basically my x y and z my x y and c position within this image for who i need to operate on so each invocation of this will get x y and z and then i'm going to do my safeguarding that i talked about before we're just going to check am i over the bounds of this image or not am i outside of the shape if i am outside of the shape the house is just going to you know the implication is going to finish right we are going to launch some unnecessary threads here um but those threads are going to just return they're just not going to do anything and that's fine we only care about threads that are kind of wandering off doing long things it doesn't matter if they any kind of short circuit finish early so anything that's inside the image is going to update the output position with the mean of the input position with its neighbors right so we've just kind of got that and i broke it down here so it's a little bit visual but you can see we're just taking all of those values dividing them by nine after they've added them together and updating that so we're calculating this blur and then for running our kernel we can do this on um our image but this isn't a very deep or very aggressive blur right we're just gonna take the mean of all the pixels we'll get like a mild blab but what if we want to blur it a lot or we could call this multiple times and this is one of those situations where we can do this blur in parallel but in order to do the second blur we need all of the updated values that kind of intermediate step so this is where we're kind of doing a bunch of parallel operations sequentially to build up the overall thing that we're trying to solve so we're going to just in python do a for loop right something that is well known to be slow we can just loop over this we're doing a lot of very fast things just inside the loop here so we'll call our blur function to blur our image and then i'm just flipping the pointers over right so the output becomes the input the input from the output so we're just going to take our image and we're going to blur it to here i'm going to blur that back into the array that we had and we'll blur it some more into that one i'm just going to kind of flip back and forth reusing the same memory over and over just for efficiency rather than having a load of random intermediate arrays hanging out so if i run this uh i think i should probably have i haven't actually run my code there we go run that run that i think i need one more flip actually i think it stays in like a flip position but if we have our we now have our gpu output which is our blurred image and if we just pull that back onto the cpu with copy to host we can do matlab in show again and see our nice blurry number oh you mean as in it would go off the out of the bounds um so at the moment around the bound is just reading noise i haven't done this in like a super intelligent way right so you might want to put that inside your candle like oh if you are zero maybe you only want to be reading the positions within the image it's still it's still reading one which is just kind of off the array into ram noise lab it's an overflow basically yeah it doesn't throw an error and you can see there's some like weird darkness around the bounds of the image where the noise is just kind of coming in and making it a bit blurry um across all the channels yeah yeah exactly so this is kind of this is what i'm saying about this is like a much lower level thing like you're operating with this device you don't have a you don't have an operating system to kind of help you here which is kind of where like you know numpy and the operating system are like working together to make sure you're managing memory nicely um in this you you have access to the ram and you can do bad things as well as good things um dara yeah yeah exactly it's it's it's definitely it's definitely got that vibe to it there are other things to help you out we have um other packages like rmm which is like the rapid memory manager that helps you do slightly more intelligent things so we're kind of quickly back to the next lab so i'm going to give you another 15 minutes to do this i'm i've forgotten how short i know it was actually but i'm going to ask you to run through basically the same thing but this time i'm going to ask you to do a grayscale basically make this image black and white um so instead of blurring when we're doing this grayscale we're taking the mean of each of the channels right we've got red green and blue we're taking a mean so this is actually a two-dimensional calculation rather than the three-dimensional calculation we did before we only care about the x and y-value and then each of our kernels is going to operate on red green and blue and update red green and blue so this is basically the same as the blur one but this is two-dimensional instead of three-dimensional so give that a go please put your hand up if you need some help also again rest breaking news break email if you need an email uh yeah question our input and output are both three-dimensional but the calculation is two-dimensional yep gonna take right okay i'm gonna get going again my jupiter just died but let's work through the solutions to this next lab and then we'll move on to something else i think we were here in the grayscaling one so this is pretty similar to the first one right we're going to import matplotlib and kind of get ourselves set up with dependencies and things um we'll import our number image and just make sure we've got our logo here and then we'll push it down onto the gpu once it's there we'll work out the size and shape of our calculation so as i said this is the two-dimensional calculation because we need to read all the channels in so it still has to be multiple behind so it has to be a block of 128 so we're going to choose 16 and 16 to multiply up to 128 and then we're going to calculate how many blocks we need to do that so it's pretty similar to the last one this is just two dimensions rather than the one and the three that we've seen already then our function here we're going to get our x and y position where am i in in the shape of my calculation this is only two-dimensional this time and again we're going to do our boundary check here um i was just uh chatting to somebody doing the thing about like these boundary checks work both ways as well because before um we were like writing off the end of the array you can also read in from off the end of the array and it doesn't warn you about these things you have to like be conscious of making sure you're doing boundary checks when you write your accounts we're then going to take the average across the three rgb channels we're not going to include alpha in this just rgmp um and that's going to give you your average right it's going to put us on this on this gradient from black to white and then we're going to update our output array but we're going to update r g and b with the mean each one will make it the same so this is kind of why we've got this two two three dimensional things and we've got a two dimensional calculation they're going to be reading in three values writing out three values across those dimensions so if we run this then uh we need to do this once because it we need to make it black and white once and then if we pull it back we can see it goes grey so this is how we can do all sorts of different like image processing things we can work on these large arrays hopefully you can kind of see how we can use the gpu for these kind of operations that are parallel right each operation we did on each of these pixels was kind of independent of anything else around it sometimes it's not always fully independent like in the blurring we had to wait for each step to completely checkpoint before we can move on to the next one so we you know we can do that kind of in python calling down into the gpu and really do like a mix and match here to get the best now to move on to like a slightly different topic before we jump into higher level libraries so far in this network in in one of the earlier notebooks i said to you like oh this is really exciting we got to this point we wrote this function we ran on the gpu and isn't that exciting we got here and you all very kindly trusted me that it had run on the gpu right because it looked just like something else you just called a python function and it ran on the gpu how do you actually be sure that it ran on the gpu how do you know what's going on so let's like go off on like a slight little detour into not i'm not going to go as far as like debugging um because you have to start pulling out like nvidia toolkits and stuff really getting deep down but let's have a look at like observability of the gpu so in the beginning i asked you to run nvidia smi to have a look at what was going on on your gpu mainly just to make sure you had one that it was there and that we could see it and that new drivers are installed and things running it again now we can see i've got like a few things in my list around this it might be a bit longer because i've just restarted my notebook because things died but i can see i've got a python process here listed down in the list of processors now they're saying oh this python process has been speaking to the gpu it's been doing stuff on the gpu and it's allocated 163 megabytes of memory on the gpu so that's like one way we can look and see okay my python process is actually talking to the gpu we can also kind of get information here about how much memory is being used how much memory we have all together and his gpu has got 48 gig of ram or whatever you also have 16 i think other ones i chose on google cloud it also just tells you about power outputs and temperatures and all that kind of thing this is kind of just like a useful overview of stuff but what about actually like utilization how much memory am i using as a percent is here but what about how much am i pushing it there is there's a zero percent listed here and it says okay these gpus are doing nothing at the moment they are not they are idle they're not busy but we don't want to be running nvidia smi over and over you could do watch nvidia smi or something and get like a you know periodic table that is going to be updating but we have other tools that can help you reach into this after you've had like a peek over on the left hand side there might be a little icon that you don't recognize in jupiter lab if you're a jupiter lab user there's like a tiny little picture of a gpu on the side this is because uh in this environment we have the mv dashboard extension installed i just search mv dashboard this is both a standalone dashboard for your gpu and also a jupiter lab extension you can install it and do both um you can run it's very slow you can run in the dashboard from the command line um and it will just start like a web server it's kind of inspired by the dash dashboard if you've if you've used that um you can view it in a web browser but also it has this nice jupiter lab extension where you can pull the plots out so i want to see okay how busy are my gpus let's grab that plot and smash it over here how much memory am i using let's grab that plot let's make it over here i can now see some utilization of my hardware i can also get you know you can get overall machine resources like cpu and other bits and pieces uh if you've got multiple gpus that are connected via envy link which is like the modern successor to sli if you're into gaming from a while ago the gpus can talk directly to each other by a little clip that you put on the top and we can see the like throughput through that that network as well so now if i go back to my notebook and if i do my black and white again is that so fast okay sometimes when i'm doing things these graphs show stuff but i think i'm this is such like a minimal application that yeah the gpu just is that fast let's try doing maybe the blur one is a bit heavier oh did you see it flicker anybody see a flicker i'll run it again just to see if it does it again just at that time oh it's just sad what did i do no i just yeah there's like a thing yeah okay that there was a flicker some of you saw it that was actually my cuda context initializing which is a slight red herring but it means i can actually come and do it in other notebooks as well and you might see that flicker um yeah there was right um the gpu is not busy in any of the things that i'm showing you i don't we might make it busy later but um you can see here now i'm going to run some code and my gpu is busy this is kind of like opening task manager or activity monitor or whatever your preferred thing is to see inside your machine windows is quite good at showing you what the gpu is doing um linux is less so these are useful tools for finding out what's going on so this next notebook is going to kind of dig a little bit into how this works um if you have a look there's there is a little gif on here this is on like uh it's quite small isn't it bigger you can see there's like an animated screenshot thing this is on a server that's got eight gpus in you can see a utilization bar for each of those gpus and it's running through this is running through a big dash graph and each gpu is busy doing stuff but hopefully you get the idea that at the top you've got gpu utilization that i've put up here they're doing stuff they're still not hugely taxed in this workload um the memory is below that and then below that we've got the the memory throughput between them so these gpus have got like little envelope connections to each other and they are handing data back and forth between them and you can see that that going on as well so within this environment it's quite a nice way to see what's happening but if you don't have that i don't i'm not sure if we have the proxy set up properly actually but you can if you run mv dashboard and go to like port 8000 all of these plots are just like on a page and you can click through some tabs and view those videos questions you can just always have this open if you want to if you want to see what's going on um now this the way that this works is we have something that's kind of it's part of the the driver actually if you look at the nvidia smi output um i thought i mentioned in here okay maybe ignore me anyway we have this this pi mvml right which is the nvidia markup language right this is the python interface into the nvidia way and it's basically just a way of querying the gpu and saying tell me about yourself you know um so if we can we can do this manually in python and we'll just kind of dig into that for a minute so you can see if you ever want to build your own things or maybe print out some information about the machine that you're on in a python script i'm vmware is like a really useful way of just getting stats about the hardware you're on i see it used a lot in like the http community if you're going to submit onto a node maybe you just want to print out the make and model and ram and all that kind of stuff of the the node you get allocated or something so if we import pi vm out we have to initialize it this just kind of sets us up to be able to communicate with it and we've got these calls like we can do device get count tells me on my workstation i have two gpus i can say what's my driver version um and then i can grab some handles right so the way that this works in nvml is you get like a handle to a device this is like an object that represents the device and then you can use that handle to ask that device things about itself so i'm gonna loop over the number of devices i have i have two and i'm gonna get a handle for each of those devices by its index you see that our devices get indexed i've got gpu zero and gpu one everything we've done so far just select it just selects gpu zero to do whatever it's doing and we will come on to like more about stuff in a bit with multiple gpus but um everything's just happening on gpu zero you've if i grab that we get these couple of handles this is just mapping onto c you can kind of see this so we can see um but then we can do things we can say get device name for my gpu so let's blue pivotal all of our gpus and just get our device name and have a look at those we can see how these are quadri rtx 8000's like nvidia smi told me that they were i can also say can i have your memory info and see how much memory is used for all of my gpus i can say oh i'm using one gig and nothing basically like i said everything's happening on one gpu and i can see how much is free uh you know these are rounded numbers but um you can kind of see i can query into my device i can find that information and this is what we've used to build these dashboards basically these dashboards are just written in python you kind of hack about with them and play about with them and get started for yourself so before we move on there's like a few other tools as well um there's envy top which exists which i i won't i won't go through that effort installing it in this environment but mbtop kind of gives you top style view onto your gpus you can see utilization as graphs you kind of get the processes as well but like in video smi glances uh also has i think i don't think it's gonna be any pictures but you know glances is like a utility monitor on unix systems i use it quite a bunch on my laptop and it will show you kind of utilization of gpu as well right so there's a bunch of tools out there already that just allow you to see your machine but they're just not necessarily in the places you would expect to find them so these are a few places you can go to find those things now the other thing i want to show you before we dip back into some code of actually doing things is to talk about we've talked a bit about number and at the beginning i like briefly mentioned like high torch is a thing and tensorflow is a thing and they have gpu support we're also now going to go on and talk about some other libraries like coupe i and qdf these are all gpu libraries as well and when you're doing a piece of work it's very unlikely that you're going to be using one tool you're most likely going to want to use multiple tools so let's look a bit about how things play together as we've seen memory exists on your cpu you move it down to the gpu you do stuff to it these arrays that we've been moving around are just arrays of data right in like its simplest computer sciencey form and they've got fancy clothes on right they look like this numpy array they look like this number cuda device array or whatever but they are just like an array wrapped in some python stuff so that we know what to do with it in our code but one of the big efforts that we've uh kind of pushed on within the rapids community and kind of broadly over like gpu data science pi data stuff is let's make everything play nicely together and let's try and avoid any copying of memory wherever we can avoid it because that's like the big pain that we see all the time right we've seen cpu to gpu memory copying but what if when i want to switch from one tool to another do i have to like copy my array from one tool to another like that's going to end up like exploding how much memory i'm using on my gpu i've got this like transient stack i don't i don't want to do that so in order to deal with this we have kind of two two protocols really that have kind of come out of the community for doing handoff between one library and another do you have dlpac which has been around for a long time which is kind of allows you to convert your array into this kind of intermediate representation of the array on the device um with a bit of metadata about what it is and how big it is and where it lives in terms of memory addresses and then that can be like sucked into something else that can read in the old pack so you can basically hand one array to another library via the you know you can do a lot of libraries have like a 2d alpaca and a frontiel pack or something that allows you to just move around but then beyond that that we use a lot within rapids we also have this cuda array interface and it might seem familiar in terms of like other array interfaces that we have in that numpy world and the way that we do dispatching and accessing within the numpy community a library can come along and implement the cuda array interface on their gpu array representation and if they do that it means we can just cast directly from one array type to another array type they will just check does where you're going have kudo array interface if so here's the array data that you need it'll call that method and it will come out the other end of what it needs to be and it just means everything gets really cleaned up and we've already got good adoption within the community so we've seen number a bunch we're going to talk about coupe in a bit i've got a little pie torch example here although i don't have pi torch installed in this environment so i don't want all these cells because they won't work but you can just hand from one of these to the other and it'll just move around it won't copy it'll be really neat so if you want to do some kind of workflow where you're going to load some data in with coupon maybe and do some numpy style stuff mush your array around do bits and pieces and then you want to switch to pi torch to do some model training you can just hand that array over from kupai to pytorch and then just get going with your tensor question yes they do sorry purple is dl pack and yellow is kudo ray and face i think the legend got chopped off um so let's have a like a quick little look at this so if i use coupe i which i know we haven't come onto coupon yet but if i create a coupon array we should see my memory usage increase yep oh my gpu is busy allocating my coupon array my memory usage increased that i'm going to keep these blocks open so you can see a bit more of what's going on and if i have a look at the type of my uh coupe ira this is a coupe i core a which we'll play with more in a minute but this is an array it's on the gpu and i can just hand that to number if i want to do some stuff in number i can just do two device um as i've been doing with my numpy arrays but if i give it a coupon array number will go like oh can i can i just have your memory addresses and i'll just use your array so if i create a number array nothing changed right my memory hasn't increased nothing has changed i have one array in memory i just now have that array in coupon clothes and number clothes and they can each operate on it and they will each modify that same array it's basically having two pointers to one array um it's just that they're different branded pointers maybe and then if i wanted to i could also import pi torch i won't in this example but i could do torch.tensor and give it one of these things i have to explicitly say this is going to be a gpu tensor because pytorch will do cpu and gpu we just say okay this is can you make me a cuda tensor from this array and pytorch will also just create a new pointer to this array so you can like hand around between these things you can do some steps in one tool you can do some steps on another tool this is all part of the goal of like as you can see gpu computing will give you acceleration it'll give you more performance but it's up to us to come and meet you where you are in terms of the tools that you're using to kind of just make your lives easier so this is one of those places where we're really investing in like interoperability and everything to make your life easier i suppose so yeah sure uh if you got out of memory error i would either restart with your notebook kernels to like or stop up your notebook channels to release all the memory from pointers or make the array smaller if that array is too big that's like an eight gig array so you i don't know what if you're on the binder you should have got 16 but okay so yeah let's now head back into into code world and have a look at coupon we go for about two hours and we're about we're just over halfway through the material the material is really going to ramp up and get a bit faster i might go on a few more tangents but thank you all for still being awake during your fourth tutorial uh on a wall after that so let's talk about coupey we're basically going to leave behind all of the horrible new words i introduced threads and blocks and all those kinds of things and we're going to move up a layer of abstraction to just numpy we're going to move up to the numpy layer so if i import numpy in coupe i i'm also just going to i'm going to do you'll see me do this a bit this is basically just so i don't cheat in any of my timings i'm just telling python to wait for the gpu to be done because sometimes coupon is helpful and like asynchronously dispatches stuff and then carries on um and so i'm i'm harshly telling it don't be helpful here let's let's actually see what these things do so if i wanted to in numpy uh create a big array of ones i would do it like this i would allocate this array of ones or take you know half a second or so to create me an array on the cpu i can also do exactly the same thing on the gpu create myself a big array i've got my little synchronized here just to make sure we're actually timing this accurately if i run this it only takes 110 milliseconds to create a big array of points because we're using all of the threads in our gpu to just push all of those ones into our memory so we get some little performance increases just in terms of constructing big mt arrays we can also do all the same kind of standard numpy stuff you would expect so we can multiply our cpu array by five we can multiply our gpu array by five just goes a lot faster same thing much faster this is kind of coming full circle back around from like right at the very beginning we were comparing bad python for loops with poorly written gpu code with numpy now we're right upper coupe i which is just well-written gpu stuff you're like at that next level of performance but i think it's i mean hopefully it's just been interesting to see how gpus work and how you can write code for it but also it's useful for you to have all of these concepts in your head that we've introduced so that when magic stuff happens like this for you and stuff just goes really fast you understand maybe a bit more about what's happening and when things go wrong you maybe understand a bit more about digging back out from failure states so let's also just have a look let's take um our array and just do more do some more operations let's do a couple of sequential operations um these are operations which if we did them in our kind of un sophisticated way with number at the beginning we would be doing lots of memory copying back and forth to the gpu right so if i do this in the cpu uh it's going to take 511 milliseconds if i do this on the cpu coupe i should do intelligent memory things for us it shouldn't do any of these um you know unnecessary copies so i run this again this is like less than an order of magnitude faster no more than obviously order makes you faster um because it's just done these kind of things i think you can see the utilization kind of ticks around we can see what's going on occupy has not 100 coverage of the numpy api but it's got a good chunk of like the most common stuff um if you run into things that are missing you can open issues and that kind of thing but they're very responsive and fleshed out um most of the time you'll find what you need and so it's got like a whole implementation of learner algebra stuff you could do like an spd if we do that in numpy that might take 292 milliseconds now if i do this on the gpu we might end up waiting a little bit longer the first time we do this and this is all around setting up that memory moving that memory around setting up context and doing kind of um bad things with small amounts of data but those things are done now so if i do this again um it should get faster maybe faster maybe it's topped out there because it's still possible to do things in coupon that are slower than numpy so you kind of got to think about am i using big arrays am i using things that are like hard and slow if i'm doing stuff that's already fast performing in numpy great use numpy numpad's awesome but if you're starting to feel pain around performance explore coupon as an option because on large problems it'll be good at solving them we're at a point where most of the things are equivalent or faster um some things are just algorithmically not possible but yeah it's kind of just it's been implemented and it is there and you should see some gains um sometimes these gains are done through like intelligent memory management rather but yeah there's some things like you know i come from like weather and climate where there there's lots of things algorithms that just make no sense to do in parallel they're hard to put on a supercomputer or an abc where you're just kind of stuck in a single threaded land and not everything's going to be great as well it's down i guess we tend to like if you go on the rapids website there's like the graphs of like this is 10 times faster this is a million times faster this is what you know we've profiled a whole bunch of stuff some stuff is in the millions of times faster range but other stuff is going to be slower than that that's fine to just keep using the cpu for those things is there any question oh so i know i'm just i'm just this is this is everything you need to do it i'm just doing this to make the timing accurate if i took this away it would probably falsely say that that was oh maybe not sometimes it falsely says things are like faster than they are but that's literally just there for my showing you stuff but don't worry yeah so i've just imported coupe i as cp in the same way you would import numpy as mp you can use most of the same stuff also to move on to the like next little bit numpy can do dispatching to other numpy like things so you can get into a state where you import coupe i you use coupe i to create your data and then you just use numpy like normal right and we can do numpy dot in our svd and give it our coupon array and numpy will look at what it's been given and go that's not a numpy array that's a coupon array coupon you do the thing and it will just handle dispatch and hand off to coupey to do the thing and the results will come back up so you can do mp.whatever on a coup array and it'll behave exactly the same so if i just run this little example here it does the same as as doing cp so this is a bit like with a cuda ray interface you can pass data structures around you can also just use you can use coupon arrays in places where you would normally use numpy arrays quite a lot of the time and that will be fine um there are some caveats i i did a bunch of work with the x-ray folks around using coupon within dusk and within that and like there were places where you know the tools were doing numpy dot as array and trying to convert the arrays trying to make sure the arrays were numpy arrays sometimes sometimes that's because you want to pass a scalar in um and it's just like oh there should be a numpy scaler so it just tries to convert it and that was clobbering the occupy array so there's places where work needs to be done but often it's just it's totally valid to just try passing a coupon array and a lot of the time you'll have success and sometimes it's just a bit of refactoring to get around errors that are coming up so the last little bit here is the questions we had about devices before so basically everything we've done we've just let it do its thing it's put everything on device zero you've already got one gpu so i haven't noticed but with coupe i you know and all of these tools but with coupon you can say i want you to create an array on a specific device so instead of creating it on device zero can you make me a big array on device one wasn't that big was it still not that big but you can see i made an array on the other gpu this time by just specifying which device that that's on you can also tie yourself in nuts i guess with this if you have memory on different gpus you have to think about how does that memory move around um but in a little bit we'll come on to doing multi gpu and multi-node multi-gpu work so i've got like a very short uh exercise here for you to just go through coupe i um maybe test your numpy knowledge a little bit just try and do a few operations just to see how familiar it feels doing in coupons so let's give this five or ten minutes please use your post-it notes let me know when you're done or want to move on i'll wander around if anybody has any questions so just check your hand out and move on to qdf so this one was reasonably just exercising the api and showing similarities to numpy so we can use a range to create a big array as we have done a few times we can have a look at how big that array is right that's kind of like a four gig array roughly and see how many dimensions it has we can see the size of the array the shape of the array and then we can use standard operations like linspace to calculate that that kind of array so this occupier can assign these arrays for you directly on the gpu these don't have to just be ones and zeros and whatever i've done before you can use kind of normal stuff you can do random and you can do all kinds of like sorting this would be like a paralyzed sort i can see the gpu it's quite busy doing that sort then that's busy second time i guess you can do reshaping so hopefully that's kind of giving you a bit of an insight into how using these lower level tools we built up these apis that look and feel familiar right but they're just leveraging the gpu where they can to make your life easier with more performance so in that vein let's move on to qdf i'm actually just going to restart all of my channels because i'm eating up my memory a bit here but if i import qdf um qdf is pandas as coupe i is to numpy so it is this data frame library that is is gpu accelerated but if you dig under the hood um the arrays that make up the data frame are coupon arrays and they're kind of built together so you can use them in conjunction in the same way that you would use pandas and numpy together you can use qdf and coupy together so let's kind of have a look at this api and explore some of the ways that it's similar so if we import pandas as well as important qdf we can do our read csv in pandas to load in a small kind of data set here we can do exactly the same in qdf read the csv in and you'll get something there's the gpu setting up there we go all right so we get exactly the same looking thing right this is a data frame um you look at the head of it it looks exactly the same you can see my memory is ticked up again so this this data frame is sat now on my gpu when i call head it just grabs the head of the data frame and just moves that back to the cpu and it actually does two panels under the hood and then blocks out the nice representation of pandas here so we can see it's the same kind of thing um we can do all of our normal operations that we could do we can change the column names we can drop columns we can kind of play around with our data frame we can see it's got over a million rows we can do things like oh i want to take this is like wikipedia page view data so there's a column of like which language of wikipedia it is we can just count the number of pages that are in english and these operations are running on the gpu this should just look like i'm doing some pandas stuff but this is just happening on the gpu i could do a group buyer i could group by language and then do a count on each one of those i did that you can see my gpu utilization these aren't hugely taxing it because this isn't a hugely big data frame because i want you to run it in binder but you can see all of these kind of data frame operations are working and doing the same kind of thing now yeah let's catch up together yeah so we can kind of do our nested accessors and all those kind of things these arrays behave the same have the same interface but i've already like had some questions about like what about the core things that might be different here what about like d types does it support all of those d types does it support the same kind of things and it has like a good overlap of the api it covers like a lot of standard stuff it has most of the standard d types that you would expect but i think i'm not sure if charles do you know do we update yeah okay so we can support like dictionary types inside data frames but these are like recent that's a recent edition i think um and like this is kind of ever expanding and i know like panda support is expanding in terms of types of supports and kind of catching up so everything we've done so far on the gpu has all been around numbers numbers we've been playing with arrays and i think something if you're kind of in the gpu community gpus have a reputation of like not handling strings as well or not having as mature support of strings and pandas is a heavily string based tool right a lot of stuff you're going to be doing in engaged frames of string base and so we have another package within rapids called q strings which is trying to give this high level string support in gpu world because i think it's funny like strings are something you like to take for granted in python and another language but it's something that is like non-trivial compared to dealing with numbers but because we have good fast string support on the gpu we can do things like uppercase and lowercase um and all your kind of normal string operations and those operations are being paralyzed on the gpu so if you're going to go over a column and you're going to up case everything one thread will get a row from that column and it will just work through an uppercase and you know we've got all the stuff written for doing doing those kind of things and q strings is like a really interesting library because we see these things being used in different ways i remember somebody telling me a story about um data like data formats and how people store data like a really popular way of storing data sadly still within the pandas community is csv which you know is just like a fact of life i guess is having to use csv and one of the challenges around csv is is a string based data format and when you read in a csv file you just have to keep reading until you get to commas and new lines and kind of store stuff but then you have all of these strings and you have to do a type inference on every single one of those because there's no type information about what's going on in there and it turns out gpus are actually really good at that job because you can just parallelize that it's a hugely paralyzable problem is just look at every one of these strings and do a type inference on there and so reading in data with qdf from csv can be really really fast compared to pandas because we can hugely parallelize this load so if you are stuck in a land with csv you can use qdf to get huge reading performance maybe even use qdf to write out some parque and then we can switch back to pandas like we've had folks come into the rapids community and just use qdf really heavily for bulk converting csv to park a and then moving on with device so there's a hand i mean yeah it's parquet seems to be like the most popular that we see and get asked about and like we've driven a lot of support into that but like we also have like an awk reader and and you know we have other formats support parque is just the one that we seem to hear about more often and get demand for um obviously within like the geosciences community net cdf hdf5 like is a popular format um like a there's like various things but yeah park a packet comes up a lot so the other thing we can do in qdf is we can do udfs so you can write a function and apply it over your data frame and these functions can be pure python functions or they can be the number functions that we played with before and probably a few other things as well but we can just use apply to do this so if we do like a really basic udf here um if we want to take a number and just do like some filter on it so say if if this number is less than five we're just going to round it down to zero and just chop everything off at that point we can write a very simple little bit of python to do that and then in qdf we can do apply here right so we're going to take the requests column number of requests for a page and we're just going to apply our udf over it um uh we can see these all get chopped off because there's whatever things but when we call this apply that function is being taken it's being compiled down through number into a cuda kernel all of the configuration around threads and blocks and all of that kind of stuff is being decided for you it's being executed on the gpu and the result is being assigned back into the data frame so this is why we did like a whole bunch of setup so you can see it's actually really like quite neat you can write a basic function you can give it to qdf and it will do all this magic that you've been doing manually on your behalf and kind of set things up for you but also if you have reason for kind of getting under the hood and wanting to to dig down you can also just write these things directly if you import a number you could write your own kernel where maybe you're going to take you've got in column and out column these are the you know if we're working on like a series of data in pandas in qdf you'll be given a pointer to the whole column and you'll be given a pointer to a new column that is kind of created for you in in memory space and that column is going to be replaced at the end it will swap the pointers around it's kind of giving you the input data it's set up the output data for you it's given you pointers to both of those things it's giving you any arguments that you want to happen uh based on your your function call and then you do your same kind of stuff you do you know get my index which which row do i operate on within this column and then you do your kind of thing we were doing before you take some value out of the input you do something to it you put it into some place in the output and we can run that on our data frame using the for-all call so we say for all the things in this column um we're gonna oh no it's on there wrong way around it's on the coo on the function itself right we have our kuda kernel we do the kernel dot four all so instead of doing our square bracket configure call it we're saying like for all and then giving it some like data structures that it can operate over this is like another number feature we've not touched on yet but we're going to say take for all of the things for all of the things in the length of the column we're going to give it our input column we're going to give it our output column we're going to multiply by 10 and we can see we get our audio column so that kind of handles that for you and then we can also do rolling windows so if we want to do some kind of view over our table where we want to like roll over it and do some kind of neighborhood operation we can define a function like this where we say i'm going to do a neighborhood mean i'm going to take in a window this window is going to be an iterable and we can calculate something say i want you know if i want to do a mean i'm just going to have a counter i'm going to iterate over the iterables in my window and i'm going to update that value and then i'll divide it by the length and then we can do our column here right our data frame dot requests dot rolling for rolling mean and we can give it kind of the window size and the start position um i forget what true means here let me just remind myself uh center right so this is when we roll over the window who do we update in the output do we update the top or the bottom or the center so we're saying the center here so we're gonna roll over that and we're going to apply our neighborhood so these are how we can kind of create little functions yourself that you're going to apply over your code and these will be just in time compiled down to like fast gpu stuff that can then be run for you so we're doing quite well checking through this stuff we've got two more notebooks we've got about an hour and a half after the session we've had a few breaks so let's let's dive straight into the qml notebook um and then maybe take like a 10 minute break at the end and then we'll go through i'm going to do a bit on like dusk and multi gpu so i might spend a chunk of time talking about dusk before we dig into that but then we should be kind of wrapped up a little bit ahead of time you know a bit of time left back to go find some folks for dinner so let's talk a bit about qml kumao is the the psychic learn of this ecosystem it follows the same api and there's a lot of stuff that should be familiar if you psychic learn i appreciate this group there was like fewer hands up for psychic land but let's dig into this anyway you can do some cool stuff so when you're working with scikit-learn it's common to use numpy and pandas in concert with their right to do your data loading and manipulation and all that kind of thing same exists in this ecosystem you would use qdf to read your data and pass it to go so if we read in some data here we have some like health data about some people what we're going to do is we're going to try and train a model a very basic thing to predict whether or not somebody has diabetes based on some other attributes about their health so we're taking some data here i think this is this stuff is pulled from some like kaggle examples we're gonna first we're gonna drop off our outcome column um as to whether they have it or not and we can have a look at that so we can see we've kind of got this binary array of whether this person has diabetes or not and then we can use kind of standard things that you would expect to see in psychic learn so if we want to split our data out into like a training data and a testing set we can use test train and split same as you would in the cycle and we're just importing it from qml here instead of from scikit lab so if we just assign that out decide we're going to use 20 of our data for doing that qdf is going to load the data and it's going to hand off to code ml that will split that out we'll have a few data frames here for running through and then we could just train like a neighbor's classifier here and do that with qml that was like way faster than i expected it to be okay it was really fast to train that one um and then we can do predict on that and give it some data and we can scroll right so this is kind of very similar this is you know this has given us like a 69 accuracy it's not the best model in the world but the main point i want to show you here is that it's the same api it's the same interface if you're used to using scikit-learn um using qml it should feel very simple again if there's things that you're using that you find don't exist raise an issue you're kind of just trying to gather like priority more than anything of like how can we implement all the things that folks want but we hit the majority of stuff that our users are asking for at the moment so that was like a very brief overview of come out yes well i mean i guess like that in that context we're talking about like machine learning stuff is mostly like whacking straight lines through lots of data kind of level of of things which you know psychic lens bread and butter whereas you know it's it's useful in conjunction with like deep learning as well but like pytorch is great we consider pytorch to be like part of this ecosystem we can cast between all of that it's all i think that's the thing like rapids is a thing where like it's an nvidia initiative right to contribute to the data science ecosystem and the pi data ecosystem but most of the time we're just contributing to existing open source projects whether that's contributing code to pi torch or to coupe i or or building our own tools like udf and then just making sure compatibility is there telling people about it trying to make more performance there because at the end of the day if all these tools are really good and they're really compelling then you're all going to buy nvidia gpus to use them and then we're happy right that's kind of the plan so there's got to be a plan so okay that notebook was much faster than i remembered so i'm just going to dive straight into desk um and then we can kind of wrap up so everything we've done so far has been on one gpu we like touched briefly on how we can like create arrays on other gpus but what happens when you are really pushing the gpu hard you're pushing you know you're seeing a big red bar on utilization and it's just busy doing stuff well i i encourage all of you to get to that point before you even start worrying about anything i'm going to mention in this notebook and it's like a long road to go to to being at that point where you are really saturating like a high-end graphics card but once you do get to that point you're then going to want to break beyond that and that is great you can do that you can add multiple gpus into your machine you can buy multiple machines and cluster them together you can have a nice big super computer with lots of gpus in and you can run basically the same kind of code that we've been talking about here across those machines leveraging all of this hardware together as one in concert um but this is adding a layer of complexity on top and if you can get gains without doing that then you should do that first by this kind of we talk about this a lot in das whether it's cpu or gpu um do do whatever you can do to squeeze the most performance out of what you've got on a single device single machine whatever before you start worrying about distributed computing because things do just get more complex but eventually if you've got big enough data and big enough problems you'll get to this point where you'll need this and this is where das comes in so apologies if you came into the dash tutorial you know what dusk is but let's kind of do a bit of an overview of das dask is a python library for scaling out your python code the way that it effectively works is it decorates python you can decorate python functions or you can use the built-in collections to lazily compute code it lazily executes your python functions and it does this by building up a task graph that describes what the computation would look like if you did it so you can build up like a series of function calls those functions have inputs and outputs um and then when you get to a point where you say okay i want this to happen that graph will be passed to a scheduler the scheduler will pass tasks to different workers those workers will do the thing and then it'll get combined back together into a result and so this is really powerful if you have a laptop with more than one cpu core maybe you're not using numpy or something that's already got multi-core capability you can really just leverage your local hardware by using task to like scale out your python calls in a distributed way but then also your code doesn't need to change if you want to add another machine and another machine and then move to the cloud and get some vms and get bigger vms like everything stays the same you just make your cluster bigger you have to start worrying about how those things talk to each other but um the code stays the same so as just like a very small example here i'm going to create a distributed dash cluster this is my cluster object this is just going to fill my machine with das basically using local cluster it tries to do things intelligently here so on my machine it's created four workers with 12 threads each no oh yeah this is smaller than i'd expected okay anyway has created four workers with 12 threads and nearly 100 gig of ram because that's what's in my machine and i can see my workers here it's just decided how to carve up the cpu and stuff now once i've got the cluster having this client object existing in my global scope is enough to tell anything that ask does to use this cluster um but also we can interact with that client directly so at like his most lowest level we could just say das client can you please just send this function call off to some worker to calculate this for me i'm using the futures api so we can just do ten plus one that's executed on a dash worker for me or i can use the high level collection so i could use dash array which looks and feels like numpy and we could create a big random array with a thousand elements in we have to chunk this array up in order for daft to be able to distribute it it's going to like put pieces of this array on different workers those are just numpy arrays but it's going to put all these little bits of array around and then when you do your operation on it like taking a mean it will do like a mean on each chunk and then aggregate the results together and do a mean on those to get your overall answer so it's kind of breaking your work down into little pieces um a lot of work has gone into these collections to try and make them as performant as possible you notice there's like a parallel here what i'm talking about you've got gpu acceleration with things that look familiar and then you've got das distributed acceleration that looks like things that are familiar and what we're kind of building up to is like you can smoosh those things together so you then get really fast hardware and lots of it to do distributed computation so if i just run this array example um i have to explicitly tell das to compute this thing because dash tries to be as lazy as possible it tries to not do any work if it can help it um it'll only do things if you actually say no i need the answer to this before i can move on here so um you have to call dot compute we can see we've done this operation this is run on desk now that was like a very very high level overview hopefully you've got the concept that we have high level collections we have this distributed api that lets you kind of chunk your work up and then to get that um to get these clusters you need to actually start the dash processes now when i did this before in this little bit of code here um i let das do this like auto config thing i didn't give it any configuration it just had to look at my machine and made some stuff what this basically does under the hood is it starts a dash scheduler you can run this on the command line if you just do dash scheduler and then start a dash worker and points it to that scheduler um the auto inspect like made a few of those just based on my machine but we can do that in like lots of different ways and lots of different places as long as you've got somewhere with a python environment that matches the one that you're working in that has disc installed you can just like ssh in or submit jobs or whatever you know hpc cloud it doesn't really matter where you are as long as you can run a command you can like pull a dash cluster together but that's not very fun really so we also have these cluster managers like local cluster which we've seen already that tries to wrap that up for you right so we can from das distributed we can import local cluster and we can say make me a local cluster which is what the auto config did but we can make this this cluster automatically and we also have like a whole bunch of other cluster managers so if you have a kubernetes cluster we have cube cluster if you have a hadoop cluster we have yarn cluster if you have various hpc systems we have like sg clustering and slam cluster and all these kind of cluster managers that are just aware of the hardware they can run on if you're using the cloud we can like spin up vms to start the processes on if you've got containerized platforms we can start containers it doesn't really matter where your stuff is you can run that there like super popular one that we see come up a bunch is ssh cluster which just sshs to other machines and starts to ask processes on them and connects them back to where you are it's great if your colleagues have got desktops or workstations that they're not using enough you can just steal those by ssh into them and then strapping them all together into a little ad hoc super computer it's great it's really helpful for doing these kind of things but by default desk is kind of staying within the pi data ecosystem right it's numpy it's pandas it's psychic learn it's these kind of core libraries and so if we want to do gpu stuff we have to give das a little bit of knowledge about gpus and we can do that ourselves right we could start with desk workers and tell them about the environment we could set config we could do all sorts of things but again that's not particularly fun um because there's like a few things we need to consider one is that each das worker should have exactly one gpu and so as functions get submitted they shouldn't be having to worry about distributed gpu stuff they should be doing the kind of automatic things you were doing where whatever we're doing happens on gpu zero if i've got eight gpus in my machine there should be eight workers and each one should just be able to see one and it should think of it as gpu zero and it's trying to do the like less complex version of things and we're just kind of aggregating those so we need to start a few workers we need to tell them you have this gpu we actually like mask out the rest of them and say ignore these this one is is gpu zero now um we also need to tell a bit about memory management and how things are configured and you can do that by hand but if you don't want to we have another package called das cuda that lives in the rapids ecosystem which just gives you more cluster managers and more tools to start things up the two main things that you'll see is local cuda cluster this behaves a lot like local cluster that i showed you that inspected my cpu and started some workers but instead that's going to inspect my gpus and start some workers so i have two gpus in my machine so this has started me two dash workers and it has done all of the configuration each worker thinks there's only one and then a different one there's also another command called das cuda worker so to replace das worker if you start things up in a more manual way or you want to start things up by using any of the existing cluster managers like cluster or whatever you can just say to it or you should run das cuda worker instead of dash worker um to do the like auto detective hardware um and start like sub workers on on gpus and we can connect a client to that and then we can carry on as normal right we have this nice das cluster it has gpus now i have these two gpus in my dash cluster and i can i can tell it to do stuff so in a very basic way i could say here's a is a number kernel this is a very unsophisticated one it's just gonna count forever and not do anything um but i can just say to das look submit my kernel this call is the same as our number call before we're doing the config fit we're saying how big the shape and size of that number kernel is we're not actually invoking it because we're going to let that do that later we're just saying here's the here's the number kernel can you do that for me and it will send that off maybe you saw my gpu flicker a little bit there and that has finished so we've kind of taken some gpu stuff we've handed it to task that's done the thing it's given us the answer back so that's great that's a great way of kind of dispatching what we've done already on to remote machines but then we can also start mixing our collections together so in dusk we have dash ray that pretends to be numpy in rapids and gpu ecosystems we have kupai that pretends to be numpy but what if we told das your unit inside is not numpy anymore as kupai can you just can you do your high level desk thing but just use coupe coupon as the like smaller arrays or or the same with pandas right that's data frame don't use pandas use qdf as your internal representation and again there's you can do that you can definitely do that you can tell ask what to do there is a bit of setup and config that you have to do to make that happen so to make your life a little bit easier if you install qdf you also get this das pdf package that you can install which is basically just all the data readers from qdf that have been pre-configured to do the das thing so instead of getting back a qdf data frame you get back a das qdf data frame so we have a look at that um we could create ourselves an array i'm just using uh from delayed and a little noddy function this is basically the creators a big array here a big data frame with many pieces in it it's going to have 30 partitions a big stage frame chunked out 30 partitions each of those partitions is the like demo time series from qdf so you've got kind of made up this big aggregate thing now but we just have one data frame right we have this gddf object this is a gpu distributed data frame it's got all these bits of data they're sitting on gpus in memory these partitions are probably evenly spaced across the gpus they're quite small so you can't really see them on the size here but then we can operate on this data frame like we would with any other data frame we can call head on this and das will go to the first partition and say can you call head that is a qdf partition so it will call the qdf head pass that to das that's we'll pass that back to us it kind of looks magic and we just get the head of our big data frame back we could also do a length on our overall data frame this again das will say to each partition can you do a length and then i'll aggregate those for you if we do a length on here we should maybe see both of my gpus are doing stuff now right these chunks of this overall data frame are shared between my two gpus they're each going to do stuff this is how we can start breaking beyond and then you know we could do a group buy or some kind of more interesting operation just to make the gpus busy and to show i've got two here if i had eight here eight would be flickering if i scaled up my cluster to multiple nodes i could have hundreds of gpus with chunks of my data frame spread across them and then do a big operation that's going to leverage all of those so that's great that's fun but the downsides to doing this is these workers these dash workers when they're doing things everything i've done here so far is like a uh reduce right so each partition is going to do a thing it's going to get an answer it's going to treat those up do the thing again to those and then give me the result but what about operations where i need from here to speak to here and i need this to speak to this and you're going to have communication going on within your data frames uh the group buy is an example of one that that would have happened now dasc uses tcp for all of its communication it listens on a port the schedule and workers talk to each other over this layer and when a worker wants some data from another worker it just opens this connection by the tcp port pulls the data across that's great it works we've just seen it work but that's quite i like in cpu land that's fine but when you're using like accelerated hardware you're leaving so much performance on the table because you're having to go all the way through the network stack together say i've got the two gpus in my workstation at home i've got my motherboard with two gpus plugged in they're on the same pci bus plugged into the cpu but for one to talk to the other it's going to talk up to the cpu into the linux kernel into the network stack to figure out what's the ip address and where am i going to go oh it's here you're going to come back out it's going to pop back down the pci bus into the other gpu right it's a very big round trippy kind of way of doing things it works but we can do better so in order to kind of resolve that we have like nice hardware features right they're on the same pci bus there is like a pci switch i don't know a huge moment about pci right but it's a bit like a network they can talk to each other on that bus they could just talk directly by a pci i've also got that little like mv link bridge clip thing plugged into the top of my gpu so that they can do mv link direct communication between each other that's like another protocol that's on the table that they could use if they knew about this and then maybe you're in a data center where you've got many servers with these gpus in they might have infiniband network cards connected over some fancy network and supercomputer that kind of network communication can also completely avoid the cpu the gpu can talk to the network card it can pop out of the switch back into another machine through the network card straight over pci into the gpu cpu doesn't even need to know the talking these kind of communications can happen and they do happen they're very common in the hpc community but we can do this here as well using ucx so ucx is like an alternative network protocol that we also support in dasc that basically has a look at your hardware topology between your dash workers in this context and it says right tcp is here it's fine we can talk about tcp but can we do better can we upgrade this connection so upgrade to pci express it will upgrade to infiniband it will upgrade to mvlink and so you know you might be in a setup where i've got 16 gpus eight in in each server i've got two servers right so these gpus can talk to each other really fast on every link and then between server they can talk really fast on infiniband and ucx will just kind of figure all that out so if you just set your dash protocol to use ucx instead of tcp which is just like a flag that you set you have to install ucx and you set a flag when you create your cluster this network upgrading will just start happening automatically so that's like a really neat way where we can just get so much more speed out of this and it's something that like you just can't do in the cpu world that's also has support for like other network protocols if you are interested um but like this one is like a great one for performance other ones are maybe maybe more targeted at flexibility i know on some hpc systems or clouds platforms you can't expose like a tcp port and it's kind of hard to get access to the scheduler but you can like proxy your jupyter notebook or something through like a web proxy we do actually support web sockets so that you can use das through the like jupiter proxy thing which is which is useful so um by default you'll get tcp but you can use other protocols like ucx and websites yeah so i think if i have a good answer for this there's a really good blog post on the das blog about like choosing chunk sizes but it's kind of in the context of cpu chunk size selection um i don't have like an easy answer i guess for that other than i'll make a note that we should probably write a blog post about this and get somebody who knows more than i do so to communicate that because it's a good question cool right the last little bit that i want to do it's probably going to take five more minutes uh and then i'll give you a bit of time back um an hour back i guess so we're gonna just come up to three out of nine so again thank you for being awake it's a warm afternoon um what we've discussed here so far is like how can i go from doing cpu disk to gpu disk which is great and we can do all of that but what happens when you're like well i've got one gpu or something i want to like mix it in i want to just dip my toes into like i'm going to do a big glass calculation but i want to see if i can speed up this step this task this bit of stuff can i rewrite this at number can i use coupe i here and just do a bit of it um we also support like resource annotations and desks so you basically say when this function is called by das it should be run on a worker that that can supply these resources and so that it's like an arbitrary thing like we would in this example i've got like oh i'm going to set the resources of my das cuda worker to two this is on my workstation i have two gpus so i'm going to tell the dash scheduler oh this worker has got two gpus so you should use those if you get any gpu tasks but this uppercase gpu uppercase gpu here this is just like an arbitrary word and this is an integer this could also be like memory or cats or like whatever number you want to measure for this machine but task will just make sure that if i've got a gpu if i've got a machine that's got this annotation i've got these two things and then somebody writes some code where i'm saying okay i want to annotate everything in here to say these tasks need one gpu before we submit them then the dash scheduler will just make sure that those tasks land on a worker with those resources so you can then create these kind of hyper clusters one of the places that i found this to be particularly compelling is when i'm loading data in from cloud storage or something like you know someone like s3 making a connection to s3 and pulling data from it is very very slow when we're talking about like gpu scale like performance and stuff like you have to go through http and tcp and tls you have to do encryption and like all sorts of stuff to like get a data stream out of s3 um compared to just like reading bytes from like hard drives and so it's really common in that space it's slow relative to that stuff but it's like infinitely paralyzable and so you can get incredible throughputs from a storage medium like s3 by just distributing data loading so if you just have like a thousand dash workers hoovering data out of s3 you can get really really amazing throughput and so that is is one of those situations if i'm reading data from s3 i want lots of das workers basically i'm trying to get to um but if i'm doing maths or computation i want gpu workers and so in my workstation at home i've got two gpus and however many cpu calls and if i do cpu task reading in from s3 it will use all of my cpu cores to do my data read in parallel and then it will use my cpu course to do the operation that i'm trying to do and i've tried recently like oh can i can i switch over to the gpu version and time it and see how it goes i was just doing like a mean of some x-ray data set reading in some czar thing from s3 and i s when i switched over to the gpu one i only ended up with two workers because i've only got two gpus which meant that that data read slowed down massively because i'm only making two connections for s3 and so doing my timings my calculation my mean that i was doing sped up by an order of magnitude but my data reading slowed down by an order of magnitude and so i ended up kind of roughly where i started and it wasn't that much better but then i could try starting a hybrid cluster where i have cpu workers utilizing all of my cpu calls and gpu workers using my gpus and then i can choose annotations to say right cpu workers pull the data in from s3 and then move that to the gpu gpu workers you do the mean and then give me the result back and that allowed me to get like the best of both worlds right i've got this nice parallels i'm using all my cpu cores and then i've got this accelerated computation using the gpu this is where resource annotations come in this is like a less mature part of the space that we're like actively working on to make this smoother but it does work today so that's pretty much brings us back full circle to the beginning thank you all for still being here i guess um hopefully this has been interesting to you in terms of actually running some code on the gpu getting to grips with what these things are i've introduced some new words um you may be thinking i never want to hear those words again but like it's it's an interesting um space that's like under active development there's a lot going on here there's still a lot of work to do but please i encourage you to like try out these tools if you have gpus go and see how they fit into your workloads raise issues get in touch with us um there should be links around in places but like the main place to come through to us is our you know our website is rapid ai which has links to our github and twitter and we have like a slack that you can come and join and like just come and come and speak to us about what's going on we primarily focus on data science type workloads but like most of what we're doing is just broadly within the pi data ecosystem so um you know we've touched on all sorts of spaces here don't be put off by data science if you don't that's like necessarily see yourself as a data scientist um if you're a pandas user or a coupe a numpy user or any of those kind of folks like this kind of stuff should hopefully help you so with that i will say thank you very much and yeah i'll stick around for questions if anybody has anything they want to ask or chat about [Applause] [Music] oh yeah i'm going to turn off that binder cluster in like half an hour so if you want to save any of those notebooks i mean they're all on github but if you've made changes and you want those make sure
3,Introduction to Numerical Computing With NumPy,https://www.youtube.com/watch?v=bveHFn0G4Zg,um so before we actually dive into the slides i want to kind of provide an opening statement if you will of like why should we care about numpy like we have the python programming language there's like wonderful things like lists why should we care about numpy especially when we're doing scientific computing so our intro here so numpy provides multi-dimensional arrays and mathematical operations that we can do on these arrays so if we think about kind of vanilla python if i had two lists here just ordinary python so this is creating a list a and a list b if i wanted to add these two lists together this isn't necessarily what we're doing from what we would want from a mathematical standpoint as far as like an element by element addition this is a concatenation so if we wanted to do some type of mathematical operation that would add these element by element we would have to do a for loop or something like this right we like instantiate an empty list we'd say for value a comma value b and zipping these together we'd say result dot append val a plus val b or if you know list comprehensions right we could have done that all right val a plus o b for val a val b and zip right so this for loop isn't hard to kind of understand what's going on right we're taking each element of a we're zipping these two together we're taking each element of a and adding it to each element of b right just that's what we would expect from a mathematical operational standpoint and the for loop isn't you know difficult to write but it gets tedious what if we had three dimensions right what if we had this kind of multi-dimensional array what if we had all these different things it's it's hard for us to it wouldn't be fun to have to like work through this and type this out by hand and that's where numpy comes in so we traditionally it's a convention we usually import numpy as np it's an alias we can import this library as the alias the numpy alias np it's like if you were to import pandas as pd or something like that it's common convention in python and what we can do is convert these to arrays so i should say i'm going to type some numpy code for the next two or three minutes we're going to cover everything that i cover in this slide so don't worry if you don't understand it again this is kind of my opening statement and then we're going to go through all of these things extensively okay so we have this array and now when i add these two things together this actually gives me what i expect it's not a concatenation anymore okay numpy one of the main reasons that it's so powerful is it does something called vectorization so this for loop is occurring but numpy abstracts that away for us and it actually pushes that down into pre-compiled c code so numpy's very very fast it sits on top of c it'll push this down for us and we don't have to worry about writing for loops when using numpy arrays if you're using numpy and using for loops it's a little bit of a hint that you might be able to do something better you should rarely have to use for loops when you're using numpy it's because of this idea of vectorization we'll talk about something later on called broadcasting as well that's very convenient it allows numpy will do some optimization for us to allow us to define things in one dimensions and it kind of stretches arrays and adds them together for us and we'll talk about that in a little bit later on but this idea of not having to write a for loop getting that push down to c is something that makes numpy very very powerful okay so we have this this idea this concept of vectorization we have this idea of being able to have large multi-dimensional arrays and do these mathematical element by element manipulations on these arrays okay the other thing that's really nice is we get um data types assigned to our arrays by default on my machine when i create this array i have a 64-bit integer or 64-bit float if you're on windows it might say 32. we can change the precision we'll we'll talk about that in a little bit but we say that numpy arrays are type safe type safe they are homogeneous they all have to be of the same type so this makes things efficient as far as a kind of memory allocation where we don't have to do type checking when we're adding things together we're enforcing a specific type that on the back end the arrays are assigned that metadata and it's stored there now we'll cover this but if you notice my array whoops np array i have one two three these are integers we'll talk about assignment here um whoops later on but i want to introduce this to get it in your heads early on i'm trying to assign a float here maybe i should do it like a three or something 13. so what's happening here is oops i'm fumbling over my keys n p a dot d type there we go npa and what i tried to do so when i created a i created this as a list of integers and it stored the data type as an integer 64. what i'm doing here we'll talk about setting so python zero based indexing what this is saying is take whatever is at the zero position of a and set it to the float 3.3456 it truncates this it doesn't round it it truncates this because this array has been set up to store 64-bit integers so we call this type coercion we'll talk about this in a slide in a little bit but the point of this in this introduction is to say that these arrays are type safe they all have to be of the same type okay so if i had my array font one two three and i declared the data type up front if i look at my a now and set this this will come through as expected because my array has been set up to hold floating point values it's not going to truncate this okay okay here okay so we have this idea of vectorization very very powerful we have this idea of having metadata that stores the type of information that our arrays hold and then we also have something called ufunks with a c not a k universal functions these are functions that have been optimized to be to perform on numpy arrays it they perform element by element so numpy has a sine function it has a cosine function we have these things that can we can apply to an array that will function element by element and we'll talk about this later on but these are optimized optimized mathematical func operations that we can perform on arrays and we'll talk about a lot of these throughout the course and then the last thing that i think is helpful for us to understand or in the kind of opening statement if i look at the type of what this array is this is an nd array n-dimensional numpy handles multi-dimensions very very well better than kind of your vanilla python list so if i had this kind of list of lists lol okay this is a list if i say np lol it's really not a list of a list it's a numpy array so maybe i should just say numpy array if i look at this list look at this array if i wanted to grab this five here through indexing using a list i'd have to say grab the last the the row at index one and then i have to say of that grab the item at index one right it'd be really nice if we could do something like this and provide some type of just tuple or some type of way to grab this five here without having to index twice and that's what numpy allows us to do i could have done this but that's taking two indexes right indexing one to get the first the row at index one and then indexing again this is a more efficient way it allows us to handle multi-dimensional data much better so another reason that we should consider using numpy is if we have large dimension of data if you have you know an x and a y and a time domain or maybe you know you're in four dimensions it gives us the ability to do this an index much easier than with a list of our list okay so far yep both so yes you're saving some keystrokes doing this but there is um this idea of indexing in python this isn't just a numpy thing this is what we call syntactic sugar where there's a dunder get item a dunder set item and what's happening here is when you have one of these you're calling that once that dunder get is calling down that slice one one here you're having two method calls essentially you're selecting the first row and then of that row you're also selecting again so you have you'd have to do that for a list but in this case it's computationally more efficient to do it in one one shot so another reason see did i lose my i lost my link here one quick question yep what pointed into lol there on your last two lines sorry uh this one here so i said np array was equal to i wrapped it so i created it from the listed list yep so there's a scientific python ecosystem.png that i uploaded here has anyone used pandas matplotlib okay so the other thing that's really nice about numpy this weird kind of pyramid-looking scene numpy so this this is kind of the scientific stack when we think of python all these things operate very well together numpy talked about actually ships with pre-compiled fortran and c code that's why it's really fast it'll push down some of those for loops in there but pandas if you didn't know it is actually built on top of numpy arrays so when you store things in pandas there's a like.2 numpy or if you call like dot values it's going to give you the array back right so pandas actually is built on top of numpy so all these things interact very well together so if you're familiar with some of these the numpy array is really kind of the base array structure that we use in these other packages as well so it performs very well and allows us to kind of interact with that throughout yes so we're going to talk about that so if i say one here it's going to default to giving you that row same with the zero numpy has this idea of taking filling in things for us we can use a colon to do the same thing but it's inferring essentially what's happening in a two-dimensional case it's going to give you at that index the whole entire row in this case because that's the zeroth dimension whereas if it was in 3d you'd get the 2x2 same thing we'll talk about that in the slide coming on but it's a good observation we'll talk about how we can use colons and things like that to specify everything okay so just to kind of summarize one numpy arrays are very very efficient we have the concept of vectorization and broadcasting they're type safe so everything in the array has to be of the same type we have things called universal functions or u funcs and then numpy rays also perform very well with multi-dimensional multi-dimensionality so we are going to and and for these reasons that's why numpy is really the standard numerical library for python so anytime you do any type of mathematical manipulations or mathematical operations a lot of times you'll be working with numpy so just kind of a lay of the land what we're going to be talking about today i'm going to go through a lot of the code that i just typed in detail we'll talk about kind of what arrays are how to create them we'll talk about indexing and slicing arrays grabbing things out of an array we'll talk about some of the array calculations that we have available the universal functions then we'll talk a little bit about kind of the data structure the metadata how it's stored kind of in the back end how does numpy handle memory and we'll talk about some more advanced topics like broadcasting and some of the other things like reduce and accumulate and things like that okay so are we okay any questions we're all okay so far so slide five and talk about introducing these arrays some of this stuff that i've already introduced and covered for you but when we create an array on slide five we have a few ways of doing so so we're always going to use this np array method or function from numpy and we can put in there a list okay so in python the list is created with the square brackets okay so i can put a list in the numpy array function and that will return an array for me i can create a list on my own and pass that in right that's the same thing as doing the above and when i look at this array so again the type whoops type of a type that was because i created it up above python has a function called type i can see the type in this case it's an nd array so nd n dimensions we have a data type which is an integer 64-bit integer but i could also so notice what happens here you notice a difference between this array and the one i created in [Music] 59 that's right so in this case when i provided integers to my array numpy is going to say ah you're inputting integers chances are you probably want to store integers so it's going to default to a 64-bit integer here it's saying ah you're providing me floating point values you probably want to store these as floating point values so notice this trips people up sometimes there's a decimal point here these are floating point values so one one dot zero and python is a float one is an integer one dot no trailing zero is a float okay so that's why you might notice here if i were to create this one two three if i create b as 1.2.3. that's right so what's happening here provided an integer a float and an integer as far as precision goes yes so you might have 64-bit or 32-bit but you can set it explicitly by calling the d-type or calling as type and we'll show that in a slide here so a way that you can solve for that is being explicit upon your creation and not platform-dependent yeah because then you understand you know exactly what you're expecting in that in that regard thank you yes yes so um numpy has this idea of a numeric tower or a hierarchy right when i add two integers together i get an integer back when i add an integer and a float i get a float okay so there's this idea of a boolean will get promoted to an integer we'll get promoted to a float which will get promoted to a complex number okay so if i add an integer and integer together i'm going to get an integer if i add an integer and a float together we have to promote up to a float because the highest we're adding here and here it's going to promote up to here for the result if i added a boolean and a complex number it would promote all the way up to a complex number so that's what's happening here i have an integer a float an integer it's going to promote all these up to a float for storage okay so if i go back to our slide here um i have a data type whoops we have a n dimension that tells us our whoops n dim number of dimensions we also have a shape okay so i have four elements here my shape is four there's also a item size so my d type right this is there are eight bits in one byte okay this is 64-bit integers and i have um that means i have eight bytes in each item so the item size tells me how many bytes per item in my array right eight bits is one byte i have 64 bits i have eight bytes that's where this 8 is coming from i also have n bytes which will tell me how many bytes i have in the array so i have four items each eight bytes so i have 32 bytes this is all metadata that's stored with our array that's on slide four okay so slide five i'm gonna create two well i guess i should say if i look at a right i have 0 1 2 3. we're familiar with the range function in python you've seen this before whoops we can create numpy arrays using the range function we can also use a a range which stands for an array range so if you didn't want to type out 0 1 2 3 right you can use range you can also say provide a step size whoops start end next step start stop thanks if you don't have to type all this out there's ways to do this so a range array range is that function okay the other thing that's nice about the arrange function is notice the difference between this and this it's going to infer the type that you want back based on how you provide the start stop and end here so i'm providing an integer 5 here it's going to give me integers back providing a float here okay slide six so i'm going to create these two arrays i have a b we can do mathematical operations through vectorization it's always going to be applied element by element so we saw a plus b is always going to put these together element by element wise right so 1 plus 2 is t plus three is five which is four seven right we can also do multiplication we can also do a power so i'm going to create an array of floating point values here x whoops arrange not array so the other thing too is so python is any matlab users python is zero based indexed matlab starts at one right yeah so python is zero based indexed so um you can think of this as up to but not including the stop value right so start at zero go up two but not including 11 which is why you're going from zero zero based indexing for a number of reasons one is the number that you provide is the number of elements you're actually going to get so 0 to 10 gives you 11 elements essentially okay so i have our x here numpy has a lot of constants so you have pi euler's number some of these are constants but if i save the a scalar c as two divided by or two times np dot pi divided by 10. i have a scalar value here if i multiply x by c what's happening is it's broadcasting this scalar value so what we don't have to do is we don't have to create a c array that has the same number of elements as x and then multiply in order to get this numpy does that on the back end for us where we can just take this x array that has 11 elements multiply by that scalar and it's going to multiply each one of these for us so we don't have to create that array of 11 elements for c we can just allow it to do it on the back end for us so if i wanted to set x it's x times c right so if i want to update x in place and say take the x array multiply it by c and store that as x i could do this right i could say x equals x times c or i could use what we call an augmented operator i could say x times equal to c okay this here works with addition subtraction division multiplication all these things we okay here and then like i said the values of b already are still the same here because i did x times c here and i stored this i didn't update this i didn't say x equals okay oh sorry and then i can take this sign right we talk about the universal function here where you take a sign do these things okay any questions so slide seven might be a little bit easier to draw here so we're talking about setting our uh indexing arrays we'll talk about slicing in a bit but again python is zero-based indexed so if we have an array of zero one two three our index starts at zero and if i wanna say a zero we talked about this this square bracket is how we index or grab from the array that's going to come to the zeroth index and give me what is there we can set or assign using the same syntax so we're saying where is index zero set that to 10. now we type we talked about this we have to be careful here if our array a was set up to hold integers at the onset if we try to assign 10.6 a floating point value into that array it's going to truncate this it's not going to round it it's going to truncate it only take the integer portion and store it into the array there's a fill behavior so there's a method called fill where if you have an existing array and you want to fill it with values it's going to do the same thing as that type coercion where it's only going to take the integer portion here so we have this array a we're saying we want to fill this with negative 4.8 but because we set up a to hold integers it's going to truncate that and fill it if we didn't want this behavior do we know how we would if we wanted to be able to store floating point values how would we do that so there's a few different ways okay i provided integers here i can say a dot as type okay so if i have an existing array an existing array a and i want to change its type i can say dot as type and save that as a new array or at time of creation i can provide the d type so there's also yeah oops so you can provide this right you have it you can provide these as well if you wanted to i'm just being lazy and not typing out the extra it works for integers as well so you'll notice here i typed float i don't think oh i can that does work so you can do a string or the actual numpy object here provide that if you want to okay okay here so slide eight talking a little bit about multi-dimensional arrays so here we have this list of lists pictorially we have you know this 2d array here when we say a dot shape it gives this to us in terms of dimensions so the first dimension here is a two it's dimension zero we have two rows our second dimension four we have four columns now it's important here to understand kind of what's happening because when we start doing mathematical operations we can if i said that i wanted the sum of this array there's four different ways i could calculate that three different ways i could calculate that sum it's on every single value in that array right or i could say some across my columns one per row or i could say sum across my rows one per column so there's three different ways i can calculate a sum and the way we tell numpy how we want to do that is using something called axes or an axis providing an axis those axis correspond to these dimensions and we'll talk about that later on when we start getting into some of these mathematical operations so if i wanted to sum the the challenging part is the more dimensions you add you're always going to add to the beginning of your shape and as you add to the beginning of your shape your dimension whatever your dimension is you're going to add one to it so what do i mean by that it's a little bit confusing so if i had ah i keep doing that i have 25 values here my dimension is one maybe i should use 24. my dimension is one my shape is 24. i have zero i only have one dimension here this would be the zeroth dimension right my shape is 24. what's at the location zero in my shape tuple 24 0. if i reshape this we'll talk about reshape i have six rows four columns two dimensions whoops the rows are my zeroth the columns are my one one so if you think about the shape right we had six four i want to go across my rows i'd specify this dimension if i want to go across my columns i'd have this dimension here okay so we're adding to the front when we're adding dimensionality we're adding to the front and we're adding one to each dimension it's still confusing hazy better example do we need another example okay another example um a is we have a is our shape of a it's five if i reshape this to be one by five so my original uh original a notice the brackets here right it's only one dimension five comma when i reshape this there's two brackets here so this is like i have a two dimension i have two dimensions but i only have one row right two dimensions but one row or five column is different than just having five values a vector of five values when i reshape this and add one to the beginning that shape becomes one five i'm adding my zeroth dimension now is my rows and my one dimension is my columns whereas here i only have a zeroth dimension i don't have more than one it's just zero so make more sense when we get into multi-dimensionality and specifying but what i mean is if i have this a there's only one dimension if i say dot sum axis equals zero there is no axis one on this array this a array whereas if i reshape this if i sum this on my axis equals zero and i sum this on my axis equals one what's happening here is my a is now multi-dimensional my zeroth axis is no longer across now it's down because i have two dimensions so when i sum on my zeroth axis it's just something down here which is just giving me one one array back down now when i have my access across one because i'm in two dimensions it's going to sum these values across and give me my 10. i promise this will make a little bit more sense in a little bit but clear as mud for now right okay okay so we have this shape the number of dimensions is important it's how we're going to start doing our mathematical operations there's some more kind of metadata the size you just think of those as like the number of cells we have in our array it's the shape the product of the shape right so if i was four by two i have eight elements right and then we can get our dimensions indexing behaves much the same way but now we have to specify these in order of our shape or our dimensions right so dimension zero going down we're saying one which is this row here right so zero one we're specifying a one here which says grab in the zeroth dimension that row and then this three specifies this dimension which is this dimension here 0 1 2 3 that gives us this 13 here so that's what's happening when we index right here in our array and it returned it here we can set as well multi-dimensions right one three we want to set that to negative 1 and that will override that 13 in our array and make it a negative 1. now there's a comment made earlier that said hey if i say a1 in the multi-dimensional case it doesn't fail it gives me this whole entire row what's happening is numpy is going to by default give you everything in the zeroth dimension if you only specify that one that one value so what's happening here is because we're only specifying one here our zeroth dimension is going down so it's going to give you this entire row here the zeroth dimension it's like saying with a colon give me everything okay so a lot of times i'm going to code this live i guess i said so sometimes in numpy we'll get this behavior where we get like a lot of decimal places or we get scientific notation we might not want that we just want to see like some rounded values or things there's ways to change the display so how the array is displayed coming back to us this doesn't change this does not change how the data is stored in memory this only changes the display so this is only like coming to you as the developer as the person wanting access to that so if we have this array i'm going to go from 0 or 1 to 3 5.5 if i multiply this by pi if i multiply this by a really large value of pi or a really small value as well so so we have eight decimal points here we see that we have scientific notation by default there are two ways to do this you can set the formatting if you set the formatting you have to remember what the default was because you have to go back and set it to the default so you can get the defaults set it and then set it back to the default or you can use something what we call a context manager if you've ever tried to open a file in python you might have used the with statement right like with open the file name as fp colon something we call that a context manager so we can do the same thing here so i'm going to show you both ways um [Music] if i get print options so this tells me some things about how numpy is going to print the output this precision and this suppress are the two things that we're concerned about in order to change the way we have scientific notation and the decimal value so this 8 is the fact that we have one two three four five six seven eight decimals and suppress false means it's not going to suppress the kind of scientific notation so i can say np so i have to use get options if i didn't know that it was 8 and false by default right mp.set options i can say precision precision is oops set print options now if i look at my array a and i say np oops a times i pi see that i only have two decimal points here whereas before i had that eight right if i did a times np dot pi times 1 e to the 8 negative 6 i still get scientific notation but only two decimals and i can say np.set print options here suppresses true [Music] pp and then if i did a times np dot pi times 1 e to the negative 6 those are zeros because this is a very very small value it's not going to give me the scientific notation so i'd have to go back and set these again and now if i do my all that over again we'll see if we get the original value if i wanted go ahead correct correct yep so that the multiplication all that stuff is stored on the back end this is print options so what the way it's displayed yep now if you're like me and you don't really you're worried that it might you know mess some stuff up and you want to be able to retain things to kind of double check the way i do this is with a context manager and i'd say with np.print so notice here not set print options but with np dot print options and then we'll say um uh a times np dot pi oops print i want to print this and then i can still access this outside if i get my print options nothing's changed right so this is a little convenient way to just display things nicely but still have access to the default behavior without actually having to go in and set reset all that stuff again just kind of a convenience to know about okay here questions um so your dictionary the problem with a dictionary well not problem if i have a dictionary notice what happens here um it's taking the whole entire object as my dictionary so like uh a0 i can't index this that's weird normally what you'd want to do is something like dot keys or dot values and that'll grab the actual like things from this is probably you want to convert this to a list again it's a list but um it's got it's going to retain the actual like object that you provide it um usually when you're dealing with numpy arrays right you want like the numerical probably not like the keys you probably want the values but i don't think i think like from pandas if you're using pandas i think there's like a dot from dictionary or something to read it in as a data frame i don't think there's a numpy dot from dictionary okay all right we're on slide 10 we're going to talk about indexing and slicing so we've introduced this idea of indexing where we can grab you know one row or one item there's also a concept of kind of slicing where maybe we want like a patch of data we have this big array and we want to grab some patch or some field or like more than just like one value one row things like that we can do that through slicing so mathematically the way we provide these is kind of like what we showed in the with the range function we have a lower or starting value you have an upper and they have a step value and they're separated by colons mathematically this is up to but not including the upper so lower all the way up to but not including the upper value mathematically here that's what this looks like and the step tells us like it's like a stride for familiar with that term right like one or two how many we have now numpy like python the default if you don't specify a lower it's going to assume that you want to start at zero or the starting point if you don't provide an upper it's going to assume that you want to go to the end and if you don't provide a step it's going to assume that you want a stride or a step size of 1. so if we have this array i'll do this in code so we can see this so we'll do this slide then we're going to take a break so slide 11. so i have this array 10 11 12 13 14. the indices for this array right are 0 1 2 3 4. so if i said a4 we're going to say a2 we also have negative indices starting at negative one so counting from the back okay so i can provide positive integers going from left to right 0 1 2 3 4 or i can count from the back using negative negative 1 negative 2 negative 3 negative 4. it's helpful to use negatives sometimes it um [Music] describes intent right so if i had this array a and i said a3 that tells me that communicates to me grab what's at the third index right it doesn't really communicate to me the second from the last right i'd have to know that this array was a shape five in order for you to say hey a3 means the second to last one whereas this describes more intent for me so you'll see something like oh grab the second to last one or grab the last one in the list i'd have to know that this list had five elements in it in order to say a4 is the last one right if this had ten elements a4 isn't the last one but a negative one will always be the last one in my array okay so we have these negative indices sometimes it makes more sense to count from the back than it does counting forward so when we come back from our break we'll talk about how we can kind of mix these together yes and we'll talk about that so there's a i'll i'll end with this so i have a so we'll think about why is this true why does this work okay we're going to go ahead and get started okay so we are on slide 11. did we think about y so why does colon colon negative one so i go back to so you have an array a why does this work and why does it print from the back forward very good yep that's right so if we go back to our slides when we slice so this variable is like an array when we slice we have a lower separated by a colon an upper separated by a colon and the step size if we do not specify any of these there's default values the default for lower is zero or start at the beginning the default value for upper is go to the end of the array and the default for the step is one so in this case we said colon colon negative one which means start at the the start at the beginning go to the end but reverse my step size go backwards make sense good so we can also specify so we can how do i want to say this we can mix and match a little bit so i can say go to i say start at the 1th right so start at 1 go up 2 but not including the third index so start here 1 2 up 2 but not including this one so i just get 11 and 12. i can say negative 2 here right so counting from the back negative 1 negative 2 oops sorry negative 1 negative 2 up 2 but not including that so that's 11 and 12 as well you say negative 4 to 3 as well negative 1 negative 2 negative 3 negative 4 up to 3 up two but not including now a positive index so a positive index tells us that we're always going to count in our original array from left to right from left to right a positive integer as our step is always going to tell us that we're counting from left to right in our original array data so if i said from negative 3 to negative oops this is going to give me an empty array because i'm trying to count from right to left right i'm trying to go from negative 1 negative two negative three to negative four from right to left but i've specified a positive step size as one i really wanted to go from if i really wanted 12 to 11 i'd need to put a negative step size here and the negative step size tells me that i can read in the original from right to left okay if i wanted to grab things from like the second element on i can provide just a start that'll say right again i have two colon so i've said start at two and then colon for these go to the end step size of one all the way to the end step size also works with right every other every other one stride of two okay here so we can start getting into kind of these more complex slicing if you will where we can start providing patterns of how we want to grab things from our array so remember we have two dimensions here zero and one dimension zero dimension one so whatever i provide right when i'm slicing this is the dimension zero this is dimension one separated by a comma so in this case if i color code this in orange this is saying the zeroth row and this is saying start at index three up two but not including index five so i get three four here okay this blue this says row four and on so start at index four before and on to the end of my array and then four and on in terms of my columns so that overlapper these four values here okay so this colon here when we use a colon we use colon to separate our slices but we can also use a colon as a single so notice this is the whole entire thing this is telling me all the things that meme give me all the things everything in that dimension is what that colon means so this is saying all the rows but i only want the second column just give me all of these okay and then this last one we can take a strided view as well so we're saying this is an actual slice so start at index two every other in terms of the rows this is all of the columns but by every other so this one this one this one that overlapper these values here okay any questions here okay when we start to talk about slicing numpy does something as bit of a memory optimization for us where when we slice certain things they're referencing the same locations in memory we'll talk about that um in a little bit when we slice things we create something called a view not a copy of an array i'm going to introduce this topic we're going to talk about it in detail in the coming slides but just know keep that in your mind as a little reminder slices create views not copies what that means is we have a larger array we take a slice so we have an array a millions of data points we filter it using a slice or indexing to array b if we alter things in b it's also going to alter things in a because they're pointing to the same reference the memory reference the same pointer we can create copies or we can do something called fancy indexing which will always create a copy but for us numpy's going to try to be a little bit memory efficient not just create copies of data everywhere when we can reference the same memory as a view we'll talk about that in the next few slides but we can set so we've sliced this data right so this is negative 5 negative 5 negative 4 negative 3 negative 2 and up that's wrong negative 1 negative 2 negative 3 negative 4 negative 5. in terms of the negative index so i say i want negative 2 and on that gives me that 3 and 4. but i can assign that values and that'll put it in here and replace that 3 and 4 with a negative 1 and 2. so just like we've seen before we can set these what i cannot do is i can't set this so if i said let me show you instead of actually telling you what's going to happen if i said a negative 2 on gives me that 3 and the 4 i can set that as negative 42 negative 42. that's going to override those for me it's going to set those values i cannot say one two three here the shapes don't match so i'm trying to grab something that's only two elements long and i'm providing three elements so it's not going to be able to save that in there for me but i can use a scalar and that will stretch it across all those for me and fill all of them in okay so the shape has to match unless you're just providing scalar and it'll fill it in for all of those right because this is two values here the last two elements do something like that okay all right throughout the course we have some exercises some of them are called give it a try some of them are going to be longer exercises i want to give the opportunity to practice using slicing and indexing there's two ways you can do this you can have your ipython console open right and you can type a is equal to np.range 25 that creates 25 values and then you can reshape that do a 5x5 array you can type that if you want or if you're in [Music] the right directory if you go there's a give it give underscore it underscore a underscore try give it a try directory there's a notebook in there open that notebook these exercises are also in there with the boilerplate code for you so we run the first cell with a shift enter i'll import numpy the image is there for you you can run this put your code in here all of these have solutions so there's a little solution tab here if you click where it says that bold solution it's going to toggle that solution for you okay i highly recommend that you try it on your own do some practice before looking at the solution but if you get stuck no shame in looking at the solution for some help okay the bottom so on the slides it's orange in here it's yellow i would say in order if you want to try to go easy to hard i'd go yellow or orange on the slides yellow red blue yellow red blue easiest hardest okay so let's take five minutes we'll do this and then we'll review as a class okay we have about 15 seconds left is there anyone that needs more time or would like more time it's okay more time ready to review okay we'll go ahead and review who got all of them who was able to get all of them good job all right good job i wish i had something to hand out i'll give you a round of applause okay so we're going to start with the orange or the yellow however you want to think about it on the slides but we have our a what we need is our last row here now there's a few different ways to do this if i knew right i knew that i had five if i was five by five and i wanted to grab the last row i could say a4 that works it's fine i could also say a negative one there's also what we could do is we could say i want the last row and all of the columns or i could say the fourth row and all the columns for me personally because i know a has two dimensions this communicates to me that i want the last row and a is a multi-dimensional array okay if i told you this what would you expect to get back there without knowing a this could be a single number or it could be a row of numbers depending on the dimensionality of a right so sometimes if you're trying to communicate intent it's helpful to add that colon to say that you're multi-dimensional right because if my a was just that just gives me a number back integer whereas in our case when a is multi-dimensional we're getting an array back right so sometimes this is helpful to be able to show intent by including that colon okay next one is we'll do red the way i like to so let's clear this the way i like to kind of set this up is if i know that i'm going to do slicing i'll set up my you don't need this much white space but i'm doing it so that we can see it i know that i'm gonna have to slice this array given the red those columns so the first thing i ask myself in this case i have two dimensions do i need to slice my rose for red let's look at red do i need to slice my rose no so what should i put in that first dimension i heard it a colon that's right so i don't need to slice my rose right so i'm gonna put a colon here okay now when i look at this i say okay there's some pattern here that's skipping columns so i know i'm probably going to have to have a step size so i would just try to say every other one and see what that looks like right so we do that by the from start to end with a step size of two let's just see what that looks like well that's kind of the inverse of what we want right so i'm starting at zero going to the end doing every other let's start at the first one and do every other okay so the way you specify a slice so there's a little bit little bit of there's a few things that we can get confused on and that's fine when we sit when we specify a slice we use colons to separate our stop end our lower our upper and our step so these colons are at an axis or a dimension level right so when i have a separated here i'm specifying dimension zero and dimension one and this in here can be a slice this is a slice with a slice this slice notation in here this comma is telling me if i want to define what's in dimension one or multiple dimensions if i have more than one dimension these this comma is separating the slices that i want over those dimensions so if i had three dimensions i'd have to have comma comma here and then provide things here if i do not provide any of these commas it's going to grab what's at my dimension zero and give that back to me okay here so for red i have all of my rows the colon start at index zero and go every other i'm sorry start index one and go every other okay let's look at i was drawing sorry let's look at blue just specify the end of that slice and why does putting a four there work the same as just doing this right there's four there so so again this to me communicates go to the end of the array all the way to the end this to me communicates go up to but not including the fourth column right or in that direction yeah okay so let's look at blue do i need to slice my rows for blue yes i do for me i'm gonna start the same way that i did before let's just start with a step size of two maybe we'll get lucky and that's what we want right so let's do a colon colon two and look at that now to answer your question here notice i'm including a comma here i'm not putting anything there it's the same thing as if i would have said a column column 2 because it's not specifying anything it's assuming that i want everything in that dimension so these rows are not the rows that i want again it's kind of the inverse so i'm going to start at one these are the rows that i now want do i need to slice my columns yes so let's try colon colon2 that's almost right but i'm including the last one i don't want to include the last column i want to go up two but not including the last one or this or okay i would use this this this to me communicates that i want to go up to not including the last column every other to every other okay here good okay so we also have something called fancy indexing and before we get into fancy indexing i want to highlight the fact that we do have something called these are called slices we're creating views here this is what i was getting into when we talk about the memory management pointing to the same kind of reference when i create so i'm on slide 15 i'm going to slide 15. when i create this array when i create b as a slice of a okay we agree that this is a slice i'm using the slice notation 2 up to not including index 4. if i say b of 0 it's 99 well that makes sense you just set the first item of b uh-oh this makes some people feel not so good what's happening is when we create a slice we're sharing memory and altering the slice alters the parent as well we can check this there's a shares memory function if those two arrays share memory if you want to say i don't know i don't remember how i created this array i want a quick shorthand to see if i alter this array it's going to alter the other one this is a nice helpful shorthand the um theorist would say well you should remember you're creating a slice so you're creating a view i'll say this is a handy practical approach to trying to check whether or not you're sharing memory now we can explicitly create a copy and this won't be true so if i say c is still a slice but i'm using a dot copy method right here's c it's because well maybe this would be a better example here's my c here's my a if i do the same thing that i did before whoops c gets altered a does not they do not share memory because i explicitly call dot copy okay so if you want this behavior you can call a copy time you create a slice by default it's going to create a view numpy's trying to be efficient here where it doesn't want to try to keep copying the same information that it already has stored okay this can be good or bad depending on what behavior you want it's something to be aware of okay so we can call dot copy there's also something called fancy indexing that we'll get into so fancy indexing will always create a copy fancy indexing we can fancy index in two ways we can create a boolean mask and specifically describe based on that mask how we want to pull things from an array so if we had a mathematical operation where we said a is greater than 5 that will give us a boolean mask back and we can use that to then filter a so i can say something like this and we'll get into this boolean masking or i can explicitly provide not a slice but the indices that i want to grab as a list not as a tuple so if i say one comma two from a notice this is not one comma there's a there's an extra list in here i'm providing the indices as a list this fancy indexing so if i said d is equal to a one comma two this is d this is a this is fancy indexing providing a list of indices that i want to grab it will not create a view it will create a copy a different array same thing if i said running out of letters here these will not share memory okay so sometimes if you're not aware of this it can cause some major issues whenever you do a slice remember it's referring to the same data as its originating array the parent array if you don't want that behavior you can explicitly call copy you can use fancy indexing if you want okay here go ahead so we're going to talk about how numpy stores data on the back end later on in memory world there's no concept of like a multi-dimensional array right you can think of it as almost like tape where it's like it starts on this memory like tape and then writes data and you're pointing to that address right so this array like a five by five array or something it's not like five by five stored in memory if we can do things in numpy that can access that same data but just change the way we access it like changing the strides or changing the shape this is why a transpose is very very efficient because it's just swapping the axes and swapping the strides it's the same data buffer you're just swapping things if you can't do that if you um so the way you think about a slice you can describe it almost as like a pattern in your data every other one start here go every other stop here it's a pattern that can point to the same data because there's a predefined pattern that you can use to understand what you're grabbing fancy indexing is used to grab things that might not have that type of pattern so you might want to grab the first and the second and then the eighth like there's no way to describe that saying start here go every other one there's no pattern for that so the reason that fancy indexing creates a copy and memories because there's no way to access that same memory using a different pattern you have to create a whole new kind of memory buffer or something like that it can't access the same data and just rearrange some of the metadata to get you back what you're trying to do [Music] okay so fancy indexing on slide 16. if we have an array zero to 80 by 10 right so 10 20 30 40 50 60 70. i can provide indices here so my indices is a list and if i say 1 2 negative 3 i can index based on these indices right so zero one and two and then negative three negative one negative two negative three there's no real pattern that i can describe here if i were to say like a every other two or something like i can't get what i actually want that 10 20 50 right there's no predefined like pattern so i can't take a slice we'll use something called fancy indexing here now you don't have to like pre-define your list i could have said 1 2 negative 3 but it's easier to see that that's actually a list by defining it before i can set using fancy indexing as well so that has now set those values to 99 at those indices using fancy indexing so this is based off of so fancy indexing using um indices as a list so that's one way to fancy index the other way is creating a boolean mask so i can create this mask one zero one zero now i'm doing this it's a little bit different than what we normally do this has so in python booleans actually get treated as integers so when i create an array of zeros and ones and tell it that the data type is an integer it's going to convert those to actual integer values of true and false if i create this mask and filter a by that mask whoops this needs to be another one so this is my mask um what did i do here what is a oh a is a lot longer than that um let's just do some one two three okay here's my a here's my mask if i filter a or i should say index a by my mask it's going to give me only the values where this is true right so true for 0 don't give me this 99 true for this 99 no 30 and a 40 and then false for the rest now i created a mask shorthand we usually don't create masks this way we usually create masks by some filtering operation right so if i have a i say a is greater than 20 this creates that boolean mask for me where i want to filter where i don't have to do it by hand i don't have to look at my original array and type out ones and zeros and things like that okay so i can take this i can say my mask is equal to a greater than 20 then i can say filter based on that mask or i could just put that in [Music] as well i don't have to create a separate variable mask if i don't want to i could just put it in so fancy indexing providing indices providing a boolean mask in two dimensions we can um combine these so i could say in my dimension zero i could provide a list of indices and in my dimension one i could provide a boolean mask if i wanted to and that's what's happening on slide 17. are we okay here yes is well established but uh just encrypt me when you were showing this it might be more useful to talk about the slices as being regular indexing and what we're calling fancy as being irregular because that's what's different about it is that slices they always you always have to pick something that's regular length the step size is always the same all the way across but with the boolean and integer array in that yes that's a great point it's a great point yep so that's a good question so what's happening here so i have a a is our array right we've seen where if i had if i had a another array b right i could add a and b together if b was an array we've seen that we can prov we can not only apply mathematical operations to array but also logical operations so this idea of like a logical and or logical or or or like a greater than or a less than or something what this is going to do is vectorize that logical expression across everything in our array just like when we're adding things together it was doing it element by element so what this is going to do is look at my array a and for every element in a ask the question is it greater than 20 and then provide a boolean result there so either a false or a true then what i'm doing is i'm treating this kind of like saying this is mask it's like mask equals a greater than 20 and then i'm passing that mask into my filter my a selection so i could have said this is you know shorthand i should i could have said things like true fault like i could have provided those true and false values in there myself as an array or i could have saved this as a mask or i could have put that in where this expression i had saved as this before sorry this is a shorthand i shouldn't have done that in ipython so there's um in python underscores have meaning okay we have this idea of like dunder methods or special methods in ipython the underscore is a reference to the output of the previous command so what i did there if i said five i could have done this taking the previous command and referencing it so when i said a greater than 20 i said mask is equal to underscore i should have just said mask is equal to a and then 20. i was being lazy that's what's happening does that answer your question about the a okay okay go ahead so i did equals equals here we have something like this um if you have an array a um and i had array b uh whoops uh that will broadcast that there's an example that i'm drawing a blank on that um this right okay i'll have to come back to that as an example later on go ahead so you have this so in python you're getting at something like this we can string these together but what's happening here is we get a um we need to do some type of bitwise operator essentially what we'd need to do there's probably a better way to do this there's an and bitwise so sometimes in python you have an and so what's happening behind the scenes when you're trying to do this this might have been what you were getting at but maybe not um underneath the hood python's going to try to access this and but what's happening is we have this array so this gives us an output an array output this also gives us an array output and it doesn't know how to what's the truthiness of the actual array object so you're saying array and array really what we want is this to be vectorized so we want to say for each of these elements here and each of these elements here put them together and give me the logical and and it doesn't do that under the hood but the bitwise operator so python has a bitwise operator the ampersand is and the pipe is ore the carrot is exclusive or um but what that does is that does what we actually want is it stacks these kind of on top of each other and computates calculates that value yes okay so slide 17 what's happening here i'm going to draw on this and then i'll code it up because there's something that we can do that helps me at least think about this remember we have dimensions right so this is dimension zero this is dimension one what's happening here in fancy indexing is we're providing a list of the row indices and a list of the column indices that we want so you can think about these as being zipped together in the coordinate system of your array so this is saying row zero column one row zero column one which is this value here row one column two row two column three row 3 column 4 row 4 column 5. so we're providing indices here irregular diagonal okay so a few things one is so we want a is six oops what i want to do here need to reshape this there you go that's backwards okay really what's happening here is when we provide indexes fancy indexes what i like to do is say okay what rows do i want so i want rows 0 1 2 3 and 4. what columns do i want in conjunction with those we want one two three four five so our indices it really look something like this if i look at a i want the coordinate like system as in terms of the dimensions in our array 0 and 1. if i look at my rows whoops rows columns uh a okay let's try this again rows columns a if i stack these and zip them on top of each other right to get the actual coordinates that i want in this array that's kind of how you can think about providing indices for your fancy indexing where zero one pertains to my zero dimension is this way my one dimension is this way so 0 1 is my zeroth row and my one column 12 23 34 45 to get diagonal now if you want to cheat there is a diagonal and you can provide an offset you can do that as well but the point of this is to display fancy indexing where you're providing indices now blue here what's happening in the blue is we're saying start at row three and on right three comma or three colon i should say row three on and then providing a list of indices 0 to 5. we don't have a regular spacing we can't slice here because we're doing every other one would give us this row so we can do 0 column this last one we're creating this boolean mask where these are getting ones get evaluated to true zeros get evaluated to false so if we put that mask in terms of the zeroth dimension true false true false false true all of which get applied to the second column which gives us two 22 52. okay so if another give it a try this is also in the notebook so if you open that notebook again scroll down a little bit should have the same data there for you um there's two different ways you can do this so there's two questions one get all the blue items you can go from top to bottom right so if you went from top to bottom you'd get 3 6 10 and 19 as your result you can also go from left to right which where you get 10 6 3 19. i'll do both of them but pick one of them you don't have to do both pick one of them either top to bottom or left left to right depends how you're defining your indices and then the second part is to get everything that's divisible by three there's a modulo are we familiar with this do not modulo notice remainder right in python do we know what the modulo is yes percent so 25 mod 5 is 0. 25 mod 4 whoops there's one some of you might not know but there's also a div mod which actually gives you both which is pretty cool so the remainder is the last one but modulo is what you want to use hint for part two okay so we'll take five minutes if you need help or have a question so there is a solution for this one as well try it on your own um we'll take five minutes and then review yes no so this is what it's kind of doing under the hood so sometimes it's a little bit confusing that it's a little bit confusing when for students when they see this kind of notation where you're you're specifying all the indices for the rows and all the indices for the columns you're not specifying them as zero one comma one two like you're not providing them as those tuples you're providing them as rows yeah so like under the hood for me conceptually when i first saw this it was like wait a second like i'm providing a whole entire list of rows and a whole entire list of columns it was just a way to say this is actually what's happening underneath the hood and a lot of the times if i'm doing fancy indexing i'll define rows i'll define columns i'll zip them together for me to look at it and kind of inspect and say that's right that's what i want and then i'll provide rows comma columns as the actual slice so yeah i would i'd usually define things like this where i define a rows list i define a columns list and i'd say okay let me just inspect this real quick okay yeah that looks right and then i would provide it in as a rows comma columns and get that so is that me excuse me is it me on the mic the mic okay okay how do we do who got both good job okay uh in the interest of time we're going to review now because we have a longer exercise that i want to have you work on so we're going to go ahead and review slide 18. we have this array reshape to 5x5 so we can go two different ways here right if i went from so if i said blue whoops i said blue from left to right we'd expect to get back 10 6 3 19 right that should be our our answer so the way i like to do this is i like to say okay what are my rows if i'm going from left to right left to right so i want 10 6 3 19. i'm going from left to right my row is going to be index 2 1 0 3 2 1 0 3. my columns my columns are going to be 0 1 3 4. so if i say a rows comma columns rows 10 6 3 19. okay do i need to go from the top down are we okay there good okay what about number two divisible by three any ideas any takers bueller euler good good that's right so a mod takes the modulo applies it to every element of my a a mod 3 equal to 0 is going to tell me when i don't have a remainder dividing by 3 which means it's divisible by 3. so i could trade this mask and then i could take a of that mask or take filter a by that mask that gives me everything divisible by 3. i could have also said a where a mod 3 is equal to 0 and put it directly in there now some students say well this is great but you're only giving me the location you're not you're only giving me the values what if i wanted to keep the same shape of a but somehow identify where those were there's something called where we'll talk about that np.where where you can provide a condition so np dot where a mod 3 is equal to zero these will give me the locations of where that's true but i can also provide if this condition is true in my original array fill it with this value if not fill it with this value so it gives me the ability to apply a logical expression to a the filter and then what return values that i want if true give me a one if false give me a zero or i could say if true give me a 99 if false give me a 13. as opposed to just filtering a and getting the actual values out so sometimes people want to know how to do both of these which is helpful okay here okay we have a break at 10 45 we're going to work on an exercise a longer exercise to practice some of the concepts that we've learned so far we're going to work for 15 minutes take a break you can continue working if you'd like and then when we come back from a break we'll review the exercise the exercise is if you click on this folder tab sometimes when you navigate it takes you down a directory so there's just another folder tab so not the big one on the left underneath the jupiter icon but the one under here that says filter if you click on that i'll take you to your root directory and we're gonna go to exercises now we're gonna do calc return if you wanna challenge and you are more familiar with numpy there's a more challenging filter image exercise that will show you rather than brute force writing a for loop how you can use numpy to smooth an image rather than kind of brute force going through line by line and column by column it's a little bit more advanced if you'd like to do that one feel free to but we're going to start with calculator return there are two different options here if you like a dot py script like you like working that way it's provided as a script if you like working in a notebook that dot i-p-y-n-b means it's a notebook you can open up either one you like both of them have a solution they're both the same solution we have solutions for all the exercises so if you get stuck feel free to look at it but if we open up calc return basically we have a stock price so we're going to calculate the return the way we're calculating the return is taking the time stamp that you're at minus the time stamp before it and then dividing by the time step before it that's how we're calculating the return okay so you should be able to do this we have the apple stock price from 2008 we're importing things here for you and we have a function called load text or load txt we'll talk about that it's a numpy function to load a text file as an array but we have these prices here which we're going to do is calculate the return here's the catch no for loops no for loops are allowed for this exercise do not try to do this with a for loop i mean you can practice using numpy so essentially what we're trying to do right is we want to take this value here and subtract it by this value and then divide by this value so that means we want to gather everything but the last value right and then everything starting at the one all the way to the last that'll give us an offset of one rather than for every item take the index before it and looping over okay so think about that let that sink in there's a little bit of matplotlib code if you want to do that we'll review it but we're going to work for the next 10 minutes or so if you want to work through the break that's fine you don't have to but when we come back from the break we'll review this okay and i'm here if you have questions if you're still having trouble with the installation if you need to run to the restroom that's fine um we're gonna work on this for 10 minutes and then take our break and then review okay we're gonna go ahead and um review how do we like this exercise give me like a thumbs up if you liked it like kind of if it was okay thumbs down if you didn't like it okay so we're going to review i know there's some matplotlib if you're not familiar with it um don't worry too much about it or just the way to kind of plot some of the stuff we're seeing so the first thing we're going to do is we're going to calculate our returns if you remember from our formula what we're looking at is trying to take the point previous to where we're at to get the difference and then divide by that point previous so some of you might wanted to i took this i took this tutorial like six years ago and the first thing i did was reach for a for loop like that's what i thought i was supposed to do so it like blew my mind when i was like holy cow this is like super powerful to be able to do this without a for loop so what we're trying to do from like a data standpoint we want to take i know if i'm looking at this point here that i have highlighted i want to subtract the one before it right just looking at the differences if i know i'm at this point here i want to subtract the one before it which is this one this one here is this one right i go all the way down to the bottom and i'm going to take this value minus this value what that means is i can collect all of these skipping the first one right so i want to just take everything but the first one i'm going to say prices all but the first one and on right and then what i can do is i can take the prices all but the last one and on and that gives me the difference right so one and on is going from this value all the way to the end everything up to the last value goes from this value all the way to everything but the last one so if i take the difference of those it's the pairwise difference that i'm looking for we can call that dips prices not princes and still wrong now there's a shortcut to this let's say you wanted to take like two there's a mp.diff if you didn't know about that of prices i need to import numpy here so imports in python you can import the actual thing or you can import you can import the actual function that you're looking for or you can import the whole entire library i imported diff here i didn't realize it so i'm just going to get rid of this i didn't import numpy as np but this calculates and i can have a offset here so a few things in jupyter lab and ipython that's good to know this also blew my mind the first time i saw it are we familiar with the question mark and double question mark so question mark will pull up the function signature and i'll show you all the parameters what it returns if it's written in python the double question mark will actually give you the source code so you can actually look at the python code that is actually written for that function source code and if you're in jupiter and you don't want it to run it in a cell shift tab will actually pop it open into a separate window so you don't have to run a cell in order to get it so if you ever if you don't know a function you're trying to understand what the signatures are and stuff like that it's super helpful there's also a so i've created let's see i created this diffs there's a handy np dot array equal function here i'm going to say diffs and is that equal to my diff of prices just to show you that this is in fact i keep doing this if these two arrays are equal it'll return true turn false there's also for some of you that might want to know there's a np dot testing dot assert array equal and almost equal and stuff like this this will tell you if two arrays are equal up to a certain decimal point and if they're not it'll give you like the average difference and stuff so if you're trying to test between two arrays it's helpful to know that there's actually a testing library to work with arrays but now that i've gone down a rabbit hole that we weren't supposed to so we have our diffs right we understand this and then the last part of our equation is to get our returns we want that to be divided by our prices not princes again okay so our returns are those difference divided by everything but not the last one okay okay here so we have this like pairwise comparison with division if we wanted to grab days this is just looking for the number of returns that we have there's also a there's a zeros array construction method if i wanted to make this zero just providing an integer will give me a one-dimensional however many zeros i want there if i provide a tuple of shape it'll give me that shape of zeros so all i'm saying here is we want the um days to go from zero to 255 i think something like that and we want our zeros to be the same length of our returns three turns um and i can plot these so we want the days and we want our returns here and i can multiply that by 100 if i want and that'll plot that horizontal line at zeros now is anyone familiar with matplotlib use it regularly if we didn't want to do this zero is there another idea that you have besides creating a zero array there's something called an ax h line axis horizontal line so instead of plotting the zeros here i could have said ax h line at y equals zero whoops uh and thank you and again i could say color is red line style solid here so if you didn't want to like create zeros in order to draw a zero there you can draw a horizontal line wherever you want just using matplotlib this is a matplotlib thing not a numpy thing just pure convenience okay so the point of this is to practice using numpy introduce the np.diff if you didn't know that existed and then understand that we can you know do this without using a for loop okay here anybody take a crack at the filter image okay that's a great exercise just to i'll show it to you so you hopefully intrigued by it um but what we have here is an image and essentially what we want to do is we want to smooth an image using the center point of the pixel the neighboring images like the top left right bottom you can think about this brute force looping over every pixel in the image and trying to calculate it we can do this much faster up to 10 times faster than just looping over line by line and column by column by collecting all the top nodes all the center nodes all the left nodes all the right nodes all the bottom nodes and averaging them all at once so this shows we have an image here there's a smooth loop which is the brute force like for every row for every column loop over it collect that pixel and create the average and then there's this smooth where we're slicing here so grabbing all every pixel that's gonna be treated as a top as a bottom as left or right as in the center and then taking the average of those all at once it's like 10 times faster and so if you're curious and you want to time it you want to go through that you can it's a great exercise to kind of solidify some of these things okay all right so we're on slide like 19 i'm gonna try to get through we have uh about an hour about 50 minutes left i'm gonna try to get through as much as i can i'll stay after if there's anything we didn't cover that you have questions on but we're going to talk about briefly some array construction methods that we've seen and then we're going to move into mathematical like operations essentially how we do them on numpy arrays so for these next slide because we've seen them i'm going to annotate them but if something if you'd like an example in code you know feel free to raise your hand and i'll i can code something up quickly for you so our array construction examples we've talked about the ability to define a data type so we can specify our data type we talked about this idea of like a numerical tower in this case we're providing integers here and one floating point value so it's going to promote all those integers and store the metadata the type of items that's going to be stored in that array is going to be that floating point value right we've seen this if i want to specify the data type at array creation i can use the keyword parameter d type right so i'm saying np dot array providing the numbers that i want and then providing the d type if i have an existing array and i want to change create a new array but change the data type or override that array with a new data type i can use the dot as type right maybe it'd be better if i wrote this dot as type so a a equals a dot as type and provide a different value in here there's a lot of different types of there's a lot of different numpy types so we've looked at 64-bit integers there's 32-bit integers there's 64-bit floats there's also something called like an 8-bit unsigned integer this is very common with images so if you think of like your rgb scale those images are from or those numbers are from 0 to 255 those are called unsigned 8-bit integers which means for every item you have 8 bits so you think about like 8 different slots it's either a 0 or 1 comparing to base two so that zero to 255 is the fact that these are eight bit integers where you can go from all of them being zero to all of them being one which is 255 so that's why you have like the red rgb scale if you see on images like zero to 255 that's where that's coming from but there's that type in numpy and we can get all that access to the data types and the number of bytes using that metadata that we've seen before okay we've used a range or an array range to provide a start a stop and a step you can also provide the data type remember if you provide an integer it will return to you integers if you provide a flow to return to you flow to try to infer that data type for you so in this case our array of integers 0 to 3 up two not including um we can also specify different intervals this is like every fourth pi essentially is what we're doing here zero to two pi every fourth pi that's happening there's a warning here it says be careful typically we say that this is up to not including so you'll notice here that this is not adhering to that behavior it's including the top value here so it's a little bit dangerous i guess in a sense the documentation says instead of using np.range if you have a non-integer step there's something called lin space linear space you can use the linear space to actually specify whether or not you wanna the end to be included what's happening here is we have floating point arithmetic it's not as precise so you're still getting the value the top value there is a ones and a zeros array creation so if you want like a bunch of ones you can say np.ones and provide the shape if you want zeros there's also an identity so matlab user's eye right i think is the identity in matlab but there's a dot identity i think on the next slide yeah so we can provide an identity right and that's just the along the diagonal ones everything else is a zero you can provide the default is floating point but you can provide like integer if you want it to be an integer for those array creation and then we have sometimes you might want to like instantiate an empty array and fill it as you're potentially like doing calculations with another array there's a ways to do np.empty or empty like so if you have an array that you want to create an empty array like now empty doesn't fill it with zeros python does something it has a garbage collection that manages memory in the background for you so you don't have to allocate de-allocate memory empty i believe works where it fills that array with things that have just recently been garbage collected so it's not filling it with zeros it's filling it with things that were recently garbage collected but you can create an empty or you can create zeros and then you can fill with a value or you can use dot full which will create a full value if you want to rather than taking an existing array and filling in every element you can just automatically create a full so this is saying we want two values filled with five but we could say two by two and say five and it would create that five by five or two by two with fives in there so we have this idea of empty we have this idea of full and fill we have identity.identity.ones.zeros various array creation techniques if we if we want so there's something called lin space and log space so this creates from a start to a stop so linear space is going to give you so this is the start value this is the end value and this is the number of items so this will create five items linearly spaced from your start to your stop this is helpful if you want to be able to like plot something or like be able to do some type of analysis where you have two intervals and you want evenly spaced things to check things at or plot things at so it's linear space there's also a log space the default is base 10 but you can so this is saying 10 to the 0 10 to the 1 in log space or you can change the base and you can make it so this is from 10 to 10 to the zeroth 10 to the one one power is what that log space is doing five of those and the default is base ten you can change the base to two if you wanted to okay there is a np.load txt who's used uh pandas before probably like the first thing you learn in pandas is like read csv right pd.read csv numpy has a similar concept it's load txt not load text i can't tell you how many times i type load text txt but it gives you the ability to take a text file and load it in immediately as an array so this is is nice and convenient if we had this text file up here data.txt we have different parameters where we can say skip rows so the number of rows you want to skip that will ignore this first one beginning a file you can say what you want the data type to be returned when you get it back as an integer you can specify whether it's comma separated i think the default for this is white space so you have to specify this if it's csv or comma delimited there's a use column so if there's a column that's um you know wonky or you don't want this is saying only use zero one two and four so we have dates in here zero one two four would eliminate kind of the third column and then there's also a comments so you can provide a single or a list and anything that starts with this or has that gets treated as a comment and it will get thrown out so essentially you'll load this in as an array for us when we specify these different parameters it's much like pandas read csv and then if you have an array you can use np.save txt to save it as a text file provide the name we can load back and forth between the two if you would like practice you don't have time for it today but in exercises there is a load text again there's a solution for each of these we have some very simple text files for you to practice reading in one more complex one and basically helps you the way i do this is just start with the file name you might get lucky and it might read it as an array and not give you any errors oftentimes you get back an error that's saying like hey there's a like parenthesis or like this weird like parsing that's going on so you might need to skip a row and then if you get this kind of other file here you can start trying to provide some of these other parameters to actually read it in so there's a solution here for load text if you want some practice there to show you some of these things the head function is really good um this allows you to like read the first like three lines of a file if there's like a file that's giving you trouble rather than like say it's like a million lines and you don't want to read it in because it can't fit in memory you might want to read like the first three lines there's a little python function you can use like cat or or head from the actual command line but it's a nice little python function to help you here if you're curious that's not numpy it's based python anyway it's a good exercise if you're looking to practice reading in text okay so some of the um we're going to talk about doing computations with arrays there's four main rules that we have to be aware of when we do computations if you come from the pandas world these are different so pandas does something called automatic like data alignment where it's going to try to align on the series if there's not the same shape it will not propagate null values for some of the reduction techniques so this is a little bit different if you're coming from the pandas world um for numpy our first rule is we're always going to check whether or not objects have the a proper shape match so if we have one shape that has three elements in it and one shape that has two elements in it we can't add those together it's not going to be able to add those together the [Music] stipulation exception with this rule is a scalar so you have a scalar value and you have like a multi-dimensional array and you add that it will propagate that all for you there is something called broadcasting that hopefully we'll have time to talk about if one of these indexes is a one numpy will essentially stretch that across the other indexes for you and actually add those together for you um there's a slide later on we'll talk about that but for now type um shapes are checked you need to make sure that shapes match when you start adding things together because remember this concept of vectorization it's trying to do that an element by element fashion and when you don't have those things those shapes don't match numpy doesn't know how to handle that the second thing is these mathematical operations apply element by element right if we had a and b and we were adding those together it's kind of like zipping them or stacking them on top and adding them rule three is a little bit of a gotcha remember when i said we had a two-dimensional array and i said there's three ways to calculate the sum you take the sum of everything in the array you take the sum across all the columns one per row or you could take it one across the rows one per column reduction techniques like the mean the sum where you're trying to condense things down will always be performed on the entire array unless you specify an axis okay we talked about the axis being one and zero and one for a two dimensional case so i have this array let's do six by four if i say dot sum here it's gonna take the sum or i could say np dot sum of a it's going to take the sum of all those values in my array but if i specify an axes it'll sum just for those so notice here i get four values axis being zero going down so across my rows one per column when my axis is zero my axis is one i'm going across my columns one per row okay so a way to think about this if you've never encountered this before is if you take the shape the axis that you specify will disappear in the reduction so axis zero what's at zero that's six that goes away so i'm left with the four all right so that's my axis zero this is going to go away so i'm left with four values my shape here if i specify axis is one what's at zero one that's a four these are going to go that 4 is going to go around 6 values so you can think about it that way or you can say a dot sum axis you can cheat take the shape of that and see if it's what you want now this doesn't work if it's a square array but this does work if it's not the same right it's okay make sense so this is a little trick to help when you start doing computations you have six by four array okay i want one per i want four values which one do i need to specify which sum or which axes i should say for my sum okay so we have to specify an axes and then the last one missing values so there's an np.nan not a number some missingness is encoded in numpy missing values propagate unless you use something called a nand sum or a nand mean or something like that which ignores null values so in numpy if i had 1 2 np.nan 4 a dot sum a dot mean these are all returning a null value because this nand this null gets propagated through that calculation but there is a nano sum or a nand mean that will calculate now notice this is one plus two plus four divided by three right that's what we want we don't want this divided by four so it's correctly ignored in the numerator and the denominator i guess is what i'm saying right it just it throws it out it notice it completely so if you have null values and you need to keep them in your arrays if you do any type of reduction technique that null value is going to propagate through and you're going to return to normal but there are things like a nand sum nand mean now you'll notice i'm calling things sometimes with a dot sum there is no nand some that has to be called from the top level numpy so some of these are accessed at the top level as a numpy function some of them you can call them as array methods okay here okay multi-dimensional arrays for right now i want you to pretend don't look at the bottom of this slide we have multi-dimensional arrays this is what i was talking about before when we're adding when we add a dimension to our array it goes to the beginning of the shape so what's happening here is if we have just four values we have a vector we would say that this is our zeroth dimension we have one dimension going this way that's our zero dimension essentially the axis equals zero and we take the sum or something across this way if we now add two rows that four in my shape so think of this as your shape tuple that four gets bumped over to the one how do we represent that from like a coordinate standpoint we add one here so now the axis going this way is a one and the axis going down is a zero okay so we've appended to the beginning of the shape tuple and shifted everything to the right if we go to three dimensions that two and that four shifts to the left so we add one here we add one here that now becomes a two that becomes a one and our zeroth dimension is into the screen so we've appended at the beginning going into the screen so what that means is taking the sum the mean and specifying axis equals zero from a three-dimensional case is going to sum in the screen into the screen axis equals one is going to sum down across my rows one per column and axis equals two is going to sum across my columns one per row trigger warning if we're in two dimensions axis one sums across my columns one per row and x is zero chrom slot uh sums across my rows one per column this is different depending on how many dimensions your array has because of that reason you might see people that are working with higher dimensional arrays always use negative indexing that's because the negative indexing starts counting from the back of your shape tuple if you remember we're always shifting our shapes to the right which means you're always going to start counting from the back so if you specify axis equals negative one you're always going to be summing in this direction across your rows one per column no matter what dimension you're in if you specify negative two you're always going to be summing down across your rows one per column did i mix that up i think i messed that up negative one will always sum across your columns one per row negative two will always sum across your rows one per column negative three if it exists will always sum into the screen and negative four if you have that my mind doesn't really crash four dimensions as well but you have like these blocks right where you have three dimensions you have different blocks maybe you have like a time dimension or something it'll sum across your your blocks in that aspect so sometimes you might see people specify the sums or the means or the reduction based on the negative indices because negative one is always going to grab from the back of that shape tuple but if i said in two dimensions versus three dimensions axis zero is a different concept as far as the sum and the reduction goes okay here okay so slide 27 when we start actually defining some of these we have a two dimensional array right our axis zero and our axis one or axis negative two and negative one if i say a dot sum that's going to be one plus two plus three plus four plus five plus six right i haven't specified an axis but if i say axis is zero i'm gonna sum across my rows one per column down the zeroth if i specify a one or a negative one i'm going to sum across my columns one per row okay so we have a bunch of other methods that we can call on arrays so we have the sum we have a product so we can do the product across all of our values there's a min and a max there's something called a standard deviation of variance we have our mean there's something called p2p or peak to peak which is the max minus the min for all of those so we can do that [Music] as a method on our arrays as well so one thing that's helpful here let me actually code this it's a little bit easier to see so slide 29 there's a um two oops um so i have a two-dimensional case if i take the min so i say a dot min will give me the minimum for the whole entire array say a dot min axis equals 1 and that will give me the minimum for each row and each column in this case there's a max there's a min there's also something called the arg min or the arg max so sometimes when we're doing computations we don't really care about the min or the max actual value we care about where the min or the max value is so there's something called an arg max but this is a one why is that a one what's happening is an arg max behaves on the flattened array so if we flatten this this tells us that our maximum is at the index one here based on a flattened array that's not really helpful right we run it in the original shape so there's something called unravel index unravel index and i can provide the location that i want in terms of a flattened array so this is a one and then the shape that i want that to convert that into so a dot shape and this will tell me this is at row 0 column 1. so it converts the actual it undoes the flatten essentially and gives it to you in terms of the coordinates of whatever you're providing so my a is two by two if i just did the r max it tells me one because it's saying flatten it out and it's giving me this well i don't want one one doesn't help me if i'm trying to find it in terms of my original array we can unravel it in terms of here zero one and it shows me zero one here put it in the right coordinate system if you will okay there's also something called np where we've seen that before but i'm going to demo it again here let's see negative two to two square this this is slide 30. so we have this array i can create a mask here that is where a mod 2 0 so i have my mask i say a dot where that mask is true this will give me the np.where mask sorry let's try that again i have this mask it's booleans say np.where based on that mask it's going to tell me the locations of the true values so np.where is going to accept either a boolean mask or a condition or a condition of trues and falses and it'll tell me where those things are true notice that this is a tuple so if a was equal to mp.where a is less than two it's a tuple because it's giving it to me in terms of the shape of my arrays it's a two-dimensional array so it's giving me in terms of my different values this is the first row column zero the first row column one so it can tell me where things in my array are located but it also allows me to reconstruct an array like we saw before so if i had [Music] a is if i had a here and i said mp.where a mod 5 is 0. this will give me the locations of where that's true if i provide something else in here that whole entire first column is the only place that it's divisible by five and i'm saying where that's true give me a one else give me a zero it allows me to fill in for that boolean mask essentially as well okay if i have my array of one two three four five six we have the mean again i can specify an axis of zero or an axis of one we also have a standard deviation and a variance right so this same with my variance any statisticians i think we can provide the uh degrees of freedom if i'm not mistaken so it treats um treats this as the actual population so if you're doing any type of like statistical you want to treat this as a sample you can change the degrees of freedom to one if you want but again we can provide we can i'm calculating that axis equals one but you can do the whole entire array or you know column or row-wise okay so we give it a try here copy and paste this i'm not sure if this is in is this in our notebook okay it is so you can do this in the notebook here i want to grab the maximum of each row the mean of each column and the position of the overall minimum okay so let's take five minutes we'll do that and then review and continue on okay we're going to go ahead and review so we have our array a the first thing we want to do is get the maximum of each row so if you look at our shape if we want the maximum of each row how many values would we expect to get five or six five five that's right five so what we want to do here is if we want five to remain that little trick is okay i want the six to disappear it's at axis one so i would say a dot max axis equals one that will give me my five values back again if you aren't sure you can always do something like this if it's not square right say oh okay that's the right one that's the one i want to use axis equals one or you can look at the shape and say i want this one to disappear so i'm going to specify xs equals 1. okay so that's the max of each row and i want the mean of each column so it's going to be accesses zero so i get six values there and then i want the position of the overall mean so there are the position of the overall minimum so there's a few different ways to do this so the first one is i can say np dot arg max i'm sorry arg min of a well that's not very helpful it's just telling me in terms of a.flatten it's at the 15th index so i'm going to say unravel index a dot arg min a dot shape if i look at a row 0 1 2 column 0 1 2 three is that zero so option one i can use unravel index option two i could have said np.where if i knew the minimum was zero i could have said where a is equal to zero and that will give me the location or i could have said where a is equal to the min oops okay so you can use unravel with uh argument you can use where anybody do this a different way i think of a different way we could do this i took the min and then took the men of that argument and i took the min of each column whoops so take the minimum and take the argument minimum for each row and each column you could do something like that as well right so this is my row index this is my column index take the minimum for every row take the minimum for every column and then find which argument is in there so let's say um so np.unravel this is only giving my first occurrence this will give me all of my occurrences and if i did this trick it's only going to give me my first occurrence thank you first occurrence reverse yeah but where will give you all the locations okay questions okay here all right i'm going to try to do a flyby on broadcasting um the last 10 minutes there's the exercises directory has some really good exercises for practice outside of the course they all have solutions so if you're looking to try to brush up on some of these things or see how we actually use them in practice they're real world examples they're great to go back and look up and they all have solutions so feel free to look at them during lunch or during your time here at some point so array broadcasting a lot of the times in numpy we can think about this like vectorization where we want to do element by element calculations well instead of always having to create the same kind of shape if we want to do larger operations like a four by three we don't have to create this four by three in order to add these together if one of these has a dimension where there's a one in them oh i'm sorry thank you very much thank you sorry about that so slide 35 um i kind of hinted at this before but if we have the easiest way to think about this is if we have a two four by three arrays and we add them together it's easy to see how numpy is just going to take both of these they can like lay it on top of each other and sum them to get a 4x3 output if we have this 4x3 and another matrix or array that has three elements we can describe this as one comma three what numpy is gonna do is stretch that in the other element for us so that we get back a four by three so we don't have to create another four by three or take our existing array that has three values and create a four by three by repeating those numpy is going to handle that on the back end for us and do something called broadcasting where it's gonna stretch this across without copying any data and create that 4x3 array for us and the way the reason this is very powerful is because we could even have a 4x1 and a 1x3 and that's going to stretch this out and stretch this down and give us that same four by four array so numpy is going to try to do this optimization on the back end for us if the shapes allow for it so it's not going to copy this information over we don't have to repeat certain things it's going to try to do this for us which is really convenient and really nice when we do mathematical operations so visually this is kind of what's happening the rules essentially is if the trailing axis so it always looks from the back from right to left as far as the shapes go so if the trailing axis is a one or they have the same broadcasting can occur so in this case what's happening is we have a four by three and a four by three that checks out that's fine we have a four by three the trailing axis for this one by three or three comma is a three which matches this three so it can be broadcast this is a four by one this can be because this is a one the trailing axis we can stretch this across and stretch this down to get our four by three so the idea here is they must be the trailing axis and one of them can be one the smaller array and they must or they must have the same if not we won't be able to broadcast the idea here is we have a four by three and this is a four so what's happening here is this last axis this isn't going to work there's no way to broadcast that down and and add those things so one nice way about doing this is there's a np.new axis we can take this this shape currently is three comma whoops so slide 37 a is np dot array 0 10 20 30. b is np dot array 0 1 2 a dot shape b dot shape i can say b dot reshape to make this one by three or i could say b np dot new axis everything in that direction my old direction we can reshape it this way if i try to add a and b it's telling me it's not going to be able to add those together because we have a 4 and we have a 3 comma but if i make b now i have a one here whoops did that backwards the last column the the see now that i've thoroughly confused you you have a i have b i've essentially converted that so it's a one by or i'm sorry a three by one so i've stacked this in column so i have four as my a i have three going down it's going to stretch each of those across to give me c depends how i want to on the slides they did a maybe i should do it that way if a b we're saying y is equal to a and p dot new axis plus b keep doing this backwards the trailing so if i look at this this converted my a kind of stood it up so it's taking that stretching it across stretching my b down and adding those together this is helpful when we start doing things if you've used numpy before there's something called a mesh grid so you might have seen this where you're taking like given an array of x that's most famous or most widely used in plotting where you have like given an x array given a y array create a mesh grid where you're having one point for all of those like pairwise all those points across it's a mesh grid but that will sometimes copy your data and not be as memory efficient when you do you can do the same thing with broadcasting which isn't going to copy your data and there's a little bit more memory efficient so these two things on the same slide do the same thing where the mesh grid is taking one and two three four and five and essentially giving you all those points and then taking the sum whereas here we're creating the new axis and allowing numpy to broadcast that for us so it's a way to do kind of the same thing in some regards i want to [Music] have three minutes left i want to move on the last thing i want to talk about is kind of the memory buffer so there's in the universal function there's things that all universal functions share in numpy there's a dot reduce there's a dot reduce at there's a dot accumulate and on this on these slides they serve as kind of a reference point to show you how those kind of work where reduce is going to reduce those things down it's like a sum it works the same thing as our sum if you wanted to see um accumulate gives you kind of the intermediate step so reduce would give you just everything sums so here one plus two plus three plus four it's going to reduce that down to one number accumulate is going to give you the like intermediate steps so 1 plus 2 is 3 3 plus 3 3 plus 2 down down the line here so 1 plus two is three one one plus two is three three plus three is six and then six for scores ten did you like the intermediate steps if you wanna do some type of accumulate or something like that there's also a reduce at so you can specify specific indices of where you want to like reduce things at not just the whole entire array so if you had these indices where you had like monthly values where you wanted to condense them down you could do something like that but the last thing i'm trying to squeeze in here is how numpy handles this memory block and this starts to make sense why things are designed the way on the back end from numpy from a like view um and a like copy standpoint if we think about the way numpy stores this in kind of memory world there's no idea of like a multi-dimensional array so when we think of something being three by three in memory this is just like a contiguous block it's just storing the data there and we have a reference to that so we need some way to be able to say hey for us as developers this should be three by three so we need some type of metadata to describe the data buffer and then convert it to like what we expect as like a three by three array and we do that through the metadata and that metadata we have dimensions shape and then strides these strides are what determine how our array like visually is displayed like to us our understanding from a python view so this in in memory land you have this contiguous block but when we say the strides are the number of bytes that you have to skip over in each dimension so what we're saying here is when our strides are 24 this is our dimension one or remember our rows so to go from one row to the next row i need to go from zero to three how many numbers is that that's one two three numbers this array we're saying stored 64-bit integers remember eight bits is one byte so if we have 64 bits it means we have eight bytes so we have 8 16 24. it has to skip over 24 bytes to go from one row to the next okay if we want to go from three to six go to that next row we're going from three to one two three right that's 8 16 24. in terms of columns this is dimension one right this way so we need to go from zero to one that's only eight eight bytes so this strides here tells us how things are getting mapped and this is why a transpose is a very very cheap operation because it's pointing to the same exact data buffer all you have to do is flip the strides and flip the shape so you don't have to change anything as far as a numpy array goes you're changing the metadata when you call a.t a transpose you're just flipping those two things that's why it's very very cheap and that's why anything in numpy when you start reshaping things is a very cheap operation because it's just changing the metadata it doesn't have to create a whole other data buffer and hold all new sets of data in order to create that okay so that was like 100 miles an hour i know that was like very very quick but it starts to make sense why numpy's behave the way that they do sometimes when you understand kind of how the data is stored okay we're at we're two minutes over i apologize for having to go through that very quickly um i'll hang out if you have any more questions or anything i can clarify again the exercises are a great resource outside of this class i'm going to save my history so everything that i typed all my typos included in the ipython shell i'll upload so you guys have access to that um but thank you so much for a wonderful tutorial i appreciate your you know engagement thank you all and you are free have a wonderful sci-pi too by the way for all of you that's your first time hopefully this kicked it off right so welcome we're glad you're here thank you so much and i'll be here if you have any questions thanks [Applause]
4,Creating Beautiful Geospatial Data Visualizations with Python,https://www.youtube.com/watch?v=cjfqCHHp-AE,so this is um so i'm going to talk a little bit about um the format of this is going to be quite interactive hopefully we'll see how we get on i'm going to do a bit of a just a crash course in geospatial data in like different data types whether they be geometries whether they be rasters and some of the different file types that you may come across although we're going to skim over that because it's not that important and then some of the different libraries that are used so geopandas cardopi rust stereo that are used in geospatial data visualization but also geospatial data analysis which is quite um you know it's a growing area i guess um and then going forward so that's going to take maybe 20 like 15 20 25 minutes something like that and then i'm going to shut up and we're going to do some like interactive exercises um so for example we're going to look at you know points and lines how to generate like quite pretty maps from points and line data um i'm going to give you like some code and walk you through how things work roughly and then let you go off and try and do it on your own with some different data sets and then i'm going to move into doing similar sort of things with polygons um so um realize i haven't actually defined what polygons are but we'll get to that in a minute but yeah we'll go through polygons and make some you know some interesting maps there do a little bit of analysis play around with some of the ways that geopandas and shapley can um manipulate that data then we're going to go on and look at rasters so this is a it's one of those areas that's growing like a lot at the moment with satellite imagery um being the amount of satellite imagery data that's being collected is huge no there's satellites going up every day so libraries like rasteria can be used to analyze that data but there's also loads of other types of data sets where you can have like raster images but with a geo reference built into them and rasteria is a library that we can use to analyze manipulate but also plot and make you know interesting maps and then i'm going to finish by trying to bring it all together and just have like points polygons lines being plotted on top of rasters so the idea with this would be you could have a satellite image and you could have you could generate a visualization in python where you i don't know you've someone in your organization or someone has created polygons to describe certain things within an area and then you can plot those on top of a satellite image to show off like you know where they are um but i'm gonna try and but you know we'll see how we get on so anyway that's a rough overview of what's gonna happen hopefully um and yeah we'll see how we get on i'm gonna do a little bit of background on me so i was uh i was a computational chemist for i did a phd at university of bath in england and i did stuff like this so i made like phase diagrams um showing how like um nanoparticles change shape whenever they're exposed to different conditions um to be honest the only reason i like the only part of what i really enjoyed was making visualizations so this is like some of the stuff that i would make i find that quite cool and i also don't think the science has gone into this is particularly robust but you know the figures in the paper look quite good um but so i left academia two years ago two and a half years ago and i went into a tech startup in bristol in the united kingdom called geolect which is a um we work in like the maritime sector and we mess around with uh we tracked on you know we tracked on oil tankers stolen oil tankers we track illegal oil um we just sort of we map things basically but with a sort of intelligence aspect so like this is an example of like a satellite image where i've like overlaid ships to two different like ship trajectories to show them meeting in the middle of the ocean so what they've done here is they've like had two oil tankers one's iranian one's not have met and swapped oil or exchanged oil in the middle of the persian gulf which you're not allowed to do um and then we do like from a data science point of view we do like network analysis on this um so if two ships meet who do they then go on and meet with and so on and so forth and you can like build up these really cool networks um but this size has nothing to do with like the geospatial data visualization this is just me nerding out with networks um but i can talk about this later if you want um so yeah you build up these cool networks and then you can like color points depending on um you know what that vessel was the red point in the middle is um like an olfac sanctioned ship oil tanker um and it goes out into the middle of persian gulf and meets with all these other vessels which then go on and meet with other vessels and that's how oil like illegal oil makes it out of iran and into different parts of the world which is quite cool anyways so that's sort of that was sort of me jumping from academia into geospatial geospatial data science i guess um on the side because basically like the background to this whole sort of python maps project is that i had no idea what i was doing like trying to do all this i had no idea what i was doing so i started downloading data sets and making interesting images because that's just oh that's another thing as well we do like we map like shipping lanes and things like that so this this is a data visualization that i did that went on cnn which i thought was quite cool i was shipping lanes in the black sea just before war broke out and it doesn't look like that line nobody is sailing anywhere near that line um but yeah so anyway so i so i went out so i left academia i needed to learn how to like how all of these libraries work so i just decided to like download data make pretty plots um because in order because you know getting a nice visual at the end of things is quite a nice way of i don't know it's it's a nice endpoint to things um so using all of these different libraries which i need to use for my job um but i then start i learned how to use them by making data visualizations in my spare time um and then all of that sort of work is now like i post them on twitter get some likes quite like the validation of social media likes who doesn't um but we but what i'm gonna do in this talk is basically hopefully show you so all of the things that i had to learn the hard way because a lot of these libraries like they all they you know they say they link together they don't really link together that well um there's quite a lot of bugs when geopandas worked with matplotlib and so on and so forth um right introduction over has everyone downloaded the repo has everyone cloned the repo okay lots of nods lots of thumbs up good stuff um if you cloned it want to say it like 10 o'clock maybe 11 o'clock the data sets all but one of the data sets you'll need will be there so if you just pull a pull from the repo again um you should get them so there was a couple of data sets there was a there's a shape file of countries borders there was a shape file of something else oh time zones um and there was a raster of uh forests like where the world's forests are like forest density tree cover and that's not all been pushed onto the git repo um actually this is them uh so someone made a comment about like pointing to areas where there's lots of data the third this one here the nasa point uh one of the things that nasa have um nasa earth observations have put up loads of like really cool uh raster data sets um the point of this is like i'm like i'm gonna like pass over to you for like 20 minutes later on and let you download some and you know make your own maps and based on what we've discussed and so just the point of that but the point is just to link you to all of those data sets there's some really cool ones like there's like forest fires and they go through time so you could download all of them and make a gif of where forest fires are over time which is pretty cool it's something i want to do but i just have the time um these are all the libraries that you need geopandas pandas numpy shapely maple lib cardigpy rust hero rio x-ray if you did this on wind like this if you just type if the script if you use the command that i put on the git repo that will work on windows you're all i'm seeing lots of macs around the room um i think you were saying that if you get rid of some of the um the versions it works fine okay uh geopandas can be a nightmare to install okay okay well that's good um if anyone has any issues i'm gonna with max i'm gonna point them towards you um it should work on windows uh but yeah i didn't think about mac um because i don't have one so there's nothing i could do um but yeah okay so we can we can maybe touch on that in a little bit whenever we come to come to that um i'm gonna just zip through these like there's a couple of like there's some learning outcomes i want to basically like highlight like when it comes to like points and lines like points and lines are geospatial you know geometry types hopefully at the end of this you'll understand like what they are how to generate them um you know how to plot them how to manipulate them um how to reproject them projections are a thing i haven't mentioned yet which we'll get to um and we're going to look at port data and airlines and airport data to sort of play around with that which will hopefully be quite cool and you get to make one of my favorite maps um with polygons and again some sort of thing read manipulate and produce like create your own polygons understand how to plot them and how to manipulate them um and again reprojections rasters um so i touched on this with satellite imagery but it can be anything any sort of image with a geospatial component to it in fact it doesn't even have to have a geospatial component to it it can just be an image but i get quite i get a kick out of like getting my holiday photos and loading with rasterio and then replotting them with like a weird color map which is quite fun i think um but yeah so with this but with rasters like they're a little bit different like the geospatial component can make reprojecting raster is quite difficult again i'm going to talk about projections in a minute um which is where libraries like rio x-ray come in so i'm going to talk about um you know harass stereo and matplotlib all linked together and then hi you can use other libraries to sort of manipulate that data and then finally i want to talk about combining the two because again you're using again like rustiro doesn't link very well with matplotlib and geopandas all in one so there's a little bit of playing that needs to be done but we're going to go through and talk about that right introduction done any questions great okay yeah sorry um yeah i'm sure yes um so i pushed all but one of the files that you'll need to the git repo so if you pull from the repo uh no like this morning like a couple hours ago so yeah if you pull from the gate reaper now they should all be there um natural the natural earth data set for example is huge um but it's quite a fun one to play around with yeah i'm not sure okay probably needs to be with pip that's all right yeah we can see if that doesn't work i can come and and come and have a chat right geospatial data so i'm gonna i promise i'm not gonna talk for the full four hours like i'm gonna let you guys go and code um but i need to give like a bit of an introduction to all this stuff because i don't know how much like what your backgrounds are and it's meant to be a beginner course uh right what is geospatial data i've touched on it already there are basically there's kind of two types there are shapes which are things like points which are literally just a combination of x and y coordinates or latitude and longitudes um there's lines which is an infinite connection an infinite number of points that link two locations um there's polygons which is um well any sort of any sort of enclosed shapes like a circle triangle anything like that that can be um enclosed by a line um so actually i've put it on the screen here so like on the bottom left of this that's a point just a single x y coordinate um then this is a line so it's a collection of um points um and then there's a polygon so just uh like an enclosed enclosed line strings and then multi polygons are a collection of polygons that can fit together um so for example in like country terms um if you were to take i don't know if you were to take a polygon of france it would be a multi-polygon because france has got lots of islands around the world that would be included within it um it's just something to be aware of and then finally rasters which is um a two-dimensional picture um of like pixels effectively um so that's that's that's my that's one of my holiday photos with the viridis color map which is um quite fun right points so this is a map that i made with points this is shows the world's co2 emissions in 2019 which is quite horrifying um but this is these are the sort of maps that you can make with um hopefully what i'm going to teach you and also just just with like geospatial data um but like at their core like points in geo pounds and other things are used they're created with shapely which has a um a point object and so you create a point just with point um also by the way this note the notebook accompaniment to the this particular slide is um in the repo so if you want to go in and play with it and play with the different things that are up on the screen feel free to do so um but there's going to be no exercises in this bit because i'm going to try and i'd like to rush through it and get to the juicy stuff but this is just to sort of give you an overview um but yeah so points like there are you know there's lots of different like properties of a point so this is a i've created a point of what's called null island so zero zero um you can get things like the area obviously the area is going to be zero because it's a point like it's a single location you can get the length again gonna be zero the boundary all of that sort of stuff but then you can also get the x and y coordinates out of it um so this is just a plot of what lots of points look like you can create those points um and then you can scatter plot them to show where they are and like to be honest like a lot of the plotting stuff that we're gonna do like at its core this is kind of what it is it's just firing lots of matplotlib functions to make it look better so line strings are sort of the next step up so this is a map of rivers in colored by their basins in north america thought i'd you know theme it based upon where we are um and these are these are a collection an infinite each line is essentially or it's represented internally as an infinite connect an infinite number of points that link two locations well they yeah they map between two locations um so again shapely has a has a method called line string which is what um is what lines are represented as um so i've so here i create a line string going between zero zero and one one um again you can you can you know get properties and like the area again is going to be zero because it's just a line but you can get things like the length um when it comes to units and things um be really careful um because so in theory it should all be done it'll all be done in um latitude and longitude coordinates um but be careful when you start to reproject things and the units can get a bit funny so for example if you were to reproject from um off the top of my head let's maybe pause that that's a complicated bit but if you were to reproject your data um and change the coordinate reference system the values that you're getting out of that will change and it's important like you can go from degrees to meters for example um but yeah you can get you can get the binds of the of the object um and you can get um the latitude and longitudes so that you can like here you can plot them and again this is just what two lines two lines when plotted look like um but like to go back to the original point like that rivers map internally like that this is what it looks like it's a collection of very very complicated yes but line strings um so polygons um there's a map of this is a multi-colored map of the world's time zones which we're going to make later um again shapely comes to the rescue we've got a polygon object so you can create a polygon is created with a list of points which are then all mapped together so this is effect this would effectively make a triangle i guess um but this this is where things like the area then start to become you can get the area you can get um the length the length will give you the boundary the length of the boundary you can get the binds of the object so um max max x max y um min x mac min y etc etc and you can convert the boundary into a line string um which is quite useful and something we're going to talk about later um and you can also you can get the you can create a point from the center of the object and like yeah i've just created a polygon and then plotted it um yeah um yeah they are um yeah they are i've never actually thought about it like that but yeah they must be if we're being honest um i know so in the company that i work at we always put in an extra point at the end for which duplicates the first point um to make sure but i actually don't think you need to but we can try we can check it and try later um i can't scroll to the right but yes yeah they're not and so and it's joined it up anyway okay that's good spoil um this is a sort of an example of what a multi-polygon is so i've just created four polygons and plotted them on the left um i've then used an operation called unreunion which joins polygons together so it finds um yeah just it just joins polygons together basically um so i've done that on the first um three of those triangle of the squares so the blue green and orange to create uh this one but if you then do it on all four of them you create a multi-polygon so it's a it's a single object that represents all four of those um squares but it accounts for the fact that one of them doesn't touch the rest of them um so when it comes to if you were if you were plotting or manipulating the data you can extract multi polygons back down into polygons and but that's that's all a multi-polygon is and that is something that we're probably i'm pretty certain the data sets that we'll look at later and there'll be multi-poly bonds there um right so just introducing the geopandas so we've looked at shapely which is just uh which is a library to create these um um these these objects these geometries geopandas is it's pandas with a geospatial component basically and in this case what we can do is we can we can create data frames with all of the data and information we want but with that geospatial component and it's agnostic to what type of data is in that geometry column so pandas had so i should maybe be a little bit clearer geopandas has an extra column effectively which is a geometry column to represent um these geospatial objects and it doesn't matter if they are polygons points line strings it deals with all that for you so i'm just showing an example here where we take where a list of points list of lines and a polygon we've got some warnings there which we can just um ignore for now but i created geodata frame from each of those objects then i can cap them together to combine them and then you can all of those operations that i sort of pointed out earlier so looking at the area the boundary um you know the centroid length whatever they whatever they may be you can then do those within the data frame so you can apply them to the entire geometry column which is what i'm doing at the bottom here um and then you get if you then just look at the data you get it's like the points obviously aren't going to have an area but the polygon will so geopants will take care of the mismatch between types for you which is quite cool um for the boundary again like you're not going to have a geospatial object you can't have a boundary of a point because a point is inherently a binder it's it's inherently its own boundary um but you will do for a polygon so you'll get a l a line string back um and then the centroid like with a a point the center of a point is itself so it'll just replicate that um and then the same with lines and polygons there is a center to those so it will it'll take care of all of that stuff for you um so it's just quite a useful little it's a it's a very useful library [Music] and then again so going to that function that i mentioned earlier like una reunion where you're combining polygons there are a few different ways that there's a lot well there's lots of different ways that you can manipulate geospatial data again pan geopandas has got the ability to do quite a lot of that for you and again apply it universally throughout a geodata frame and so in this case i'm just creating two different polygon geodata frames and using a couple of different overlay methods so this is where you take one data frame and you you merge the geometry columns using different um methods uh so what so this this on the left is the original so it's a series it's just two overlapping squares um you can do the intersection so it will just find the parts of those two i pretty i probably should have colored these two differently to make it clear which one was which it looks like they're the same thing but they're not and but you can do the intersection which will just find the parts of those polygons that overlap um you can do a union so you go from having two different data frames to a single data frame um you can go for the difference so find the parts of the polygons that don't overlap um or you can get the difference and it will take the difference of one from the other um but yeah this is something we will probably touch on a little we will be touching a little bit later hopefully so does anyone have any questions before i move on if i've flown through that i apologize um but we've got a lot to cover yeah which one's intersection oh um so just look at the um oh yeah okay look at the accent the axes yes that's what i meant by i wish i should have i should have colored these differently um so there are two things plotted here it's just um yeah i didn't i didn't i didn't scale them properly so apologies uh geoplanned as well has a plotting function and which is built on matplotlib uh it's what i i would recommend using it for most plotting i think i think it works quite well there are just some quirks that we can maybe talk about a little a little bit but it's really simple to do you can just get your geodata frame and just do geodataframe.plot and it will create a nice you know basic you know representation of the data but there are loads of you can you know with any sort of with any sort of native map plotter with any like matplotlib um layer in another library there are you can you can bring in all the extra maple and functionality to customize the plots and that's again what we're going to talk about a lot today right rasters that's a rat that's a plot that's a topography plot of pakistan and it comes from a raster image um and some of the things you can do so this is where we use for rasters we use um rasterio um it's quite cool library so this is again just showing off one of my holiday the same holiday photo as i showed off earlier and you can just open the open the whatever object it is so i'm just doing a picture but you this could be it could be a satellite image it could be um you know a geotiff it could be a tiff any of these geo-referenced data sets and then you can like with stuff we sort of looked at earlier you can then go through and get some of the properties so um width height the bindery the crs or the coordinate reference system and this is just a picture so there's going to be no coordinate reference system but um if it was a satellite image for example there will be a that will be built in and if that's built in then you can start to mess around with and play with projections and things like that and you can also bring in other geospatial data sets and overlay them on top okay i mentioned projections um you're all been like if you ever looked at a world map you've been lied to that's not what the world looks like um you're looking at a specific projection of the world like if you look at what what projections are is they're taking a they're taking the globe and they're projecting it onto a 2d surface and there are loads of different ways to do that and so there's loads of different maps that you can get so this is a it's a shame this is not scaled properly but this is a shipping lanes map using i can't even remember the name of the projection but it centers it on the north pole um and that's one of the things you can do in geopandas um so this is a i'll talk about the code in a second but the image is a mercator i think yeah mercator projection so this is this is the ones you've probably seen in world maps maybe in school and things like that where the the low latitudes so whether it be yes like in the round like between the tropics are scaled quite smallly whereas when you move away and you move towards the poles and things are stretched um and so if you look at this map like the between 0 and 30 you know that's quite small 30 to 60 this this space has been stretched and then 60 to 90 has been stretched further um it's like have you ever seen has anyone ever seen um one of those maps where someone will have like you can grab like a country polygon and move it around the world to see how it scales oh i've got some got some nods uh i'll share that i've got it i've got it somewhere i'll share it later in the slack channel but it's quite cool to see like russia's actually not that big i mean it's quite big but it's not that big um africa is massive compared to the rest of the world and nobody really like genuinely i don't think that a lot of people know how big africa actually is um but then this is another projection that can be used which does the exact opposite thing so it squeezes the higher altitudes and makes the altitudes latitudes and makes the lower attitude christ makes the lower latitudes much bigger a little bit more realistic um what i'm doing here is i'm using cardi pi to do this so carter pi has got some when it comes to shape data so points lines polygons and things like that it has got built-in weight built built-in functions and methods to reproject that data so if you have the coordinate reference system um which is well we can pause that and talk about that at a later time but if you've got what it can do is it can take you know the the starting projection and it can reproject it into a into something different so that's all i've done here like i've just taken um i'm taking one of their no geodata frames i imported it earlier and i've taken that and reprojected it with mercator and then i've done just done it again here and reprojected it with the lambert lambert cylindrical which um i've not really heard of but anyway carpi's got lots of built-in functions like this which you can um which you can use right we're done on the introduction we're done in the introduction nice time to do some coding hopefully um it might be we've been i've been talking for about half an hour so it might be time if anyone wants to have a quick break before we go to the next bit let me just you know toilet break or get some water for a couple minutes is that would anyone like to do that okay sort of watch should we say let's meet back in five minutes and we'll go through how to generate a plot of the world's ports and to generate a nice airways map okay um this is the co2 emissions map again uh i think like i've i've written a medium article or i've done something i've linked the data set for this um but if anyone wants to play around and try and make something like this later with the data just hit me on slack and i'll send it to you um because it's quite cool like if you zoom in if you like if you zoom in on this map you can see like you can kind of see it here like individual roads um you can see shipping lanes like this one coming out of the malacca straight and up into the persian gulf and into the suez canal you can actually see the nile like it's that little like hot spot there um you can see airways all of these curved lines in the north atlantic are flight paths between europe and america like it's a really cool data set if anyone is as interested as i am just give me a message and i'll send it to you um that's lines what's that that's um that's undersea cables again this is a freely available data set which um if you want after going through this tutorial if you want to have a crack at making something like this i'll i'll send you the data right so the first little example um is around plotting points but lots of points so a geodata frame of points um so there's a data source it is the world port index and literally just a list it's like i think i don't know like 5 000 uh latitude and longitudes representing ports and with loads of metadata um that's crap um to describe those ports i'd like we like i work in in like the maritime sector and the world port index comes up a lot as like this is a great data set you should use it but the metadata is very sparse so it's just like just literally a list of latitude and longitudes i've put the lat longs in a csv in the resources folder so if you've pulled the um git repo recently that should be there so you don't have to go and find this but that's where the source comes from um right so so the way i'm going to do this is there is um there are two parts to this particular exercise there is looking at ports and then there's looking at airports and then there's looking at airways so i'm going to sort of hold your hand to go through the ports example and then let you go and try and do the same thing for an airport the airports example i'm then going to sort of talk you through the airways example because there's a little bit of i imagine probably quite bad use of pandas but maybe clear use of pandas hopefully um and then at the end there's a longer little exercise that i'm gonna let you have a crack at and to make a map of airports and airways so yeah pandas um simple simple job just load load csv as you would any csv using pandas um and this is what it looks like it's 19 000 okay so this isn't the world port index because the world port index has three thousand instead of nineteen thousand this is a data set that comes from geolect the company i work for so just don't tell anyone that you've got it that's quite funny what yeah like yeah like it's fine could literally be anything um so okay included within this are smaller harbours um shelters demolition yards anchorages and whereas the world port index is like the big ports like long beach um rotterdam et cetera et cetera but this will have much more data to be honest it'll make the map look a little bit better because of you more stuff that's quite funny anyway um so the first thing to do is so we have uh we have latitude we have literally just two columns of lat longs the first thing to do is to take those lat longs and create a list of points so that's what the first line's doing it's just a bit of less comprehension to take all of the latitude and longitudes in those on those two columns and generate a point from each of them um so this first line will create a um just a lit a list of points the second line then is we're creating our geodata frame so this is taking the original ports data frame as like our input and the coordinate reference reference system is eps g4326 this is just standard degrees so latitude and longitude like degrees um and then geometry refers to a geometry column so what are we going to the the geodata frame has like the thing that makes it unique is that it's got a geometry column so then so whenever you create a geodata frame you have to pass in something geospatial um and that's what this line is doing so this will create a geodata frame of ports so it'll have latitude longitude and then point for each port everyone happy enough sweet so this will then so then you can generate a plot so i've imported matplotlib i've i've made the figure look bigger because it's otherwise quite small um to plot it it's just literally port geodata dot plot and if you want to bring in that sort of matplotlib axes object um you use ax equals ax and it just tells geopandas plotting function what i guess parent plotting object you're you're using and this is the map you get so it's pretty bland it's not amazing but it shows you where the ports are then again we touched on in the previous example you can then reproject the data so i like to use what's called a robinson projection which is i in my opinion the most realistic projection um it still has some flaws um just kind of curves the world a little bit so new zealand is still there but it's just about to fall off the map um [Music] which is funny so this is done with carter pi so in the in the plot.subplots you know object you provide um a projection keyword there are other ways to do this i think this one's the cleanest personally um so this is so cardipi has a representation of what the robinson projection is um and that's what we're passing in we're passing in the robinson projection to the plotting object um and then within geopandas when we plot the object we can transform our coordinate reference system the plate carry whatever that's called is a little bit complicated all we're doing with that line is telling geopandas that we are working with degrees we're like we're working with um yeah we're working with degrees um so it's internally it is taking the geospatial object it is matching it with the with cardi pie's like way of understanding geospatial data a way of understanding latitude and longitude from a degrees point of view and then it will convert it to the robinson projection at the last step um i don't know i only vaguely understand that but that's roughly what it's doing under the hood and this is it it and i am reprojected so i'm gonna now pass over to you guys for maybe five ten minutes and let you play around with customizing this to see what you can produce i'm going to walk around and have a look and offer some tips and talk maybe a little bit more around the libraries and that we're using and this is my attempt um it's dark mode because why wouldn't it be um but yeah so let's let's maybe take five minutes let you guys play around with the code and i'll come around and see if i can resolve some of the installation issues that people are having as well um and we'll start again in about five minutes we can move on to lines all right okay cool get cracking openflights.org data set this is from 2019 i want to say so just before covid so this data is probably all irrelevant now but you know it's a tutorial um so there's two data sets one is airports so it's the latitude and longitudes of airports and in a minute i'm gonna get you guys to go away and in the same way that um we've read in and used data from the ports and create a plot do the same thing with airports and maybe even plot them both in the same map see what see with two different colors see where ports and airports are that could be fun um but the data set sort of exists there is like there are the latitude and longitudes of the airports as well as like unique identifiers so some metadata there is also in this data set uh a roots a roots data set the routes are not they're not lines going from airport to airport they are they're literally just linking to airports so it's given the unique identifier for two different airports um so what we will need to do is we'll need to create well from those the points of the destination airport and the source airport we can create lines to join those two and then plot the lines and you get this like perfect map showing exactly like if if you had a flight going from denver to london exactly what it would look like if when as the crow flies in reality it doesn't look like that um i'll show you a data set i'll show you a plot later of using um data from aircraft's black boxes exactly where the flight paths go and it's completely different um and it actually changes every day because flight paths have to be changed based upon weather and things like that so that's a that's a side nerdy point yes the first thing so load load the airport's data set um there's loads of different information within it and i a t a is latin long are the most important um columns really because that that's the unique identifier on the locations which will be used later so yeah so have a crack at that go away let's maybe take 10 minutes for this one or 10 15 minutes builds you know make a plot of you know world's airports see what it looks like why don't you also maybe bring in the ports data set as well and plot them both together and change the colors um i'll walk around and we can chat around different bits and pieces or you can ask questions around any of the things i've talked about i appreciate as well that i'm like flying through this really quickly and just offloading loads of information um there's lots of libraries there's lots of different things to get through um but i'm around to help ask questions too so yeah let's give it like 10-15 minutes and go while you guys go through that and we can we can then carry like i think again go back to the submissions thing like i've not put any background onto this i think it looks quite cool to be able to just use the geospatial data to map the world um but i was just reminded like things like what you were doing um with bringing in cardip you can bring in like um effectively like stock images to plot your points in the back on up sorry on top of which is quite cool but i've not included it here but if you go to the you know if you just go to card documentation it's quite straightforward but well actually to be honest it is i have sort of included it here anyway um in [Music] yeah here so this is now i haven't never mind i haven't sorry i've um brought in the grid lines from cardify yeah the country the country boundaries come from [Music] this it's a geo a built-in geopandas um data set so that's one thing you could do if you wanted you could plot this alongside your points so if you just um import the data set by um well using that line in the second notebook um you could then include that within your within your plots but i mean it's not it's not my way i don't like doing it that way but um i can see why you would do it that way yeah exactly but if you're trying to tell if you're trying to tell a specific story then there are there are reasons to do it um yeah i think we should maybe get back to it so how did you get on any any good plots everyone everyone succeed some nodes that looks that's good um all right so we're not gonna move on from points to lines um which is always fun oh so this is just my solution so i yeah so what the first thing i did was i created a airport geodata frame um just exactly the same way that we did with the ports geodata frames it was a bit of a copy and paste job and then i made this so same bait to be honest it's basically a copy and paste job from the from ports but i changed the color to crimson which i don't actually think looks as good yeah right so the next bit we're going to work on is looking at the looking at line strings so we're going to read the roots data frame um so sorry data set i'm going to be careful with terminology um so sim roughly what we're going to try and do here and i'm trying to try not to butcher the explanation is i'm going to show you how to how to create these um these line strings and plot them and then there's an exercise at the end where i'm going to get you to combine both and plot the airports um on the map as well as the lines that link them and then also for an added bit of fun scale the size of the airport points according to how many flights um go there just to make a cool little map but the first thing we're gonna do is we're gonna i'm gonna show you how to create the line strings um on mass so the main so the first issue with this is like they aren't the data doesn't come as line strength the data is literally just um a series of links basically between two airports so our route is defined as um airport one to airport two and so on um so i think in this data set there's like 50 thousand fifty nine thousand um i've just plotted the columns but there's about 50 odd thousand air routes and these are doubled up as well so you know you could have 20 20 different routes between the same airport so there's some plotting considerations for that later but what we're going to do is we're going to take the take these um take each of these routes so these just basically key pairs of airport 1 to airport 2 and we're then going to use the airport's data frame that we made earlier which contains the latitude and longitudes of those points to get the point for the the source airport a point for the destination airport and then we're going to take those two points and convert that into a line and then plot that line i hope that makes sense this can this this bit can be a bit convoluted i think um well that's literally what i just said isn't it i should i should look ahead and see what i'm doing so um the first thing to do is um this little bit of code what it's doing i i'm duplicating the airports um the airport's data frame the reason for this is i want to have i want to duplicate the i want to have a data frame for the source and the data frame for the destination um which is all i'm doing here so i'm taking um source airports destination airport destination airport's just a copy of source airports and then i'm renaming the columns in both of them to underscore source underscore destination the reason for this is later on i want to have 0.1 source 0.2 destination um and then all i'm doing is i'm mark then merge create with my roots data frame i'm merging in the source airports and i'm doing this base so in the columns of the roots data frame source airport is the iata code that links to the airports data set the same for destinations so what i'm doing here is i'm taking so the roots data frame is having first of all the source airports merged in based upon that that key then the destination airport is merged in based on that key um so for each route in the i routes i've got some i've got some confused faces i i appreciate this is this is a bit of a explanation the the goal of these lines is for each of those roots um it will have the a iata score iata code for the source iata code for the destination latitude and longitude for the source latitude and longitude for the destination that's what this is that's what i'm trying to do with this little bit of code um and then this is creating the the geometry so this is creating the line string so again same is same as before where we create a geodata frame and we have the original pandas data frame the geometry object and then our coordinate reference system this is just some mad list comprehension um taking for each of those routes and generating a line string for the source latitude longitude point and the destination latitude longitude point um there's going to be an exercise coming up in about five ten minutes and i can go around and maybe explain this these last few slides in a bit more detail if anyone wants to um but yeah i might be plotting um part of the beauty about the whole thistle like when it comes to plotting shapes i think is that a lot of a lot of the code is reusable so i like you'll probably see like i use the same sort of approach throughout but what's really clever about the data frames is that the the keyword arguments that you pass in can change depending on the geospatial data set that you've got one thing i've never actually checked um is what happens if you were to plot a geodata frame that had points and line strings within it because we saw in the seconds notebook that that's popped perfectly fine within geopandas but i've no idea actually what would happen if you would plot that data and then pass in keywords like line width how that would be i'm genuinely thinking out loud i've just thought of this like i don't know what would happen to the points if you then pass in the line width argument but anyway it's geopandas is clever in the way that it does it so in this case i'm just plotting the line strings color white uh line width 0.01 and these are the sort of things you can play around with um and then i've gone a step further and generated a map with um a different projection for one and i'm using this cardepi um geodetic argument for for the transformation what that's doing is it is converting the line strings into it's mapping them onto a sphere essentially it's not quite what it's doing under the hood but it's mapping them onto a sphere so you get the curvature um and that's there is there then you get a map of um air routes no the next exercise is to try and generate something like this so what this is is just the air routes with the airports um on the same map um i've tried what i've also done is i've scaled the size of the points the airport points based upon the number of i in this case actually both the number of journeys and to and from that airport but you could just pick i don't know uh planes landing as opposed to planes landing and taking off um but i'll come around and talk you through how to do this and help you out with it and and then yeah maybe 10 minutes or so i can go through a worked example on the board except there's no board i'll go through a work example on the slides um but if anyone's got any questions stick up your hand i'll come and help i reckon gallery is the place to go yeah ax dot stock image okay yeah i mean i think in this tutorial i'm very much showing off my style as opposed to generally how to do it well like with that said like a lot of what's a lot of what's here is the basics for these libraries hopefully you could not just go read the documentation and you know customize things however you want cardify's documentation is quite good they've got lots of information around projections which is quite useful um so [Music] the coordinate reference system for the data is in in the geodata frame is in degrees um what you have to do is you have to what you're effectively doing is you're passing you're using the using the cardepi reference system i guess um you're you're representing that data within the cardo pi object so that card pi can then reproject it is my understanding of what's going on i mean i think like on this one and that's the projection for the plot yes you're saying here everything on this plot is going to be robinson and then whenever you load any other data the transform is the thing you're saying like okay the data that i'm loading is in whatever projection it's in in his case it's geographic or clock correct or whatever that is um but it could be like utm or some other arbitrary projection and you have to tell carnify that the data that you want to plot on this robinson point isn't some other projection so it knows what to do with it if you didn't if you took that off and you tried to plot it it wouldn't it would either show up in like some weird corner of the locker nowhere at all or something because it doesn't know it's just going to take the numbers and assume you don't tell it anything it's going to assume that they're in robinson and try to follow the map and if you don't tell because yeah what would happen if you if you didn't have it you because the because the because the the axis units effectively are so big everything would get forced down into basically zero how is everyone getting on with that um yeah i've got quite a i guess you might call it a hacky solution to do it um that's the other thing as well is these slides which contain the answers i guess um are all in the git repo as well and so at the end of if you go away and you want to like approach this later or copy any of the stuff that i did you have you have the code i'm going to give it maybe a couple more minutes and then i'll talk a little bit more sort of approaching the halfway mark um and there's [Music] yeah three more notebooks to go through i think this one's probably the longest might be wrong there um but i'm gonna try and aim to finish with sort of 15 20 minutes 15-20 minutes early so that if anyone wants to come and play around with code or talk a little bit more generally about the stuff than we can do that might be ambitious though because there are things to do still things to go through is correct uh yeah i just thought it looked pretty there was literally nothing else to it um it's been pointed out you could call them based on country which would probably look cooler well i mean it depends on how you sorted yeah it would depends on how you sorted the country like if you did it alphabetically it would just be a complete scatter gun um or if you did it randomly yeah i don't know give it a go and see what happens uh but this is kind of this was kind of meant to be done on it's it's sort of meant to allude to like time zone like doing it by longitude it doesn't quite work that way but again that's another thing you could do is you could take the time zones data set merge it in to this data set and then color it based on time zone does it again even better yeah dude i have times in then right i'm gonna keep talking i think i've ever spoken for this long in one continuous sitting it's exhausting right so what have i done i have um so i've perhaps stupidly um i've done it based i've combined both the number of flights taking off and the number of flights landing to scale my points um it might have been it's probably simpler to do it just based upon um you know flights arriving because it's you know have half the lines of code um but yeah so i've done um in the roots data set source airport value coins to get the to get the number of um aircraft that take off from that airport um same for the destination so i've now got two series which are um with the index is the the airport code and the values are the number of flights that have either landed or taken off um annoyingly my thing hasn't scaled correctly let me just open the actual code that might be simpler does anyone use like jupiter slides and enjoy it i think they work oh really i can't i can never i'd like do you know how to like manipulate the css to get it to look the way you want is that possible okay so at the end of this can you teach me because i'm awful at it um which one have we got see yes so i um i create two data frames um basically i create two data frames with these with these values and then merge them together so that i've got um airport accounts which it which contains um the airports the code for the airport as well as the number of aircraft that took off the number of aircraft that landed and that's what this airport accounts data says um i then add those two i create the actual coins by adding those two columns together source and destination um and then i merge that back into the airport's data set so i've now got um the airport's latitude and longitudes the airport's um you know code that describes it as well as the counts of the number of aircrafts taking off and landing and that's then what i'm gonna and then i've just created recreated perhaps unnecessarily actually um the points geometry column and then the geodata data frame i've also then divided so that that i've created a marker size value which i'm going to use in the plot this was pure trial and error and so that point value of which which sort of looks like well like 900 they'll be like 900 vessels taking that what is atl i wonder anyone get an idea atlanta you all got it straight away um oh really okay um yeah so i've um so i've taken those values and i've divided them by ten that was trial and error and that's something that you'll just have to figure out yourself if you ever work on these um and then i plot it so same as before um [Music] i've so i'm transforming transforming the line string so you know we get this nice curvature and going with what a white color i've added an alpha value so that the lines are quite see-through so you can see the airports underneath um and then plotting the set the airport counts as well so this is and so i'm using a marker size and i'm using that marker size column that i created um again i'm using a small alpha value because um just to reduce how much it i don't know how high like in your face the plot is again these are all things that you can just tweak and this is just my style and the way i like doing it um this bit is what you were all sort of asking questions about i have used the word use the keyword column which tells um tells is that you are cut like it tells you it's the column to to color the data based upon and then it uses a color map so i've just used jet because uh i don't know why i use gem and that's it and that's the plot that you get so how did everyone get on did i everyone got got there in the end nice close did anyone color the points on something different and get an interesting plot all right fair enough yeah it's slow yeah slow um like that's one of the problems with i don't know if this i mean i work in geospatial data science i don't know if this is a problem anywhere else but plotting and rendering of things geospatially once you get over a certain amount of data becomes very very slow um like even loading data like geopandas if it tries to load i mean anything over like 200 megabytes really is be it's gonna it's gonna struggle i think the highest i've ever managed is like 1.5 gigabytes and i just had to like press go and then go and you know go to bed and come back the next morning and all of loaded like it is quite it doesn't it doesn't scale well on your laptop it's not that's generally something i've not looked into though as high this scales if you would then go into some sort of like cloud environment or bring gpus in i don't know how it would i would improve data sets and also yeah really large data sets um i mean this stuff is good for data sets yeah it's very good it's exploratory data data analysis really um but yeah once you start going into like once you get lots of you know big data it's not it's not quite big data it's a good question i'm not sure um to be honest though it will be the geometry will be the limiting factor i think um because the stuff in the other columns is just metadata it's just floats and floats in some strings and whatnot um and like so that comments about the geodetic or whatever it's called um i don't know how many line strings there are here there's probably like i think it's what like 60 000 possibly yeah it's about 60 000 line strings each one has to be transformed i don't know how well that's vectorized under the hood um but typically whenever anything card whenever you bring in card pi for any sort of transformation so for reprojections it gets very slow when it when you start to scale with data again this is where you might want to use other technologies um yeah all right unless anyone's got any more questions we can have a crack at polygons our next geospatial data type um which one's this so this is um what's this showing this is the world's um exclusive economic zones um so this data set comes from marine regions or marine features.org um which is actually a really cool data set so what you were sort of alluding to earlier mike the trying to get hold of time zones data for ages trying to get hold of a country's territorial waters or its exclusive economic zones like that data mapped and you just couldn't get it um it's only recently like organizations have like made this data freely available um so i make maps out of it but it's really useful from just like a sanctions point of view so if you're if you're tracking vessels like we do at geolect all the time um often you want to track if a vessel goes somewhere where it's not meant to go and that typically relates to think these sort of features so actually having like an iranian territorial water shape file is really useful but for years that just didn't really exist unless you unless you've got an intern to make it manually for you right so the data sets should all know so the natural earth thing so natural earth is a is a big sort of accumulation of data they've got rasters they've got shapes they've got all kinds of stuff so they've got like line strings for railways they've also got their own ports and airports data set they've got all kinds of stuff it's just like if you want to after this tutorial hopefully you're now all in love with geospatial data and if you want to go away and just start exploring things you can go and play with this data set um there's so much like useful thing so many useful things there with that said i've pushed the polygons that you'll need the two shape files i've pushed that data to the get repo this morning just so that you can specifically have those those data sets we're also going to use um open flights data again and so the remember correctly we're going to we're going to make a map of the world's time zones first of all we're then going to use uh so i'm using i didn't actually know that time zones were within the open flights data but what we're going to do is we're going to merge in the open flights data with the time zones and then plot time zones between our current sorry plot um flights going between our current time zone and the time zone that i'm going to be flying back to later this week the uk so it's a little bit of like polygon manipulation and also um like analysis or just sort of you know data manipulation uh so again sorry my um my thing hasn't quite uh it's not wide enough but i'll open this as well so you can see but the first thing is just reading in the data so the two data sets are the time zones um any 10 meter timezones.ship and admin countries.ship um these are just the first one is polygons representing the um the world's time zones and the second one is representing the world's countries so it's um we're going to use it kind of like a base map really um you could use the cardepi one but i just i wanted to just plot things out right uh so yeah the first it's a again like i said earlier like it's quite geopandas is quite nice with shape with shapes because you're able to i was trying to figure out what that was yeah no that is the shape that is the correct shapefile yeah japanese is really nice in the sense that you can just like the way that you plot lines points and polygons like once you take all of the keywords out to like line width marker size it's all virtually the same thing so in this case like what i've done here is i've just taken that data set the loaded the time zones data set and then i've plotted it um and then just applied a little bit of styling so i've when i was riding this a couple of weeks ago i decided to not use dark mode for some reason i don't know why um so that's yeah that's the world time zone what i think is quite quite cool about this data set is you can see you can see some countries borders appear um yeah but anyway i'm gonna give pass this off to you again um this is sort of my attempt to this map i just had fun with it um and made a like nice multi-colored map with a mercator projection why don't you guys go away reproject the data you know find a projection that you like is the other thing as well um [Music] which i wanted to mention um here is a list of all of car if you go if you just type in card pi projection list there's a list of all of the different projections that you can you can get so some cool you know weird and wonderful ones there and so you don't have to just use robinson because that's the one that i told you existed um go and have a look and play around with some other ones and so yeah reproject the data um using the the column and color map pairing that i mentioned in the last exercise use that to maybe color the time zones differently uh i'd like this one is just mad like i don't think you should do it the way i've done it um and then also plot the world's borders as well so take that other data set that you've loaded in and plot the world's borders so that you can see how time zones line up with with borders like just looking at and like once you do that you'll be it'll be you'll be able to see it much clearer like the chinese one for example is a bit crazy to me that like it spans like five different time zones and all the countries around it are like you know five hours ahead i don't know i think it's interesting so yeah let's go and have a crack at that for the next sort of 10 minutes um and then we can reconvene and talk about um well there's another exercise so maybe just 10 minutes this should be quite a simple one um and also so that we're going to go straight into another exercise after this so if anyone wants to do you know bathroom breaks or anything like that feel free to have to to go whenever um and if you do and you miss something and you wanna just give me a shot and i'll come and talk you through it is it slow or is it okay um yes i do yeah i usually do it in cardi pi uh there are times there are a few things that i'm not showing in this workshop there are other ways of reprojecting the data i think using cardi pi is that is the easiest first step to be honest um also i think at the last sci-pi there was a there was a workshop on carde pie and i had was that the last one of the one before and i had a watch of it it's quite good i just thought it was a nice continuity to use cardepi um but there are there are other ways to do it to go really into the weeds like cardiff uses there's a thing called pi proj which is a library with with like in fact actually we're going to we're going to touch on it later on i could maybe shut up but there's a there's a library called pi project with um 100 150 different projections like mad mad mad projections um and cardipi has got it's selected a few of them the developers have selected a few of those and they use they use pi proj they've got a series of classes under the hood that um convert your data into some sort of pipeline object and that's what that's what reprojects it um what you can do is there's it's quite easy to hack in your own projections from like extra ones from pipe rods which is how i made that um oh wait ty made this one this square upside down one um by taking one of those weird weird wonderful projections from piperodge and then hacking it into cardify just by replicating the way that they've set up their code but yeah cardiff is the way to do it because what you another thing you can do is you can take points data if you've got enough points data you can take the data um so in fact actually that's how this map was made you can take this takes like i think 80 or 8 billion ais updates so that as a ship moves around it's pinging its position this map took like eight billion of those updates and then built i built a 2d histogram from that data and then there's ways to then reproject the histogram data which is quite cool i think um but like as i say i'm going to try and leave a little bit of time at the end and i can show you guys how to do that but i'm not going to go through that in this tutorial because it's just complicated for the sake of complicated so is that ship position data is that available um no it's not available well i mean it's available like you could get it if you wanted you would just have to yeah you have to pay for it so um this company spire is a satellite company based in washington i think they've got they've got a constellation of satellites that collect because it's just this is just radio frequencies um and they they collect that data and then you can buy it so for all of the stuff that we do because it requires it's all shipping focus we've bought that data um and so i've taken it made some made some maps just for like for the company's point of view a bit of marketing because it looks cool and from my point of view for some social media likes yeah but you can get access to it so the other plate if you wanted you could go to um there's a company there's an organization called global fishing watch which has some ais data um that's available um there's a there's a danish organization cannot remember the name of them um that give out ais data samples nasa there's a science on a sphere if you come across that yeah so there's a science on a sphere data set which is a raster of shipping lanes and which you can which you can use that's not ais data but it's a way of getting that um getting it loads of there's loads of geospatial data sets out there uh right so i'm gonna go through this quickly this shouldn't be too complicated i first of all i've plotted the so i'm using a mercator projection which is the one that stretches the the higher latitudes and compresses the lower latitudes well no the central latitudes i should say so i plotted the countries um underneath so this is i think this is true in all plotting but like the let the order in which you put in your plotting statements dictates like where they sit in terms of like the layers of the plot so i've put the con i've plotted the countries first so they're sort of in the background and then i'm plotting on top of that the um um the time zones and so i'm using the map color eight column uh can't remember off the top of my head what that corresponds to but well it corresponds to time zones and again i'm using jet which i don't think you need to do there's other color maps and yeah this is the map that you get why are you all laughing um i mean again i might i need to like avoid nerding art there's a there's there's some really cool like color theory it's like talks on why i think it's very this is the best color map because of the way that everyone okay you've all clearly heard it everyone's nodding um yeah it's whereas jets just awful yeah particularly is anybody here is anybody here colorblind statistically they should be i mean it's something like 20 yeah oh i'm really sorry okay this is another exercise that um i just want to send you straight off into um and i want so the reason i'm using this one is because so in the seconds notebook or the second set of slides i talked about this idea of um merging on overlapping polygons and like finding the the overlapping parts of polygons um i think it's a cool little exercise in the resources folder i've uploaded to geojsons um actually just as a side note i haven't really discussed like the different types of geospatial data file that you get there are the most common two that you're probably going to come across online at the moment are shape files and geojsons shapefiles are quite old as a format people are starting to go off them geojson is a new new kid on the block well that's not really a new kid on the block it's another file format but anyway i've uploaded two files one's called rome roman empire dot geojson the other one's called mongol empire.geojson i think but you'll see it in the code um what i'd like you to try now is to use oh there we go is to use this um well load both of the data sets and then use these sort of methods to find the overlapping regions um the parts of the world that have at one point lived under roman empire and also mongol empire i think there should be two areas also just as a side note doesn't it look like the mongol empire is like a bear about to eat the roman empire i think so um right so i'm going to walk around um for the next sort of yeah the next sort of 10 minutes and chat to you all about this um but it should be relatively straightforward and you can make a map like this so all i've done here as well is i've taken like i've turned off all of the axes i've turned off and everything just so that you've got the two well technically three polygons and but you could you could do whatever you wanted you could add in the you know the countries layer you could add in the cardify layer just to like highlight some of the context um but yeah i'm gonna have a walk around and have a look at how you're getting on and you please ask me questions if you're interested to move on these tutorials are long i feel like i've been talking for ages and we've still got rasters to cover um yeah so i threw that i threw this um this sliding i think this comes from this is from geopanda's own website um it just shows the difference between the different um operation the overlay operations that you can do um [Music] so yeah so the solution is you know you load load both of the empire geojsons um just with japan's read file and then you do uh overlay and you use intersection so um this one and this is gonna give the parts of the two parts of the two um geojsons that overlap with one another as a new data frame so this overlap data frame is now the parts of those two data frames that overlapped and then i've just plotted all of them um and just changed the colors and that's all so there's two bits i think there's there's obviously this big one in sort of turkey and there's this tiny little one up here um which i didn't i didn't spot it first that's where the bear's mouth is uh right so the next sort of thing we're gonna go through is combining everything um into one so polygons lines and points in one plot and then that then we're done with we're done with shapes and we can move on to rasters so if you don't if you didn't enjoy um talking about shapes then it's all done um so i'm gonna sort of walk through a few bits of code um and then set you off on another little exercise um so what i've done is like what i've done is i've taken the overall goal of this exercise is to make a map which has should maybe show it off before we um before i explain it but the idea is you're going to make a map that shows flights going between texas time zone and the uk time zone and plot the airports in those as well um because one of those is going to be my flight home in a couple of days um so what i've done is i've just isolated the two time zones into their own data data frames so i've taken that original time zones data frame that we had earlier um and i'm taking so the uk is the name of that time zone is zero um and the name of the texas one is minus six or six hours we're six hours behind the behind the uk um then i'm combining them into two sorry sorry combining them into mining the two data frames into one just with concat and then plotting them and so this is these are the two different time zones plotted plotted together now we're going to bring in our old friend the airport's data frame so yeah i'm using the time zone tz yeah there it is and so this is already part of it so you don't really this is kind of a bit of an exercise in futility but we're going to do it anyway um so anyway load that in as we did earlier um and also load in the routes as we did earlier um do you need a charger i don't know if it could stretch but there's there's empty plugs down here oh i see um so again so i'm gonna rush through this because this is stuff we've already gone through um again so making a data frame of um points for the airports so an airport's data frame um and then i'm joining i'm doing a spatial join realize actually this is the first time we've touched on this this this is a spatial join in geopan this is where you are taking um you are finding data okay in the simplest example you're looking at you're taking points and polygons and you are finding all of the points that are within polygons or outside of polygons and so this predicate um argument there's a couple of different like keywords you can you can go for so if you were working with line strings for example you can go you can use the keyword intersects so if a line intersects with a polygon we're looking at points here so i've used the keyword within so this is going to look for using the geometry column in airports and my flight so my flight remember is the uh timezone the two different time zones and airports is the um is the list of points i'm looking for all of the points or all of the airports that are within the polygons of my flight um [Music] this is just sort of a bit of housekeeping um the this both data frames have the keywords sorry the keyword the column name name um and so when you do a join or a merge even i mean pandas if two things exist you'll have left and right um underscored to them and so i've just renamed them again we can't really see that on this but i'll go on to um i've renamed name left as airport name and name right as time zone name um just to make things simpler later because name gets used and there's more names so you have like name left left just it doesn't work right anyway enthusiasm uh so this is just printing the um the unique airports they are all the unique airports in these two different time zones um obviously not all that's been printed so that's a little bit silly i'm gonna switch to the notebook because i want to actually show off all of the yeah that should have been okay so similar sort of thing to earlier um [Music] we're taking the the airports and we're taking the um we're getting the source and the destination airports as their own um as their own as their own data frames um and then doing merging based upon the routes in order to get for each for each route between uh the uk and taxes well the uk timezone and texas time zone um the latitude and longitude of the source and latitude and longitude of the destination for each unique route so it's just the same as what we as as we did earlier um and then what i'm doing is i'm removing with this line i'm removing um any routes where the time zone that go within the same time zone so i don't want um for example uh uk in the uk london to glasgow flights because i want things that cross between those different time zones and then right finally just again as with the last notebook we then take that and convert it into line strings and then make a geodata frame and when we plot that we just plot the lines this is what you get so here are flights going from actually all i think the uk to yeah through ice um i guess the us and mexico um so as with earlier you can then plot the data on a on the same map that we made earlier and this is what you get um so yeah so it's it's literally the u.s and mexico so the exercise um i'm going to walk around and sort of chat chat you through this because it can be a little bit it should be doable what i like to do is to make a map that shows all the airports um within those two time zones so points for the airports as well um also clip the image so add in some sort of x-limb or while in so that you just show instead of showing um that's really irritating instead of showing this entire map we're just showing between like basically the uk and um the edge of this time zone um and also with the line projection account for the curvature of the art so so bend those lines and yeah play around with some funny projections as well um and yeah so i'm gonna walk right let's again maybe take like 10 10 15 minutes on this and i'll walk around and i'll help help you out this is what um this is what i generated what's the airport what's the what's that most northerly us airport u.s people which one's that okay this one where's denver i thought danville was around here denver over in this one i was at one over okay okay does this mean um well we'll hope there's no bug but yes let's see oh yeah sorry so i'm i'm i swipe hang on yeah you're right so i've removed with this line with this bit of code here i'm removing anything that i'm removing flights within the same time zone i just want ones that cross between different time zones yeah like it's whether you have flights from west africa going to the us within within this 2019 data set right yeah it's from these places here or indeed portugal i guess that's potentially surprising see guys this is why this is why we have geospatial data so we can have these arguments and discussions um yeah i went to a conference in denver a while ago my friend went from portugal and i think he had a layover in florida i think so if perhaps that's part of the explanation you have a layover somewhere i don't know because also like heathrow and gatwick are pretty huge airports and the biggest the only other one that's large would be the goal and it's in a different time zone anyway back to coding r and r and 20-ish left and so it'd be good to get into the rasters sooner rather than later because they're a whole new data set a whole new type of data set um okay that's really rendered poorly i'll go through it on the notebook again um what have i done here so okay so what i've done is i've i've just created a i've i've indexed the the airports that are within um the the two time zones that i'm interested in based upon the ia ta codes within the roots and so it's just a little bit of pandas to get the airport latitude and longitudes that we want um which i actually know i think about it that's probably surplus requirements because those latitude and longitudes are in the roots data frame anyway but who cares um so i've then plotted the countries so we've got that um background sort of image or at least the borders and i've then plotted the time zones again just to give just to sort of highlight the the six unique time zones that exist between these two points and then i've plotted the airports and then i've added excellent so this has gone back to that sort of point earlier about um being able to it's not the robinson projection or this in fact is the mercator projection isn't it yeah the unit the underlying units aren't in degrees um so you've got to convert your xlim to whatever unit the projection is using which i think is probably meters but i'm not entirely sure there's probably a way to figure out what those values would be yeah uh yeah there's bind to me it's just not something i've it's very rare that i will apply x and y limbs so i've never really bothered to figure it out if i'm honest um but there would be a way because it's just going to be a mathematical conversion yes all of the projections are the maths involved is within i think each of the the classes so i think you can pass in latitude and longitudes and get out and the values yeah i think so it's just not not something i've ever sort of toyed with in anger um right um i think if you guys want to take maybe a five minute break so we're done with shapes points lines polygons multi-polygons we're done not quite done we'll be back to them later but we're done for now and we're going to have a crack we're gonna look at rasters um i think just because we're changing tax slightly and the the the background that we're gonna go through is a little bit different maybe take a five minute break grab some water go to the bathroom and then we can kick off again with with rasters in five minutes start onto rasters my favorite um so rasters they like we i mean we touched on it earlier they are they're basically a 2d representation of um some sort of geos well it doesn't have to be geospatial data like it can just be an image in this case we are using rasters which are geo-referenced images effectively so a geo-referenced 2d array of um pixels effectively um although it can be an nd array this so this particular map that i'm showing is a population density map of vaguely rest western europe um i wanted to use this as the example to go through but the dataset is just too big like i think it's i think this data says like three gigabytes um which is just too much to ask you guys to download and then wait five minutes for it to load and you know and do things with so um we're not gonna we're not gonna work with this but i'll if again if anyone's interested in trying to make this stuff i can give you the give you the data sources at some point um so the two data sets we're going to use the first one is a forests data set so it's just it's an old data set from like 2003. the reason that i've picked it is because it's quite small and also it's interesting um i think uh but it's from 2003 so the date is way out of date um there is a more up to date so if you follow the link this particular data source i've put it into the get repo so you don't have to go searching for it but if you follow the links in the on the github page um there's also a link within this repo to hire um to sorry to a more more recent data set and a much higher resolution data set so if you wanted to play around with this you can and then the other data source is a surface temperature data source that comes from um that nasa earth observation site again in this case i've put the data set into the git repo because um just to make life a little bit easier um right so we're gonna be working with rasterio primer in this tutorial in fact we're gonna work exclusively with rasterio in instead of uh geopandas um so you know a bit of change uh so the first thing you to do like raster you open the data set and then read the data set um so this is a so i've created two objects a forest file and then i read the data from the forests file um [Music] the i just a little bit of data exploration with this um there are the minimum value is zero maximum value is 254. um and the number of unique values there's 102 unique values exploring the data like this with rasters is really important because no um no well no data points in in your um in your raster where there is no data recorded so in this forest data set example somewhere where there is no forest like the ocean um how those values are dealt with is different depending on which raster you're using which can be quite frustrating and it can cause problems which i am going to take you through in a second and like when you so when you plot it and if you use so what if i use the green's color map you get this plot um so effectively what's happened is if we go back to these values there's 102 unique values so the [Music] the density of forest is mapped on with 101 colours so from 0 to 101 and no data is being given the highest value in the uh in the data set so 254. so when you plot it when you plot your plotted on this when you plot the map with a color map and you get this sort of crazy plot where it looks like the oceans are full of trees full of life um i really hate the way these have not rendered correctly okay i'm just going to open up the the um the notebook um so basically what's happening is this plot sort of highlights what's happening the when you use the when you use a color map on a on a raster um what's happening is the data is ending at the data that we're interested in the forests data goes is mapped between 0 and 101 because it's 101 um values like 102 unique values one of which is the 254 that's applied to no data and the other 101 are the the values for the actual data so what happens when you use the color user color map is the data is the the data that's used to the data set is mapped with the basically the bottom half of the color map and then there is no data between 101 and 254 and so all of this color gets wasted and then this value the last value in your color map gets used um for no data which is why the oceans are really dark [Music] so we're going to get right down basically um so just just to like make highlight what i've done all i'm doing is i've just extracted a color map um arranged bindery so that um i'm mapping a color map between 0 and 254 to highlight like what's actually happening and then i'm just plotting um that i think i think this little bit of code like i'm plotting a color map is quite a useful piece of code which is why i included it and i didn't just throw up an image so if ever you want to sort of explore your color map and how it maps to your data you can pinch this piece of code um background color like so it's background colors cut off but it's just it's it's like a dark color but not quite black um i think or maybe it's light so what i do is i take i'm taking my forest data set and i'm taking all of these values that are 254 so these no data values and i'm just setting them to zero so i'm moving them to the if you think about it in terms of this image i'm taking all of these values that sit up here and i'm moving them to the bottom of my color map now in theory whenever we plot our whenever we plot this map the oceans will appear very very light because they're going to sit down at this level and the you know the areas of the world's dark densest forests are going to appear very very dark because now the data is going from 0 to 101 and hence they will sit at the top of the those values will sit at the top of the color map and so it'll be much darker and that's all i'm doing here so i'm taking so i'm setting all these no data values to 254 to zero and then i'm making a color map with 101 colors so there's going to be the same there's the same number of colors in my color map as there are unique values within the data set um and then i'm creating a listed color map from that oh i'm i'm setting a new color at the bottom because i wanted to complicate things so i wanted to i wanted to put a custom color for for xero um because i quite like not quite white um colors but you can sort of ignore that it's not really that important but anyway i'm making a color map with 101 colors to use on this data set um and when you plot the data this is what you get so it's now much clearer and you have um really dark areas where you would sort of expect so the amazon rainforest southeast asia the congo parts of parts of the us actually which i was quite surprised at i didn't oh i've not really been here very much so i don't know how many forests you've got um but that's roughly what it looks like um the next thing i want to talk about is how to reproject data so i mentioned this earlier to someone um this library called pi proj which is a um i mean i'm pretty i don't know exactly how it links to it but this links pro pipelines library links to gdal which is a geospatial sort of coordinate reference system like mammoth um thing i don't i we can maybe talk about that a little bit you know afterwards um i don't quite understand how it works but pi project is basically a python library that is you know has got all of these different projections and the mathematics required to translate between the different projections but it's quite hard to work with and so cardi pi uses pi project whenever it's um whenever you project from robinson to mercator or whatever um but they've only card pi has only implemented a couple of these projections um for some reason i think it's i don't know why to be honest um but because we because this is rasteria we can't use card pi so we've got to use something else if we want to reproject data and that's what i'm going to talk about here um this piece of code just shows all of the different projections like if you run it it will give you a big list of different projections that you can choose from so later on i'm going to get you to reproject some rasters and hear all the different options that you can use so we're going to use this library called rio x array and it's quite a new library i've not come across i only came across it recently um but it's a library that allows you to um re-project using um pi piperage uh raster data um so this piece of code what it's doing is so we're now moving so that i'm moving on to this um surface temperature data set the reason i'm doing that is because it's much smaller this is quite a computationally intense process i'm not sure why it just is and so that forest data set if you run this code on it'll take sort of 5-10 minutes which is just a little bit too long i think for this exercise so we're going to use this other data set which is much smaller and hence this operation will be much quicker so what it's doing so what am i doing i'm loading see okay so i'm loading the data set the original with with rasterio the reason i'm doing this is just because i want to plot um i will use it later um but anyway so we opened that we opened the data set with rio xray um i'm then printing the original projection or the original um coordinate reference system so it's this epr eps g4326 so it's just the classic default what you would what you get normally um i'm then using pi proj from string so what what this line is doing is it's um it's loading a projection object and it's taking it from this is coming from piperodge so project equals robert rob robin is the robinson projection and i'm then reprojecting the data so this open file that we have and dot reproject and i'm passing in this projection object or this um coordinate reference system object um so this will take might take a minute or two and when you run it i'm not sure how long how slow it will be and then if you plot if you print the new projection of the new coordinate reference system you have this all of this information that comes in and this comes in from from pi proj um and then the final thing to do is to convert that so it's still technically an open file i think so you then convert that into a numpy array and plot it which is what the exercise is going to be but you then we can then plot it and it will be a nice newly projected um data set so that was a bit of a whirlwind i went through that quite quickly i'm going to walk around and sort of chat to you as you go through this but the exercise here is to plot both um so the old data which we've loaded so um original surface temperature and the new data surface temperature so plot both of them and see how different they are because of the way that this data set's set up they're they're going to have projection they're going to have issues with their color map so take the [Music] lessons from above so this stuff around color maps and create a new color map that's going to accurately represent that data and indeed you can reproject it if you want be careful with the projections that you pick like i have shown all of these different projections that exist whereas it we've used we've used robin for the robinson projection so in theory you could use all of these i suspect what you'll find because some of these projections are utterly bonkers what you will find is that a lot of them won't generate data or they will generate data but it'll be like smashed into a single point so play around with it and see what you get um but i'm going to walk around and sort of yeah i'm gonna walk around and see how you're getting on and be on hand to answer questions yeah this is that's that's what you should get to but like i'm not sure the answer even though you've got the answers right i'm gonna go and sort of walk through what this looks like um did everyone have a good crack at it nobody ran into any issues cool that is good news um did anyone run into any issues trying out a weird and wonderful projection no well that's good um as i say some of them can be some of them can some of them can be a bit hit and miss in what it generates um but you can get some really cool ones but yeah anyway so what i've done is just yeah this is just the two plots so i've ran that code that's above and then i've plotted both of them side by side just using um subplot and that's the other bonus about um rasters so again so i can't remember who i was talking to about this earlier like the idea it's quite hard within geopandas and cardify to use plot.subplot and have like a nice well-arranged series of plots but because in this case we're effectively just well we're using just native matplotlib for the plotting in this case at least it's very easy to to build like nice little plots like that and sort of um you know layer them together and so there's color map issues that need to be fixed um so the same so the same sort of thing as earlier um no no data values are mapped to 255. it's the highest value in the data set and it scales between 0 and 255. so what i've done is i've i'm operating on both of these here but so i'm i'm taking that value of 255 and i'm moving it to zero i'm flipping it to zero so that it's nice sits at the bottom of the the color map and then i'm generating a color map i'm using red the reds color map i don't know don't really know why i chose that um oh because it's surface temperature of course yeah you want you want hot to be red um and then i'm i'm and then i'm making i'm making a a boundary norm object of 255 color 256 colors and then when you plot your data this is what you get and so the background color is not the background is now nice and light and it scales up to hot areas um and on the left hand side it's just the standards like standard projection and on the right you can sort of see the difference um in terms of projections so it all worked relatively successfully what i'm doing here is i'm just trying on a different projection um so this is using um projection equals mrec this is uh the mercator projection um and you get this crazy plot again it looks i think actually to be honest antarctica being included is relatively interesting um right so the final exercise in this is i think it'd be cool if you all go to this um nasa website and pull down some tiff files of your own and create your own map so i'm just going to show you how to do that because i realized that it's i've not actually said said that and it can be a little bit picky um so what's this can somebody explain to me what albedo is okay okay uh all right so so i'm gonna download one of these so just go to on the on the right hand side file type to geotiff raster um and then click the resolution that you want and that'll be it and then just copy that to wherever it is you want to do your day do your data analysis and play around and see what see what sort of cool maps you can make i think there's some cool ones here like um let's see i think there's a forest fires dataset which would be quite cool to plot yeah land that'll be it there you go active fires that would be a nice you know depressing one but yeah um so anyway i'm gonna again i'm gonna walk around for another sort of five ten minutes let you play around and see what sort of maps you can make and then we'll move on to the final final tutorial which is combining rasters and um shapes into one into one plot and hopefully we'll be finished sort of around quarter past which will give us maybe 15 minutes if anybody wanted to go into like any more detail about things but yeah give it maybe just give me maybe sort of 10 minutes to play around with things and i'll come around and if you've got any questions just give me well ask them so [Music] nearly done it's nearly times you all go and have a delicious alcoholic beverage instead of listening to me it's the final thing i want to talk about is is rasters and shapes um rasters and shapes together can be a little bit funky because you're using so rasterio itself has its own like internal plotting functions and geopandas and cardify have their own and they sometimes don't really play that well together so i'm going to give some examples and we're going to work on a piece of you know hopefully bring together all of the stuff that we've done so far over the last like three and a half hours um into one final piece of um work um what have i shown there that's a that's a map of shipping lanes which is a raster and a um map of the flight path map that we made earlier m on top of each other so this is using both geospa both geospatial data types the raster and the and the shapes um so data scores that's quite funny and so river basins uh that should be included it's um i can't remember it's like hydro basins underscore southam dot shape i've pushed that to the git repo so that should be there topography uh is not because it's quite a large data set and i'm just open it yeah so if you go to this website and download it and i think it's a 312 megabytes so it's quite large so i couldn't push that to the git repo otherwise get would have got okay hub would go angry with me um what we're going to try and do here is we're going to [Music] we're going to plot a topography map in this case of south america because it's got lots of rivers and we're going to overlay on top so that's and that's a raster data set and we're going to overlay on top river basins and then we're going to play around with the color map so that you can see how topography well obviously is going to define where the river basins major river basins are um i've given you an exercise what do i want you to do yes so using what using what we've done earlier it can't um go away and just in the next couple minutes it shouldn't take much effort download the data and then use rasterio to open and plot the data set just any sort of plot it doesn't matter just to explore what it looks like and then use geopandas to open them um so the uh river basin so yes the excuse me the topography looks like this um this is actually just a bit of a side note it is this is also it's not just topography it's topography and bathymetry as well um when plotting this on when plotting um these sort of maps with color maps they look really awful like you you can't tell like so this is just using a viridis color map you can't really tell where the high and like outside of the himalayas the rockies and the andes you can't really pick up any other detail really the reason for that is because the color map is just doing so much heavy lifting going from what's the deepest point on earth it's like minus 1600 meters or something like that all the way up to mount everest so like again what's that like 8 000 meters something like that um and so you've got this mask this really really low point and it's really really high point on the two at two extreme ends of the color map and then actually the rest of the world kind of exists within like quite a narrow range in the middle of that and so that's why everything just get most of what you see is just sort of mapped to this to the similar sort of color um but we'll be addressing that a little bit later because that needs to be done because the andes will wreck our map if we don't in terms of the basins if you plot the basins you get a map of south america as you'd expect there's loads of little this data set has major basins and little basins major basins would be like like within the amazon within the amazon river for example there are loads and loads and loads and loads and loads of little mini tributaries that feed into it each of those tributaries is kind is part of its own little basin within a larger basin so what's been plotted here are all of those little basins but within this data is cap and those larger basins are captured and those are what we're going to plot um what have i done here so i'm so there's a series of colors that over my career i've decided that i like um so what i'm doing is i'm creating a data set um of these color a data frame of these colors and i'm mapping those colors to the i'm mapping those colors to the basin's the mage madge name is the is the column name of the major basin in this basin's data set so i'm just taking those those names converting them to a list and i'm making a data frame of basin and colors i like and then in this next cell i'm merging the basin's data frame with my colors data frame using madge name and basin so that i get this nice plot of colors that i happen to like and [Music] and basins so it's like all sort of what you'd expect you've got this massive red one where um the amazon is you've got this quite large purple one where i think the rio de la plata is um and i don't know the rest um if you do well we'll talk about another time but anyway so this is what you what we get to um so what we're now going to do what we would like to do is then take these basins and plot them on top of the topography to see how like changing in topography basically governs where these basins actually exist um but the first thing that has to be done is we've gone through i've sort of showed it earlier like this is a global data set the topography data set or the topography and bathymetry um we're not interested in any data outside of south america for the purposes of this exercise so we're going to use um quite cool little function within rasterio to clip out data to remove data and it's called mask and what it effectively does is you provide it with a polygon or some sort of shape to be honest actually that's not true it has to be a polygon because a line like a line and a point aren't going to work you provide it with a with a polygon um and assuming then this is this is the bit that you need to get right you need to make sure that the um coordinate reference system of your or the projection depending on how you want to think about it of your um polygon and your raster are the same and then what this function essentially is going to do is it's going to overlay the shape on top of the raster because they are because they are geospate they're both just basically referenced so you think you can map the two data sets together and then any data that's outside of that polygon is going to get chucked away and you'll just be returned with a cropped image or a cropped um 2d array of um all of the raster data that existed within the polygon that you passed it um so what that that's what this that that's what this the that's what this piece of code here is doing so map musk.mask you provide it with the image which is what we called our open geotiff or our open tiff object earlier um and then you provide it with a list of you have to provide it with a list of geometries to use in the in the clipping so i have just taken um [Music] i've taken all of the geometries from the i've done a little bit of list comprehension here and i've taken all of the list so all the um geometries from the basin's object and i've passed them in um i'm redoing it a second and then redo it a second time there's a bit like this is a bit hacky the reason being so what you get returned with is sa array which is the um the 2d array of value 2d array of raster values within the polygon um what i wanted to do and then so you can provide an extra keyword for no data so if you think about it like south america is not like a perfect rectangle um val the bathymetry data for example so the c floor data that exists around that that's going to get mapped that's in order to it can't return to an array that isn't like n by n it can't return like an exact south america array to so all of the values are on the outside um get mapped to something or they're they're given a no data value uh there's a default for this i can't remember what that default is probably zero but i don't know um but what you can do is you can provide your own you can like hard code what you want it to be so in this case i'm just i look i do i do the operation once because i want to know what the minimum value in my data set is and so i can then do it a second time and then i can say that the no data value is going to be one value less than the minimum so that when we come back to the color maps later um we don't have to do all of this like messing around with um moving things moving things around where are we yes here we go so that's what that's doing um [Music] so what i would what this is an act this is an exercise we're running a little bit short on time but i think you've got plenty of time to have a crack at this um we touched on it back in this tutorial um so using this idea of geodata frame dot boundary i want you to use that to convert the basins polygons into line strings so take those polygons and then take the data frame and use the boundary operation to get line strings corresponding to the edges of them and then plot those um and let me know how you get on that should only take five minutes then i'll sort of walk you through it um which which shape file well so well any shape file right i mean like how do you save the file you want to load when there are like eight files um yeah so you're talking about this this idea basically um to tell you the truth i don't know how all the data breaks down in these different files but the dutch this this file represents this is this is the data that you're opening um within that file it points to all of the other files that exist for you know there is there are things in it that it needs um but what you're doing when you're loading a shapefile you're loading the dot dot s hp um no it does not there's a it's well there's an xml but that's a different thing um and that's one of the reasons why shapefiles are on the way out because they're just a bit that's just a bit mad [Music] yes is everyone else doing all right yeah just a bite solution looks like quickly um because we are starting to a little bit short on time we will be all right um what i've done is i've done a bit of what's quite funny to me actually is that what have we done here ah right i say so what i've done is i've taken all of the the problem that you have with this is that each major boundary consists of uh a series of smaller polygons um so what i've done is for each major boundary i am um using i'm doing a bit it's list i'm basically doing list comprehension i'm looping through all of the major major major boundaries i'm then performing a unary union operation on all of the polygons included within that um that basin um i think this is yeah well maybe paul's influence so that that's that's a bit of madness this comprehension which i can talk you through if um you need then what i do after that is i then do the boundary operation so again another bit of list comprehension which actually probably isn't needed but yeah so i loop through all of the geometries all these binders all these large polygons representing the major boundaries and i get the boundary opera the boundaries of those polygons so i know i have a series of multi-line strings for each major basin and then what i do beneath that is i create a geode another geodata frame and so pandas data frame with my basin names and then i use the geometry that i use is these multi-line strings and then when you plot it this is what it looks like so it goes from earlier where we had a a series of polygons now down to a series of lines now the reason i've done this is because when we come to plotting it on top of the topography map if we have a series of polygons we would have to apply quite a heavy alpha value to make them see through so that you can see the topography underneath which um would kill a little bit of the the readability of the plot um but by doing it this way and just plotting the boundaries um we can it it will look a little bit better um okay so then i think moving on to the sort of the final thing so now we have so from earlier we got our um uh what's it called our south america specifically our south america um uh topography data set um what we're going to use we're going to use rasterio has got its own much like geopanz has got its own sort of plotting functionality rash stereo does as well again much like jiopan this is built on top of matplotlib um so you need to import this function called show and then you can plot the essay array which is what came from the mask function earlier um this transform equals clip transform clip transform is one it isn't is something that's returned from the mask function and it relates to the uh it's the information about the geo the the coordinate reference system and i've used the jet color map again because i'm not friendly to people who colorblind um and then i'm plotting the new basins object which is these which is the line strings of the boundaries on top and this is what we get um so the only problems really that need to be sort of like now is just customizing the plot so i'm gonna just i think blast through this now um and let you guys have a play um as far as like messing around with the color map is concerned i think that's something you guys can maybe have a crack at yourselves the problem in this instance is that so we need to add a background color so we need to remove this this blue value that this blue is the lowest value in the color map what i would do is remove the or replace that with something lighter or yeah with something lighter and which is what i'm doing with this background color and then adding it in at the bottom of the array and so when you plot it you get this um what i've done here and this is what i'm going to get you guys to go away and have a play with for the last five ten minutes of our session is whenever you mess around with rasters like color maps are just the most you spend a lot of time messing around with color maps um the problem that we have in so in this case everything is blue because the and the andes are so big and so if you think about it in terms of you've got like south america is not flat but it's pretty flat when you compare it to the andes so you've got this massive mountain range which get all of the all of the like the diverse colors in the color map are mapped to that that and by comparison then the rest of the country looks quite flat because you know compared to how tall the andes are it's it's not very tall it was a poor explanation but it's the same problem that we had earlier with the forests dataset for example um so what you can do is you can play around with one the number of colors that you have in the color map that you use and two the boundary norm object so binary norm is this um it defines how you map values in your data set to colors in the color map basically so what i do here is i say um so this value range object i defined earlier as the difference between the maximum value is that what i've done yeah it's the yes it's the difference between the lowest value and the maximum value in my south america array and so what i do then later on is i say this maximum oh no i know what it is so it's the value range is the maximum value in the in this data set so the highest point in the andes effectively so what i do is when i bring in my color map i lower i only want i bring in a smaller amount of colors that are needed to map to every point so let's say there's a hundred let's say for the sake of argument that the um the south america data set has i don't know it's a hunt like highest point's 100 meters and there's a hundred and everything is mapped to it to the nearest meter um what i do is i say i only want a color map with 50 colors and then when i define a boundary norm object and the binding normal object maps colors kind of it maps colors to values i'm basically saying anything above 50 just color it the same so when it comes to when we then plot this a little bit later on everything above a certain value in so anything in the andes basically is colored the same and so all of these the intricacies start to come out at the lower altitudes um so yes i think with that it's probably best if you have a play with it um and then we can have a com yeah we can have a chat um yeah over the last 15 minutes so i think i'm going to walk around over the last 15 minutes if you guys want to have a play um i've put in but it's not been ran it's not i've not yeah this is it um if you would like there is there are more data sets that exist there's more like river basins data sets on this website fao.org have a if you want feel free to download one and make you know this map up for europe or north america or whatever um or indeed do it for a specific country but that's pretty much us so i think you go through and have a play with this and i'm gonna walk around and we can have a chat around color maps and stuff but we are pretty much out of time and i think everyone's flagging yeah you could you can log scale it but um yeah you can like give it a go and see what it looks like i don't like doing it because i just don't like the way it comes out i think yeah i think in the end you run into the same problems you could probably get to something that looks good in the end but it might be really hard i mean yeah right so explaining it to somebody yeah other thing you could do is like split your like you really want to customize your color map just like split it
5,Introduction to Property Based Testing,https://www.youtube.com/watch?v=za7SPz5APKM,let's jump right into it so we've got four hours in this tutorial and i intend to let you all take lots of breaks because i'm aware that this is going to be an intense experience the tutorial is sort of broken up into four parts the first is just what is property-based testing anyway maybe you've got some idea from the blurb but hopefully i can give you more of an idea here then we're going to talk about describing your data that is how you work out what inputs your text should accept and how to describe those then common test tactics or design patterns and then finally some of the practical stuff that other tutorials or documentation sometimes skip over like how do you configure this what's the difference between a regular ci job and what you would do maybe if you wanted a nightly one that kind of thing so we'll talk about the settings and the performance issues so property-based testing 101 uh to start with let's talk about what testing is in the first place for the sake of this talk i'm describing testing as the art of science of running your code and then checking that it did the right thing so there's a bunch of other cool things you can do to make sure that your code has less bugs that are not testing putting assertions in your code is a great habit but it's not by this definition testing it's a great complement to testing it makes your tests more effective but it's not in self-testing using a type checker like my pile pyrite depending on your project can be a useful way to avoid spokes but it's not testing using a linter like flake8 or highland getting your colleagues to review your code making sure that you've got enough sleep the night before or coffee in the morning off as you prefer all of these are great techniques for having less buggy code but they're not what i'm going to be talking about today what i am talking about though in testing there's a whole bunch of sort of different subcategories or subdivisions of tests and just like a few kinds of those there are what people call unit tests where you have a relatively small thing that just checks what some small piece of code does you might have an integration test which is a unit test with a bigger unit people keep arguing about the definitions here but that's the only one i've found there are snapshot tests where you run your code and save the output like take a snapshot of it and so that in future you can rerun the code and check that you got the same output that can be quite useful for a lot of things there's parameterized tests we give it a list of inputs and outputs like a table and make sure that every entry of the table is held there are fuzz tests where you just put in random stuff and see if your code crashes those turn out to be embarrassingly effective uh and then what we'll be talking about today are property tests or property-based tests which are essentially the same as funds tests but you can add assertions about what your code is they can check for example if you save your data and then you load it back you get the same data you just saved that one's pretty easy to screw up and also quite important not to scroll and then lastly there are what we call stateful or model based tests those are a pretty cool generalization of property tests but because this is an introduction i'm basically just going to tell you what they do and if you ever need them you can go consult the documentation so with apologies to my friend david he mentions that every time someone uses reversing a list twice to demonstrate property-based testing nutrients it's not a drinking game he just doesn't like bad examples the example there is that if you reverse the list twice you should get the same list but this is like almost impossible to step up and it's not really a problem that anyone would ever tries to solve in the first place so it's not a great example um on that basis let's talk about sorting lists sorting is a little bit more complicated than reversing and what i've put up here are three tests that you might write for a sorting we're just imagining here that you don't trust the sort of built-in but python's built-in functions are in fact pretty much so we might test that if we sort the list one two three we get the list one two three and if we try this with floats three two one we get one two three and we can also do this with strings if we try to sort b c a we get abc what do people think about these tests yeah they're okay but they're pretty short so it seems to do about the right thing but it's not super convincing you could you absolutely could and if you were going to do it forever you might want to use a test or you can just write the list of all the inputs and outputs and that makes it much easier to add more examples if you're one of your clients or another person when your team reports a bug you just add it to the list and send everything to write a whole new test so that's a pretty good start um but it'd be even better if you didn't have to think up the correct output by hand as well to say this input is something you want to test and so sometimes you have a trusted reference implementation this is actually maybe more common in science than in other domains where often we'll be like re-implementing some older simulation or we're doing performance optimizations or we can run in single threaded or multi-threaded mode so often you can in fact just compare it to something else which you think is more likely to be correct or even just have not fully correlated but when you don't even have that all is not lost if we're testing a sorting function we don't need to know how to correctly sort of list to check whether or not the output list is sort and so here we're saying like well if we take all of our inputs and we sort them and then we look at each pair of elements in the output they should be in order what do people think of this test okay it is more compact yep great question would this break on a single item it would actually not break on a single item but the loop would execute zero times because that second slice would give you an empty sequence and then when you zip that together you've got no evidence so as long as the call to sort it raise an exception this test would pass on any single element list which seems fair right a single element list is always sorted it does not assert the link if you define sorted as return empty list this test would pass that seems like a bit of a problem actually so maybe we should check the length of this and we could also check that we have the same set of elements that we started with otherwise you could just return range length what do people think about this test it catches that one it doesn't catch duplications if for example you passed in one two one and the sorting function returned one one two this chest should pass you might wonder like is this a realistic bug but if you want to be rigorous about testing that's just the kind of educates you want so the mathematical definition of sorting is that it's a permutation of the input such that the output is in paralyzer so here's that definition any issues with this test now this one should work for anything where you can compare the elements the main problem i think is just that calculating every permutation is pretty slow so we might want to check that we can use the collections.counter class so we say we have the same number of each distinct in the input as the output um congratulations we've just invented coffee based testing so the sorting function has these two properties right that the outputs are in order and that there are same elements in the input as the output and if you check those properties that's what we call a full specification then this test passes for every function which is the correct sorting function and fails on some input for every increment yeah yeah you could let's serve this in here yeah that is in fact a full third of this talk of this workshop so part three we're just gonna be talking about design patterns of different properties that commonly come um but the other thing i want to emphasize is that what we call partial specifications are still really useful right you don't have to be able to pin down the exact behavior to find a lot of parts so having invented property-based testing the real question here is where do we come up with these weird edge case inputs uh and my favorite answer is use hypothesis so if we have the same body of our test function here i've just aligned it all with the comments in the same space we can say with hypothesis the given declaring and we say that our argument is one of a list of some mixture of integers and floats or a list of strings you can't sort a list which is mixed numbers and strings because you can't compare novels to strings but you can compare to floats so we're willing to have a list with those mixed in the same numbers does this seem fairly reasonable turns out this test fails now because of not a number so if you compare not a number to anything no matter whether you compare it's less than or greater than or equal to it always returns false so specifying what a sorting function should do with another number in the mix is a lot more complicated and so often i just say like i don't want this test to generate i don't think it's over or well infinities are fine they've got to be comparable and so that's property-based testing in a very small microscope um the main advantages here is that hypothesis helps me by generating data that i wouldn't have thought of myself and that's part of the reason having this project right you can centralize all the effort of devious people coming up with horrible counter examples into one place that can generate something um instead of checking that we get the correct result we check that the result we've got is not input so this is a slightly weaker you might want some traditional unit tests as well or you can add those to a hypothesis test as i'll show you later um but the last thing is that i often find that writing property-based tests helps me find bugs in my understanding of the problem not just the code found where before i actually put this talk together i have never really sat down and thought what should my sorting function do if it hasn't not a number it's not obvious um and then finally a point that i didn't show here but you often don't even need assertions in the body of your text often just putting really weird inputs into your functions will cause python to raise an exception somewhere and that's often worth finding out on its own this is especially powerful if you put those assertions in the code that you're testing so the error is raised as close as possible to the point where something that you didn't expect all right so we're going to take a brief interlude here to make sure that everyone can open up our exercises for this tutorial so i just see a quick show of hands who has already had a link to this sentence right most people but not quite so i'm going to give us a moment just to make sure that we can set up if you're having any trouble raise a hand and one of this will come around yep uh great question i think it basically depends on the kind of thing you're doing uh so at work we've often had to sort of use the conditional because we often run in optimized mode but we don't want the assertion to be optimized out for things which are expensive that you don't want to check every time instead of just in testing not in production user statement is great i i think this is basically an independence kind of answer which is why it's engineering yeah yeah that that really depends so for hypothesis itself the way i tend to do this is that well or my philosophy about the assert statement is that the assert statement should only be used for things which will always be true unless there is a bug in your code right so if it's something where the user might pass you a weird input that should definitely be a conditional because that's a situation which you might actually need to handle if it's something like um oh there was a bizarre bug a while ago on pie pie where something wrapped improperly on 32-bit windows and that we eventually caught by our insertion because we thought this was impossible but we were just going crazy we started asserting sort of every invariant we could think of and eventually one of them testing yep yeah i would think of it as being pretty much like a unit test the same way that a parameterized test is yeah like in terms of logic you're writing a test like paired with the domain of possible inputs for all whatever or like given any whatever this should be true excellent can i see a show of hands a hand up for everyone who does have it working have you got the tutorial installed and working or open in a binder but yeah okay two minutes uh if you have downloaded and installed things or if you clicked on the mybinder link and had a notebook open then we're good uh all right jump into part two of the tutorial so at the end of this one we'll have our first sort of substantial block of exercises and at the end of that time our first break that you can get up stretch legs and so on so describing your data this is the bit where you work out how to get all the things that you might want as inputs to a test right so i showed you before lists numbers and strings but there is in fact more to python than lists and numbers and strings who here has ever used a python data type that is not a list ninja flow forestry most of us great that's a relief um so i tend to think about hypothesis we call them strategies because the word generator was already taken so i think of hypothesis strategies is coming in a couple of different categories the first is for scalar values things like none booleans data times date times numbers text that sort of stuff there's also collections of those lists tuples dictionaries etc and then we get into the kind of more specialized stuff so the first is that you can modify all your strategies by mapping and filtering with whatever chord would you like there are certain special strategies like just and sampled from a few ways to do recursive data because that does come up in practice and then we'll talk about how to infer strategies so you don't have to write them out by hand all the time so scalar values as i mentioned uh the short version here is if it is a built-in type and python or a common standard library type there is probably a default strategy for it so it includes dates times numbers strings and all the built-in specials that's pretty simple the numbers all tend to have a minimum and a maximum value and for things like floats you can also say whether or not you want to allow infinity not a number whether you want 64 or 32 or 16 bitcoins and whether sub-normal numbers should be allowed if you have never heard of sub-normal numbers congratulations i'm just going to walk through a couple of examples here so this test looks at a package called binary or not which has a helper function is binary string and so my colleague david wrote this very simple test he said well given any thing for the binary strategy which is the byte string just call his binary string on it and we won't check the output we'll just see if it crashes works really well i think just test it no need for an assertion just call the thing uh it crashed it turns out the is binary or not function uh used the char date library internally which tries to detect what character encoding this is for a unicode string and when charge returns 100 confidence that it's a particular encoding that is not intended to convey that it actually is a valid string for that encoder but the binary or not package at the time thought that it did just a simple miscommunication based on the docs uh and so this bug was actually in some sense a bug in the understanding of yours let's look at another one there's the mercurial tool which is an alternative to get and it has this function to encode a byte string in utf-8 and so we wrote a simple test that well if you take a binary string and you convert it to utf-8 and then back to a binary string you should get the string you started it seem reasonable you might be able to guess where this is going uh we found another bug and this one i think was something about ucs16 surrogate characters but the point is there were valid inputs to the function that caused it to crash so this isn't even an exam an example where it returned an output that didn't round trip everything were returned did in fact run correctly it was just that some possible inputs made it crash instead so of course the point of finding these bugs is to fix them and it was fixed very promptly what about date times so if we start with the date time and we get the iso format string and then we pause it using date util then iso four like that it turns out that we could get a bug here as well where if the year had a single non-zero digit like it was a year between 0 and the 9ad and the year was equal to the seconds digit then it would swap the month of the day the year and a month so um who do you think they ever would have written a test to call this not me either there is kind of an open question as to how much this kind of bug matters how many people are dealing with particular date times before the year 10. it doesn't come up for me very often but i think it's a good example of kind of power of hypothesis about making general statements all right so much for individual scalar values there are also collections uh the simple one is lists you can say what you want it to be lists of you can put a minimum and maximum length on the list and you can also say that it should be unique or unique by some particular function for example you might want a list of numbers which is unique by absolute value a list of strings which is unique by the lowercase version um you can also use things built on top of this like the dictionaries or sets or iterations strategies the tuples strategy is a little different to lists tuples is always for a fixed number of elements where you specify a strategy for each index separately so if you want a tuple consisting of for example an integer and a float and a string you can do that or if you wanted a 3d point you could describe that as a tuple of three floats um there's also fixed dictionaries if you want a specific set of keys uh with a corresponding different kind of value for each these dictionaries is how you specify that this is feeling a little data dumpy yeah so if you want particularly like a json object with a name key and then some string that would be fixed dictionaries in your class name equals text and then it would generate your dictionary where it had a key that was named and the corresponding value was any possible unicode stream so compared to the faker library hypothesis is much more specialized to finding like weird data that breaks things so faker tends to emphasize and related libraries sort of realistic data but we tend to argue that realistic data like you would see in production is good for performance testing but because it's the same kind of data you see in production it's less likely to trigger the really bugs for you uh so let's look yep um uh it's it's not a secret exam exactly but it is the kind of thing that we've written papers about uh so the very short version is that we use a random number generator that choose between alternatives um the rest of the secret is that we tend to think about this as taking some random string of bits and then passing them into whatever values you ask for so from that perspective every strategy is secretly just a parser yeah we will get into that other settings uh but yeah that's that's related to how we make sure that we can find minimal examples and we always replay previous values so this should never be flaky let's look at a somewhat more sci-fi flavored example let's suppose we're doing some advanced statistics and we want to calculate the mean of the list of numbers so the mean should always be at least the minimum and at most the maximum element right seems reasonable and the standard library has a statistics.mean function which should implement mean so let's see what it does if you give it two very large inputs they overflow to infinity when you add them together it's kind of classic problem so much for lists do people kind of understand how we get here uh i mean it craps out when you run out of ram but you can get a very large entity before you run out of ram uh hypothesis tends to restrain itself to 256 bit or so if you need larger integers than that like for a cryptography thing you'll tend to be generating them as flight streams instead um but the other notable thing here is that the minimal example that it's found and reported to us is not actually any list that this happens to it's the shortest possible list that you can get this kind of overflow from right because you need at least two elements in this bathroom so we've got a list with exactly two elements and these are in fact the smallest quotes such as adding them together exceed it so it's kind of nice that we get minimal examples not just the rank of examples um supposing that you want something a little different the map and the filter methods are available on every strategy and quite useful that you generate an example and then you apply the function that you're mapping over but people like used map with pandas for example or not built in most people is pretty similar here if you want for example integers.map string this will give you a piece of text like a string object which consists of digits maybe with minus signs if you didn't want the minus sight you could pass min value equals zero to your input strategy filter on the other hand is great if there are just like a small fraction of the inputs that we've generated that you don't want you pass in a filter function and if it returns false hypothesis we'll just try again and it will generate you a new example that eventually passes that filter function that kind of describes in a nutshell why it's not always the best idea as well if you want for example integers dot filter is prime hypothesis is really going to struggle to find your primes by rejection sampling so we might find like two three five and then just give up but take like ten minutes trying to generate more things um and so here's two different ways to generate an ordered pair of integers using these methods the first is that we could filter right we say well we have this pair of integers and we want to throw away all the ones where they're not in order the more efficient technique is to sort them and then turn them back into a tuple right so that way all of the examples generated by the strategy on the inside will make it all the way through the outside making sense so far cool some of the special strategies the just strategy is for you have a value but the api requires you to pass a strategy instead one classic example is that the date time strategy requires time zones to be provided by a strategy so if you want all of your date times to be from the same time zone you can use the just strategy to give you just that time zone pretty simple sampled from is pretty much what it sounds like it's an element sampled from some signals so maybe you want an inner join or an outer join this also works well with the nums if you're using numbers including flag numbers it will just magically does exactly another two special strategies are one of and nothing one obviously union of strategies like the vertical or operator that we saw earlier if you want uh integers or floats you can do one if you want arrays or lists you can do one-off arrays lists and nothing is a special strategy which is like the empty set you can't generate anything from nothing because there's nothing there and this means that if you pass one of nothing as well as some other stuff but nothing just vanishes so it's quite convenient if you're writing your own custom strategy functions and there are some cases that where there's just nothing valid you can return nothing and then that will sort of drop out at the union later or it will raise a fairly helpful area if it makes it all the way to the music unfortunately you can't in general subtract strategies or take the intersection which would be natural if you think of these as sets of values because those map or those filter functions that you apply can do arbitrary other things they might be stateful for example if you're using the django extra it will generate you for example a user object for your web app and make sure that the corresponding data is in the database it's a little hard to work out how you would take the intersection of that and something else so they're more like recipes than sets of values the builds strategy is another special one who here has ever written a custom class or a custom type most of us well if you want an example of one of those you pass the class object or any function or callable object to build and then you pass strategies corresponding to the positional and keyword arguments that you want to pass to that and hypothesis will draw the values for the arguments and they call your thing and then give you it also has some nice magical type inference so if there's a required argument type annotation that you don't pass a strategy for hypothesis will just work out how to call the point recursive data who here has ever had a recursive data structure like json this defines every possible json object using hypothesis it's a recursive data type base case is that it can be none or a boolean or a number or a string and then json objects can also be lists of any json objects or dictionaries with where the keys are strings and values and so it takes two and a half lines to express that with hypnosis oh strictly speaking nan is not allowed in jason you want to exclude me i mentioned before that you can also infer strategies from types the idea here is that it's designed as a time saver because our goal is not to make you spell out everything it's to make you spell out just enough to make sure that it's unambiguous um so if the type inference isn't working you can always write a strategy back we try to avoid that thing where like it's magic until it stops working that you have to write everything by hand in hypothesis you should always be able to write just the bits that you want on standard and have everything else and if you have custom types you can also register a particular strategy for those if you have a class where for example the age is an integer but it can never be negative then you can register with hypothesis that whenever it's generating an instance of this class it should respect that constraint we've also got some others who here has used regular expressions they're pretty convenient if you ever want random strings that match an arbitrary regular expression hypothesis can give that to you which is pretty useful who here has used numpy let's do this again if you want elements for some numpy data type hypotheses can also infer those for you and then if anyone here uses django which is maybe less likely at sci-fi than python we've got you covered there as well a few design tips if you decide to write your own strategy inference functions uh which is a very useful pattern to save time and make testing easier i usually end up with a little hypothesis utils file in most of my test suites it's always a good idea to err on the side of generating too much insertion if you generate too much then whoever's writing the test has to manually constrain it audible if you generate too little then the tests pass when they shouldn't and in general i tend to think that missed alarms are much worse than a little work to avoid false alarms in testing does that make sense has anyone here had a bug where you had a test for it the test didn't catch the bug yeah feel this and the last tip is that in hypothesis we're very careful to make sure that you can always customize anything that's inferred by passing in just the arguments that you care about and inferring the rest i think that's a really important one um i will skip through this django example the short version is say you have a project on something like github and we say well you've got some limited number of collaborators want to charge you extra once your team gets large you say like well if we take some list of users and some project with a random collaborator limit then in taking each person in turn we can either check that if we're already at the limit then we can't add a person or if we're not at the limit we can add the person and then they're not they are right this will pretty quickly find a problem where you didn't specify that the users had to be unique and so this is kind of a bug in your test rather than bug in your code but those manager all right last special strategies uh for designing your own strategies there's a couple of techniques uh the simple one is composite very closely related it's flat mark but generally speaking don't use light map it's a cute toy but it's less readable than composite most of the time and then the data strategy so these are tools for when you need to generate some kind of input to your test that has an internal structure something that you couldn't easily generate using that or filter for example if you need a list lots of valid indexes trying to do this with filter by having a maximum size of the list and a maximum index you would end up rejecting a lot of examples especially for very short lists you usually have an index that is too large um so what composite lets you do is it lets you draw examples from a strategy inside a function that defines a strategy and so once you decorate it whatever you return gets wrapped up in the strategy again as one of the examples it could be generated however you don't always need composite often you can save a bit of time by for example having this input validation outside the what i call the inner composite this could be useful because you have very expensive validation of your inputs which is often worthwhile when you're defining a customer strategy you don't want to run that validation every single time you generate an example you just want to run it once when you create a strategy but in this particular case as well we don't actually need composite you can just have a function which returns a strategy so i'm teaching a lot of fairly complex tools here don't reach for the most powerful ones reach the least powerful ones you can get away with and functions which return things are a great trick and then data is kind of like composite but it runs inside your test so uh the upside of this is that it's incredibly flexible and powerful you can choose what kind of value you want hypothesis to give you based on what your test has done so far and how your characters behave the downside is that this is incredibly powerful and flexible and that means that the reports that hypothesis give you about what happens have to be a little more complicated because it's not just items for your function we're also now talking about well inside your function here is the sort of list of things that you ask for um so to summarize like definitely use the data strategy if you need it but check if you can get away with composite or just standard strategies first all right lastly some tips about where to look if you like i'm pretty sure this must exist but where is it if it's in the standard library or it's a built-in it will be in the hypothesis dot strategies module which we usually import as st for sure um there's a couple of exceptions like if you're before python 3 9 then you need the zone info backboard but we just depend on that as part of the standard library in future python if you're using numpy the corresponding strategies live in the hypothesis.number so that they're there if you have numpy installed but if you don't have numpy installed hypothesis still works for you if you use pandas that's in hypothesis.extra dot pens you want to guess where the array api extension lives anyone you got it it's in hypothesis.extras dot array api um we also have a lot of third-party extensions which are listed in the documentation those will provide strategies they'll provide inference for things uh various plugins i haven't written all of those so check them out for yourself yeah anyone could just publish a hypothesis dash whatever package on the python package index uh and if it looks like it works i will listen to the documentation but that doesn't mean that i've sort of done security betting or anything it just means that this is the thing which notice that it is a hypothesis extension so it's listed as potentially interesting all right we come now to our first set of exercises so in the repo that i sent out there are four sets of exercises the first one is sort of an introductory set which is more for future reference so what we're doing here is the describing your data exercises i'm going to say we'll come back together at 3 pm we'll take about 30 or 35 minutes for the exercises and then 10 or 15 minutes at the end of that as a break so you can grab something to drain stand up and stretch your legs sound good all right let's get to it um welcome back everyone i think i saw like three or four of you stand up and take a break so i admire your work ethic and feel sorry for your legs um in this next section we're going to be talking about what i've called common test patterns so this is basically where i tell you about some particular properties of property-based tests that are applicable to a wide variety of code but to do that i'm actually going to use a tool that i developed after sci-fi 2019 when i showed up and heard a lot of people say something like oh hypothesis that sounds really cool but i'm just not sure how to apply it to my code and like every programmer i know that social problems have technical solutions so i sat down and i wrote some code that would solve this for me so if you've installed hypothesis you will have this hypothesis shell command and if you ask hypothesis to oh no that's bad um [Music] this is what i get for not trying my live demo in advance if you ask hypothesis to write you some tests um you will if you installed hypothesis so that's that's that's my demo right you've got to install hypothesis for this to work but if you haven't okay okay but if you've installed hypothesis in a python environment which definitely exists then you get a couple of what we call ghost friday patterns the ghostwriter is a thing that writes code for you and then you can pretend to your colleagues that you wrote the code and it makes you look really productive but the other thing that it does is it shows off a couple of very common patterns and so the first thing you can do is just let hypothesis try to work out which of these patterns is applicable to your code you can just point it at some module or some package and it will write a bunch of tests you can yeah it will produce the source code and write it extend it out and you can pipe that into a test file and then just like if you've got type annotations you're done you can go home early and if you don't you can use that as a template which is customized to your code i think it's pretty cool but then again i would um but you've also got these four common properties that the ghostwriter understands so the first is equivalence i mentioned this earlier but it's a pattern where your code should do the same thing as some other code maybe you're re-implementing a function which already exists in numpy or sci-fi and you can check that it does the same thing there's also the round trip one so the example here is json that if you dump some object to json and then you load it back you should get an equal object there's also the item potents property which is if you call the function on its own output you should get the same output again so for example sorted should be item quoted um there's also uh the binary op one which i just put in to show off really like how often do you implement binary operators in python um but if you do then you can use hypothesis to check that your binary operator is associative and commutative and has the expected identity element so let's write a test for the sort function um well okay so we don't actually have a sort function in the hypothesis tells us that so how about the sort end function and here we have a quick test this is pretty similar to what i showed you earlier in the talk but a little different when i was writing tests for sorted at the start of this tutorial i neglected entirely to mention the key argument and the reverse flag and so this test doesn't check that those two properties that we had earlier that it's pairwise or we have the same elements but it does check that we can pass the reverse flag and so the ghostwriter has already kind of proven itself useful there we can also check you know as a known pattern that sort of is item potion and so the ghostwriter will then sort the input argument sort the output of that and then assert that those two results are equal and again i would use this as a starting point right but the idea is to kind of help you jump in and apply it to your own code faster if we wanted to test the two functions are equivalent like the two i'm doing here are the aval built in and the ast.literal eval function and these should be the same or anything that literally val works on what you're seeing here is actually some of the limitations of the ghostwriter because these functions have different names of their arguments the ghostwriter can't quite tell how they're meant to match up automatically so what you've got here is a template that you can work off where you would need to edit this to make node and or string and source actually the same argument to both as is this test function just calls both of them and well hopes for the best it's probably not going to work if you want to test a binary operator like edition who here trusts the addition operator in python some of you okay the rest of you you can test it so what we've got here is a somewhat longer one we've actually got multiple tests from here we are not quite sure what the operands to addition are maybe we should say they're integers or floats you could also pass strings right python can do string plus string uh but you've got to have the same thing so that's a global constant here and then we test that it's associative and we test that it's commutative and we have a sort of placeholder test for the identity element in this case it would be that adding zero to anything we can add well should give you the same thing for 10 4 strings though the identity element will be the empty okay so far so good let's write a test for gzip compression what property might we have here in this case the ghostwriter will look in the module and notice there's also a decompress function in that same module and so we'll guess that if you compress some input and then you decompress that output you should get your input back so it's automatically detected that there's an equivalence going on i have a list of regular expressions and format patterns this is the secret of a lot of hypothesis it looks like it's magic but just like real stage magic it merely consists of somebody who has practiced a lot putting in an unreasonable amount of work to make um i i think the answer there is you should edit the out output from the go strata right it's a starting point not it's not designed to be the only test you ever have and so there are a number of places as well where the ghostwriter output is deliberately incomplete where it actually requires some work from you to fill out for example here data is nothing and so that nothing strategy it will actually fail to generate if you try to run this test you'll get an error that says i can't generate anything for the nothing strategy you need to fill that out and you've also got a comment to that effect that's right source code so that bits up to you if we write a test for json.doms uh well we've got the loads function as well uh but a lot of what you learn is that json takes a lot of arguments there's a lot of different ways to customize json loading and saving um so i just produced a cut down version of this where i also specified that what the object should be right and you might remember from when i was talking about recursive data this is how we can describe all possible json values that it's one of those scalar base cases or it's any list of json values or any dictionary with string keys and json values following so far cool it's so nice to be able to see my audience and not be teaching online and then we have this simple test we dump it to a string we load it from a string and then we check that we got a thing equals what we started so let's just run pi test on this file what do we think is going to happen will it pass yes no why not everyone thinks we'll pass okay why won't you pass okay you're a very suspicious crowd and and justifiably so it does in fact fail uh so we've got two things here uh the first is that it fails because nan is not equal to n okay fair enough if the input is not equal to itself then the output will not be equal to the input the other reason it fails is that if allow nan is false and the object is infinity then the json library will actually say that these non-finite values are invalid in json so the json spec strictly speaking only allows finite numbers python by default will allow you to save infinity negative infinity in there but there's an argument which for historical reasons is called allowman but it also disallows so let's see what we can do about this um we can add and assume which is like a filter that the object is equal to itself and we'll say that we always allow non-finite numbers right fairly small diff do we think this fixed test is going to pass maybe gosh i have made you a paranoid not sure about the assertions it still fails well it should be but i wouldn't want to give away the game too soon um and so what we see here is that a list containing then compares equal to itself it turns out that python has a performance optimization because lists often have nested sub collections where if an element of a list is equal by identity to another element of the list then it will just return true so if you have a list and compare it to itself it always compares equal even if it's got nands in it if you put the same nan object in two lists those lists will still be variable if they're the same index until you round trip it through json and you have a different than an object then it fails uh so the correct fix is in fact just to say that hypothesis should not generate dance do you think this one's gonna pass yes yes yes we're confident now maybe you're not so suspicious after all no so it preserves nance so it just represents them as nam in the json output but this is an extension to the json format technically so you can pass allow nan equals false and then that will become an error if you try to encode it or decode it all right and this one doesn't need parts so hopefully you will enjoy using go strata on your own code as well the name of the module or the name of the file yep if you do hypothesis write numpy you get like a thousand lines of test output um the sprints are coming up let's leave it at that yep um do you have hypothesis installed in the environment uh no if the command is not found it'll be a install issue or something i will come over and after i've done this for complete about that uh so the question was do we use the same random number to generate the examples each time we'll cover that in the section after this by default it varies but we have various anti-flake based techniques all right let's jump into design patterns for tests what i've called tactics to distinguish them from strategies so a couple of common ones there's the fuzzing or the does not crash properly which is just if you call your code with arbitrary inputs that it should handle whatever it does it should not raise an exception um there's round trip and equivalent ones which we've just seen ghost written there's metamorphic properties which is an amazing mouthful for a very useful thing and there are some situationally useful properties so just call your code on random inputs it's embarrassingly effective so that's about all i'll say like just do it if you're not sure what property to test write a test with no assertions just call your code and see what exceptions do round trips the ghostwriter will find many of these for you but the basic patterns anytime you save data and load it anytime you encode and decode to some format anytime you send and receive data between processors or over a network anytime you're converting between different data formats this is the thing where often different data formats that can't represent exactly the same thing but it's a really useful exercise to work out what what is the largest subset that both formats support and check that it round trips that subset losses or you can even use round trips for things which aren't strictly a serialization or a load save for example if you multiply then you divide or you convert each way that kind of thing so anytime you have a function which should undo the result of some other function it factorize and then multiply for example um round trips are i think the single most useful and single most powerful property for you to test every code base has at least a couple right if your code base never reads or write output you can replace most of it with pass and then it will run much faster as well but round trips tend to be completely critical to your code right if you can't load or save your data what are you even doing they also tend to have relatively complicated inputs and outputs because everything that goes into your code has to be loaded or properly saved at some point but there are also places where errors are relatively common because you're loading and saving logic or you'll send it over the network logic will typically cross many layers of the stack right it's not purely written in python it's usually blowing together a couple of drivers or a program or dealing with multiple layers and finally round-trip bugs are often prone to silent failure where you go to write some complicated data out and something gets written to disk but a little loss and detecting those needs a test if it's not going to crash but just produce wrong output you've got to actually write a test from so if you take nothing else away from this property test your breakfast equivalent functions uh i think are great whenever they come up they are a little situational you don't always have an equivalent but it's often pretty easy to go like well i have this multi-threaded thing let me run it with one thread and with eight threads and check that i always get the same result um the old version of the new version the fortran version versus python version yep how do you test the old and the new version uh it can be a little tricky to set up i'm just repeating the question for the recording um it can be a little tricky to set up but often i can just make a copy of the repository check it out under a different name and then import both into the same test you do have to be a little careful to avoid name collisions there but it is doable and for equivalent functions important to note they don't have to be perfectly equivalent right often we end up writing functions which overlap the sum of their behavior or you implement a faster version for a subset of a function in numpy for example on those you can test that it's equivalent just for those inputs where it should be the same or as a more general pattern that it's the same unless you get a particular exception yeah so you can use this for any case you're like well the results should be within a certain episode that absolutely works here you almost always have to do that with floating point outputs as well if you're using not literally the exact same code or sometimes even the same plus or minus you know one part whatever it is that said machine learning of tools can be a little tricky to test in this style because machine learning tends to optimize for getting things right most of the time whereas hypothesis will find the very weird very rare inputs where it gets it much longer than usual maybe that's what you want but it's not always absolutely right if you have some domain or some set of inputs be like no it absolutely must get all of these rights within epsilon it's great for that all right another general property and this one is embarrassingly simple just check that the output is reasonable if you're calculating a probability it should probably be between 0 and 1 if you're calculating something to do with energy that should probably be conserved if you expect a number check that you got a number check that your array has the expected number of dimensions that it's the right size and shape it's got the right data type if you're getting strings check that they're not empty check that they don't have normal characters in them the other thing is that i try to put these assertions in my code rather than my tests and that's so that if it comes up in testing or when i've deployed or shipped my code or i'm using it for real i want to find out about those arrows by having a crash as soon as possible because these are bugs and it's much easier if that crash comes as close as possible to the time when something invalid or something unexpected great question do i track that or do i just let it crash and have the user report it i think this really depends on your domain right if you're writing the code as part of like a six-person team at a university just letting it crash and having someone walk over your desk sounds like a great answer if you're running youtube or something you probably have very sophisticated logging the cases in between it really comes down to what are you doing what are the consequences if it crashes for your users how bad would that be so there's a lot of questions where the answer is just like it depends but it does depend all right um and you saw this briefly in the binary operators there are a couple of like ointing colors like classic algebraic properties um because property-based testing was invented in haskell that's why it's named property-based testing um that's it they're actually pretty rare in python so i don't tend to think about these ones much compared to just testing round trips and then my code doesn't would i use a separate library to validate that i got the expected data i think that depends on my problem so if i'm dealing with data frames there are a lot of helpful libraries to like give you schemas for various kinds of data um i'm also just fond of the assert statement uh but yeah like it kind of depends on what your data is and what exactly you're checking probably whatever domain you're working in you have your preferred tools for checking that your data makes sense i would just use those a big part of the hypothesis philosophy is that like hypothesis users are also python users and can be trusted to write code so we try to give you tools to write code instead of tools that let you get out of writing code even the ghostwriter right it gives you a starting point for you to write tests rather than doing it all for you okay and i mentioned towards the start that a more complex form is what we call model based or stateful testing so what i've been talking about through this whole tutorial is about getting hypothesis to generate inputs to your test functions from a certain point of view though generating or choosing which action to take out of a set of possible actions is also just a different kind of data that could be generated by hypothesis so if you have something where there's like a sequence of api calls like a tree or a graph of possible actions you can use hypothesis to explore that in this kind of state machine style this is particularly useful things which have stateful apis but it would be a whole nother workshop and sci-fi tutorial community's not accepted this year so i'm just telling you that this more powerful style exists you will not need it very often i've written these about four times in total but if you do need it you can go looking for it metamorphic relations metamorphic relations are relationships between two inputs and their corresponding outputs so a standard property of the property based test is a relationship between the input and the output but especially in science often we don't know the relationship between the input and the output because that's why we're writing our code to find out but we might know something about the relationship between input output pairs for example that if we double the amount of energy in the input then the amount of energy in the output should be correspondingly doubled or have some other kind of invariants like that so a couple of classic cases one was from compiler testing you generate a little random program you generate an input and run it and then you randomize every part of that program that wasn't executed by that particular input compile the new program run on the same input and if you get a different result it's compiled turns out to be pretty common metamorphic relations are basically how i think about testing in variances so if you have something like conservation of mass or energy right well generate one input you can check that the energy is in the output or you can check that if you modify one you get the corresponding modification in the output we use this to test astrophy actually we would take what we would generate a random timestamp and cycle it through a few different formats and then we would add a small epsilon to it and cycle it through the same and the two outputs should be about as close together as the two inputs when they weren't we knew that we found a precision problem make sense properties are a little tricky because they depend so closely on the specific domain that you're working on so i can't really give you many great examples specific to your name like happy to talk about that but the bottom line here is that when you're writing property-based tests the first thing to do is write some assertions in your code second thing to do is just call your code and see if it crashes and the third thing is to write property-based tests for any round trips you can think of at that point i really do think you've got at least 80 of the value you're going to get out of property base testing the more to the point once you've done this i think you'll know what it's like to apply a hypothesis to your code base and you'll have a much better sense about what other things it might be worth using it for but here's where to start okay uh alternatively though since we've still got some time left we can do some exercises on common test tactics so this is going to be the next notebook the one number two or o2 which is on column test tactics let's give this to 20 past four okay i think it's 20 to 5 let's jump into the last tutorial chunk of the day i'm calling this one putting it into practice and the idea is it's the kind of practical tips that become useful when you actually use hypothesis for real you know at work or in open source projects but don't show up if you're just using it in a jupiter notebook in a tutorial i'm aware of the exercises i've given you a little small a little artificial and the goal of this section is to teach you the other things that are useful to know when you're going to use it on ten thousand a hundred thousand line projects things where you have ci running for example um so i'm also going to encourage questions for this whole section just shout them out i'll repeat them for microphones they get recorded and then i might tell you i'll answer that in four slides uh so let's jump into it so we've gone through the kind of principles of property-based testing described what it is looked at generating data and then the kinds of properties that you might want to test your code so this last section is like miscellaneous stuff that's worth knowing it doesn't pick up i'm calling out design patterns for the property-based test suites we've talked about design patterns for individual tests then how do you fit this into a broader testing strategy i'm going to talk about settings and how you configure things whether and how to share the example database and what that actually is and then look a little bit about coverage guided fuzzing and the hypothesis and testing ecosystem okay so the first thing i want to tell you is that you should not just write property-based tests i like them a lot hopefully you guys also understand a little why i like them but i would never recommend that you only write property-based tests often i will use hypothesis for the first test that i write just because it's a good way to get a quick indication of whether the thing works at all but as a test suite i worked on projects where we have one test that uses hypothesis which just sort of throws in any possible value and sees when things crash to a few projects where almost all of our tests were property based and this was mostly for tools that were converting between different formats which is somewhere where property-based testing is really powerful but most of the time it's going to be like some fraction typically maybe between 10 and 50 of my tests use hypothesis and the remainder are classic unit tests snapshot tests various others um the second thing about a property-based test suite is that writing custom strategies for your project can be a really powerful pattern so we talked about that a little earlier where you can use the composite decorator or just write a function which returns strategies often for example if you're working with data frames you'll have data frames that represent a couple of different types of data and the data frame strategy in hypothesis.extra.handlers is kind of complicated because you can specify things about what columns what rows what contents all this sort of thing so being able to wrap that up into a sort of more abstract function which describes it in terms which are relevant to your particular project can often be a great way to do that and that means that when you need to update the strategy you can update into one place and then all of your tests will use that new definition this kind of additive rather than multiplicative scaling really helps once you have a couple of people working on a project much more than a few days yep um do i usually put in a config file or where um yeah often i'll have like a test utils file somewhere sometimes like called test.pi maybe my package will have a you know dot underscore test util sub module like where exactly it doesn't matter that much but somewhere where they can be here to get to and a couple of patterns for this the simplest one is just like at the top of your test file assign strategies to global variables this works strategies are pretty much stateless so you can just then use those in the given decorator for each test another one is to write functions which return strategies that's maybe more scalable if you want to customize them a little bit and then finally register type strategy for custom types makes it very easy to pick those up all over the place and i would usually register those in something like a conference so whenever i load up my tests it does the registration before i start executing all right uh hands up who uses a debugger who uses print debugging yeah pretty much all of this print debugging is actually great but you might have noticed with hypothesis when you call print inside your test function it gets executed many times so hypothesis also ships with this notes function which is print but only on the final example and that means you will go less crazy with like a hundred times however many print statements you have lines of output the other one that's useful if you do want to summarize over all of the times it's executed is the event function and what this will do is it will ask whenever you do it to a string and then it will show you what proportion of inputs had each distinct value so for example you could check that you have positive and negative things about an even amount of time or see which operation you do on what proportion this is mostly useful just to check that things aren't much rarer than you expect right you get about an even mixture rather than you know if you use the filter for example that can skew the distribution quite lopsidedly the thing to note is that event is not printed by default event is just shown as part of the statistics so if you pass dash dash hypothesis shows statistics on the command line uh it will print something like this for your test function say well when you were generating examples uh here were the run times and he was the sort of fraction of time spent generating data compared to writing running your test code that tends to be a higher fraction when you're generating things like numpy arrays or data frames because numpy operates very quickly but filling out an array of numbers sort of one by one chosen by hypothesis takes a little longer uh and we've got the custom events where you can see that you know generating lists of length two here was a little bit more complicated than the single element lists and then while shrinking this tended to run much more quickly because we were dealing with smaller examples make sense to people cool settings um so there are a couple of things uh you can determine your settings using profiles which can be set from content conf test dot pi and you can choose a profile using pi test flags or you can write your own code to activate whichever setting based on your environment variable or literally anything else right it's just python code you will know how to write python you can also use settings as a decorator on a single test if there's just one test that you want to run for say ten thousand examples instead of a hundred just write at settings max examples equals ten thousand um and you can also set some of these from the pi test command this is another quick and easy option the settings that you are most likely to care about and you can read about all of these in the documentation uh there's a deadline which is how long is each individual test case allowed to take before hypothesis warns you that this is taking suspiciously on by default that's 200 milliseconds so one fifth of a second uh for some things particularly if you're using big number arrays this might be too short in which case you can just set along a deadline or even disable it the other one is max examples hypothesis by default will try 100 examples and then if none of those have failed it will give up removal if you want that to be faster you can turn it down and be correspondingly a little less likely to find bugs or if you really really want to find bugs you can turn it up and take corresponding longer performance analysis is not always easy but if you run twice as many examples it will take about twice as long this is an easy case i mean i showed you the statistics well they'll tell you both the average and sort of range of variation i was also asked earlier uh do we use the same random seed each time the answer by default is no but there is a setting for this if you want to run exactly the same set of examples every time you run your tests you can use the de-randomized setting or if you need to set the random seed for reproducibility you can say dash dash hypothesis c equals whatever number that can also be useful in reproducing things that failed in ci on a remote machine where the database won't replay things for you automatically i do tend to recommend running in non-deterministic mode because it makes it more likely that you'll find a buck but this is a bit of a trade-off right where the more likely it is that you find a bug if it's a rare bug sometimes this can come up and make the tests fail for an unrelated change and sort of a question about your team's workflow whether you prefer that to happen or whether you want that to happen in a separate build so that your pull request tests are always consistent another issue that can lead to flaky tests is if you use a random number generator who here ever uses a random number generator lots of us the problem with random number generators is that they generate random numbers and if some random numbers make your tests fail and others don't that will be sad um so the first mitigation is if you can pass a random object like random dot random uh hypothesis can generate random number generators for you and under the hood it will then generate you random numbers which are secretly evil numbers and it will then give you like the shortest simplest sequence of random numbers that make your test track um but if that doesn't work hypothesis actually does seed and then restore the previous state of python standard library random number generator and also numpy's random number generator would be using that so if you expect those to vary then you need to use the random module strategy which will make sure that they actually get different seeds every time instead of the same seed for every single example uh hypothesis will generate sort of arbitrary numbers for you and then if it ever fails it will then do its usual replay and shrinking technique um if you have another global random number generator well you can register it um if that's in a library tell me and i will try to handle it like a vote for you okay you might have noticed when you run a test it took a little while to fail the first time and failed very quickly when you rewrite it that's down to the hypothesis example database every time a test fails we save the underlying state so we generated something with that random number generator then we turned that using a strategy into the data that your test wanted and then we called you a test function we saved that intermediate layer in what we call the database and for every failure we start the test when you run it next time by replaying everything we've ever seen failed before and so your test shouldn't be flaky even though we randomly discover the factors so to reproduce failures step one is like well just run the test again if you're on the same machine it will just replay whatever that example was if you want to share this you can use the at example decorator this was in some of the exercises did everyone get there some people i see a few nodding hands a few shakes so at example uh is just like ad given but instead of passing strategies you pass the exact values that you test and then sorry it will try running those exact values every single time before it moves on to generating new branding this is great for regression testing or if you're concerned that you want to make sure you get great code coverage every time and not risk hypothesis is good at this but make sure that you always hit every branch instead of relying on happening to randomly generate them you can make sure that the at example inputs get you good coverage for everything you know about and then generate more random examples using given so if you change the test that modifies the database key so it would need to regenerate whatever failure was in there in practice this doesn't tend to be an issue uh any order of the decorators will work um we thought about requiring a special order we thought this would be a pain in the neck everyone would have to remember let's just make it work no matter what absolutely so if you have an existing parametrized test this is a great candidate you can convert each of the current examples to an app example decorator and then add an ad given decorator to generate more of them if you use like two parameterized calls to get a cross product of things um you might want to write your own little like measure decorator that does that in terms of this all right so these are the simple ways to reproduce any failing examples you have if things fail in ci uh we have a print blob setting which will print a base64 encoded blob uh which will guarantee that it reproduces if hypothesis can tell that it's running in ci it will enable this by default because it is a horrible experience have things fail in ci and not be able to reproduce it ask me about the numpy their issues someday so reproduce failure is a temporary um you can put it on when it fails in ci it will print out the decorator you can copy paste it into your code and run it to make it fade if it doesn't fail then it'll raise an error so something's wrong here you told me to reproduce failure and reproduce what's going on if it does fail it will also add that example to your local database so when you take it away it will keep failing for the same reason make sense okay the next step is well maybe you just want to share the database uh the database is just like a directory of files they're addressed like a hash of the test function in a fairly complicated way and then the contents of byte streams so you can just share this in the same way you would share any other mobile files um that said you're probably better off using something like readers if you're planning to share this with a team rather than trying to check it in to get and have this binary blob of data that changes at the time so i've actually had really good luck doing something like this we say well we'll have a server which holds the database for the whole team and the ci so if ci fails you reproduce this locally by running the tests it just pulls that a c um this particular example is actually saying that in development mode we'll use a local database and then a read-only wrapper around the shared database so the ci can add failures there but local runs don't delete anything that's already with me so far another neat trick which i think is probably more relevant for scientific numerical code than most is what we call target or guided testing so hypothesis is mostly a sort of heuristically random black box explorer target gives a particular label that it can try to optimize so you figured it a number and optionally a label and it will try to do a hill climbing search to maximize that this is particularly useful for example if you're asserting that two functions produce the same output within some epsilon you'd say try to maximize the difference between and that will often do much much better than an undivided search in turn this means that like your chance of improving about of finding a bug like with each test input actually improves over time rather than staying close um a couple of uses that i've found great well target the your epsilon in this difference between things below an error threshold or target the number of elements in some collection or the number of unprocessed jobs in the queue target the mean or the maximum runtime of something target the compression ratio or as a way to avoid filtering right if you want to write a test that applies to both valid and invalid data you can just target a sort of binary whether or not it's valid so that it tries to spend relatively more time generating valid than invalid data without rejecting the invalid data entirely the generalized form of this is if you don't know what to target conceptually you could just target each branch of code or each line in your program so well each time you execute an input that covers some bit of the code that we haven't seen covered before try more variations and this is particularly good if you have some sort of sequence of branches which are only rarely taken you'll be sort of multiplying the probabilities together and find out how likely it is to make it past all of them by chance but if you have coverage guided testing then it will actually try variations on each furthest through step that it's found and so it's additive instead of multiplicatively unlikely there are small examples here i'll honestly say like consult the documentation if you want to do fuzzy um i also have a tool called microphones which is designed specifically for hypothesis and it gives you nice log plots that you can sort of draw straight lines on if you use a very very fat marker um as to hypothesis update cadence um the way we manage this is that every single pull request results in a new release so if you report a bug it will get fixed reasonably quickly and then it will be released as soon as the bug fixes merge maybe like 15 minutes late see that helps avoid the problem where you fix a bug and then it sits on master for two months while you wait for it to be released then you need to work around in your own code which is not fun uh the other thing this means is that at the sprints we might have five or six new releases of hypothesis and so if you're using it at work you should probably be pinning your dependencies uh update on whatever schedule works so that's my kind of miscellaneous tips for using hypothesis happy to take any further questions now all right well thanks for coming we'll have a last set of exercises to close out our time together or circulate around but to summarize please test your code with hypothesis don't have bugs anymore just in c thanks very much [Applause]
6,Convex optimization using CVXPY,https://www.youtube.com/watch?v=USaishDES9s,i guess to start off uh how many of you just raised razor hands have used cdx five before um okay great no this this is perfect um that's not not a requirement not an assumption at the end of this course you will be totally totally trained in cdh pi for optimizing all kinds of different things to start off i'm going to give a broad overview all right false one um but yeah i'm going to start off uh let's go over the the structure of the four hours briefly so i'm going to start off with a lecture about you know what is optimization what is convex optimization give you some examples then we're going to uh make sure everybody is set up can run like at least a little test script there's like a hello world thing that you can do uh then we're going to riley we'll talk about sort of the rules of how to set something up in cvx fire and there's some exercises around that uh and then phillip is going to lead uh just a very focused uh exercise session starting with some material about about finance and other things but lots and lots of exercises in that section on many different topics to start off i'm going to give a definition of one particular definition of a mathematical optimization problem the standard form here is that we minimize than uh uh we minimize some objective f naught uh applied to this variable x which is you know a vector of dimension n here we have inequality constraints that f5x is less than zero use different different functions fi and then we have equality constraints now this is just one particular way to think about an optimization problem but it's one we're going to stick with for the course and there are many many variations of this like for example you might want to maximize instead of minimize you might have multiple objectives that you're somehow balancing there's other kinds of sets that your functions rely on but they all they all in some sense can be mapped into this standard form so broadly what would you use optimization for well one case would be to choose some action some good or some best action which might be for example the trades in a portfolio that's a very common application of this this technology is choosing an optimal set of trades in finance control of something like an airplane right you have different things different knobs you can turn with any sort of vehicle that you're controlling in an airplane i guess you have the ailerons or something and so you want to choose the best usage of all these knobs of all these device all these aspects of the thing you can control in order to you know for in terms of whatever whatever your objective is it could be getting to a particular point in a smooth way something like that you might have some sort of logistics application you want to route those packages as efficiently as possible in x is like the plan for how you're going to move them allocation of resources like for example capacity on a fiber optic network and you have different constraints that limit what you can do um limit how what positions you can take in your portfolio for example and then you want to you have some metric of cost that you've come up with that you want to minimize um or you want to maximize profit which is the same as minimizing negative profit right and you for whatever your application you you come up with some mathematical expression of what you care about and generally this is going to be a sort of a combination of multiple factors so you might be looking at uh expected profit but also the risk for your portfolio um you might be looking at fuel use versus um like the time it's going to take to get to where you want to go but there's different ways of combining all these things into a single objective also engineering design this is a really interesting application where x represents a circuit some kind of device we're gonna have a cool example of this later and you have different constraints on x from the manufacturing process uh and whatever performance requirements you have on this circuit that stuff some sort of clock rate uh and then your objective is going to wait various things you might care about like the cost you know the power consumption whatever but this is a really really nice application finding good models well this is probably a lot of you are familiar with machine learning in some form or another this is sort of the machine learning use case where x is not really a physical thing that you're doing or building it's just these more abstract parameters in your model and here you're not as likely to have any constraints you might have some like that they they might sum up to one or something something just required by the the model that you're using but the constraints are less common in this situation uh and then you want to minimize like your training error or something like that with perhaps a regularization term inversion this is a this comes up in imaging which it was something i worked on for a while and the idea is that i i have some measurement um like for example an mri machine is actually measuring a fourier transform of sort of the volume that you want to see so that's a pretty interesting case and so from this fourier transform you want to go back uh to the original thing uh but maybe there's noise uh or various other issues with the measurements like some measurements are missing like lots of things could go wrong and so you know rather than just applying the inverse fourier transform you want to have some more robust methodology and so one way to do this is through optimization setting it up as an optimization problem encoding different prior knowledge you have about uh the image or the the volume that you're trying to reconstruct uh and this is this is pretty much universal now in imaging in all kinds of applications um like for example you may have heard of this um uh the image that was taken of a black hole you might have seen that i mean so that's like you know it's not that's a reconstruction in a similar manner to what i'm talking about here uh pessimization or worst case analysis uh this is related to idea of like adversarial examples and deep learning and the general ideas you have some variables that are out of your control and that are either under the control of the you know subject to the environment uh or you could say they're in the control of an adversary uh and you want to find what's the worst thing that could happen in this situation what's the worst thing the adversary could do and that's a good way of you know testing the robustness of your choices of your setup right because maybe there's you optimized like for example you optimized your trades but then you want to get some more sense of like how badly things could go and this is also a way to look at a lot of different ways different problems that arise in like choosing actions and so forth it's interesting mathematical perspective too optimization based models are where you have some agents and you model the agent as taking an action that that optimizes some function like for example in economics you have this idea of agents maximizing their expected utility right they're solving this optimization problem and in biology you have this idea that organisms are optimizing or maximizing the reproductive success react you could say there are certain models of cells that reactions are running to maximize the growth of the cell and then a few applications of this in physics 2 for example electric currents minimizing total or electric circuit minimizing total power with the the current being sort of chosen quote-unquote to minimize total power and often these are very crude models but but as you perhaps know crude models can be extremely effective and they can work very very well in practice so the key takeaway here is that optimization arises in pretty much any area that you could imagine i mean i could have gone on with more stuff more apple more situations now the bad news is that while you can set up anything as an optimization problem you can't necessarily solve it and there are different exceptions to that rule convicts optimization is a particularly nice set of optimization problems that you can solve and can solve very robustly and quickly and that's going to be the focus of the rest of this um and i will say deep learning is not convex so we're not going to talk about deep learning unless you guys want to we can we can answer some questions but uh unfortunately not maybe someone will figure out some day how to make it convex here's the standard form for uh yeah we did thanks so much yeah here's the standard form for a convex optimization problem so harking back to the general form that i showed you before we have this variable x with the subjective f naught of x and we have inequality constraints and then we have equality constraints but here notice the equality constraints are linear so that's a requirement for convexity you can't have non-linear equality constraints another requirement is all these different functions have to be convex this is a formal definition but basically it means they're kind of like bowl shaped like upward sloping curvature that that's that's sort of a simple way to think about it um there's a lot a lot you could say about this but this is you don't need to worry about this too much know that there's like you know a certain set of functions that are allowed now the reason that this is such a nice subset of optimization probably i think is is threefold like one reason is is the theory there's really nice theory about these problems and from that theory you get things like uh duality which is this really nice way of measuring the effective constraints on the problem there are many many applications of duality and economics where prices are derived from dual variables in these problems and that's something you don't have in a lot of other optimization settings you can verify the optimality of your solution which is by no means easy like in many many settings like you you get some solution from your quote unquote optimizer like you don't know if that's that's really the best solution possible or not but here we we can verify it and this is very useful in the algorithms that have been developed for solving these problems right because it's you know obviously a very helpful stopping condition uh and it lets us certify that we have the globally optimal solution uh yeah there are many many algorithms that have been developed for different opt convex optimization problems uh and they have nice theoretical properties but they're i think more importantly they work very well in practice like there's a lot of engineering effort that has gone into this and you can have like trading engines that are running you know every day or every hour and people have all have confidence that this is going to work every single time it's a unification of a lot of things like linear programming that uh in other topics and optimization and many many for example machine learning and statistics uh this is more like classical machine learning what my advisor now calls shallow learning uh like uh like uh you might like uh different kinds of regressions like logistic regression things like that these are convex finance as i've mentioned logistics supply chain management a lot of interesting applications in control um we should have probably for for just sex appeal included some video of the spacex rocket because that actually is solved using essentially this methodology that's that's a really cool application uh it was also something similar was deployed on one of the mars rovers i'm not sure which one but that that was pretty exciting uh vision and image processing sort of related to that imaging stuff i was telling you about networking uh just allocating you know different packeting schemes or allocating bandwidth different engineering design situations uh combinatorial optimization uh here com so you'll see later on that convexity is more about continuous things but even so it can actually be used to solve or get a be a good sort of subroutine for many discrete problems as well the general approach for all these application areas is to sort of think about your problem and formulate it as convex this is kind of an art not necessarily so easy but fortunately people have you know been doing this for a long time and so anything you're interested in you'll probably be able to find some similar application that has been developed before um we'll you'll see the materials later on but my advisor's book has just like hundreds and hundreds of different examples that are all derived generally from real real applications so this is this is something that's i think more and more accessible or a piece of the process now once you've done that and you've put it in say cvx pi as you've got as you guys are going to learn how to do then you can solve it numerically using off-the-shelf software so you know this is an area of optimization that's well developed enough that it's basically technology you don't have to play with like learning rates or anything you don't have to babysit it it'll just work uh in terms of the formulation there are a few tricks uh you're going to get to play with this later but there there's just sort of some standard things you learn for how to handle this um and there is some art in figuring out okay well i want to include this in my model not include this uh but i actually think that makes it more fun more interesting now for medium sized problems with a thousand to maybe a hundred thousand variables this is basically you can just use off-the-shelf tools they're open source solvers there are commercial solvers that can maybe go a little bit higher in terms of dimensionality generally based on interior point methods which are very reliable methods uh very high precision and they can exploit the sparsity of a problem and yeah this is used all the time in in finance control various other settings uh you could run this in a control loop you know at uh at 10 hertz and it'll you know work reliably larger problems going up to say a million to a billion variables here you're going to have to do a bit more work you can use things like lb fgs or stochastic gradient other methods you can i there are a lot of packages for this there are a lot of different ways to do this it's generally going to take a little more more tuning and babysitting to get these to solve and you will have to think more about your computer architecture things like that but it's definitely very doable not a huge burden and this might arise in machine learning for sure also maybe image processing that can get those problems can get pretty large as well modeling languages are another important piece of the ecosystem and that's what sort of our focus is going to be on cvx pi as a modeling language and the idea here is that these model languages allow you to express your problem in a high level way so often the solver standard form like the input to these solvers is not really that user-friendly you have to be kind of an expert in optimization to be able to express your problem in that way and so we created these these uh high-level languages to make that process uh just something that is handled automatically just like a compiler will automatically convert your code you know into assembly you don't need to worry about that and this has made things a lot more accessible the originals here were yall nip and and cdx which are in matlab and then cdx pi which i'll be talking about uh in python and there's also a julia version and an r version if you're interested in those languages cvx pi has been developed since 2014 uh it's now a really really nice community and it's awesome that that that we can you know have the three of us here to talk about it uh it has a nice system for verifying convexity that riley's going to tell you about later today it's open source it comes with open source solvers so you can use it all in a fully open source way but you can also use whatever solvers are are available like commercial solvers all kinds of solvers it supports uh parameters uh which are you know symbolic constants um which is it's a small thing it sounds like a small thing but it's actually enabled a lot of very cool applications like we have automatic differentiation through problems like you can put your problem in a pie torch we have um code generation also based on parameterization so that that's been like a really fruitful area and as you'll see it's it's just a normal python library it's very easy to mix in with other code and build a really uh nice code base uh on top of cvx pi um and it's used in many many different research projects companies classes all over the place and it's hard to know how many users we have but i think it's probably tens of thousands here's a cvx pi example problem right so we import cvx pi we define a variable x so we've created an instance of the class variable with dimension n where n is defined outside this snippet and then we start setting up our cost function so here a and b are going to be numpy arrays so 2d and 1d arrays and we've overloaded matrix multiplication and minus and all these arithmetic operators so that it's captured by cvx pi and it forms this expression tree and then we apply the sum squares function which you know takes the square of everything then sums it this is a cd as you can see this is a cvx pi function and this is an important aspect of cvx pi you have to use the functions that are defined in the library you can't do black box functions and that's to verify guarantee that everything is going to work out guarantee convexity here's another function uh the norm so the sum l1 norm so this is the sum of the absolute values of x so here this is kind of like a lasso problem if you've heard of that we combine these together and we get this cost function which is an expression cdx pi expression uh we construct a problem object uh and then we minimize uh the cost so that's our objective and then we have a list of constraints which in this case is just one constraint and we've overloaded less than or equals to construct this constraint object and this is saying that the uh the largest absolute value of x has to be less than or equal to one then we call dot solve on this problem and notice there are no parameters here and generally you won't need any it's it's very reliable and it'll just handle everything for you you can of course if you no more want to use a particular solver particular solver option you can do that as well but general i think ideally you would it would just work and it generally does so that returns the optim yeah um no this is the new python uh symbol for matrix multiplication yeah yeah since python 3.5 um to disambiguate times and and uh inmate just disambiguate like element-wise multiplication and matrix multiplication what type would be a i mean is it is it is it a specific type well it can be variable or is it just is it just a matrix just anything like np yeah essentially anything so normally it would be like we support a lot of different uh like formats for constant values so normally it would be like a numpy array um and the array it could also be like a sci-fi sparse array um those are the main ones we support but but generally any anything that you would use to express this maybe one more question to understand you said this is like like a lasso setting what would be what would be in the a like typically what how would it look like oh does the question make sense um like when i look at it from the view of someone who does machine learning then you say this is a lasso sort of solving thing how would the a look like because if i if i think about a lasso thing i think about data points is this the same thing yeah so i guess yeah it's not set up to be super clear but it's based as a logical problem basically the a would be your your like data like your training data and then x would be the fit like you could say x are the features that you're fitting right and then it's a linear model so a times x um and maybe you could you could include the constant offset in this this setup and the rows of a would be your individual data points yeah yeah yeah and the entries of b will be your labeled observations yeah uh and then you're penalizing the the feature weights to try and get them to be sparse um and then there's there's some sort of constraint on this i'm not sure how you would interpret this exactly yeah this is just to have have a constraint in this situation i yeah i know very very nice questions uh so here yeah a b and gamma are constants uh in different many things are supported but generally like numpy arrays and well after you call solve it assigns the like solution value of x to this dot value field so all the variables get a solution value which you can read off later these model languages have had a really big impact since cbx in terms of making convex optimization accessible to non-experts it's i think really nice how easy it is to experiment with different formulations right you can just try all kinds of different things and without this i mean it would it takes quite a while to get it into the standard form and so this makes it you know much more flexible and i think more recently i've started to see how it enables like more complex models um like for example i work a lot now at my current job on sort of object oriented model structures where there's lots of components coming together in interesting ways and it's it can get it can be really quite quite complex and something that would be very difficult to even write down as like a single problem like on a whiteboard so that's something i'm pretty excited about that there's sort of new frontiers here and there may be some losses in terms of trade-offs it may be slower than if you you know really were if you knew exactly what you wanted and you just write a custom method yeah it probably will be faster um but it not by much necessarily it depends on the the application like if it's something that people really worked on for a super long time then there may be very specialized solvers uh yeah writing out the uh like the cost functions and the constraints and everything yourself like not using the cvx5 functions what exactly do you mean by customizing it yeah well yeah well there's there's different ways like i guess for example like let's suppose this was like a lasso problem like it's not exactly let's suppose it was well you could do cvx pi or you could use like you know scikit-learn right and scikit-learn is probably going to be faster than cvx pi i mean look for the lasso it definitely will be right you're just not really coming it's just kind of embedded right you're not coming at it from the optimization lens at that point yeah i mean it's it's more of this functional you know it's like we these are the different functions that that you know would support like in scikit learn like you can do this model or this model or this model right and so people can really you know spend a ton of time making this model you know as optimizing this particular code path as much as possible here it's mapping it into a more generic setting so it's not like it's the solver is is not as specialized generally with solvers like the more specialized they are like there there are benefits to specializing um and so there's sort of an inherent trade-off between generality and and um you know speed performance in some some sense uh and that's a but so there is some gap um but that's something that that we're trying to you know we've worked hard to close and and i think once you really get into this like you know you're going to have so many things in your model or there's so much flexibility that it's not really going to map anymore into anything in like scikit-learn uh and then uh another limitation is scaling so this is not this doesn't yet scale to like the levels of like pie torch or something like that like you as you saw like a million to is billy and that starts to push this um so that's something we want to we want to fix uh but for for most settings i don't think you need other than machine learning i don't think you really need so many variables um but we would like it to also just work for such large problems here's some examples that i think are very cool radiation treatment planning this is a real example from work that was done at stanford in in my advisors group and the idea here is you know someone has has cancer and you want to treat it by radiating it and you basically have you know some sort of machine that they're in and that the person is in and beams can be shot from different angles right and you want to figure out uh basically how to deliver this this dosage to the the tumor um using these different paths that you can you can shoot the beam from right so each each beam has some intensity uh that's non-negative uh and then you basically voxelize the the patient right to this 3d model of the patient and each voxel is going to receive as a consequence of this radiation some amount some dosage and this is actually a linear mapping which you know we need it to be for convexity but it is um it's there's a lot of number crunching that goes into getting this matrix a uh behind the scenes but you know once you have that then you're good to go and you know ideally you would deliver the desired dosage to all the tumor cells and no radiation to any other cells but of course that's not possible because in order to get to the tumor you have to go through other cells right other other voxels so you need to kind of trade this off uh you know trade off the unintended dosage with the real dosage or with the intended dosage and just in terms of dimensionality you could think of there being like a thousand beams like that's how the discretization of the locations uh and then maybe like a million voxels um so the per take they'll focus on like a particular area of the body so like a million voxels and the way we approach this through convex optimization is first we write down just like the the basic you know physical constraints like that non-negative beam intensity and then the relationship between the beams and the the dosage uh and then we want to penalize the dosage somehow to like get get the behavior that we want and what these uh researchers concluded is that a really nice way to do this is to have basically a particular penalty for overdosing and then a particular penalty for underdosing which is going to depend on each be different for every single voxel so you know if it's a tumor cell it probably doesn't matter too much if you overdose it right that's fine but if you under dose it that's really bad conversely if it's like a non-tumor cell overdosing is extremely bad under dosing is fine and so by setting these weights nicely you can get sort of the exactly the treatment plan you want and here's an example for a real patient case and this has i guess 360 beams 360 000 voxels and this was still when the validation stage of the research so they validated that the met that the treatment plan generated by this approach was very you know essentially the same as what the clinician came up with uh but this was computed in only a few seconds uh using a gpu versus the original plan took hours of weight tweaking um and so it's clear that this is like you know getting into potentially new methodologies for this application and there's been a lot of follow-up work since then on like for example they actually made like this nice python package for the clinicians where they can explore the different plans and right if it only takes a second to compute you could really look at all kinds of possibilities um and so this is an application that you know is really uh really nice to see another very cool one uh more recent is hyperloop system design uh this is by kershan uh and bernal in 2021 so hyperloop is kind of a futuristic concept for mass transportation uh and generally it's like the idea i'm no expert but the idea generally is that you have these two or these pod pods like these vehicles that are shooting around through this low pressure tube like some sort of low pressure enclosure and this is a very exciting area for people in involved in in transportation because you know it's it's a clean sheet problem there's no real limitations on how you can do this you can really look at it from first principles uh versus a lot of existing things like if you it's very hard to just come up with like a new plane there's a lot of restrictions on that are like a totally you know crazy design and get that actually deployed into the world uh and it's a very complex design problem because all these different components are connected uh and you know your choices for like how big the pod is going to be is going to have like many ripple effects for the rest of the design and so ideally you would sort of consider everything together and here's a visualization of the design relationships like the connections between the components so this it was organized hierarchically uh when they did this so there's like a pod like the vehicle it is all these different sub components uh and then there's different aspects of like the fleet of pods like the way the pods can relate to each other and then the infrastructure to you know support the whole thing like the tube and whatnot and and all together like this generated a problem that had about five thousand variables and six thousand constraints uh which you know actually only takes like a second or so to solve um very very fast uh and so they could really explore the design frontier for this problem because generally you don't have like some objective it's like oh this is exactly what i want this objective boom i'm done usually you want to kind of trade off different aspects and see and explore the possibility space and so this is an example of a parameter sweep the details i'm not going to get into the details here but the idea here is they have they're starting off with a particular optimal design where you can the pods charge at like 10 amps and they want to see what happens well what if the pods could charge faster like how would that impact everything and this optimizer just sort of you know re-crunches everything and adjusts all these different factors and so you know as your chain as you're increasing the charge um it's like oh saying okay well then i need don't need as many charging stations or uh my pods can go faster like all these different things so so it's like a nice uh paradigm for for doing this kind of work this is a more stylized example for control so the idea here is that you have uh states which are like basically all the information about your system to know like what's going on a particular moment so like you know it's location velocity acceleration like stuff like that uh and then you have actions uh that can be taken like you can apply this thrust or you know this turn on the move the ailerons stuff like that and you need a linear relationship which seems a little bit constricting um a linear relationship in terms of like how the states update into the next state and then how the controls impact it but this is actually not such a limitation people will often linearize nonlinear dynamics uh and it and that tends to work fine if you if you run it at a high frequency um so so this is a very common approach to control um and it handles like things like lq it includes things like lqr if you've heard of that uh and then you have you know different constraints on your states uh we have like special ones for the terminal state um xt and then stage so we and then we have costs for each stage of this process like x1 through xt including the terminal state so this is just kind of a stylized thing here we're doing a little tiny problem with eight states and two controls 50 time horizon of 50. uh and here uh we've just chosen random dynamics that are that are sort of reasonable for a such a system like this uh and then we are constraining the largest absolute value of the controls to be less than equal to one so controls have to be between negative one and one essentially uh and then uh we want to target x equals zero right so we've just kind of defined the state so that that's that's our endpoint that we want like zero is where we wanna be but of course if we're not you can always just offset things right it's it's just an offset uh and then it's very traditional to penalize have a least squares penalty here like this is how you get most like standard controllers uh and there's often a closed form solution in that case uh and you can get like a linear mapping from your controls from like your state to what your control should be but you don't need that you don't need to have a closed form solution uh necessarily you can just you know run this optimization a loop um and that and that's very effective and so we initialize this randomly and you can see a few of the control a few of the states being driven towards zero uh and it's interesting to see that the path is not you know it's a bit of a winding road right they're going away from zero then back towards zero right this one's going up and down up and down and then the controls kind of behave like you would expect uh they're sort of shooting between one and negative 1 though there is some concern for being close to 0 because that's in the objective just very very briefly this is a machine learning example here we have features x i uh feature vector and then we have this is a binary problem so like spam not spam something like that we have a linear classifier so we just evaluate this this linear function that we learned uh beta you know is the feature waste then then nu is this constant offset and then you look it's that what's the sign of that that'll tell you if it's plus one or minus one and this is this is how you formulate an s a support vector machine uh but we've added in you know since we can uh some regularization to make the features sparse so this is a svm with feature selection and here we evaluated this approach we you know do parameter selection on how sparse we want things to be right basically as we increase lambda it becomes more and more sparse and there is an optimal sparsity because we included some features uh that are not necessary that are like not part of the real true uh feature set and so around you know lambda's .5 that's like the best set of features for this this little example so you use the optimization to find the lender or did you use the optimization to solve the machine yeah so we for any particular we fix lambda then we solve the optimization problem that gets you the the feed that fits the the features and so but then we do a out outside that we do like a loop over different values so you're not using um optimization to find the laptop that's right yeah um well i mean you could come up with more efficient algorithms than this i mean it's yeah there there are i mean yeah this sort of gets into convexity it's like so lambda is not the problem is not convex in in beta nu and lambda together um if it were then you could just include lambda in the you know cv cvx pi could just handle it and it would be one problem um so here uh if you fix lambda then it is convex uh and so yeah you could come up with some method that's gonna look at all these things together um potentially as a newbie how would i know that the problem is no longer convexify if i include that is it just something you know because you tried it or is it something you see because you have some yeah there's actually it's it's a great question it's very well defined and riley is going to give a whole talk about how to do that um but this is like you're going to be experts by the end of this like you'll you'll know exactly when it's convex and when it isn't and then here's an illustration of how the the feature selection is working as we increase lambda it starts off if limit is very small then there's there's no feature selection as we increase it these features more and more features are driven towards zero until here we have four different features and at a certain point we don't have any features and that's just a constant offset uh in summary convex optimization problems arise in many different applications you're gonna get to play with a lot of these today we have a lot of exercises for you and they can be used to solve you know solve medium scale problems generically meaning you really don't even need to know what solver you're using and then if you have a very large problem or a larger problem you may need to to think a little bit more about what solver you're using but that's not necessary there's a lot of support for that as well and then these these prototyping tools or these high-level languages really make this this all very easy um as you'll see today uh we'll talk later about resources and we'll post these slides there's more just like a reference all right so now we have our first uh like exercise session and if you go to your uh that um uh repo that you cloned so the cvx short course uh you'll see in the readme that there's this hello world example so we want to take this time to sort of make sure everyone has a working environment can run the hello world uh and then if you have like extra time like there's also another exam there's another um exercise that you can do like an ex it's called extra uh and these are from the readme so you can see in the readme all the different exercises we have for today yeah sorry little lately the slack maybe to the cvx short course repo yeah it's in the slack um it's it's at the banner of the slack uh they they're not in the repo right now but we'll add them yeah timer here i guess that okay and zaba daba do okay view slideshow okay so um in this talk i'm going to cover like one of the most important things when it comes to using cvx pi namely uh how do you get it to accept a problem and not yell at you with an error saying problem is not dcp that's that's something that uh you'll hopefully see with decreasing frequency as you use cvx pi over time um the so this this standard form we saw in stephen's talk uh the objective will be to minimize a convex function convex functions uh constrained to be less than or equal to zero and linear equality constraints now that's a a very general form it's not the form that is used algorithmically so cvx pi exists to make it easier to access really high performance numerical solvers that are written in typically c the standard form of those solvers is surprisingly simple you only have a linear objective function and linear equality constraints then there's this funny looking thing saying x belongs to a set k which this the the term here is that k is a convex cone the simplest cone is the going to be the cone of just element-wise non-negative vectors but out in the wild too much um yes what a parameter um a that is one of the most common cases of parameters yes like a regularization parameter something that's going to be held constant during optimization but what cvx pi will do when you use these parameters is it will perform really like most of the transformations in getting to this format but it'll stop one step short so that when you update that parameter it can very quickly update that problem data so this can be useful specifically when sort of training some shallow learning model and you're trying to do cross-validation so you need to solve a bajillion problems that are more or less the same any other questions cool okay um when it comes to solving a convex problem really the i just see this says use an existing custom solver this is kind of a ah okay yes yes so this is like um like something in scikit learn or something this is if you uh do not uh bring the light of cvx pi into your hearts um the if you're trying to like uh show off or get published in nurips or something then you can say oh well we developed our own customized solver that its scales to multiple gpus or something um but this last one here is really cvx pi's philosophy uh some very very brilliant people have dedicated their lives to making at least dedicated their professional lives to making these uh conic solvers stupidly efficient now on to this question of um how to make sure your problem is convex how to have cvx pi not yell at you steven showed the definition of a convex function before the that definition is that a function of an average is less than or equal to the average of the function values if a concave function the inequality goes the opposite direction a concave function the function of an average is greater than or equal to the average of function values affine functions that is like i mean linear functions plus an offset are convex and concave and this is just sort of the vaguely general form of a scalar affine function the one one for the curious wondering why uh convex functions are so important or so useful for optimization the there are many different answers but one of them is that if you have a convex function any tangent line is a global underestimator this is the basis for like a lot of those customized algorithms that people use this isn't as relevant in the in cone programming but it's uh it's a nice fact okay boy i'm trying to carry three things with one hand all right so in order to use cvx pi you need to be aware of a small library of convex functions that you can use as building blocks and then we're going to use these functions to construct more sophisticated convex functions so one of them uh just raising a variable x to a power p if p is greater than or equal to one or less than or equal to zero then this is convex if p is less than or equal to zero then we need to restrict the variable x to be positive and this this will happen automatically if you ever write a cvx pi expression with like a one over x if you use the inv pause atom that's the name for it in cvx pi then the constraint that x is positive will automatically be enforced that's i mean unless i'm yeah okay cool it would just be really bad if i got that wrong so i wanted to double check um um this is uh ah yeah sorry the al for um non-even powers for for uh for yeah for non-even powers greater than or equal to one you must also restrict to um x greater than or equal to zero because yeah you're you're right that uh x to the power three is like this squiggly guy which is very much not having the property there um the exponential function oh sorry did you have your hand raised okay the exponential function typically we like base e because we are uh enlightened folks who um uh who use base e for all our exponentials and logs but you can also use base two or base anything else it will still be convex at x log x so this is the negative entropy negative entropy is a convex function why is this important it's because we like to minimize convex functions so minimizing negative entropy is the same as maximizing entropy so like in in many models arising throughout like physics or information theory or wherever else where the sort of your prior assumption is that some unknown parameter should lead to maximum entropy in a system you can optimize for those parameters with appropriate convex models in cvx pi we have the alfine function this guy here x transpose px this is uh this is called a quadratic form what we need having this p with the curly greater than or equal to zero that means we need it to be symmetric positive semi-definite that's like a long term that means uh well the symmetric means what you think it means it equals its transpose positive semi-definite meaning that all of its eigenvalues are greater than or equal to zero these are the best of all matrices well positive definite matrices where all the eigenvalues are positive those are really the best of all matrices but positive semi-definite are second best um any and all norms so the simplest um well you can take the matrix to be one by one in which case it's just uh like it's p times x squared where p is some greater than or equal to zero so that's positive semi-definite is when is when that scalar p is greater than or equal to zero positive definite would mean the p is positive in this one by one case so the graphing it in the like in larger cases can be tricky but one of the most common forms of these is when p is the identity matrix in which case saying x transpose p x is x transpose x is in c the x pi's parlance some squares this is actually i'm glad you you've gotten us to this point because this shows um three different ways of writing an expression which will have very different results in cvx pi if you try to use this cvx pi will be mad at you because you're multiplying two variables and cvx pi is not going to say ah well are you multiplying a variable times itself it's just going to say you're multiplying variables this is not something we let you do except under like super uh specific situations and we don't um particularly try to check for those because what happens is you get an error when you see this and then we tell you to use either this form specifically with the function is called quad form x p so this is the c the x pi function you would use or if you're really in this case when p is the identity then we will tell you to use this one the difference being a matter of efficiency and how quickly cvx pi can recognize that this is convex as opposed to recognizing that this is convex because here it's going to have to check the eigenvalues of p and this can be very expensive if p is a large matrix i that that that what helped me a lot because those were important points so thank you um all right now this this one this last one here max element wise max is a convex function and you can see like conceptually it's a sort of a piecewise linear thing if you plugged in um different affine expressions of a single variable x you would get a piecewise linear curve um here are uh examples of concave functions uh say the log of x square root of a product so this is allowed specifically because two variables and square root this is the geometric mean so you can go on the wikipedia page look up geometric mean if you haven't encountered it in the past this is a concave function x transpose px that quadratic form if all the eigenvalues of p are less than or equal to zero this is concave and as you might imagine if max was convex uh then min will be concave okay now here are some less basic examples these are the kinds of things that um in order to present them to a solver in an appropriate way it really really helps to use cvx pi because if you try to use like a generic thing that that isn't um aware of of the subtleties and how you represent these things then you can run into numerical problems so one is that when viewed as a function of two variables x squared divided by y where y is positive this is convex now the really crazy one is x transpose y inverse x where y is a symmetric matrix and all of its eigenvalues are positive when i say this is convex i mean this is convex when viewed as a function of x and y there are lots of situations in um control theory where this type of thing arises or at least in the control theory that some of my uh um sort of lab mates were working on at caltech because they would often come to me with a problem say how do we approach this and then we end up staring down one of these things and and that's good because we're looking at a convex thing uh the largest eigenvalue of a symmetric matrix this is convex concave functions the log of the determinant so the um conceptually what is what is the log of the determinant that is the sum of the logs of the eigenvalues this is uh i think this arises in gaussian process regression but i can't actually tell you how the determinant raised to the one over nth power this is the geometric mean of the eigenvalues of a matrix one also has the log of the gaussian c cumulative distribution function this can be useful in constructing some optimization model where you need some condition to hold with sufficiently high probability and in the same way that where max the maximum element of a vector was convex and the minimum element of a vector was concave we have the maximum eigenvalue of a symmetric matrix as convex and the minimum eigenvalue is concave yes uh ah yes so it is it is a stronger statement to say that when viewed as a function of x and y oftentimes um if we're not so lucky as to face a truly convex problem then maybe we're in a situation where if we hold one block of variables fixed then the problem is convex and so we do this alternating approach c the x pi isn't built for that but one of the great things about having it being embedded in python is that you can um write pretty sophisticated code to to make use of the functionality it does offer to accomplish much more sophisticated things yes yes although you wouldn't really want to minimize the the consider the regularization parameter a variable in the convex optimization problem because the objective the loss function is linear in the regularization parameter so like there's a question of what you would actually trying to be minimized minimizing in that case with all those previous examples the go-to thing is to verify convexity by the definition but you can also check if the second derivative or in this case the hessian the second derivative matrix is positive semi-definite now this is all very mathematical stuff it would be great if you didn't have to concern yourself with this and indeed you don't that's the point of cvx pi is to make it so that you don't need to worry about these first two methods instead we're going to take this third approach starting with a small library of functions like the ones i showed you on the previous slides and then consider transformations which preserve convexity so we can scale a convex function by any non-negative number so if yes no no there are many there's cvx pi probably has like what 30 40 sort of yeah um you can you can actually compose in this last sense so if you have a convex function and then you have a convex increasing function then the composition here is convex and so you can build more like materially more complicated functions using this last point here um but you do ultimately have to work with cvx pi functions as building blocks yeah um so some examples that for which we now can infer convexity based on the build the the small family of convex functions i gave earlier and the composition rules this the two norm squared so that's uh this guy x transpose x some squares this is convex and so when we compose that function with an alpha transformation this is still convex so we're good to go with the first term i also said that all norms were convex so we have here the l1 norm that's the sum of the absolute values and lambda our regularization parameter was non-negative so this term is convex and then the sum of convex things is convex so this this is like the quintessential example when um you're being tasked with constructing a convex function starting with simpler ones a much more sophisticated example is the sum of the k largest elements of a vector and um the i i will give a hint and and see like um how quickly the connection can be drawn there will be a little exercise shortly hint being you can use the fact that the um largest element of a vector like that function mapping from the vector to its largest element this is a convex function and you also have that taking a sum is a convex function so any thoughts as to how to get from those two building blocks to this guy being convex so it's uh actually i'm not going to say here i'm going to wait until the exercise slide which i believe is coming up very shortly um for those who want to who uh find like this optimization stuff very compelling and you do more reading on your own um this function here is called the log barrier for a convex set that's the set of vectors x where each of these functions f each of these convex functions is negative um but that's just a that's like a higher level math example not something we need here um i presented a list of rules on the convex calculus slide and it is personally that that is how i learned how to show that something is convex was with kind of that handful of rules but if you're clever about things then you really only need one rule if you have a convex function h then you can compose it with these other functions f1 through fk if h is increasing in its ith argument and f i is convex then sort of that part of the composition we're good to go now look whenever h is decreasing in its ith argument if the corresponding fi is concave again you're good to go if um any for for any uh entry where fi is affine and this might literally just be like the say the the map from a vector x to the jth entry of a vector this this is an affine transformation it's a very simple one but it is it's one of the things that's kind of meant to cover this case when you have these conditions uh you're done so um unpacking that rule for a funny-looking function here a function of two variables u and v i want it to be convex when both of those variables are allowed to to change at once the starting point we can say that x logs x log x divided by y turns out this is convex in x and y it wasn't on the list that i showed you before but it is in fact convex in x and y it's also decreasing in y we have that our variables u and v are positive we know that u plus one is alphine the min of u and v this is a concave function and because u and v are positive both of these are positive so by going back to the rule or the the one rule from the previous slide you can conclude that this thing is convex so you don't need to take any derivatives this is great news because it's not differentiable you don't even need to plot it um which is good news because plotting two functions of two variables is surprisingly painful you can just apply that that one rule to to see why this guy is convex okay now what cvx pi does is it takes an algorithmic approach to that it considers an expression i put a capital e expression because that's the like the name of the class the and then it's going to look at the arguments to things called atoms that define an expression so the addition operator we've overloaded that operator it whenever you use it there's a a corresponding cvx pi function steven is it called like add or plus yeah you don't actually need to know the name because you're always going to use the addition operator but the cvx pi internally says well the addition operator preserves non-negativity the addition operator is affine it tags all of this information and then it parses this expression tree from the bottom up so going back here what cvx pi does is say that at first it's going to start at step 2 then it's gonna go to step three then it's gonna go to step one and say ah yes given that the things in two and three are true i'm able to use this fact and the whole thing's convex so this isn't really what um the way a person would go about it but it's a very convenient way to implement it um what's the word programmatically okay this is another example but i'm gonna skip it oh well sorry i shouldn't skip it this website is your friend so when you're getting an error cvx pi is saying something is not dcp that's it's not compatible with disciplined convex programming then you can go to this website and this will parse it will produce such a diagram for you where you can see sort of where problems arise when cvx pi is trying to verify that a function is convex and i have to emphasize when cvx pi is trying to verify that the function is convex because this one rule i said the title of the slide was the one rule to rule them all i i mostly just wanted to to make sort of a joke um uh there are plenty of convex functions that cvx pi does not recognize as convex in fact um you can construct extremely simple functions that cvx pi cannot recognize as convex and this is why uh you your yourself in the future is grateful to yourself now for attending this to know the differences between what cvx pi is happy with and what it is not but given all that framework with cvx pi you minimize some convex objective or maximize some concave objective you have constraints that are expressed with these binary operators less than or equal to greater than or equal to double equals and c the x pi is going to parse the whole problem from the bottom up to determine if if it's uh going to accept it uh you've already seen this sort of the the vague appearance of cvx pi here um here is an exercise for you in you don't need uh to use a jupiter notebook for this you can just pull it up in in terminal because it should be super super simple come up with a cvx pi expression expert for which this condition expert is dcp is false so i just want to like i think that's a lot to digest and so i wanted i think this is uh yeah we have this exercise but also i think this is a good time to play with that website the dcp.stanford.edu website like let's we're going to take like uh some time now just play with that website i know this is like a lot of math it's like early in the morning um but if you what it really it really will click like if you just use the website there's this quiz mode just like do that for like 10 minutes yeah let's do that let's do that and like all of this will like come together you'll see you'll see what we're talking about um and then like just a little bit of context so like why why do we have all this it's like like people put a really a lot of a lot of thought into how to make this as user-friendly as possible like really thought about this a lot and it boils down to like they boiled it down to to this um rule about how to compose things and in terms of like basically it turns out that you can just have this like library of like 40 things and really that's all you need and it's surprising that that's true but that's how it turns out to work and you know sometimes people will come up with new things they're like oh i can't this isn't in the library it can't make it we just add it to the library so it's like an evolving thing yeah we've like people people make pull requests um but yeah this is again i think there's a lot of math so i just we can take some time now just go to dcp.stanford.edu um and you know once you do this as riley said you're going to be like really this is this is the hard part like once you get this that's everything you need to know there's going to be lots of examples later you'll you'll see lots of applications yeah the the i guess this is the first time i have uh been involved in giving an introductory tutorial to anything cbx 5 related um and i think it's uh apparent that i typically speak to like a pretty mathematically oriented audience um so like the fact that i went over here and emphasized oh three different ways of writing things and cvx pi will like them to varying degrees this is just because i've seen so many people learn these this lesson and then it's made a big difference when they learn it but this is something you learn later on realistically so yes let's go to dcp.stanford.edu and work through some examples there i have one question yeah can you give me an intuition why cones are the geometry that we care about here um if you have any con so a convex set is a set for which if you have any two points in the set then the entire line segment connecting them is also in the set any convex set you can turn it into a cone by lifting to one higher dimension you can like say if in two dimensions you have a like a an ellipse in the plane you can stretch it down into three dimensional space now you have your your ice cream cone so to speak and it's uh for why that's algorithmically easier i don't have a quick explanation but essentially it's that if you're willing to lift your problem to one dimension higher everything is a cone but let's let's go to dcp.stanford.edu i guess i'm gonna just uh set the scene here a little bit more for some of the examples i already talked to some folks that have some experience experience in f in finance if you don't that's totally fine um i'm gonna introduce it basically from a yeah not assuming any knowledge perspective and yeah like the background is um like we've seen the the theory part now we have seen like the the goal of optimization now it's gonna be a bit more about application specifically because the nice part of of optimization is especially convex optimization we can actually apply it yeah and there's many examples from different fields and people will actually solve it and you already solved some toy problems and the good thing is like thanks to yeah projects like cvx pie the problems that people will solve in the real world are kind of just you know not too too too much bigger than that so they might have some more assert statements or some more checks but like or some more business constraints but the overall problem will not be like insanely different from what we are going to solve today and i think that's very nice and it brings together like i think convex optimization in general brings together like people that maybe just have specific domain knowledge they may don't know anything about programming they may don't know anything about math but still like within you know a few weeks they might be getting ready to to solve some problems in their field and it's basically from different directions somebody who's a very good programmer but maybe doesn't have done like the math in a while can easily pick it up and also apply it to different domains so no matter which uh domain you're coming from or direction you're coming from i think uh yeah you can pretty easily get started and solving some some real world problems so yeah as my i mentioned my background is in finance both from academia and also where i worked so i feel most comfortable starting with those examples so that's acid i and we have basically n assets here so that's going to be referred to as the weights and you can think of it i have you know a hundred percent and i want to allocate this to um different assets and that's why we're going to say like the sum of these to these different assets one to n and therefore the w is our portfolio allocation vector um they have to add up to one but that doesn't mean like it has to be a convex combination they can also be a negative so this means i'm gonna short this asset which is kind of the term you would use and that just means um i'm lending it now i'm selling it and then i'll be buying it back later and hopefully if the price went down i'm gonna buy it back at a lower price so i made a profit that's why you call short and then if you are you know if it's greater to zero you would be long that asset and then that's why you say if all of them are greater equal to zero it's a long only portfolio and if you have shorts then yeah it's it's uh you induce basically what's called leverage um and there's different uh formulations what you call leverage some that's also like not everyone uses these notations and everyone just says says leverage but you don't really know what they mean so you always have to ask like can you please write down mathematically what you mean when you say leverage and one common definition is this one so you just basically take the d1 norm so the sum of the absolute values of your weights which means for example if you have this would equal to let's say two that means you can be short with a total of 50 and then long with 150 so it would add up to one and you would often have constraints of the type you may not be like the l1 norm of your weights may not be more than two or three which kind of gives you a yeah an upper bound on how much you can borrow um yeah they're going to be returns that we want to basically find the best weights that give us the most return with respect to some constraints on the risks maybe and yeah we model them as prices which will change over time if the price goes up yeah we make a return and we most often look at the uh when i say returns i mean the change in the prices at the end of the period basically with respect to the where it was at the beginning of the period divided by where it was uh at the beginning of the period so basically that's just a percent change in the price yeah and and that's uh more easy to work with for a variety of reasons that we'll also see later one of them being that if we want to have the portfolio return like how much did we make across all of our different assets and our weights that we chose it's just the inner product of the transpose of this return vector and w so um yeah commonly you would assume that these returns are random you you can model them as like a random variable for example one that has a mean mu and a covariance that we call sigma and uh this might be a wrong assumption i mean it is a wrong assumption so it's not random it's like people to coming coming together in a market deciding on prices where they want to buy where they want to sell so it's a dynamic system but we're just you know uh trying to approximate it with this random variable and hopefully by finding solutions that are good for our random variable we also find solutions that will work good in practice but that might or might not be the case actually depending on how good your estimates are like any model will not be the true world it will just always be an approximation and depending on how good that approximation is it might or might not work in practice um so we define the expected return as the mean of our portfolio and the variance of our of our portfolio returns would be the risk yeah sometimes people uh do the standard deviation of the returns which they then call volatility yeah it's just a standard deviation again everyone speaks a different dialect every field so this would be the volatility or we will we're going to stick with the standard deviation um which uh yeah you can see this one is convex we already saw today under one condition yeah we have some condition on sigma it has to be positive semi-definite and then also the square root is um convex too but uh people are for historic reasons i think they would really quickly realize that this one is convex and therefore kind of also how it emerged typically it's written it's optimized over the variance but then when you plot it you're going to plot it with the standard deviation so because people don't really know that maybe this one is also convex and obviously we want to have high returns we want to have low risks and we have then as always usually in optimization you want to have some trade-off between between those like there was the example earlier you want to have some of trade-off between speed and efficiency maybe here you have to trade off between return and low risk so yeah just writing our formulation or our form for the optimization problem so we want to maximize the returns minus basically this gamma which is kind of a variable that determines how risk averse you are because what we are subtracting is basically the risk yeah that we just defined and if you are very risk-averse this will like every increase in the risk it will hurt you a lot so you have a large gamma if you basically don't care about risk you just want to maximize profits you you're going to set gamma to zero so this will just parameterize um yeah how risky words you are but in this context it's hard to interpret as a as a raw number so you can just think of it by changing it you're going to trace out all the different combinations that for different risk aversions people might choose um and it's going to be like the pareto optimal line yeah we can also have other constraints which are just denoted here as being in this convex set w um which in in like that this can be empty like we can say we just want to have them sum up to one we can have infinite leverage and anything goes but we can also make more restrictions we can say for example always have to be positive or we can have say what we saw today the k largest uh assets might not be more than than x or or y so we can have a whole different set of constraints i'm going to introduce some of them later um and basically yeah we're trying to optimize for this risk adjusted return as it as it's called and we're going to have a trade-off how does this look like so we have 10 assets here in this example and their risk here as the square root so the standard deviation is plotted against our expected return here on the y-axis and we see like if we optimize our portfolio and we don't care about risk and we say we're in in the long only case so i cannot borrow so what am i gonna do i'm just gonna put everything in the asset that gives me the highest return so if if if gamma is zero then your vector will be just a one in the entry for the highest returning asset and then if you say i'm more risk-averse then kind of i want to make trade-offs so i'm going to find portfolios on this so-called efficient frontier which just says like here it's plotted for two values of gamma if i'm risk averse with like gamma equal to 0.2 um seven 0.29 i'm going to end up over here so i'm going to have this amount of risk and this amount of expected return and then if i'm more risk averse i'm going to have less risk but also less return so typically if you have more risk you're also going to get more return and notice that like all of these points on the green line they're better than all of the red points right because they're going to have some covariances so sometimes one asset will go up the other one will go down so what you get as a benefit what people would call diversification benefit so you're going to do better than all of the assets individually except for like the one case where you are all in one asset so this is how it typically looks like and you want to be ideally over here you want to have zero standard deviation and high returns but that's not possible right so you're gonna have to make some compromise based on the assets you're working with and um yeah gonna end up somewhere on this line here for this example and then for any given value of gamma you can basically see like what's going to be the return and the risk for this portfolio allocation and you can see if i'm willing to accept a higher risk yeah i have a bit higher return here in the expectation but also the variance will be higher indicated here by this wider curve in blue versus if i'm more risk averse i'm going to accept the lower expected return but also the risk would be would be lower here as i mentioned there can be you know very many different types of portfolio constraints the simplest one is be like would be for example the leverage limit that we introduced that we say that the one norm must be less than um let's say two yeah that kind of introduces an upper bound on your leverage it can also contain things like you want to be market neutral where you say basically the m's are the weights that are within the market indicated by what everyone is holding so just the total market capitalization of the asset i which you can just simply calculate by the number of shares times the share price you know how much is the market valuing basically this asset and you look at the covariance yeah it's just m transposed sigma w and this is the covariance between the market and your own portfolio return and you can do things like you want to set this to zero meaning i don't want to be correlated with the market and this is very very valuable for example because if there are cases where everyone is doing good the market is doing good you're just investing in the market yeah you're going to be fine but maybe there's a there's a crash in the market so you're also going to crash with your own portfolio that's very bad because maybe in those times you're going to need more money so it's good if you're uncorrelated with with the market meaning um yeah you don't follow the broad market trends and this is typically what's also hedge funds are doing like that's where the name hedge comes in you typically don't want to depend on the market but you want to make returns that are more or less independent of the market or at least uncorrelated um there can be yeah a whole lot of other constraints saying like tech sector constraints saying this this stock i is in sector i and all of the stocks that are in sector i in total may not be more than uh some some share or yeah you can do different crazy constraints and and maybe we'll see some of them later on um here's an example for different for one constraint that we say okay that uh this basically means you cannot go short yeah the sum of your the weights the absolute rates must be one the sum of your absolute weights can be two and the sum of your absolute weights can be four right so i can employ different amounts of leverage and i can see obviously if i if i can do more leverage i i will get more risk and therefore also more return but i'm going to do it strictly better because i can still use i choose to not use all of that leverage right so this will obvi obviously be better but you might be constrained here for legal reasons or practical reasons so not everyone will have infinite leverage limit so and that just means that for a given risk aversion parameter here or a given level of risk you will do better if you can choose to borrow more and i think there's an example here we can see for the same risk limits one two and four we can see how would the portfolio actually look like yeah for one which is on the very left we can see we never go negative yeah and we just invest a bit here here here here and here but as you notice it will it's a very sparse solution right because we don't invest in any of the first five assets and if you're doing like a one norm lasso as we just saw earlier today you will be familiar that this often leads to sparse solution and indeed this is what we see but if you can if you can employ leverage you can see that you will basically short the assets that you also didn't want to go in so now you can even short them before you just couldn't buy them now you can bet against them yeah so that's kind of makes sense that you would start um yeah betting against the assets that you didn't want to invest in in the first place now you can make it even better by by not investing into them and and betting against them then i think we're gonna talk about some variations you can continue i'm just wondering is this just like how you would do it in academia was this how they really do no this is how they would really do it um so the the problems are actually um the problems we're going to write up today in the in the exercises are as i said a factor of maybe five uh in complexity away so it's not a huge amount if you just look at the length of the problem that they would typically solve maybe it's you know five times longer because they add some more business constraints and sector constraints and and all of them at the same time but conceptually it's not much harder than that but i think what's the hard part is not solving the problem going back to this one here so as we said like the mu encodes what are your expectations about the returns right so that's a very hard question so how do you come up with those with the data basically of your problem and that's that's i think that's the more tricky part and this basically means what's your private estimate of the returns of the assets and um and typically there's a whole lot of variance into going into these estimates and you this could be like just the mean of the last 10 years yeah this would be a very simple model it could be some crazy neural network that takes into account how many trucks have stopped at walmart delivering products so you you you expect if there's a lot of trucks delivering product you might have a higher mu for the entry of walmart in your stocks right so this can have various degrees of complexity and um same goes also for for the sigma so solving the problem once you know the data is easy and then yeah getting the data that's kind of the hard part yeah i'll say it's easy because cbx pi exists yeah and there's also libraries building on top of tvx pilot make it even easier so you just basically give it your data and say okay i want to do mean variance and and uh yeah it will be even easier but as we will see today also um writing it up in cvx pi is going to be just a few lines and yeah if you have good estimates for mu and sigma it would actually work in practice but that's the hard part yeah that's that's the problem you don't easily get then and i think there was some some story that that the advisor of our group i mentioned that there's like labs in berkeley i think that sell covariance matrices for hundred thousand dollars which in this context is actually nothing so i would sell for tens house oh yeah but maybe yours is not maybe yours is not as good though right so yes yeah so so yeah that's that's the level of sophistication that goes into these covariance matrices um or also the muse in particular because it's very sensitive to the muse all right so yeah we can have different constraints that we add on top of of this problem um and we will get solutions that might look different depending on on the the constraints that we set and yeah as i said there can be many variations on it we can basically turn around the problem and say give me at least this return and minimize the risk associated with that or we can say the other way around i want to have maximally risk tolerance of of some amount and maximize the return that i can get for this risk they're all going to end up end up on this green curve so it's basically just different ways of expressing the same problem one interesting thing is what you could not do is maximize the risk you can only minimize the risk and that's a common theme again in optimization typically the problems we want to solve are the problems we can solve which is good and there might be different reasons for that some might be chance some might that people just worked more hard on the problems that they want to actually solve um but yeah it's it's a good thing that we can minimize risk and not maximize it would be worse the other way around yeah and basically here we can see how we can express basically this the the risk also as a second order constraint too we can add more variations we can say okay shorting typically is not for free yeah because i have to lend it from you and then i sell it and then later i'll buy it back so in order for you to lend it to me you probably want to be some you want to have some some profit from that too so whenever i go short for example i incur some costs here they would just be linear i can include transaction costs which is very important because if i solve this problem every day and we will also have an example of that later maybe the optimal portfolio will radically shift every day because my my muse and my signals will change because i have one more observation that goes into them and just minor variations in the in the in the inputs can lead to widely different outputs which is kind of very brittle optimization in that sense because it's very sensitive to the inputs so we can say things like we want to penalize deviations from our previous portfolio and we can do for example um just like the absolute we can just say the absolute deviation the sum of the absolute deviations we can also say we want to square them for example yeah so that would make sense in a in a setting where the more you have to trade it's not getting just linearly more expensive because um yeah how the market microstructure works basically the more you want to trade on one given day it's going to be more than linear uh expensive for you to do so so you can do like quadratic or you can just do three halves which i don't know if there's a good way to reason about it but it's commonly used in practice too so yeah any of those values would be would be reasonable then like this is what you would typically use to solve problems where you have like 10 or 100 assets and you can reasonably expect to have enough data to to estimate the covariance matrix but what will you do in practice if you have 10 000 variables right so the covariance matrix will be very big so you will probably not have enough data to just use the sample covariance matrix for that one um so what you would do for example and without going into too much detail here um you can basically do like what they call factor models which if you heard of for example principal component analysis this is basically it and it will reduce so for each for each asset you will basically break it down into exposure to these different factors and then they call it loading on these on these vectors and then you can reconstruct the covariance matrix basically just from the exposure to the different vectors plus then some risk that comes on top of it which is not explained by the factors so this would just be a diagonal matrix that's why it's noted d here so you can basically your risk and your and your asset comes from the exposure to the different factors and you can they even have them like names for these factors some of them might exposure to um you know to energy prices let's say so some companies might have a huge exposure to energy prices if you're maybe shell or exxon maybe you have obviously huge exposure some other companies might have only very minor exposure to that and like the global development of all prices as we see might affect one company way more than another so that's basically how you can think about these these um loadings to these um factors and then this is what they would call the idiosyncratic risk component which means this is just a risk that you incur that's totally not explained by any of those factors like let's say your factory burns down that has nothing to do with the market it's just like a risk that only affects you and that's why this will be diagonal um yeah and you can just run the portfolio optimization based on this model and it will scale much better it will scale like if k is the number of factors you choose and k is comparatively small and in comparison to n which is the number of assets you have then n k squared is much much smaller than n cubed basically and yeah we're not going to go into detail about what exactly how exactly we derived this model but basically basically we just say we can instead of the actual covariance matrix we can use this factor factor covariance matrix instead and here's an example just like to see that the computational advantage saying that okay if we have 3 000 assets which is quite a lot and your covariance matrix will be quite big already if we solve it in the in the classical way we would yeah come up here with uh like three-minute solve time uh also there's some progress this talk was also or this slide was already given a few weeks a few years ago and this was 800 seconds so just like the servers are getting faster the the compilation is getting faster too so i think that's also a good way that things are not getting so much slower but then the factor model just solves in under a second on your laptop one core single thread with open source solvers so i think that's that's a good way of seeing like you can solve real world problems on your machine without having super expensive equipment or or super high computation power and even even the 3000 asset one which is quite a lot like that's right um i just wanted to point out the cvx pi osqp yep here osqp is a numerical solver like the whole point of cvx pi is to make it easier to use numerical solvers you see on the whiteboard there it says prob dot solve solver equals mosaic so in this example you would say prob.solve solver equals osqp yep so like in in general if if there's a name for a piece of software thrown around for we use cvx pi and blank that and blank is probably a numerical solver which you specify with solver equals length and also i tried this particular example yesterday by doing just the one code change i make i was writing solver equals mosaic this went down to 10 seconds so just to see like okay you can you can get sometimes better performance by by changing to some servers that are proprietary um or like better optimized for your particular problem some servers might work well on some problem class but not as well on others um so yeah that's that's important but if you wouldn't have cvx pi right so you you write your problem um in your particular form you formulate all of the constraints so that they fit exactly to the interface of the solver um writing it to solve for mosaic would maybe take you a week because you have to redefine all of the interfaces some of them are very similar some of them are very different so the beauty of this is just you change literally one keyword argument and you get all of the interfaces that are also part of the cvx pi ecosystem basically this will be taken care of and it will just match the format that is required by the different solver right you can also add your own servers by the way but typically you would choose to use pre-existing general type solvers yeah [Applause] and then like you can do so this is pretty let's say well known like if you take a finance class most of those things would be you know you would you would have heard of those but with uh you know cvx pi and there's also an example about that we can do things that uh now you you basically at the end of the this small talk you wanna you will be you will know more than many of people working in finance actually know so i think that that's a good that's a good part um what you would could do is for example say you know i bought your covariance matrix for 10k and then maybe you will sell me a different one for 5k and you will maybe sell one for for 20k but i'm not sure which one of you is correct maybe you because i paid the most or you because you're very you're very selling it uh very much selling it to me you're the best sales person maybe so i don't know which one will it be um so what what what can i do i can just say okay i believe neither of you but i will optimize my portfolio in a way that the worst case that can happen if either of your coverance matrices is correct that i'll suffer the least loss basically so i will optimize my worst case so this is kind of a mini max game so what you can do and again we will see more of it in example we can say we have a first of all we can say we have a fixed vector right we say we already know the weights what would be the worst thing that could happen under any of your coherence matrices so i could just you know try all of them but also i could just write it as an optimization problem and i think that's not the quite interesting part so i can just try all of your covariance matrices and i will know what will be the risk and i just take the maximum risk and i say can i still live with that or not if not i might need to change my weights but how will i change my weight so i will just change it and then i'll just try all of them again and then i see did i do better or worse kind of there must be a better way so you can think i will tell you different covariance matrix but maybe i'll complain because uh you know it but only in five years yeah that's sometimes a problem you only find out later how good it was or not so what i could do i could say um given your three covariance matrices find the the weights that um will again make me least unhappy in the worst case so that's kind of a problem where you combine portfolio optimization with this worst case analysis i think the way it's framed here it will basically just uh compute what's the worst case but you can also combine it with like finding the weight such that the worst case will be least bad so that's a different example and i guess like not too many people notice because it's very easy to do and now it's easy to do in cvx pi so you could just you know come up with different covariance matrices there's like a whole lot of literature how you would choose these covariance matrices that's like shrinkage if you if you've heard of that or like just a sample covariance matrices factor models all of these different different types so what you could do is like maybe i don't trust either of these methods but i can say say i do maybe three of them and then i just say okay i want to do at least that in all of them and maybe that would be better than just choosing sample coverage matrix and yeah i guess we're not going to go into too much detail about how this particular one is constructed but i would say this is enough of like finance background now we're going to solve all of these all of these problems that i just briefly introduced and there will be more description on each of them for the all of the exercises that start with 13 and you can choose to do them if you say i don't care about finance that's not interesting at all we also have some other examples that are into biology there's one about i think the speed uh the speed efficiency trade-off um and then there's the energy one that's uh yeah you're optimizing i think the level of a battery i think so yeah feel free to to query me on finance things or optimization things any of us uh or energy things or linear algebra things so yeah whatever you you want to do feel free to do so um and we just be around um yeah checking in i just wanted to add one thing quickly so um if you guys are in the slack i posted a bunch of stuff in the slack uh like resources um and one thing that's particularly nice uh is this pi portfolio opt which is built on it uses cvx pi um but it's like even more user friendly and like if you you could really like just use this and just get some market data and like go to town like it's it's really actually extremely easy to use and people use it in hedge funds so maybe maybe this class will have be payoff when you're when you're interviewing at a hedge fund yeah yeah but sorry go ahead i think it's it's essentially been replaced by this other one i mean we we did that but we like didn't really get very far with cba's portfolio and this one is much much more developed cdx unfortunately it has gotten really into only maintenance mode so it's cv portfolio it's no longer actively developed oh i didn't know that more recently uh um martin who developed it he's gone to work at a hedge fund and someone paid him to basically no no not that but like it's just a lack of time i think so ah so if anybody wants to contribute now that you're all experts in in finance and civic pilot feel free to reach out yeah as always open source it's always dependent on contributions cool then i'd say yeah just choose the problem that's interesting most i think you don't have too much time so and they take like probably like at least 10 minutes each or maybe more so yeah just choose the one start with the one that interests you the most and maybe that's kind of the order you should go for yeah imaging so this this uses cvx pi no it's something i did that was related ah okay i threw it in there for the heck of it it's kind of a similar idea but it's not cvx ply based different very similar syntax yeah i mean i wrote it it's it's the same it's like the same idea but it's like these problems are non-convex uh typically or like people like to make them non-context so and also it's yeah there are a lot of it was pretty interesting it's like um like that they require uh more special treatment with to solve efficiently and there are also some extensions built immediately on pi so for example i think uh riley you mentioned before when when you only fix one part and it's becoming convex and or like you linearize one part and then it's convex and then you can basically solve the sequence of problems that would um you know get at least a good solution maybe it's because it's not globally convex you might not get to the global optimum but you can still make a sequence of convex optimization problems that might get you to a good solution so there's also packages this would be called dc ccp for pccp or convex concave programming basically and yeah i think there's some other ones uh i think that the code generation that was mentioned before is also like built on top of cdx pi but not immediately included yeah so some extensions already exist you know you're welcome to to hang out as long as you want um but i just wanted to say a couple things at the at the uh four hour mark so first of all thank you for spending four hours with us um that's a big commitment i really appreciate it uh we all do um and yeah um this is uh you know something that that we've all been working on for a long time and and it's really cool to share it share it with people who are interested um in terms of like follow-up materials so i would definitely recommend joining the discord i think there's a lot of interesting stuff that people post um and any questions that that arise like that people will be very happy to answer in terms of like other things you can look at to learn um and also this is on the slack so so uh look at that there's a book about convicts optimization um it's it that's a real that's sort of the key key one um it has tons and tons of examples like hundreds um and then there's like hundreds more in the additional exercises there's also a class at stanford that's uses the book that has more of like a lecture format and like videos and so forth it's the same material but it's you know it's a different format yeah the videos are on youtube yeah so you can look at that whole thing yourself and then also yeah the slides are online i post a link in the slack and then we also put them in the repo and and then yeah this these packages there's a lot of interesting packages there are others that that i'm forgetting like there's a lot of um quantum stuff that that uses cvx pipe to some extent uh so that's that's another another area uh but yeah i hope uh you you've i hope we've been able to like communicate sort of how how broadly applicable this all is uh and i think one thing like the thing i liked the most about studying this topic in my phd is i started to see at the end like how all these fields were connected and sort of these common patterns that occur again and again and again across these fields and i think that's what all this really enables like having this this consistent methodology for how you approach these things thanks [Applause] we did have a getter but it was we we've switched to the discord yeah and for longer discussions um you can also use the github discussions feature like everyone knows github issues there's also github discussions um people mostly use the discord like there's long conversations on discord sometimes because people will come with everything from like help with this dcp or why is my problem taking so long i'm honestly amazed at how good a job stephen does responding when there are like 600 people on any other
7,Eroding Coastlines: A Geospatial & Computer Vision,https://www.youtube.com/watch?v=iob7R70VTvI,welcome to sci-fi everybody um my name is monsi shah and i'm here with my colleague kevin lakkai and we're really excited to present for you all a tutorial on analyzing coastal erosion using geospatial imagery in python i guess we should start by introducing ourselves hey i'm kevin lakai i'm uh i'm a software engineer at planet and um i've been there for about a year and both monze are on the developer relations team where we build tools for users like yourself to help ease the pain when you know using any sort of geospatial data or our data in particular yeah my name is monsie shaw um kevin as kevin said uh developer relations team at planet labs also been there for a little less than a year um my background is in it's kind of varied it's like environmental science data science i sort of come at it from all different angles so don't feel worried if this is your first time doing anything geospatial related with python we are here to help all right so what are we going to do today we are going to be um talking about first the core geospatial concepts associated with remote sensing analyses um and computer vision and then once we cover those we're gonna look at them in python so how can we use python tools to perform some of these analyses um and like do this raster processing and then finally y'all are going to apply all of what you've learned to a real world scenario of coastal erosion any questions so far okay cool all right so just starting with some core geospatial concepts just out of curiosity who's worked with geospatial data before okay so a handful cool anyone completely new to python it's okay if you are yeah okay awesome you're very brave to be here so i'll see all right so first thing to know about geospatial data spatial data is special there are characteristics of working with spatial data that are different from working with other kinds of data or even other kinds of imagery for example geospatial raster imagery can be multi-spectral so that means that the imagery has multiple bands for example a visual color image you're looking at might be made up of the red band blue band and green band that are all combined to get you that visual image and that's a consideration um to keep in mind geospatial data is also spatially referenced which means that an image you're looking at of the earth ties to a specific um place on the earth itself so it's not just an image but the image ties to like a physical location and it also because of this has spatially relevant metadata that helps tell you where on the earth you're looking at just an example of how raster data works so if you look at the image on the right just say this is like a beach for example so you have the jungle or trees and you have some sand and then you have water these are pixels arranged in a grid and the similar pixels have similar values um sorry pixels that are similar will have some similar values so if you look at the image on the left you see that generally all the water pixels would have the same value all the sand pixels would have the same value and same with all the tree pixels the pixels are also represented as numpy arrays so this is the sort of special feature of it that will allow us to manipulate this data as we go you might also have heard of the concept of resolution when it comes to geospatial imagery it's a little confusing because there are so many different types of resolution um so let's just go over them really quickly the first is spatial resolution this is probably what you think of most commonly when you do think of the word resolution so if we're looking at let's go back to that beach image say we're looking at a picture of a beach spatial resolution is how much area on the ground does one pixel cover so for example if you're looking at that image we were just looking at you might say okay one pixel on this image covers a three by three meter area on the ground and so your spatial resolution would be a three meter spatial resolution you also have temporal resolution which is the frequency between image captures so say you went to the beach in 2018 before kovid and then you went in 2020 right before kobe hit and then now you go to the beach again this year your temporal resolution would be every two years or so and that's how often you would capture those images there's also spectral resolution which gets uh to that band concept i mentioned earlier so this is the number of spectral bands we're measuring um most common you have rgb and like near infrared but there might be other bands you're looking at too and then there's also radiometric resolution um this is bit depth of pixel values so how much information does one pixel contain this is a little uh confusing but i like to think of it in the context of color so if you have high radiometric resolution or high color depth or bit depth you'll see colors a lot more clearly and actually i have an example of this so this is a really good example um with low bit depth you're kind of if you're looking at this leaf image you it's kind of black and white looking you might get some green with a little more but then if you increase your bit depth radiometric resolution you get these colors really clearly here so that's just an example of that all right and then talking about the bands again um we are going to be working with multispectral imagery so imagery that has more than just one band of data so again if you look at the image on the right you see a sort of natural color image like what you might expect to see if you're um just looking at the image with your eyes but that is made up of several different bands red green blue and others as well yes oh no no yes please um go for it yeah yeah um that's a good question i think that's yeah i think that does make sense um yeah computationally for sure but also from a like a modeling perspective so bit depth goes as 2 to the nth power so if you have a bit of one you'll have two colors if you have a bit depth of well i don't have anything but let's say 16 which is like our analytic data you'll have a bit depth of 2 to the 16. so like ten thousand different shades of a certain color then you add let's say just three bands you have three times all that so yeah it it scales exponentially so if you if you're trying to model something at a very high tempo resolution with you know years of data you're going to probably want to lower that bit depth if you don't have a super computer any more questions so um we've talked about some general basic geospatial concepts let's talk a bit about the imagery we're going to be using today which comes from planet so a planet has a couple main different types of satellites on the left is the planet scope dove satellite which is the same one you see right here definitely take a look at it later if you haven't already it's pretty cool um these ones are about five or so kilograms each they're quite small they fly in a large constellation and they map the earth's land mass every day at about four ish meter resolution um they're small but they they travel fast around like twenty thousand kilometers per hour and they take pictures in eight different spectral bands we're going to be working with four of the bands today but they they take eight um and we also have yes uh they there are many different doves that fly around the earth together and together they image the whole earth i have a a picture i'll show you it'll make a little more sense in a bit um we also have sky stats which are bigger than these doves um about 100 kilograms they take higher resolution data and then they're they're bigger as well they're about the size of like a mini fridge um and those are tasked satellites did you want to add anything kevin about ourselves we we tend to nerd out a lot about this so all right so this um this was a video it's not going to work here but you can see there's sort of a narrow band um on the earth where these doves are circling so if you imagine the earth is turning and these doves are circling the earth slowly they'll cover the whole landmass of the earth as the earth turns and here's just another graphic sort of saying the same thing it's a little bit blurry all right any questions every day yep hi cadence satellite imagery cool all right let's talk about geospatial tools and libraries so the good news is um python's open source we also have lots of geospatial tools that are open source that we use um some foundational software that's really important gdal is a library for reading and writing raster and vector data and then proj is used to convert coordinate reference systems which we'll talk about in a bit anyone used g dollar crotch before nice and then also with python libraries we've got rasterio used to read and write geospatial data represented as arrays then we also have numpy which works well because it also works with arrays matplotlib is used to plot data generate charts and then opencv is a computer vision library used for image processing anyone worked with these before yeah nice awesome okay the fun part so um if you all haven't already go ahead and go to the link on the screen here this will take you to our repo that we'll use for the rest of the tutorial you should be looking at something that looks like this is everyone there anyone want me to put up that link again yeah okay all right so um what we're going to be doing now is before we dive into the coastal erosion example we're going to be going through some key steps and like working with the libraries um for examples before we dive into the big the main sort of part of the tutorial so if you go to the repo it looks like this um as kevin mentioned just really quick for the data download piece there are two ways to use the data that we're going to be working with today the first way i recommend highly is using google colab um anyone used jupyter notebooks notebooks before okay mostly familiar with that all right cool yeah google collab is a way to basically run that in the cloud so you don't have to download the data to your machine it's all just sort of there and i'll walk you through that in a second um the second way is to run it on your local jupiter instance so then in that case you would have to download the data and clone the repo um any is anyone here just so i know doing it this method doing the jupiter i think okay a few of you all right um if you run into issues when you do this you can always switch to the google collab method i'm gonna be running through it on collab here but just know that if you run into issues this one is always available yes yeah so um when we go through these in collab the notebooks will once you work with them if you add any code or if you change anything once you exit out it'll go away um if you want to save it any changes you make to it um i'll show you in a second so you'll want to go to it's kind of like google docs where you like save a copy to your personal drive so you don't want to go to like file save a copy and then you can save a copy here and then work with it and then anything you do will be saved yeah another quick note make sure if you haven't already so for this uh you won't need to download data but for kevin's part later you will need the data and that is a large data set it took me like an hour and a half to download so if you haven't already click on this xero underscore download underscore data notebook in the repo and yes his data needs to be downloaded oh yeah right here okay um but it does it does take a while to run so either have it downloaded to your local machine or just run it here through co-lab yes yeah you can either run it here um in collab if you're using it this way or you can download it to your local machine yeah does that make sense oh yeah sure it too um when you try to run things in collab you might get this message notebook not authorized or not authored by google that's okay um you can just run it anyways all right so going back to reaper i'll just do it later i'll download it okay um you want to make sure you keep this open in a tab because we'll we'll keep coming back to this so for now if you want to click on this first notebook in the repo one underscore by stereo underscore first look and then you should be looking at something like this and then in your notebook you want to click that this blue button that says open in collab and then you'll see this page that i'm looking at has everyone been able to get to this point either open in collab or on your local jupiter instance okay great awesome so let's talk about reading data with my stereo so in these examples um later we're going to be using a different example with kevin but in these examples we're going to be using an image of houston flooded during hurricane harvey in 2017. um so this is real data that was this imagery was released in the days following hurricane harvey um and first responders used this data on the ground to like help like facilitate rescuing people um and like just in general rescue efforts during that hurricane so this is real data that we're using so first thing you want to do if you're using collab you're going to want to run this line just by the way for those of you who haven't used notebooks before notebooks allow you to run code sort of line by line so you can see what happens step by step so to run lines in collab you want to click this play button that's next to the code cell and the way that you will know if it's finished is you will get a check mark oh okay great prices are ready okay so um [Music] all right it might take a second but after it's done you should get a check mark and this um raw stereo install and also data download you'll have to do it for every notebook that we do today so just fyi has everyone been able to run this line yeah okay great um all right so loading a data set first we want to import raw stereo so we're importing the package go ahead and run this line and then the data that we just got above um this is us loading the data in so this third code block we're opening this data using my stereo and then calling this image in my stereo calling it satellite data and we're printing this so here we can see this output shows our data set that we've just read in then we can also look at some basic information about our data set so if we run this next line we see again the name of the data set all of these sort of numbers and then visual.tif and then we also see the number of bands in this data set so sat.count is three so we have three bands in our data set we can also work with the bands so if we run this first line we can see the indices of our bands this is important because if you worked in python before you know that python is zero indexing meaning if you have a list of values the first value is actually value zero it's a little confusing but with bands the first one is one so first band is one then two then three and then because we know we're looking at a planet scope 3-band image we know the band order so you might be used to seeing if you've worked with geospatial imagery before rgb red green blue in this case our band order is blue blue green red so by running this line we're taking our image and we're saying we want to establish the band order as blue green and then red we can also look at the data type and we see it's a unsigned 8-bit integer and so we know it's stored as a numpy array yes yeah i i think you'd have to know um you might be able to tell like if you visualized your image it might look funky if it was blue green red and you thought it was rgb if you visualized it like the colors and stuff would be off so you would know to flip the bands that's usually how i find out i like visualize it i'm like oh that doesn't look great and then change it and then we can finally also look at the shape of our image so if you want this line we can see that our image is about 4 000 ish pixels by 8 300 pixels so we get the image of it any questions on this first look of our stereo sorry um [Music] yes yeah um they probably have values but they might also have zeros they might have some um like nin values as well um no they they should all sort of have different values yeah we'll we'll read the bands in a second you'll see like how it will work yes oh this one um okay so this image file this second cell here is the path to the file and that we've gotten through this line that we ran at the beginning so here in this third line the variable image file is this path so when we do by stereo dot open we're opening the path that ties to that file oh okay um i'm actually to be completely honest this is the first time i've used collab in this way with like the data actually in it i'm not sure exactly how it's working but it's this content part of the url that is pointing it to this part yeah if you download the data this way as we've done here at the top and then in collab you can use like slash content and use it more questions oh yes yeah that's a good question i think do store more kevin do you know oh yeah yeah i think it's because it's a geospatial image it's like different from a regular like i'm not totally sure on the mechanics of it foreign so so yeah i've had so many situations where i open up an image and i think like my computer's broken or something because it's all black and then i realized that it's because it has extra bands or it's higher bit depth or something so i have to open it on some geospatial software all right so now that we know how to work with raw stereo we're going to use it to extract bands and then compose it as seen from it so go ahead back to the repo and then open the second notebook so two underscore raster bands dot ipymb then same thing there click on the open and collab button once you get there and then you want to again run this if you're using collab run what's underneath option one so the get the data and then pick install stereo so are we all here yeah okay okay so um we want to first import our packages so we're going to import rasterio again that like we just did but we're also going to import numpy um and also matplotlib and i've just used shortcuts here so here i'm just saying that every time i refer to numpy i'm calling it mp and same thing with matplotlib dot pi plot i'm just calling it plt by the way i say numpy i think kevin says numpy i don't know the right way to say it all right and again we wanna get our image file and then open it using my stereo so first we're going to look at the metadata of our image so this first line we're getting our images coordinate reference system so we see that this this corresponds to utm universal transverse mercator coordinate reference system a coordinate reference system is basically um when we we're looking when we're talking about earth data the earth is round so we're talking about it in a 3d way and then when we're working with images we're working with 2d images so there's a bit of a transformation that has to happen from the 3d to the 2d and that's what a coordinate reference system helps us with so in this case because we know that the coordinate reference system is utm utm uses meters as the projected units so we know that the units we're working with here if we're talking about the projected coordinates are meters so here this is telling us that the the left um corner bottom right and top these are the locations and meters and what this is in reference to it always helps me to look at this map so just for general reference utm divides the earth into 60 different regions and each region is like six degrees wide so this is what that's referring to within the region um the position in meters okay we also want to get the dimensions of our image so if we run this line we see that our image is 25 000 ish meters by 11 000 meters width and height and then we also want to get the rows and columns so we see this is in pixels so our image is about 4 000 inch rows by 8 300 columns and now that we have both the dimensions in meters and pixels we can look at our spatial resolution so if we divide the length in meters by pixels we see that our image has a three meter resolution so each pixel in our image is representing a three by three meter block and just for a sanity check yes our pixels are square i've never encountered a situation when pixels are not square but just to sort of make sure that's the case yes yes three meters by three meters all right we also want to um look at our pixel coordinates on the globe in uh into geographic coordinates so um this is telling us that in the utm coordinate system that picture i just showed you all this is our top left corner coordinate and our bottom right corner coordinate again sort of just like a sanity check type of thing these numbers no that's a good question um these are you can look at the eastings and northings so i would have to i would have to look at this find the zone that my image is in yeah so this is um this is referring to within the zone itself so i would have to find the utm zone and then go to that zone and check it from there yeah yes yeah that's a good point thanks cool and then all this we sort of did this manually but all this information is available in the images profile so if you run this next line side.profile we see the bit depth we see the pixel dimensions band count all this stuff is right here so we can see that very easily questions about melissa yes yep yeah yeah that's to do with this easting and northing thing so within these cells moving to the right goes up and moving up it gets higher yes that's a good question yeah i'm not sure is any does anyone know i would have to look that up but yeah yes um yeah there is an attribute i think it's called like the image dot transform or something that will help you transform an image or at least like get the reference system then you can transform it from there other questions awesome so now that we know a little bit about our image we can use it to compose a scene so again if we look at our band count and indices we have three bands one two three and then again we know it's a planet scope three band image so we can assign our bands blue green red so we have our bands here and then here's the fun part this is um sort of what we were talking about earlier where what kevin was mentioning with the different bands and how they can be stacked on top of each other to get that visual image that we're used to seeing so this line on numpy.dstack we stack the red green and blue bands on top of each other and so this what we get is our visual image something we can see with our eyes and then to actually look at that image we can use pi plot to plot it here and here we go voila yes yeah so um it does look a little bluish i've tried it in a few different ways otherwise it looks reddish it might just be the way that um it's visualizing it here uh i'm sure if we pulled it up and like calibrated the brand the visual components of it would probably look a little better there's generally some calibration that you need to do with images in preview okay kevin are you familiar with this uh it might look a bit reddish if you do that yeah this does look better that is a good call um i know the order is bgr so i'm a little confused yeah that was a good call thank you uh okay got it i changed it on line 17 all right sorry yeah line 17. so this is where it was rgb and we changed it good call on the iron shell thank you okay so just looking at this image really quickly um it's kind of blurry but we have flood water here that's this kind of brownish stuff we're seeing which it makes sense that it's brown now um because before it was oddly blue um and so this is good to know that we know we're looking for water in this top right corner of our image so that's just something to keep in mind as we go also here in the left side we have clouds that we're seeing um and also you see the clouds shadows if you look closely this is also important to know because sometimes when we work with imagery we're using things like reflectance values to get information about the imagery and then sometimes because of that clouds can read as water values so if we're not careful if we're doing like a classification or something it might think that these sections are water even though they're clouds so that's why you kind of always want to look at your imagery make sure you know what you're if you're looking at clouds or what you generally might expect and we'll see this kind of play out a little bit later yes yeah so we've worked with my stereo to read in our data and then we've also looked at our metadata associated with our image and then we visualized our image so far yes okay so now that we have taken a look at our image we can get into more computational stuff so we can um can compute our ndwi learn about it compute it and then calculate a land mask from that okay so back to the repo uh notebook three and then same thing i'm sure you all know the drill by now opening collab and then run this first line all right so again we want to import our packages and load our data same thing as we've done before and now we're going to calculate ndwi so in this example because we're looking at flooding it would be useful to be able to say oh this is an area of water versus this is an area of land and get an idea of what we're working with without um sort of not in a visual way but more in like a computational way so we can measure something called the normalized difference water end index or ndwi this is related to plant water content and it can help us identify areas of water and we can calculate it by using our bands so the calculation is green minus nir near infrared over green plus and ir if you're curious about ndwi you can find more information here you might have heard of also like nd vi um vegetation index there's many different indices and ndwi is just one of them so the first thing we're going to do when we're working with ndwi is we're going to allow division by zero this is kind of crazy like i feel like middle school high school teachers would like yell at you if you tried to do something like this but the reason we're allowing division by zero is because in our nd this is supposed to say ndwi in our ndwi calculation um parts of our image might be clipped or might have like zero or null values and if this happens in the denominator of our calculation we have um this addition and if both of them are either null values or zero then we would have a division by zero so by allowing division by zero here we're just making sure that if we run into any null or zero values um that it doesn't break our code yeah so that's just if this denominator is zero we just want to allow that um it it would just i think it would just remain zero or null it's just allowing it so the code doesn't break okay so again we want to read in our bands and you might notice here we have an extra banner working with we're working with the nir band this is because uh the past two notebooks we worked with we were working with visual data which is just like you can see it with your eyes you don't need the nir band this time we're working with the analytic file and the analytic file does include that nrr band so that's why we're when we read this in we have this extra band we're accounting for then here we're calculating ndwi so again here's the calculation um green nir and we're dividing it and then once we have our ndwi calc we can visualize it so same way that we visualized the previous image with um i am show all right so here's our visualization of our data set and you can see that this is showing the values so the higher values correspond with those areas we saw earlier that we knew to be water just from looking at it with ndwi um pixels that have a relatively highest value generally greater than 0.3 are are generally associated to be water values while pixels that have lower values are more unlikely to be water but in this case because we're looking at flooding data the line between land and water is a little murky the water is a little murky we're gonna be using a slightly lower threshold so we're gonna be saying that anything higher than zero for now is gonna be considered water this is you can kind of play around with this a little bit um there's not like a hard and fast sort of thing this is something that people generally experiment with just a bit i that is a good question um i'm sure you'd have to do some more image processing if you wanted to get into like the nitty-gritty um there's probably some other sort of calculations and things you would have to do um masking would help we'll talk about that in a second but i think generally it's better for more like clear areas of water would you say kevin yeah so now that we know our ndwi values and we've sort of established that we're using zero as a threshold in this case we can create a mask which says that basically anywhere where the values are higher than our threshold which is zero we want to call those water call those pixels as water pixels and then anywhere where the values are lower than our threshold we want to call that land pixels so that is what we are doing in this line here and we'll work with this a little more in a second does that make sense everybody are we all good with ndwi awesome okay so now that we have our ndwi values we can talk a little more about uh interpreting this so what does it mean how can we sort of get more clarity on what's water on and what's land so notebook number four back to our repo masks and filters and then open collab so yes opencv yeah you might need to import it if you don't already got a little bit in partnership are we all generally here yeah okay um so this first part is kind of just doing exactly what we just did so fluffy to just run all these lines here we're just calculating the ndwi again just in this notebook so we're doing the same thing allowing division by zero setting the bands calculating mdwi and then getting that masks so once we have the masks we can look at them so just to recap the masks showed us once we had our ndwi values what pixels based on our threshold are we classifying as water versus length so we can look at our masks so first let's look at our water mask and then let's look at the land mask yeah this is it's kind of a funny image it's like you look at this and you're like what am i looking at it's just like shapes and holes um we in the visual image we looked at we saw this whole area at the top right was supposed to have water in it um we see some water here there's a lot of holes though where we would expect a bit more water versus the land it does make sense here if you see this this like curly shape here that's where we did expect the water to be in our previous image so that sort of makes sense but again we there's sort of a lot of uncertainty around this image of where the land versus water values actually are yes yes um that is the clipped um portion of the image that was like nan or like yeah zeros yeah exactly yes that's what we're going to do so that's where filtering comes in um so if we wanted to clean up this mask and get a better idea of where the land and water areas are we can use filtering the morphological filters we're going to use right now come from cv2 and there's a couple different filters we're going to use one is called closing filters and one is called opening filters i'm a visual person so i like to look at like look at this what it um looks like so let's pull this up this is from the opencv documentation so you can see that an opening filter is when you have um areas outside of your main image that you want to close up personally i think the opening and closing the terminology should be flipped because for me this seems more like closing but anyway i don't like the rules um so this is an opening filter where we're clearing out these extra specs outside and then a closing filter which to me kind of sounds like opening because we're getting rid of these specs but anyways we're clearing them out of the main part of the image here so the way we do this in action is we establish a kernel size so for our closing um i've just said 35 and then for opening i've said 15. you can change these feel free to experiment this is saying that for the closing any patches of pixels that are within this window of 35 will be closed and same with the opening kernel any pixels that are within 15 pixels will be opened and then the rest it'll it'll ignore it'll keep them the same um 35 sorry yeah it's like a grid uh say that again yep yep oh yeah it's a 30 it's um 35 by 35. yeah so what we're doing here is applying this okay there we go um we're applying first the closing filter so we're doing pull this up again we're getting rid of where we do expect um there to be the data we're looking for we're getting rid of the holes and then we have that mask the closed mask using this 35 grid uh kernel size and then we're taking applying the opening filter on that closed data set that we already have so that's why here you see we're applying it on the mask closed data and then finally this object mask underscore close underscore opened is after we've applied the closing filter and the opening filter and then this is what i think someone was asking about earlier where i think you're asking about it um how do we make sure that the areas that like why were those areas black that were black before and this is just making sure that the areas that were equipped earlier still remain clipped we don't want um areas that were zero we don't want them factoring into the opening or closing so that's what we're doing here so we have our final mask and then we can visualize it all right so let's just for reference go back up to our original water mask this is what we had before sort of we can't really tell a whole lot this is what we have now and you can see this part on the right we can see much more clearly the areas of flooding that we saw in the visual image of course there's still a lot of this stuff out here i'm sure we're probably catching some clouds in there in in this that's counting the clouds as water this is something that you can experiment with people run like algorithms and stuff to find the perfect combination of closing and opening kernels and other sort of things you can do with it but here's just sort of a general idea of how this works to get a better understanding of the filtering i think so that's a good question i'm not aware of the like actual dynamics of the algorithm oh let's say each of these blobs is like a 5 by 5 or less so if you say your kernel size is 5 or larger it'll run through each row and then it'll say oh is this five by five or less if it is then it'll just make it black um and that's why like none of this j is affected because it's probably larger number five by five and similar goes here um it'll run through all these pixels and then if this is like a five by five or less it'll close that hole again which is weird because actually i guess closing i don't know but like this is weird this is weird that's opening but i get it thanks kevin questions on masks and filtering this is great we've done so much we have worked with our stereo we've read in our image we've looked at our metadata of our image then we've talked about ndwi what it is why we calculate it we've calculated it and then once we have our ndwi we have applied filters and masks to it my profile are you talking i'm not sure i understand so are you talking about like the the area of the image that has data versus the part that doesn't have any data okay okay yeah i see um yeah so this is something that if we were doing this for a bunch of images or doing it like again and again we would have already identified like what are the characteristics of this image and how can we change our algorithm or refine it in a way to get at the pixels we really want to i think the best answer is that it just depends um because we with the way that imagery works we might not get like this image that we're looking at we might not get the exact this exact slice of the image we might get a little bit above or below or to the side or whatever um and obviously like seasonal changes are a real thing so things look different different times of the year um different seasons different years also like the land changes over time so i think yeah they might not yeah so there's a degree of calibration that's you would want to like sort of have set as the profile um but there's also always things that you're going to want to change depending on the image that you're working with that's a good question more questions yes yeah those are pixels that um that the data set is just we're looking at a particular part of the data set and those are just like either zero values or like clipped okay any ideas yeah i want to look into it um so i guess with the clip i was talking about null values like values that just don't show on on either of the images not just not just in the border but like even in the middle of the image there might be pixels that are just not there which wouldn't show up yeah it maybe wasn't captured in the satellite or maybe it was and it's just like corrupted in some way maybe um i would have to look into it honestly i would have to like go in probably look at the pixel values at the parts that you're talking about but yeah that's a good question uh yeah there are so there are areas that are that were black that are now not there yeah but there's also some other areas in the middle of this image it looks like yeah yes so yeah it sounds like a very good guess thank you so so yeah the individual bands can also have um values that are kind of more questions these are great you guys thank you yes i'm sure there is um yeah it would be really useful just to like cut out the whole class yeah um i'm sure there is kevin do you know of one like there are like ways you can detect cloud using machine learning for instance but there's also much easier ways stuff that i don't remember off top my head but like similar sort of um spectral calculations where you can sort an idea of like how likely it is that an individual pixel is related to cloud versus land or water um but this is like a super dumbed down easy way um yeah but like there are tons of even with just like machine learning algorithms are like oh this is a cloud but that gets really complicated because you can get like wispy clouds or you can get uh regions that are like foggy working at regions that are like filled with smoke for instance and it gets a little bit more nuanced yeah but that's a really good question even like there was a there was an algorithm that kevin and i were working with earlier this year that was um about like snow we were working with icebergs um and that got really confusing because you have clouds and you also have snow and then you have icebergs and it all looks like water also so yeah it gets a bit confusing sorry well yeah you're right it is all water different forms we found um we were working on like different indexes to look at that and we found like several different papers like um sort of technical papers that people have written that use different algorithms that we've tried questions other questions cool we're in the home stretch we're almost there i know i know we've been here for a while but we'll get you guys a break soon last piece of this is we're gonna just quickly look at plotting a histogram so how can we uh use our raster pixel values to plot a histogram so same thing back to our repo notebook number five plotting a histogram and then opening collab and run your data download so all right so this one first few cells y'all should be experts by now import packages um get our data and then open it in my stereo and get our bands again okay so this next code cell there's sort of a lot of stuff in this code cell so i'll just go through it slowly um we're first going to define a new figure and a subplot um pi plot is a little weird with this kind of stuff um it just takes a little bit of getting used to it's a little bit confusing um and then we want to add a title and then x and y labels to our plot just so that we know what we're looking at um has is everyone familiar with like histograms and how instagrams generally work so histograms show you um they're a good way of looking at the distribution of your data so you can see the data values and then the frequency of those values in your data set so it just helps you get an idea of what you're working with so in this case we're gonna look at one of our bands we're gonna look at our blue band and we're gonna plot specifically non-null blue band values just to see what that looks like blue in this case because we're looking at the flooding in houston so um just to see if that corresponds in any way we also want to define the number of bins so this is arbitrary i just chose 50 you can choose 150 if you want to or like five uh depends on the granularity you want to get to you can also change the color of your histogram and then here is where you actually call the histogram where you tell it that that's the kind of chart that we want and then you can also save the chart that you create to your computer which is what this line is and then finally we show it so if we run this line okay so what are we looking at um x-axis we have pixel values and then the y-axis shows the number of pixels in the hundred thousands so we see that the pixels the highest number are around what looks like to be seven thousand ish maybe seven or six or eight thousand around there i was curious about this i was like what is this showing me in blue values so um oh dang okay we're on a different computer so i can't show you here but i pulled this up in qgis um and i was looking at the areas of water in that top right corner of the image that we were looking at earlier and the pixel values there correspond to this 7 000 ish value so my guess is maybe there's more flooding in this image than we originally thought or like maybe that more that we can see and those flooding areas are maybe what's accounting for this higher blue band but there's no there's no this is just like a quick sort of i was curious about it um i would have to probably do a little more looking into to tell exactly but this plotting a histogram will come in handy for what you'll do after the break with kevin questions comments concerns yes uh does it speak to their radiometric resolution oh yes yeah um yeah i think i think it does tie to that because that's the range of pixel values that we're looking at in the band yeah what do you mean oh yeah like if it's clipped or something yeah yeah you would see it in the histogram you'd see the the values yeah but in this case we're looking at non-null values so null values wouldn't show up but if they were i don't know some like corrupted values or just values like really high you would see that spike yeah more questions okay well congratulations y'all you made it this far you're still here no one's crying or like left yet so that's a good sign um so we just a quick recap we've gone through talked about um how to work with geospatial data the components that make geospatial data maybe different to work with than other types of imagery then we've also walked through in python looking at our stereo working with the bands computing ndwi masking and filtering and then finally how to plot a histogram so we will take a short break um i wrote this slide before i kind of knew about the timings maybe we should change this what should we say what time 9 50. okay yeah let's take a short break uh let's come back by 9 50 and then after the break uh we will have kevin lead you through a specific example for coastline erosion sound good thank you all right everyone uh we're gonna get started on this next section so just grab your seats whenever you can can you guys still hear me if i go hands-free and use this mic stand yeah cool he's good yeah that's good all right so i'm kevin software engineer on planet um my background is in astronomy and computer visiony things and i recently moved to planet because i thought planet was really cool and doing really cool things for the earth a lot of humanitarian resources being provided and just doing a lot of really cool science so i thought i'd apply my scientific background uh and my coding skills to the test at planet and it's going really well um i'm not trying to sell you something but i love it um so for this next part uh we thought we would take everything that we learned before and apply it to a real world example um um so i thought you know what would be a really good example and i went through searching through papers we have a whole a whole list of publications on our website that use our data and one of them was a region in bangladesh that had a pretty severe a pretty severe example of coastal erosion um coastal erosion is just basically a part of the coast that erodes inland um and we're going to be talking a lot about that so what we're going to be doing is we're going to be um importing and processing all the data as we did earlier in the notebooks um and creating a time series of that area of interest i'll be referring to the interest as the aoi just saying that we do a lot of geospatial data and then we're going to be creating the image processing pipeline that we did all the notebooks just basically in one in one big pipeline and then we're going to classify regions of water and land just like we did with the nd wise or nd um and then we're going to be employing just some classical computer vision techniques nothing machine learning based just more classic computer revision techniques to try and find how the coastal version is happening over that time series and then we're going to pull out some cool metrics like uh the landmass loss over time and uh identify how much the coast has moved inward and is it slowing down or speeding up uh for reference in the very very bottom and like tiny tiny pond you can see that uh there's a paper that i reference um if anyone cares it's also in the notebook but it's this paper here um they uh they're the ones who i stole the idea of them they they have this uh really um it's really great region of interest that's um that combines just like close version just to be carrying a part that part of and they have a their whole analysis victims over here and um pictures so um we're gonna get right into it let's go to the github page we're going to be going to the one called coastal analysis and then from there we'll click the open a collab button that should bring you here um so first thing we do let's download the data it's a lot of data run this uh w get command i've done this one years ago but this shouldn't take too long um you might get an error on your terminal or on your websites or on your notebook saying that you have too many active um active uh notebooks sessions thank you um i just did this is anyone running that error right now yes just terminate all the other ones um i really should just be able to do that okay right okay so you see where it says ram and desk up here if you go to the manage sessions button i've already terminated my sessions but you can go here to terminate your sessions in fact i'll terminate these ones i'll terminate the rest cereal first look and rasterbands once everyone has the data with the wget command let me know in your i'll give you all like a minute there's also a lot of text in this notebook i'm also just like writing down some thoughts as i was going through it um it's mostly just for like just getting a sense of what's happening probably gonna be reading from it um does everyone have the data downloaded yeah does anyone not have the dividend downloaded okay all right i'll give you like a few more minutes um in the meantime we'll hit install masterio and then we'll import all the packages once that's done successful good then we're gonna we're gonna use a few more packages here we're gonna be particularly just using a sci-fi package called find peaks um well we'll see what that does in a minute that's for like the last little bit okay so um [Music] now that we have all our data downloaded and all of our packages downloaded i'll just get a little bit into like what we're going to be looking at so we're going to be using we're going to be looking at a specific region of bangladesh specifically the specifically the southern region of bangladesh where the ocean meets the the mainland and we're going to be looking at this inlet here where there's lots of coastal erosion happening to lots of regions but we'll specifically zoom into one region right here and this little polygon of john is uh is like our area of interest so when we when we're imaging it you'll see that uh everything outside of us polygons can be black or white those are just like the clipped regions that's what we call regions and then this is like the coastal region that we're going to be looking at i've chosen uh the data to have like no clouds or very very minimal clouds so hopefully we don't see any of this stuff up in just for nbwi reasons any questions so far no all right cool so just like before we're going to be importing visual and analytic data the visual data is going to be really helpful to just visualize what's happening uh this image here is a visual band image and the analytic data is like the full image that's like full uh color resolution at 16 bits um and then we're going to sort the data by uh chronologically so we're going to start at the year of 2017 and image it once a year until this year in 2022 um in case you're interested i i chose the spring seasons where it's a little bit drier and there's less flooding um you can get more with that later if you're interested so we should have five data points 2017 2018 2019 2020 20 21 16. um okay and now we're gonna be starting our image processing pipeline basically what we're gonna do is we're gonna do everything that we did before the previous notebooks and just coat them in the function so i'm just going to put a function call around them uh is anyone here not familiar with functions in python okay so for those who aren't pretty much what we're going to do is put the code that we had in uh in our notebooks and then just put a little defining function called around it so def with the findings function this here is the function name in this case will be called extract spectral bands and then this is the input image file name and then in this case it'll return the return spectral bands so here we're going to call an image file name i do this a little bit differently than monzy i mean to be honest mozzies weighs much better but uh i do this a little bit more archaic um i just open uh i open each band individually with my stereo um so i'm opening the green band and the near red band and then i'm returning that to the user uh this line here so let's just take a look at how this works actually let me run this this cell so by writing the cell we let the notebook know that the function exists and how it works and okay so let's take a look at the uh the second year in our data set i chose the second year because it's a more dramatic example of what we're going to look at in a second uh so second year in python uses uh lists at zero so we'll start indexing that one uh and then we're going to extract those spectral bands so to call the function we just say what we're returning on the left hand side the function name and then the file name so we're going to be calling the analytic file the the second data pointing that file so we ran that now let's see what's happening so here i've used um python.mshow to look at the green band um yeah i mean nothing special right this is just the green band um and similarly we'll be looking at the near and front band um these are just pixel values on the x and y axes not very helpful so i'm just going to turn them off for the next rest of the demonstrations there are ways to turn those into like uh coordinate system values but ah it's not worth it for this presentation so i'm not going to do it uh and here's the neoprene um you can see that like the mirror bread is like super prevalent on land and like pretty pretty uh not relevant or not bright on the water and there's a whole bunch of science behind that uh basically nearby images heat and water is a lot colder than that uh second question great cool uh green a little bit more complicated i don't um cool i'll get on this one section here and we're using green and red to calculate the ndwr later that's just y important right now next i'm going to create a function to compose the scene just like mozy did earlier so again calling a function or creating function with the inputs as the file name and then again i'm doing this a little more archaically when launching really should have updated this and i'm just importing the red green and blue band um and uh creating the visual image stacked with nd stack function call uh and then let's call that function visual image and numpy array and then let's do intro to look at it there we go that sort of looks like the screenshot earlier this is the 2018 image you see uh land on the right water on the left and the clipped areas which are just the areas are not within that polygon are just black those are actually nand values in python um and the water doesn't look very blue because it's very very murky sandy water there might even be sand bars in there um or vegetation growth yeah any questions so far okay fly into this all right so let's compute the ndwi um as defined previously uh the npwi is just the green minus the near infrared over the sum of both those bands similar to how we did earlier we're going to set the uh the error of dividing by zero to so we're going to allow with division of zero i'm pretty sure uh it sets it to a name value but it might set to zero i'm not really sure as i said i'm pretty sure i'm just heroes i'm not sure um uh and then here we go it just i just wrapped the code in the function uh call and then we return in wi uh let's see what this looks like for this image so i'm gonna compute any wi here and i'm gonna run in show on that there we go now this is a pretty dramatic example especially compared to that houston example that water is like vibrant green really really high values of ndwi and the right the uh the land is like so clearly not water um so so the reason why is probably because in the houston example you have water on land which can be a bit tricky uh you can it's a little bit more difficult to um uninvolve those pulses from like land valleys water values and it might be also really shallow water whereas here we're looking at the ocean you can even see though [Music] we have like rivers that go inland here and these are like much less obvious values of ndwi's um there's even like little lakes probably or ponds probably the lakes actually and these are less obvious as well but when the water is much deeper it's much more clearly value and much more clear than water so that's a great question now um as much once i mentioned earlier we can classify pixels as water or land by just an ewi value we typically do that by setting a value of professionally by a value of 0.3 that's the typical value for our bands but you can see here nothing really gets up to 0.3 like maybe some pixels around here got up to 0.3 otherwise they're like more like around zero probably because it's like really murky water really sandy water might even have like sand bars over here it's like it's unclear we'd have to probably uh pass the higher resolutions satellite like our sky sat satellites to get a better idea of what's happening um but for now what we'll do is we'll set a threshold value probably lower maybe closer to zero so just capture what's water what's we'll do that in just a minute any any questions so far it's all pretty familiar probably from what we just did earlier so i'm kind of like flying through it but please tell me to slow down all right so here i'm i just define a function that says find water and land i'm very creative it gets the point across um so we're gonna in we're gonna take it as an input the ndwi numpy array uh and then we're going to return a water mask and a land mask so we're gonna do we're setting our water threshold to be zero we're saying anything that's above zero in the ndwi array is going to be classified as water anything less than zero is going to be classified as land so we uh you just do the thresholding just like we did earlier and we returned both water and land masks so i'm gonna run this code block and then here um i'm i'm running that code and i'm printing out just like just look at what the landmass looks like so you can see how there's nand values and ones there's also zeros but it doesn't print out it's actually a really really large array the nand values are associated with those clipped areas so if i scroll up a bit you can see like the top left region is like all white those are all just clipped values so then and then down here where it's blue where uh those are going to be one because they're land and that sort of aligns with what we see in the print here but we'll take a look at the office now so uh let's visualize these these masks how good are they okay here are the masks you have the water mask above and the land mask down below did a pretty good job on landmass water mask like kind of not so much uh you can see that like even though it was super obviously water there are some holes in that water mask and same with the land um so any any suggestions how to fix those holes yes filtering morphological filtering great so we'll do some uh opening and closing filters to open and close those holes all right uh so i'm gonna define a function called filter mask so it's going to take in the closing and opening kernel sizes which is just the width of the of the kernel box and then the mask in question whether it's water or land mask and it's going to return the mask that has been filtered so it's going to be a closing function followed by an opening function so just identically as before we're going to do define a closing kernel element uh we're going to initiate that closing kernel and then we're going to apply that kernel to the mask and then we're going to take the closed mask and apply that planning opening function to it as well and it's going to return a mask that has been closed and then opened um that's a lot of words let's see what that looks like for real so here i've uh i've like empirically defined some some kernel sizes for opening and closing filters uh specifically for the water mask and for the landmass now you can see earlier in the water landmass the water mask has like massive holes in it and the landmass has smaller holes and it's kind of uh i don't remember your name but in the watermelon chart you asked earlier about uh about do you need to change the kernel sizes uh yes absolutely so so even just for these two different masks the water the kernel sizes need to be completely different and in fact like over the years from 2017 to 2022 you can absolutely have different kernel sizes i just chose the maximum value for all of the years um and this will convolve best for like worst case scenario like this and like the best case scenario has just small holes and if you have small holes big kernel sizes will still work um so i've defined these kernel sizes uh so i'm gonna use the filter mask function that we just created to uh create the filtered water mask and the filter land mask still need a second because the morphological filters are kind of like long turquoise cool now let's take a look at what the filtered versions of these masks look like so now you can see that all of the holes have been filled great see and closing the river that's right yeah i i specifically chose a value of any wi that wasn't too low not to capture the river that goes inland here so in 22 you didn't see the river but there are other years where you do actually catch but you're going and then and that's exactly right you would probably see holes going inland here and that would just smudge them that's fine because um what we care about is coastal erosion uh which i haven't talked about much yet but we'll talk about that in a second um right now we're just capturing land and water mass filters uh any more questions yeah yeah there we go so now we have two very distinct masks water masks so this was just for one year so now let's run like this entire this entire process for uh each of the years so from 2022 to 2017 um so here's like this little pipeline that i've created it's a lot going on but basically we're going to run everything we just created so um we're going to loop through each here the first thing we're going to do we're going to do is extract the spectral bands the green and your red band next we'll we'll extract the visual image this won't be won't be used but what you can do is you can just like visualize this image by putting like an info call around that visual image just for fun really um we'll compute the ntwi we will take that ndwi and create water and then masks afterwards we're going to take those masks and filter all those little holes in them to create really distinct water and masks we're going to call them water and then filter or water masculine masking and lastly what we're going to do is something that we haven't done yet is we're going to uh we're going to take all the land masks and add them to an empty array so up above i created an empty array um those who are really nerdy and care this is like probably not the best way of doing this because you're just expanding the away every time so it's like slower and the computer doesn't like it very much but for this purpose it's okay um so we're going to add the the large layer mask to each to the right every single time and we'll use that for analysis later so this takes a second so let's run that in the meantime are there any questions that's like what's happening or like why we're doing this yeah ties are really tricky and it it's really tough so uh so i didn't grab that data the same data that the authors took um i kind of went wrong and go to myself but but there's a there's a lot of like there's a lot of consideration when grabbing these images so um i grabbed them in the dry season when it's not extra wet and you can have lots of variation in water um i grabbed them when there weren't any recent hurricanes or anything like that i also i also grabbed them all within like a month of each other and then the next thing i did was i also made sure that like the time of capture was like within a half an hour or so within like half day of each other exactly for that reason to not capture a high tide load time that's a that's a really important thing that i didn't think about until like the very end yeah yeah yeah so like so i i almost certainly am including some title differences uh in these images but from what we'll see in a minute it won't really matter in the end the postal version is so much more dramatic than the title differences but if you're looking at a much lower cadence say like quarterly or monthly the title differences would make it huge yeah yeah orange uh so yeah so there there's some some reason available differences i'm not sure what it is in bangladesh like how far up that goes but in some parts in canada where i live uh the bay of fundy can have the thai glow like kilometers uh don't put me on that but like a very you can't see the water all good okay does everyone has has everyone run that for loop is everyone all good cool so so now we're gonna be analyzing coastal erosion so what we're going to do is we're going to take a look at those landmass over each year and see how they change so the first thing we're going to do is look at what i call land difference so i'm going to take the first year which is 2017 and compare that to all of the other lane masks so uh specifically i'm going to subtract the landmass from 2017 uh compared to 17 22 the way to 20 by 2. so you might imagine that comparing 2017 to 2017 will just give you a bunch of zeros and 20 17 to 22 will just give you like the total mass loss um so this first cell i'm just going to do that do that uh land difference i put an np nand num call around that just because subtracting bands it's like it pans about but this makes it really easy and as monsieur mentioned earlier in the slides the resolution of our pixels is about 3.7 meters in in lonzi's data set it was three meters uh but in this one i checked the middle data and i is 3.7 uh i really actually probably should have just called it from the metadata but i don't and then the area per pixel is just the resolution squared so uh length times width because it's a square pixel and then now we can take a look at the total landmass loss so what we're going to do is we're going to uh look at that difference land from um oh sorry so land difference is actually not what i said earlier the land difference is 2017 minus 20 and 22. so we just see the total land lost in those five years and so i'm taking that that array is going to be in pixel values so i'm going to multiply that by a physical unit area per pixel and then we're going to be able to get that in terms of meters or square meters in this case and in fact it was so large i multiplied it by one millionth so we see that the total landmass loss is 11 million square meters in factors just pretty dramatic but let's see what that looks like because numbers are can be funny sometimes there we go so uh this is the difference between 2017 and 2022 landmasks and you can see in yellow is the total landmass loss um against nuances here but up here they put a little uh sea wall or like something in the way so the land didn't really receive too much there but over here in this coastal region where it's nice and soft that receded a ton and in just like a small seven kilometer strip they lost eleven square eleven million square kilometers any questions on what we're doing so far okay now we're going to take that landmass loss and look at it over a time series this might be a little more interesting so i'm going to define a time array oops basically just saying like how many years are we looking at um in a numpy right and then the next thing we're going to do is look at the uh right what i mentioned earlier which is lay mass loss over time so not just comparing 2017 to 202 but just comparing to every single year that we stored in that list okay that's run and now we can look at it in a time series so we'll have um we have years since 2017 on the x-axis and on the vertical axis we have the total area loss in 100 or in thousand square meters so you can see not only are we losing a mass but like we're losing it quickly and it appears to be speeding up pretty dramatic these are things that like governments would want to know like are we losing land number one yeah we are is it increasing and not only is it increasing but looks like it might even be accelerated uh which is bad news so uh let's take a look how quickly is it increasing so we can measure in this case like the velocity of land mass loss so just simply like uh the change in landmass over the change in time will give us like an idea of how quickly we're losing the mass so we can do that with np.diff really helpful it just takes like the last the last number and just subtracts over and then you do that with the landmass loss and also the difference in time and for those who care uh we're just basically taking the derivative of this function here let's take a look at that what that looks like so here's the the speed or velocity well there's no direction really so the speed of the landmass loss per year so you can see not only like we were saying earlier is that are we losing lay mass but we're like really quickly losing that mass and it's speeding up um yeah really fast too uh i think i measure yeah at the last year so this year they're losing land at uh 225 000 meters per year in that small strip any questions as to like what's going on here yes um yeah just like not enough resources to keep up with with like the changing um with like all the sea levels changing so climate change is a big issue and the land uh is being eroded due to the water levels changing and the more dramatic weather uh system effects and the land that is really really soft so it just gets really really quickly bangladesh is not the only place easily like this you know lots of places around including the us and uk lots of places that are like really heavily populated and have lots of resources too so yeah it's pretty dramatic yeah it's it's a pretty real thing and um there's a link to the paper up above them like they they go in detail uh much more than that or i could like read back to you but yeah yes yes totally um so a big part of this was just like me going through the data and going through uh first does anything look off and then like this this is like a homework actually uh so there's a a lot of a lot of the behind the scenes work is like seeing if the data looks anomalous so yeah was there a flooding event was there hurricane was there any of this sort of stuff uh i pretty much just use google and i just said i just looked when in southern bangladesh where there are or uh times of flooding or any sort of special weather events um and i just pruned that from the day so i just didn't choose those times um mostly this time of year in the spring it's pretty dry they have monsoon seasons in the fall and winter so choosing date on that time would be much respect um if we were to do this at a higher cadence of like say monthly or quarterly um i'll go back to this plot here lane mass loss probably wouldn't look like this it probably would be like it would look like it was going up and maybe go back down to the different planning events this is more just a smooth version that's a really good point yeah that being said i haven't done that haven't gone through it at that global cadence for heimkins so i don't know but i imagine that's what it would look like [Music] exactly exactly and like from from title uh effects as well you also see it moving down so yeah it gets pretty tricky and these satellites don't go over the earth at the same time every day so you'll capture different times today yeah that's a really complicated like analysis but yeah that would be a really a really good way of doing it um you haven't done that here but yeah that that that would be a really a really good way of doing it um okay and then one of the next things we're going to do is just look at like insurance it's it's land and we're losing it quickly and it's speeding up but is it accelerating like is the problem getting worse and uh we can just do that by taking the derivative of the velocity or um taking a difference of the velocity over the difference and around the cells and what we can see is that not only is it speeding up but it's it's accelerating uh quickly now it looks like the acceleration is like slowing down so that's a good sign but it's still accelerating which is not great um so you know if the government wanted to do all this analysis and take a look at the data they'd probably get pretty concerned and say hey we should probably intervene but um yeah it's pretty really crazy um so that's that's sort of our spatial uh work here now we're gonna go on to um looking at specifically like how much has the land receded so we looked at how much land was lost now we can look at like how much has the coast around it any more questions well so we're going to be using a um so there's lots of ways of doing this you can just do it by hand it's probably like the easiest way to do this but if you want to automate this uh let's say you have like a pipeline you want to like you don't want to touch it you just kind of want to like let this thing run in the background with like i don't know 50 or 100 or a thousand different regions here um this is like maybe a good way to do this so we're going to be using um we're going to be looking at that image this one here the cumulative landmass loss and um this image here tells us a lot but specifically what we're going to be looking at is uh we're looking at the left side there and the right side left side is the coast in 2017 the right hand side is the coast of 2022 so all we need to do is find both those coastlines and then look at their position in x-axis and see how much that position has changed so here we go so what we're going to do is we're going to take that image that mask and we're going to run an edge detection algorithm on it um there's lots of ways of doing this by hand or you can use opencv which does highly recommend the second method um there's also different types of edge protection algorithms um i'm going to be using the uh the candy edge detection just what i'm most familiar with but you can also try different different methods and uh yeah let's run it so it takes in the main difference image which i just pulled up um i'm saying there's a threshold of zero and one zero is um is all the areas that like wrong one is is the landmass lost and zero is not and then we're going to run the cell uh but really the important part is going to be visualizing it because i don't think i can do it justice there we go so on the left-hand side is is that coastal region on the 2017. on the right hand side is 20 22. that's great we've we found the coastlines um we also found a bunch of other stuff and we found like the bottom and like we still haven't been able to like automate rare coasts um now i'm gonna do some magic stuff this really only works if your posts are very oriented this won't work if like you're looking at the image sideways or like on like an angle this barely works this image but it works uh so we're going to be creating a histogram so the next thing we're going to do is we're going to take this these post lines from this edge detection algorithm output and then bin up all of the postal pixels from left to right i'll kind of go up just explain a little bit better so we're going to be looking at like how many pixels do we find that are yellow as we scan across from left to right and one might expect that you might find lots like zero pixels here a few pixels here lots of pixels here a few here and lots of pixels here maybe let's find out [Music] so what we're going to be doing here is we're going to look um we're going to be creating histogram with lump with numbing um i'm created just like a random we're not going to i chose i've been i chose 12 bits and then what what we do next is we take that histogram in fact i'll just i'll show you the screen first and i want to find these the way i found these peaks there's lots of ways to do this you can do this by hand uh pretty easily but like why do i have when someone else has already done it right so uh so i use the the sci-fi package called find peaks to just find um the maximum values in uh in the screen um yeah pretty straight forward so these values here correlate to the crystal regions and to show you that um i'm going to show you the edge direction output with the overlay of the camera here on the screen like i said earlier it's not like the perfect favorite but like it's it's a pretty good word um yeah any questions on that okay in general but this this hysterical method of finding those lines will only work when when it's uh vertically lined because of us and left to right to see where the pieces are if let's say the image was oriented in like a way that like the coast was eroding this way we wouldn't be able to find it like we would just find it was just like the history would have like a peak here because the coast this would be oriented this way so we've probably found a pretty uniform distribution of all those ones yeah totally slow transposer yeah but i uh chose a pretty pretty convenient dataset so we might do that transposing comes with its own issues you like interplay data which can get a bit funky um but yeah people transpose it or you can also like when you when you download data you can also get it through transpose cool and now we want to see how much the plus is eroding inland so we just take the difference between the two peaks um so there we go so the peats are at uh right these are right here uh in cell 30 three and then i just take i literally just take the difference of those two peaks and pull that recession and then um those are pixel values still so we can multiply that by the resolution physical value and then we can see that the land has receded uh about two kilometers in past five years um and if you want a more general sense of how much that's proceeding per year on average it's receiving about 400 meters per year which is like pretty dramatic um that's kind of like the end of this this was pretty quick because you've already learned all the things that i do with the functions um but do you have any any more questions yeah yeah it's pretty convincing absolutely yeah yeah i mean it gets a bit tricky but like you know you obviously want to catch it before you've lost two glommers uh and like catching it before that can be even tricky let's say let's say we take a look at this block right between you know year zero year one so in that 2017 time obviously like there's close erosion but like you've only lost well it's a lot you've lost about 400 plumbers like of course which is 400 liters of codes just like a decent amount that might not be very convincing and if you add more data points in between yeah you might get a lot of variation from coasts uh sorry from tides and special weather events um so yeah it can be pretty difficult to get convincing data but when you add even just like one mirror right like it's a pretty clear slowly so oh yeah so yeah that's that's that's a really complicated process um that's something that that we don't deal with that's uh we have a whole a whole data function just dedicated to that yeah to do this yeah like this is a this is like an ongoing issue with like geospatial data in general um it's called i think image image rectification and so it's a really it's a really really complicated thing but it's a really complicated processing like is this pixel in this image associated with the same in this pixel in the following year um so so these are just like final analytic and visual products that like have already been rectified um getting the raw data i i mean i think i think even in like the like blog data that you can download i think it's already rectified there too um yeah yeah so like so these these satellites are all like we all have on board like blocks and um when they go over different parts of the earth they communicate with different land stations they have like pretty a pretty good sense of where they are in space but capturing the image and sending back to earth metadata is is a really complicated process like taking all this metadata with like raw image and like converting it to a tiff where you have just like all that data associated with it and where you can say even like you know this principle here a three by three pixel uh we have a high certainty of light that has this that's a really complicated and then there's even the whole process of like there's even a whole issue of um of uh when the when the sound is not perpendicular so you can have like you can have it like offset a bit so if it's like let's say 88 degrees right the pixels at the top of the image exactly will be won't be as square as like at the bottom so yeah it's yeah yeah so all this stuff is done by like 14 people um and so it's a really complicated question nothing that i can speak to because i don't care i just get the brilliant yeah so they're they they have a wide background yeah so a lot of our programmers someone come from like geospatial backgrounds someone just come from like computer science backgrounds yeah like like myself i've never worked in geospatial until i started to kind of um just have like i you know we just know and like how to apply it to different applications yes yes right so yeah so good question so that's from mosaic this is actually not one image this is this is um i think this is a combination of four images uh that have been mosaic together and which is also also a whole other a whole other issue itself um so because what happens is as the sound light goes over earth it does something called like uh what's called an align scan so it scans the earth in the line as it rotates so so you don't get like one strip of the earth you get like as like an elongated strip of the earth um and as more silent sounds go over it covers more area and um to get like a larger area like this in this case four different solids mosaic yellow yeah so solar reflection off of water is a whole definition itself there's there's lots of issues there's a lot of like problems to solve in geospatial imaging yeah even even actually you can even capture images of airplanes flying through the sky and you can see you can see them uh being captured in different special games at different times so uh as the image is taken it's different of different satellites but on our satellites they're they're each band is taken uh like after another so like i don't i don't remember the order but let's just say red blue green or like red you'll see like basically a rainbow of like a like of uh of an airplane moving through the sky and and if you know if you know for instance the speed at which you're orbiting and you know uh you know uh how quickly you you capture the images you can get a sense of how fast that airplane is moving yeah it's pretty cool all right one yeah yeah well you can also use different software like a flight tracking software and you can associate images and be like oh this is like their kind of like 342 going to tokyo i don't know yes so the high resolutions are called skysat we bought those pretty sure from google do you know how long i'm sure yeah time like terabytes i wish for them yeah a lot yes uh yes if you have a planning account yeah you have data back from 2011 yeah daily interview right so so we we provide imagery that we also have like an analytics team that um that do certain type of analytics like let's say real detection or uh or like change detection they can find much more sophistic much more sophisticated way than this like the changing and water levels and then you can find flood bridges like in houston um but uh yeah like we'll get we get contracts from different companies or different groups or sometimes like red cross for instance to find um yeah to like do specific analysis government yeah we just have um we do business across several different industries and also one thing about kevin you mentioned earlier something about an airplane just to be clear like our our imagery is high resolution but it's not so high resolution that like you could look and like oh there's kevin giving a talk outside pi like you you couldn't see individual people so that that is something that is important to keep in mind i think planet's philosophy is that we obviously we have to abide by all of the us's regulations and all that but we take the imagery and it's whoever wants to get the imagery from us can do that yes so like the us can't sell engine to everyone but government's abide by those laws again yeah but like they're not like spiced out like resolution like you can get you can detect roads and cars um but not like yeah spy level yeah yes uh yes yes so the stuff that we were looking at was surface reflecting corrective yeah you can you can choose to download it uh uncorrected and do it yourself there's also a whole um just like kevin mentioned the whole like rectification team that's in charge of that there's a whole other team in charge of like visual making sure that like visually the images look good that's like a whole other process it's not it's not as easy as like oh we downloaded an image like there you go there's like a whole sort of thing that goes into it so yeah like you can you can take the analytic image and then do the same sort of numpy dot d stack and stack the red green blue together uh but it'll look awful there there's like yeah like once i said there's a whole team that just goes through color correction and applies color science to make the images look pretty and uh visually appealing and also like accurate that's the hard part because when you have shadows or you have clouds or you have yeah like reflecting things on the ground it can be really tricky to get a good image yeah that's that's a good question because every so i don't have an image of this on my laptop but um our our spectral bands cover like different regions of the spectrum and different satellites so we try to align ours with landsat i believe so because they have such a massive database that it would make more sense to align with just have them all cohesive um but they're not perfectly lined so like the ntwi that we create uh will be a little bit different than the ndwr that you get from the inside and so our the threshold that you get for a planet image would be different than it would be from the inside image yeah like like if you have the right threshold value you can probably get a pretty similar mask insurance company oh i don't know sounds like a great idea laughs if there's no more questions um those are our emails we have uh our cards up up front if you want to message us there we'll also still the slack channel for how much longer that's open and um here's uh here's our team's website so developers.com that's where you can find all things developer related and then we have this is i'm not kidding like i revert to this all the time it's called planet school it's basically um a much more uh sophisticated version of our presentation going from like bare bones here's like how to use python for geospatial things to like much more complicated processes um and they have jupiter notebooks that monzy creates and um and it lets you know how to like um just be like a pro with python and geospatial yeah i don't know where but but uh we will have this so we're creating a blog post for for this sci-fi event and we'll link this youtube i'm pretty sure it'll be on youtube uh to the blog post oh and there's no more questions we'll leave this up for a bit and monza i'll just be hanging out and we can take a look at the job yeah thank you you
8,Spatial and Urban Data Science with PySAL,https://www.youtube.com/watch?v=4AHJVMs7iH4,welcome to spatial and urban data science with pycelle i'm eli knapp i'm one of the core devs with pysell um typically this workshop is taught uh with serge rae who's our our bdfl of the the paisle meta package um serge has a lot going on he's in the midst of a institutional transfer so he couldn't be with us he may be on the slack although it's still pretty early over on the west coast so if you have questions and you are in the slack channel for this for this workshop please go ahead and drop them in there i'll do my best to answer them as i can but if there's only one of me it's it's tough the last time we did this was in 2020 and it was all digital and actually were surprised by how well we could manage one person doing the code and one person responding on slack but if surge isn't with us i'm all you got so do your best to answer each other or i'll get to questions during the break um so we're a reasonably small group uh if you do have questions and they're gonna be useful for the rest of the folks in here go ahead and stop me i'm fine with things being a little bit organic and we've given this workshop several times so we can remix it on the fly if we feel like we have to and we're gonna do this in basically uh four parts um i don't know if anybody's had a chance to download the workshop materials or set up your environment and see if anything worked i hope all that worked smoothly in our tests everything works there's also a binder instance that we set up for this but just a heads up it'll only give you two gigs of memory and it will crash with some of the examples later on um i i didn't do a good job of trimming down the examples i wanted something a little interesting so um if you're on binder we can get around it we'll have to rework some of the code but um just so you know that's gonna that's gonna fail at the end yeah so it's in uh sjs ray oops and then uh pycell scipy22 this one what's the best way to find the slack ah good question um so i think somebody else may want to correct me here i think we got an email from scipy inviting everyone to the slack a few days ago if you open that and then you go to channels you want to join the tutorial pycell channel on the left here tutorial pycell this should work all right so we'll we'll start uh as folks are getting set up uh i'll i'll start by just talking a little bit about uh pysol and what it is and a little bit of geosnap and what we're going to cover today so um like i said before my name is eli knapp i'm a core dev with pysal i'm a social scientist by training so my phd is in urban studies and planning i think a lot about cities and neighborhoods and and how they change over time um and a lot of my work focuses on geosnap that we're going to get to in the second half of the tutorial um geosnap draws a lot on the sort of the fundamental building blocks of spatial analysis in python which is pi cell um pycell was started about a decade ago by our our bdfl serge ray and his colleague lou ganslin um at the time they were at at asu and the the python ecosystem for spatial analysis was essentially non-existent um so things like reading spatial data writing them out building spatial weights data structures none of that stuff existed yet in the in in the python world this is sort of um pre pandas and and and pre the big explosion in a lot of our fantastic tools so um pysol started off as a set of some of these fundamental uh uh building blocks and serge and luke are sort of founding fathers of spatial econometrics and so the two of them continued their research building out modeling frameworks and and paisal has continued to expand over the years quite quite dramatically so it used to be condensed into one single package that you would install with pip or with your favorite package manager and as the library grew and grew we realized that that model was not going to be very sustainable because we've got lots of pysel does lots of different things now so we want to make sure that that folks could push on their individual pieces uh in in tandem without what one giant piece of infrastructure slowing everything down so a couple years ago we refactored the package into a meta package and so instead of pycell being one monolith now we are a set of packages and pysal the package basically just specifies dependencies on all those sub packages so there is a lib layer and this is sort of our our core data structures these are things like spatial weights and file io there's there's some computational geometry stuff in there but just about everything will depend on lib in the explore layer we've got functions for exploratory spatial data analysis so these are things like global and local spatial autocorrelation detecting we've got a package for geospatial distribution dynamics this is about how places evolve over time in space package focused on inequality so these are things like genie indices and and spatial decompositions thereof we've got a relatively new package called mom pi that focuses on on morphometrics so these are things like um floor area ratio and the the tessellation of urban space and this is a bit a bit more design friendly um we've got a package on point pattern analysis segregation which we'll get into and then spaghetti which is often a fan favorite because of its silly name which is a package focused on spatial graphs routing topology that sort of thing okay then we've got a model layer and this includes things like measures of accessibility so if you need to access health care or jobs so the fundamental delimiters of of urban space that help guide decision location decisions a package for sparx generalized linear modeling multi-scale geographic regression spatial interaction modeling uh sp opt which is spatial optimization regionalization routing we'll also cover a bit of that today sp reg spatial econometrics and tobler which is named after a very famous geographer waldo tobler is a pioneer in spatial analysis tobler is a package for spatial interpolation transferring data from one spatial representation to another and then finally we've got the the biz layer and uh spatial data requires some massaging of of the data to get them to display well um our traditional visualization tools um need to be binned uh for example to make good-looking choropleth maps and that takes a little bit of thought beyond just your typical plotting all right so we're going to cover uh a lot of the core python functionality and then in the second part of the tutorial we're going to move into geosnap and look at uh urban neighborhoods and how they transition over time if i can find them all right okay and so we're gonna do this in four pieces we'll take a break after each one and we'll do sort of a longer break right in the middle before we get to the second piece so first we'll start with um your basic spatial data processing so file io we'll do some choropleth mapping some visualization take a small break and then we'll get into fundamentals of spatial analysis so things like spatial weights matrices autocorrelation that kind of stuff after that we'll take uh sort of a long break and then we'll move into some more applied stuff on neighborhoods with geosnap all right oh one other thing um should you find any of this stuff interesting or if you want a bit more background um my colleagues serge danny and levi have put together this fantastic book out in the open it's geographic data.science um it's all written in in jupiter so there's fantastic code examples and it walks through uh in a much a much deeper and and fuller scope what we're going to cover today and they can they can cover a lot more ground in this in this uh book than i can in a couple hours here so if you want a bit more background i encourage you to check this out all right so for those of you following along we're gonna start with notebook zero zero geoprocessing and so as i mentioned before uh piscell started with sort of fundamental data structures there was no way to read and write um spatial data formats in python once upon a time and that that's changed pretty dramatically over the last several years if any of you folks are familiar with spatial analysis maybe you've done some gis before you're familiar with things like shape files and maybe geo packages you know that spatial data are heterogeneous and they come in lots of different shapes and sizes right you've got points and lines and polygons you've got vector data you've got raster data and it's it's hard to to find one package that that encompasses all of that and a one point pysol tried to do that before realizing that it's much better to let uh folks who are good at this uh play to their strengths so in the past couple years geopandas has really come along i'm sure most of you are familiar with pandas um good r style data frame structure um and a geopandas basically wraps a panda's data frame around spatial data so you've got a um a geometry column that that stores um the the bounds of of your datum so we'll get into a bit of this so we'll start by loading sort of the fundamental piece here which is geopandas import geopandas gpd there should be a file called scag underscore region with the gpkg extension we'll read that in and store it in the variable called gp and that gives us a geopandas geodata frame like i said this is basically just a subclassed pandas data frame so the columns attribute tells us what's in here this is a data set of a bunch of uh demographic and socioeconomic data from the census the american community survey i think this is the 14 to 19 time period but i i'm i'm not 100 certain that it's not it's not critical for at this juncture um now this is a relatively simple data structure right we've only got one layer inside this uh this geo package but you could store several like i said you could store points and lines and polygons all inside this this one container so if we had more than one data set in here we would need to use fiona which is another package for understanding vector data and with the list layers function from fiona we can list exactly what's what lives inside that geo package here we can see we've only got one layer called tracts all right so if we actually want to operate on these data and we want to check them out we'll go ahead and store them in a new variable called skag that's a poor euphemism for the southern california association of governments these data have nothing to do with with scag but skag covers roughly the same area that we're looking at so it's a it's a good quick acronym for us to use and again this is just a geodata frame we can take a look at the head attribute to see what's inside sorry the head method and like i said this is just a data frame right so you've got uh observations on your rows you've got different variables on your columns and the distinguishing thing here for a geodata frame versus a your standard data frame is this geometry column here at the last um that gets appended at the end right and this stores what's what's known as a well-known text representation of of the data that you have so if these were points then it would be a point with an x and a y location because these are polygons you've got the list of the vertices that define that polygon okay again just like a data frame we can look at the shape so we've got about 4 500 observations about 200 different variables you can see what's in here like i said at least one of the variables inside a geodata frame is going to be called geometry and this is the the key thing that allows us to operate and do and spatial operations on on these data so one of the first things that you want to do is a quick plot right so again this is like in if you read these data into a gis system like qgis or arcmap the first thing it's going to do is put a map right on your screen right on the interface so let's make sure these data look like we think they're going to they do so if you're not immediately familiar with the geography here we're looking at los angeles orange riverside imperial san bernardino and san diego counties here i hope i just got all those right because i'm in uh my current institution is in riverside so this is this is where i live i hope i just got all those right um and a key thing here is that if you look back up in the data frame when we looked at the head here this first column called geoid this stores the federal information processing standard code of fips code so this is what uh the federal government like the census bureau uses to identify unique geographies and this is like a nested code right so the first two digits are your state the next three digits are your county and then you get into tract block group and these are all smaller and smaller polygonal divisions that the census bureau uses to break up the country and provide data in all right so that was a geo package that we just read like i said if you've done any gis in the past then you've probably encountered shape files uh once once before um shape files are a legacy data set it's we try and avoid them as much as possible that has some odd quirks like having column names that can only be 10 characters long and it will automatically truncate them for you and so if you've got things that start with the same prefix all of a sudden you're going to end up not knowing what any of your column names are when just a bunch of numbers geopackages don't do that some of the more modern files don't do that but you will run into shape files everywhere in in spatial analysis so it's good to be able to still read them again the geopandas read file function takes care of that for us so here is a set of behavioral health clinics in southern california i think we got this from openstreetmap we'll read those in look at the columns take a look at the first few rows i guess what i should have done is clear the cells below i'll do that for the other notebooks again we can just use the plot method to see where these are um now that just looks like a a bad scatter plot at the moment it's hard to tell that these are spatial data those could be any xy values anywhere if we look at the geometry sure enough we can see that all we have here are our x and y values again this is the what's known as the well-known text representation these represent all of our x and y locations of these clinics we take a look at the address these are scattered all over southern california great but as is common in in urban data these are mixed all over the place so we've got clinics many of which might be in the same building right it's it's common for a health complex to have several different providers inside of a building right if we've got a row for each clinic then we may be over counting in space if we're not not thinking carefully about where these are so there are 18 unique addresses showing sure enough we do have several of these clinics in the same location okay and now let's put on the hat of an epidemiologist or someone that's that's trying to understand the the layout of of care of potential care in southern california um here we need to do an operation between two different data sets we need to we've got our set of polygons uh on the one hand then we've got our set of behavior health clinics on the other hand we want to understand the relationship between those those two data sets the first thing that we care about um because uh serge and i are in riverside we'll use riverside as the example so the first thing that we need to do is subset riverside out of our larger set of tracks we've got the whole southern california region here and we want to pull out just one county so we can do that with this regular expression here so we're going to match anything that starts with o six o six five like i mentioned the fips code is a a nested identifier so o six that's the state california and then o six five is is riverside so we'll store riverside inside a new variable called rc and plot it great riverside is a massive county it's it's hard to think in latitude and longitude but this is uh quite a width we can take a look at this a bit later so now we've got our our set of tracks in riverside and we've got our behavioral health clinics that we know are on riverside and now we want to understand the relationship between the two of them and we do that using something called a spatial joint um and we might want to ask how many clinics are there in each census tract in riverside county we know that there are um 18 unique clinics and we know that we've got 453 census tracts in riverside so where are these clinics distributed within the the 453 census tracts so we use the geopandas s join function to do this on the the left hand side we're going to use the clinics on the right hand side we're going to use riverside county and then we're going to select within as our predicate i think i should actually probably change this and what's going on here so one of the fundamental uh stumbling blocks in spatial analysis especially when uh when you're not familiar with gis or with spatial data is this concept of coordinate reference systems so as i mentioned before if we look up here at this map of of riverside county this is being displayed in units of latitude and longitude it's not very easy for us to think in latitude and longitude right the the globe is is round um and so we need some way of transferring that that round data onto a flat surface so that we can uh both operate on it in a ways that that makes sense and we don't have to use complicated math but also so that we can make good looking maps and all this information is stored inside the crs attribute the coordinate reference system attribute of a geodata frame so if we take a look at this we can see that our clinics data frame is stored in what's called the the um oh it's it's failing on me the the the california state plane sorry um and this is stored in zone six okay so here these are stored in uh our units our units on the x and y here are feet for the clinics unlike the decimal degrees that we have for the tracks here all right if we take a look at tracks the crs attribute of the tracks sure enough these are stored in latitude and longitude so we need a way of converting one system to the other so that we're having apples to apples comparison because if we look at the spatial join that we just tried to create we see we've got nothing in it right we've got no results because there were no tracks that had clinics within them okay so we can change that quickly so again look at the axes here we're plotting riverside again we've got latitude and longitude as our x and y chords if we use the two crs method we can quickly convert from one representation to another so we're going to take riverside county and we're going to convert it to the clinics coordinate system and we'll plot it immediately without storing into a new variable and sure enough you can see right these axes have changed pretty quickly pretty dramatically right these are now really large numbers as opposed to being in the hundreds okay so let's make sure that we save that right here we were just plotting it quickly so we could do a diagnostic to make sure the results are what we think they should be but this looks good so we'll save that back in our same data frame so we're going to reproject riverside county and store it back inside the same variable so now if we look at the crs attribute for riverside county sure enough it's been converted to california zone six all right now if we redo our spatial join and ask for clinics that are within riverside county low and behold we have some now but this doesn't look exactly right so those are our clinics that's right but this is an awfully large data frame so how do we look at how many clicks are in each track well we'll do a pandas group by operation and the index right here should be our set of census tracts i'm sorry so we're going to group by the geoid on the left which comes from riverside county and index right is our count of the index for each behavioral health clinic so when we group by goid and we aggregate the index right this gives us the total number of clinics inside each census tract again that looks like we like it so we'll store that inside a new variable called ct which is 16x1 okay great so we we've got our subset of tracks that have at least one clinic um but we don't know how many tracks are missing clinics right we still don't know where those are we might like to make a map showing say a choropleth with the intensity where we've got the color showing the number of clinics inside each tract and we can't do that with the data structure that we have just yet the first thing that we need to do is merge these two data sets back together do a simple table join using pandas so we're going to merge riverside county with the census tracts geoid is our unique index here okay great but as you said before this is an outer join so [Music] we're getting a lot more data than we actually need actually i have that reverse the first version we're doing a just an inner joins we've got only our 16 tracks that have clinics what we care about is an outer join because we want to make the the full census track data set available so that we can make a map of it so if we change our our our method to an outer join then we're going to make sure that we have all of our units from from both data sets and the problem here is if we look at our index right and this is the the column that stores our total number of sense of behavioral health clinics in each tract right several of these values are nan not a number because lots of of tracks don't have any clinics that's an easy fix so when we do our merge we'll just make sure that we fill the n a's with zero because in this case zero is accurate we know that it's a it's a true zero we'll try that merge again and this looks like we want it to we've got zeros filled in except for places where we have behavioral health clinics excellent so let's make sure we actually store that in the variable again we're going to overwrite rc now it's going to be our full data set of census tracts and the count of of clinics that they contain if we sum up the index right we recover our total number of health clinics which was 28. that's a good sign we haven't done anything wrong now the only issue is that index write is a useless column name that's only going to confuse us later index write doesn't mean anything so let's make sure we rename that to clinics or maybe clinics count because this is the number of clinics in each in each census tract and that's not going to work because we just changed it great 28 and we will preempt the next one really quickly oops this is our data frame and we could make a really ugly course map of where these are in the next notebook we'll look at how to make a better map of this this is exactly why things like map classify were created because like i said the the generic and visualization tools just don't work very well for spatial data we we want a way of binning this we'll look at that in the next notebook all right any questions so far great question so there is a package called senpai that was first written by uh one of our developers levi wolff now uh james gabordi and i also help maintain this um but the census api changes quite a bit um and this uh the last time we looked at this everything was working really well but a lot of this is built on legacy infrastructure right so if this package works great but what it's going to do is send a request to the census servers all json encoded the servers are going to respond they're going to send back pages and pages of json encoded data that have to be converted into a data frame on the back end and that can take a really long time and it depends on how many variables you ask for that can fail as it's responding to the query for you so this would be the the package that i'd recommend using but that you're still going to run into some headaches from time to time we take a slightly different approach in in geosnap by just storing most of the data from the census that we expect people are going to want to use we put that up in an s3 bucket any other questions cool all right so we've got a quick exercise i want to create a shape file for each of the counties in southern california we're going to write those to a set of shape files called fips code where fips code is the five digit fips for each county right like i said the that fips code is a nested identifier right so we need the first five characters to identify the unique counties in our skag data set i'll give you a few minutes to work on this thank you so mhm sure so let me actually so another gotcha for spatial analysis in the pi data stack is if you're you want to make sure that you never mix channels uh when you're installing conda so if you want condom port you can go all kind of if you want default you can go all defaults i recommend console but do not mix the two because some of the underlying c dependencies like g dal and geos are built on different architectures in those uh those two different channels so if you mix them you're gonna run into all kinds of problems so if you reinstall everything and make sure that everything comes would recommend mamba um as your your package manager if folks haven't used that it's basically a drop-in replacement for conda but it's wicked fast it'll install a lot faster for you okay so if you uncomment the last cell and run it it'll it'll pop up a solution that we've created for you here so the first thing we're going to do is create a new column called county on the skag data frame and we're going to use string indexing string slicing to slice out the first five characters of the geoid string and then for each of the unique values in county we're going to write out first we'll do a subset and select only the that county and then we're going to write that new geodata frame we created to a new file as i said before because we're using a legacy data set we're already encountering some problems our variable names have been truncated to 10 columns 10 characters a piece so now if we look in data we've got a bunch of new shape files in here a shapefile is a misnomer technically a shapefile is not a file it's a collection of files it's at least i'm going to get this wrong it's at least three files i think you need at least three it's generally like seven depending on how many pieces of information you have whether you've got a spatial index and a coordinate system stored all right any questions awesome okay so you may have noticed that that map we created at the very end was not a very aesthetically pleasing one in addition to that it's not very full of information it doesn't tell us much right if i go back and look at this map this looks like a big purple mess big blue mask my colors aren't great but this isn't a useful map and there's a fix for that so as i mentioned in in the introduction you know pysol started as um an analytical library for doing spatial econometrics right um sergeant luke the the founders of the library are spatial econometricians right and so they they think about um really difficult spatial analyses but before you can get there it's really important to be able to understand your data and make good outputs and good maps and we realized really quickly i'm going to use the the royal wii here because this is long before my time on the pythal team and we realized really quickly that we're going to have to build some visualization tools that sit alongside these analytic methods if people ever want to understand the spatial data that they're working with okay so map classify is uh is that the package that evolved out of that and so map classify is now actually a dependency in geopandas so when you make a a quick map in geopandas you you you pull in map classify so just to show you how this works we'll we'll roll one from scratch so first we're going to import map classify we're going to import geopandas again we'll do matplotlib and then we're going to bring in one more library called contextually and that that's going to help us bring in some more context to our maps okay again we're going to read in our our southern california data store that in a variable called gdf we're gonna can convert the coordinate system again we're gonna project our data to uh a web mercator so this is a like the if you look at a google map and this is the projection that you're often greeted with this is what if you look at maps online this is usually the the coordinate system that they use so this will be a familiar representation to you okay and then we've got a lot happening in in this cell so let me break it out a little bit so first we're going to set up our plot we're going to give ourselves a figure and an axis figuring axes objects from matplotlib subplots and this is really so that we can define the the size of the output plot so it's going to be 12x8 and then we're going to plot our geodata frame like we did before we're going to specify that we want to plot the median home value we use we want to use quantile binning to plot that map we wanted to go on the axes that we just created there's two pieces of information to color one is the interior of the polygon the other is the the line string that surrounds it so we're going to set the edge to white include a legend and trim down the the size of the the line width a little bit okay now this is still not a perfect map but it's a lot more useful than the one we created in the last notebook i think i would probably get rid of these lines that makes it a little bit easier to see and right away we can start picking out some pretty obvious patterns if you're familiar with the southern california region it's no surprise that home values are more expensive along the coast than they are in the inland part of the region so we've got a lot more yellow over here on the on the west and so this is geopandas just using um map classify under the hood we didn't have to do anything special all we had to do is tell geopin is that we want to use quantile binning that's really useful so just to make our map complete we're going to do a mean imputation here and we're going to fill any nand value with the mean that may be a course assumption but it's going to help us create a better map right now we could dig into whether that makes sense later so this is a better map than we had before but it's still lacking some context right if you don't know ahead of time that this is southern california maybe this is not very intuitive to you um so a nice thing that we could do is add a base map a set of base tiles to the to behind it right that we dropped the map on top of that's going to give us a bit more context okay and we do that with the contextually package again i'm going to add some space here so that the cell is a little easier to read and the only thing that's going to change between this cell and the one before it is that we're using the contextually add base map function so again we set up our plot we're going to plot median home value with quintiles with quantiles this time we'll make it a little bit transparent so we're using the alpha parameter to add some transparency we still want the legend and then we're going to use contextually to add a base map we add it to the same axes we've already plucked we've already plotted on we need to tell contextually what coordinate system we're using so the crs argument gets remember we can look at the coordinate system by doing gdf.crs but that's a stylized representation so we can do two string and that gives us the simplified version okay so we tell contextually we're using 3857 which is shorthand for web mercator and then we want tiles from stamen stamen is a cartography company they make really beautiful web maps so we want to leverage the great work that they do not being designers ourselves and we want to put them their base tiles right beneath our map and that looks a lot better now all of a sudden we've got some context we know exactly what we're looking at now it's pretty clear that home values in san diego and in los angeles and up the coast here are more expensive than they are in moreno valley and san bernardino and in riverside we've got a better sense for exactly the coast where the coast is and why our data cut off right there this one i'm not sure so so an aside on census tracts census tracts are uh designed to have roughly the same amount of population inside of them so you see down here in the populated parts of the region these census tracts are really really tiny because they've got about 4 000 people and it's really dense over here when you see a giant census tract way out here that's that's got some high value in it that probably means our data are pretty skewed over there there's probably this the fact that it's this large means there's probably not very many there's 4 000 people but it's really it's quite rural it's there's no population density here so i don't know what this is uh off hand could be joshua tree let's uh i'm to cheat here maybe i'm going up a little bit but let's do this it's probably five feet we go so the explorer method is a a pretty new um function and we're going this is i'm i'm shooting myself in the foot here because i'm going to get to this cell a bit later um but because this is interesting the explore method basically is brand new and this will drop you right onto a slippy map instead of a static plot so if you're not doing a publication this is obviously the the much more useful visualization here so let's zoom in it's a marine corps base [Music] 29 palms but it is right by palm springs so this explorer we basically just recreated the same plot using a different method give it the column that we want to plot use the same scheme um the explorer function was just put in here yeah it doesn't have alpha um like i said it's it's it's quite new in geopandas it's only a few months old um one of the core developers at geopandas martin fleischmann is also a pysol developer so we try and stick these things into pysel tutorials as soon as they're available in geopen as it's one of the nice things about having cross fertilization in the the developers all right so aside from that digression uh let's create a new variable called hv and that's just going to be the series of home values from our geodata frame to get a sense for what map classifies doing under the hood so if we care about how this binning is actually happening we can look at this with math classify so like we said before we're using quantiles here and we didn't bother looking at how this changes if we if we switch so quantile bins it defaults to i think five one two three four five if we want to give a little bit more variation in its map we could crank that k parameter up so instead of five bins we could ask it to give us 10 bins so now these are deciles instead of quintiles and our map should have a little bit more nuance to it because we've given it more more uh categories to plot so what's what's actually happening under the hood map classify is what's creating those bins and it's basically like a it's it's it's like a univariate cluster in a way so if we want to see what our bins look like for the k of 5 versus a k of 10 we use the map classified quantiles class we pass it our home values data we ask for in the first case five bins here's what they look like it gives us the interval lower and upper bounds and the total number of tracks inside each bin we do the same thing for a k of 10. as these are these should have the same number of observations in each one of them because of the way that we're doing the breakouts but because some values are not unique we're going to end up with not not perfect distribution across the bins okay we can store that into a new variable and then we can see what other attributes are available on the map classify classifier object in this case it's a quantiles class so the bins these define our upper and lower bounds accounts tell us how many observations we have in each of the bins and then we have lots of different classification methods right so we've used quantiles right here because quantiles tend to make good looking maps and we can also use fisher janks which is a an optimization strategy fisher jenks is going to give us a different breakout and then we can use the the adcm this is the the average deviation from the class mean the class median i'm sorry and this is basically a it's like a measure of fit it's how well our classifier fits these data um so you only want if you're going to use this as a as a measure for for for fit you want this number to be lower [Music] and you only want to compare binnings with the same k parameter so if we look at fischer jenks versus our our quantile classifier we can see that that fischer janks is a more optimal classification scheme let's let's see what that looks like in the map i'll steal this put it down here and instead of the scheme of quantiles we'll do fisher drinks slightly different map pattern in particular sort of washed out here in the sort of the middle regions i've got a lot more variation in downtown los angeles as well so you can also create your own bins if these don't work well right if you want to show a certain range we can pre-define our bins so let's say we want um what are these 100 000 half a million a million and 1.5 and then here we use the user-defined classification scheme and map classify so 61 homes up to 100k most are in the middle over over 100 and under a million a handful over a million i think i've got those numbers right looking at these quickly these these bins just yet so just in the yeah it's going to default to five bins and then you can change that with k so if you want to plot say 20. did i understand that question uh yeah so [Music] i'll cook up an example for you when we're doing the no it's a great question i'll cook up an example for you when we get to the to the exercise so really quickly let's let's go through a few different um choropleths a choropleth is one of these um these colored maps where you have the the areas of the map shaded by the the quantity that they represent so let's crank out a bunch of them first we'll do quantile bins with five then we'll do fischer jenks with five then we'll do fischer jenks with a different color scheme there is an entire i'm i'm sure i don't have to tell you folks there's a an entire science devoted to the proper way to do these sorts of visualizations right what you're trying to show here we've got housing prices um so it probably makes sense to use a sort of a single color ramp that goes from light blue to dark blue if we want to show deviation from some central tendency then it can make sense to use a diverging color scheme and we can get into some of this in a bit and as we've said before the explore method is going to pop up an interactive map and the tooltip option is nice for subsetting what information you want to pop up if i ignore that tool tip and remove it it's going to use number one it's going to take a lot longer to plot this and number two it's going to be really difficult to see what's happening because every time i mouse over an observation it's going to show me every single value for every column so under the under the hood it's got to convert our geodata frame to a geojson representation because we've got so many columns that's why it took longer to plot and now we can't see anything because all of our data are being obscured by this so if we add that tool tip option back in we're going to subset so the tooltip only shows the data the same data that we're plotting now if we pan around we can see exactly how home values change all right now really quickly we're going to create a different choropleth map for each county so first we're going to create a set of counties so these like we did before these are going to be the five digit slice from our geoid and if we loop over that set of counties and create a smaller geodata frame using only the tracks that are inside that county we can create a new plot for each one now this is going to give us a sense for how home values vary within the county instead of how they vary within the larger region right there's going to be some internal variation that looks quite different when we scale relative to the county instead of relative to the region so for instance now that we're looking at riverside you can see a lot more nuance right there is over in palm springs there's this little pocket of of uh higher priced homes sort of near the uh western edge of the city where you've got a shorter commute to the larger employment centers on the coast and all of this variation is washed out when we plot this at a larger scale right we can't see we get we get some of that but makes it much harder to see all right we'll do another exercise i'll give you guys a few minutes for that let me see if i can cook you up user-defined example okay so sure enough someone asked a very similar question over in the geopandas repository a few years ago uh and because i wasn't standing in front of a room full of people i could put that code together a little bit easier so i can put this in the in the slack if you like but the idea here is to [Music] create a set of bins and then you use the assign method on your geodata frame and give it the yb attribute so the yb attribute of a classifier tells that it's the y bin so that's going to give you instead of it's it's going to be harder to get the legend but it's going to plot the right color for the bin that you want so in this example i i didn't so you get a the bins only go from zero to five the legend no longer shows the range of actual values because we've already truncated it to the bins but the way to do what you are trying to do is create a classifier object create a new column on your geodata frame and in that column give it the yb assign it the yb attribute of your classification scheme and then if you plot on that column that's your your defined bins yeah let me put that in here perfect any other questions great i guess this is going to be a really simple question what's the significance yeah it was that's just to show what's happening under the hood it's it's really uncommon for you to use map classify directly instead you're just going to use it through geopandas by specifying what classifier you want to use and then what k you want to use the only time you're going to run into using map classify on your own probably is if you want to create your own bins and then plot those although martin may have changed that too it's possible you may there may be an easier way to do user-defined bins now i'll check that after the workshop and get back to you in the slack in case there's an easier way to do that so here we've looped over and created a fisher janks map using six pins for every county in the region all right all right so those are the nuts and bolts of reading and writing data subsetting some simple spatial joins some plotting and we'll take a short break now let that sink in and then we'll come back and look at spatial autocorrelation spatial weights we'll give about about five minutes i think we're right we're right on time um wow oh i just remove this okay i say blasphemy accommodated okay so it may be that if you already blasted it like in a different terminal it may think you're in it still so i may try another gel exists is um okay environment okay okay i've tried that thought quickly on mine okay um okay okay it oh oh differently yes um quotes uh worst case scenario we can do binder and then i can help you troubleshoot during the next break as well all right everybody back we'll get started all right so so far we've just been doing sort of nuts and bolts of read spatial data in take a look at it plot it and now we'll start getting into the fundamentals of spatial analysis and the really one of the the most important one of the core data structures in spatial analysis is this this concept of a spatial weights matrix um [Music] it's it's not really a weight but canonically it's called a spatial weights matrix really this is like the connectivity graph between observations in your in your data right this defines who is neighbors of whom in your data set um so these are are critical for expressing spatial relationships between the different observations in your data so we usually call this uh a w matrix so the the weight between observation i and and j is the the value in that in that matrix and it expresses whether or not there is a relationship between observations i and j and that relationship can be defined in lots of different ways so we'll use this notebook to look at a few different ways that you can create a spatial weights matrix we'll look at the implications of making those different choices and then we'll see what the what the weights matrices look like when we plot them so here we're going to use libpy cell lip pycel is what provides the the weights module in fact let me go ahead and clear so there's actually something showing up all right and then there are a few different kinds of spatial weights that we can create so we're going to look at queen rook k-nearest neighbor kernel and distance band so we import each of those classes we'll do our standard numpy geopandas pandas and matplotlib all right and then one of our visualization packages um we we tend to call it s plot although you'll be forgiven if you call it splot um it has a little function called uh plot spatial weights um and that is really handy so we're gonna bring that in as well all right so um there are lots of different types of spatial weights matrices that we can create one of the most common and classically used in spatial analysis is is contiguity so if you are these these work quite well for polygons right if you share a side with with a different polygon then you are neighbors right and we we borrow the terminology from chess right so if you're queen neighbors then you can share either a side or a vertex right it's anybody that touches you even if that that point of contact is just a point um alternatively you might have rook neighbors right those those have to share aside they don't they don't share uh points and then in theory you've got bishop neighbors right those are ones that only have corners they don't share a side in practice bishop weights are pretty impractical it's pretty strange to think about only having a relationship with places you have a very small connection to through the corners but just to be clear all of those actually are available so let's see what these look like we'll read in our southern california data again this time we are going to convert it to a slightly different uh coordinate system we're going to use uh utm zones universal transverse mercator and we're going to use zone 11 which is appropriate for southern california 11 north okay so the first thing that we'll do is from this data frame from this geodata frame we're going to create a set of queen weights and this is pretty straightforward we take the the queen class that we imported and it's got a class method from dataframe we pass it our geodata frame of southern california and we're going to create this new variable called qw okay that gives us a piscell weights matrix we get a warning here that it's not fully connected we've got three disconnected components and one island we'll take a look at what that means in in a bit great so we've got our our queen object and if we want to understand what's happening in here then we can look at a few of the different attributes so this is basically just a a heavy wrapper around a dictionary so we can we can slice into the weights object so for the 156th observation right because we're in python world and we start at zero that is this identifier here are the weights so for the 155th observation its neighbors are four five two eight five four seven two one three three and two seven four four the value in this dictionary is the value of connectivity in this case we're using binary connectivity right these if you share a side then you're connected so you get a one right there's no distance to account for it's either yes or no all right so when we create a weights matrix it's going to use the pandas index as the identifier for the self and the neighbors sometimes that's useful but sometimes we also want to make sure that that index is set so that these are are meaningful like right now we've got a range index and we don't know what the 155th observation is so just a reminder that sometimes it's quite useful to set that index if that's what we think we're going to be operating on here let's again say we care about this 155th observation so we'll grab it first then we're going to extend that dictionary with its neighbors so we've got our focal observation 155 and then we've got its queen neighbors and those are all now in this list of self and neighbors great and then we can use the the loc method attribute slice thing an append as data frame to grab out our focal observation and its neighbors and those are these folks and we can plot this just to make sure we understand what's happening here and sure enough everything is contiguous like we would hope that it would be if it's not then something is wrong now we can get the full dense matrix that describes the relationship between every observation in our data set by using queen our our qw object which is our queen weights and use the the full method that's going to give us both the dense matrix and the set of ids so if we sum along the rows that's going to tell us how many neighbors each observation has so observation 155 has four neighbors see what's in df oh there we go so here is our focal unit 155 in the middle here one two three four all right you can also take a look at the cardinalities right that's the same thing instead of doing this by hand we can just get the the cardinalities for 155 and again because this is binary anytime we have a connection between a focal unit and its neighbors we're going to have a 1. of course sometimes we want a slightly different representation so if you want to use a spatial weights matrix first for input into something like a spatial regression model often what you want to calculate you want to use the weights matrix to calculate what's known as the spatial lag and that's the average of your neighbors so in that case it's it's common to transform the to row standardize your weights matrix and basically what that means is we'll take your value and divide it by the number of neighbors that you have right okay weights matrices tend to be very very sparse right especially contiguity neighbors so if you've got a data set of a thousand observations and you're using queen neighbors if again if you've got like something that looks relatively like a chess board you're only going to have between four and eight neighbors depending on what what kind of matrix that you create right so most of those entries and your weights matrix are going to be zero so that the dense matrix is usually a really uh inefficient storage mechanism for that so the sparse we rely a lot on sci-fi sparse matrices the sparse attribute will give you back that uh that sci-fi sparse matrix whereas the full we'll give you back a numpy array we can also look at the share of entries in our weights matrix that have non-zero weights this is about 14 again we can do this for a set of observations that we care about so let's look at observation 100. these are the data that characterize it let's take a look at its neighbors it's got five neighbors sure enough okay these are the neighbors and then we can plot what this weights matrix looks like so we use the plot spatial weights from s plot we give it our queen weights matrix and the data frame from southern california and it's going to plot first the polygons themselves and then the lines show connections between each unit and its neighbors right so because we're using queen weights here anybody that's got a vertex or a side connected is going to be connected to the other observations here so we got really dense connections down here quite sparse up here right this guy's only got a handful of neighbors it's not connected to any of these ones down here all right now this yeah yeah so we get a couple different warnings when we created that it tells us the wave matrix isn't fully connected we've got three components and one island so the fact that it's not fully connected means that we can't move through the matrix so to speak from one observation to all the others so if we look here at the channel islands these are only neighbors of themselves and this is really just an artifact of the way that this was created these shouldn't even really be neighbors they don't really touch so our three components are this large connected graph right you can move if you if you have to force yourself to move along the network here you can move from any observation to any other observation in this set if you just move along blue lines but you can't get to these guys right there's no connection from this one to the mainland so this is a disconnected component and the same is true over here this is a disconnected component but it's also an island because it doesn't have any neighbors so disconnected component not an island has neighbors disconnect component island dis disconnected component but connected to everything else do that wrong i think it's qw that's what it is component n component maybe yeah so that'll give us three i forget what the attribute is component labels there you go that'll tell you which which element each one of these observations is a part of there we go all right sometimes it's useful to plot these cardinalities that gives us a sense for how connected the graph is right so most of these observations we have in this data set have somewhere around i don't know seven six seven neighbors they tend to be on the small side and here's what these look like as a series there's a lot to take in but what if we take the average i'll come back to that later all right and this i think is what you were getting at before so these components tell us something about the larger connectivity structure of the region so we can create and this i think is exactly what you were asking before we're going to create a new column on our data set called component and we're going to put the component labels inside of that then we're going to use our explore method again to give us ourselves a slippy map a little web map so that we can explore which observation belongs to which component [Music] note that we're not giving a classification scheme here because these are our categorical data right these these components are um like discrete bins they're they're not a range so we include the categorical here so we don't confuse that and like i showed before but this is a much better representation we've got this one blue component red component and then this one doesn't even end up showing zero one two yeah it's there the color's just really really light it's this color here let's say there we go it's still not the best color choice but now we've got three different three different colors all right we can also look at them one at a time so if we only care about component two again that's just this naval facility doesn't really have any information the census doesn't generally give lots of detailed information about military bases doesn't doesn't tabulate the information for the population there there is no civilian population there all right it doesn't have any neighbors if we take a look at this observation let's see this is my screen ratio is too small here but this is observation 4285 and if we take a look at it again it's got no neighbors right it's that island stuck out in the pacific take a look at component one this was the other disconnected component but it does have two neighbors and the neighbor of 2000 is 30 21 right there and the reciprocal is also true 30 21 its neighbors are 2 000 again these are the two observations that belong to component one where is it well it's these two why are these connected anybody want to guess why these are connected we're using queen weights right so they have to touch a side it's a good guess it's not the issue is that these are are digitized differently so this is actually one observation you see these two islands that are going to highlight when i mouse over one of them the bottom one shows up as well so this is a multi-polygon and the other observation that you see is this one so there are two tracks on this one island this one island well these two let me make sure i'm using the right terminology here so islands here in the in the physical sense right uh these are digitized as one entity is one multi-polygon but so they technically have neighbors because this one observation also touches this other observation down here in the channel islands is this actually a multi-island census tract or is it a mistake this is going to be a multi they they digitize catalina islands the catalina islands as um well catalina and the san clemente as a single polygon well so it's a multi-polygon in the census i don't know why they chose to do that i'm sure they did it on purpose um they're going to have the same fips code yep yep and then we can look at our big one this is getting into exactly what we just looked at what's happening here we already looked at that in the map so it's it's easier to so here we're digging into this the explode method on uh on a geodata frame will explode a multi-polygon so now we see that observation 3021 which was that two island multipolygon has now been exploded into two different observations but as you see like you said they've got the same fips code so according to the census they are one observation according to the real world they are two different land masses and all of that can really screw up our concept of what's connected to what from a spatial weights perspective right because those don't those two islands look like they are separated by a lot of ocean so it's good to interrogate the data like this all right all right okay so those were queen weights rook weights are similar but rook weights are uh a smaller set right so now instead of being able to share a side or a vertex now you have to share a side a corner doesn't count so these are a stricter definition of neighbors you have to have a shared boundary to actually be considered neighbors we get the same warning unsurprisingly we know that we've got three disconnected components in these data that data are imperfect now for observation 100 we've got one two three four five neighbors which we can get with the len we can slice out those neighbors from the data frame right we ask the neighbors attribute from our rook weights object for the 100th observation that's going to give us back the set of indices if we do an eye look on that that gives us the neighbors for observation 100. i'm sorry the weights so the weights are just going to be a one in this case because they're they're binary so we can plot these and there's less connectivity here than there was up in our queen graph if i can find it here we go versus this one here it's hard to see at this scale but there are fewer connections it's easier to see over here in riverside again we can plot the cardinalities and so like i mentioned before right technically we've got three different kinds of of cardinalities here right we've got queen which is you can share a vertex or a side we've got rook which is you have to share aside or technically we've got bishop which means you just share a corner and practice bishop weights are are very rarely used but just for the sake of completeness we show that we can calculate all of these so to create a bishop weight we use the w difference method we take queen weights and rook weights we difference the two and those give us bishop weights we take our queen which has sides and corners we pull out the ones that just have sides that leaves only the corners all right so our bishop weights are really sparse and unsurprisingly these places if you if we force you to share a vertex then we really truncate the connectivity graph all right so contiguity weights often make sense for polygon data right it's easy to conceive whether something shares a side or a vertex and then it's easy to intuit that those should be neighbors right they are physically next to one another right but that's not the only definition of neighborliness that we're we're forced to have we could also have a distance based weight right so we can all be neighbors in this room you don't have to be sitting next to me the fact that we're in this room together we share the same space that can be our our definition of of a neighbor set as long as it's i don't know what's what's we'll call it 50 meters right we'll all be neighbors of one another if we're within 50 meters so pycelle has a distance band weights creator and it's going to use the units from the coordinate system that your data are stored in and this is an important point right so when we read in our data set that was originally stored in lat long right it's not a good idea to do distance calculations on latitude longitude right because they don't account for the curvature of the sphere um so if we're asking for uh neighbors that are within 2 000 units of one another then we need to make sure we know exactly what those units are so let's take a look at the crs attribute the coordinate reference system attribute and it tells us that in universal transverse mercator our measure our units are stored in meters okay so if we create a distance band weights with a threshold of 2000 that's going to be anybody within two kilometers is going to be neighbors with one another okay now this is interesting because we talked already about the heterogeneity and the size of these tracks right so that introduces a lot of heterogeneity in the whites those rural massive tracks out on the eastern side don't have any other observations within two kilometers of them so they don't have any neighbors that's very different from the the dense coastal regions in san diego and los angeles where the tracks are really small and they may have several neighbors inside the the two kilometer threshold and there's no right there's no right or wrong version of a spatial weights matrix right this is dependent on the question that you want to ask of the data how you want to operationalize who's the neighbor of whom yes so this is the centroid and when you have polygon data and you ask for a distance band weights we have to we have to condense you down to some where do we measure from is it the furthest point is at the nearest point and we abstract that to be the mathematical center of the polygon so that's where we always start from now distance band weights are particularly useful for when you have point data right like we talked about before contiguity often makes sense for polygons but if we were building a weights matrix on our locations of behavioral health clinics for example right we could create a set of contiguity weights but that's gonna that works by creating a voronoi tessellation in an intermediate step so you can create adjacency by tessellating the the um the point space but uh often if you've got like point data it makes more sense to use uh distance bandwidths all right now there are some cases when a weights structure like this will cause problems right because lots of our observations here don't have any neighbors um so if we want a slightly more flexible version of a weights matrix that's not defined by contiguity we can use the k nearest neighbors um and we'll go through two different things here so in the event that your data are not projected and you have you're operating on latitude longitude uh then you need to make sure that you pass the radius as the the uh great sphere distance on the earth right the the earth is round so pycelle can account for that internally if it knows that your data are already measured in latitude longitude it can do the math appropriately to approximate how far away things are in general i recommend against having to use that it's better to operate on projected coordinates so just be in the habit of projecting your data into a coordinate system that's appropriate but if you can't then we can still account for this and we're going to show you the difference between if you do and don't account for the earth's curvature so first we'll create a weights matrix that's called knn8 bad right if we if we ignore the the earth's curvature here are our k nearest neighbors our eight nearest neighbors for each observation okay and there's the histogram all right and then we're going to pass in the the the actual radius using the the great circle distance and we'll create the same weight weights matrix from the same data um but this time we're going to measure distance appropriately and if we take a look at observation 1487. we can see that some of its neighbors are the same but not all of them right in particular these last two are different if we ignore the curvature of the earth and we're going to define our neighbor set incorrectly again we can test whether these two sets equal one another sometimes it can be hard to do this comparison because these are not ordered lists and so we have to cross make sure that those are connect instead we can just do an equalities test in these two sets are these two equal do they have the same neighbors and they don't all right let's break for a quick example here i'll let you guys work on that and then we'll look at two other white structures really briefly serge wants us to know that he is available on the slack in case anybody has questions he can't see what's happening so he's flying blind but if you have questions drop them in the slack with a little bit of detail and surge should be able to respond to you so there are several different ways you could do this obviously in this case we just define a simple little for loop that tests for set membership it turns out two percent of the tracks have incorrect with neighbors if we ignore the curvature of the earth okay really briefly to other kinds of weights matrices that you'll see in practice um now the two that we just covered contiguity and distance band these these tend to be the most common um but another data structure you might come across is a kernel based weight and this is when the actual concept of weights it gets activated right because you might want to discount for distance in our in our simple version of a distance band weights we're saying that we all our neighbors as long as we fall inside some predefined threshold so like i give you the example of the the room right we can all be neighbors if we're all in this room together that's fine that's a binary uh definition but then we also might want to allow that to have a little bit more nuance right so the the folks here in the first row are closer to me we might be closer neighbors than the folks in the back row right and we might want to account for the fact that there is this distance decay in our neighbor relationship so i'm the first law of geography is that everything is related to everything else but near things are more related than distant things it's called tobler's first law and so the the idea of a kernel weight is to try and encapsulate that idea right so we can be neighbors but we're more neighbors if we're close together and we're still neighbors if we're far apart but like if you live on the edge of the block we are a little bit less neighborly that's the idea and you create these just like uh just like we've done with the others so we'll create a new object called kernel weights we'll do a kernel from data frame we give it the data frame we give it a k so in there are two ways we could parameterize kernel weights we can do them based on a distance or we can do them based on on the k nearest neighbors so here we'll say give us uh our the 10 nearest neighbors and then we need a waiting function to apply the weights as we move further out so we can do a simple linear decay we can do a gaussian decay and there are i want to say two or three other different decay functions you can you can create your own if you if you really care about it um but the idea is that you you might want a different waiting function for folks that are that are far away to help discount for space now when we plot this this is going to be this is going to look exactly the same as it would if we had done a k n weights right this this graph is just showing us the binary connectivity between neighbors it's not showing us the the quantity of that connection right but in this case we actually do have oops let's say we look at 155 again [Music] right so now our neighbors in this dictionary are the same as they would be in a k n but the values that quantify that relationship are continuous it's not just a one zero and that's how we can account for observations further away counting less in our model for example all right so this is showing the the kernel bandwidth right because we didn't use a fixed bandwidth because we used a k n in the kernel this is showing how far on average these um each unit has to to travel out before it hits its k so as we knew out here in the rural areas these because these tracks are so big you've got to travel a really long distance before you can hit 10 neighbors and that the opposite is true of downtown los angeles right you hit 10 neighbors really fast because the tracks are small and close together okay the other thing that you might want to do are block weights and so these are we'll do a quick example here so we'll split the region into quadrants first we'll grab the total bounds cut them in two that'll give us basically a a cross in the middle of the region and we can use those to define quadrants so if you are west of our central meridian or east you're going to be in different quads and then we do the same thing for the uh the horizontal and then once we have that structure we can ask for block weights now these are quite dense right because everyone in here is like the example that i used in us being in the room together right that would be a block weight we're all in this room together then we're all neighbors together these are much denser weight structures than ones based on contiguity right because we're all neighbors of one another all right one more quick exercise so it depends on the question that you want to ask so um and and so a canonical example in urban economics is what's called a hedonic price model for housing right the the idea is that you can a house is a bundle of attributes uh it's it's construction quality it's access to schools it's access to jobs it's air quality but it's this bundle of things and the price is the composite aggregate of all of those features of a house and you can't observe the the individual prices that people value of better quality construction or being in a better school district all you can observe is the total housing price so you could basically build a regression model that tries to partial out the components of a home's price and so in spatial analysis you often want to include a spatial term to make sure that you're not to one to make sure that your estimates aren't biased because spatial autocorrelation is probably present in your data near observations are going to be more similar to one another you want to remove that so that you get fair estimates of and so this is a case where you want to think carefully about what's the neighborhood of this house yeah you're going to get a strong signal if you use contiguity weights because the house the price of a house is going to be really similar to the ones next to it but it also might make sense to use a continuous weight because a house that sells you know two miles away is still probably going to have some influence on my house's selling price right like if you go to zillow you see a house that sold three miles away that's going to change the way that real estate agents act in this neighborhood because that house selling has sent a signal to what it's worth now you can use something like a kernel weights to help account for the fact that well my real the house is really close to me should count for more than the houses that are far away i don't know what's going on there um but there's there are no hard and fast rules for any of these things so block weights might be useful for trying to uh if if you've got if we want to split this by county for example if we thought that the spatial relationships were going to be different in los angeles than they were in uh in riverside then we might use block weights as a way of saying well riverside and and los angeles are going to have fundamentally different structures because that that county line really does mean something yeah there's spillover across it but inside the county is going to be a lot more similar to a cross so you might want to operationalize block weights to try and take advantage to try and investigate that question whether that's true or not i probably just didn't answer the question and talked a lot at you but there there's no hard and fast answer for when you use these different weight structures it's really dependent on the question yes yeah so this is basically like group membership weights instead of near one another in space weight so this is a bit like a hierarchical model all right i think the question is what we're going to get to in the next notebook i think is what you're asking so this is going to take a long time to draw right because this is a really dense weights matrix it's going to have to draw a line between every single observation in each one of our quadrants this is where the finder crashes mine has crashed yeah so this we got a lot of observations in here it's trying to do a lot of work it's going to run out of memory you know my this is a reasonably high powered machine it's still running all right so now we're at 10 let's take another short break uh and then when we come back we're gonna do spatial auto correlation we're gonna we're gonna skip the weights disparities notebook because it's relatively short and i think the the content in the other ones is it's going to be a little bit more compelling so we're going to jump directly to four after the break we're jumping straight to notebook four we're gonna skip uh notebook three on weights disparities we're going to jump right to spatial autocorrelation so thus far we've talked a lot about spatial data um and like fundamental data structures for doing spatial analysis and now we can pivot to some of the more fun stuff and start actually using those to do spatial analysis and look at some questions and what what we actually use some of these weights matrices for and the the heavy lifter here is the esda package from pycelle the exploratory spatial data analysis is the acronym so we're going to do our standard imports again pandas geopandas snumpy maplotlib libpysal contextally and the the key one here is esda i always forget how disorienting this is i can try and clear all the cells so that there's actually something happening on the screen when i'm going through this and it's not all all canned but i always forget when i'm going through these that the i have to crank up the aspect ratio so that i can only see like one cell or two at a time and i i can't see what's coming in the notebook and while i i know these narratives pretty well because i've done this as you scroll through there's still some some stuff that you missed so if i blast through any of these cells too quickly without describing just stop me and have me go back because it's easy to get disoriented in this in this format all right so we'll read in our data set again for the southern california region we're going to subset so that we're only looking at san diego county so again we'll do our our string slicing on the geoid so that we pull out county 073 which is san diego if we call the the info method it tells us just like pandas does how many columns how many rows what our date our data types are and then because we want to do an analysis of of home values we're going to drop any observations that are missing and then we'll convert to coordinate system three eight five seven so something that came up during the break was you know what what how do you choose these coordinate systems what is this code that you're using what is three eight five seven what does that mean um coordinates i i am not an expert in coordinate systems right that is an entire field of earth science unto itself um [Music] but if you want some background on coordinate systems and what is appropriate in certain places epsg.io is a good place to go find an appropriate coordinate system for your your region um and i i i'll just leave it there there are a few different systems that are worth being aware of in the u.s we've got something called the state plane system so every state creates its own projected coordinate system that that is good for using in that state some states have more than one california is big enough that it needs two to be accurate um and then another system that we've also used in in the workflow so far is the the utm zones uh and that's a that's a global system so that they slice the globe into columns basically and each one of them gets a number all around the globe and there's a north and the south and those are are often good um good coordinate systems to use there is a little trick in geopandas um geopandas has a method we're going to use it later but i'll just tell you now um there's a method called estimate utm crs on a geodata frame and that will look if if your coordinates are in if your data is in latitude longitude it will make an educated guess as to the best utm zone for your data so it's a really good first approximation for getting a good a good projected coordinate system it's not always perfect it's not foolproof but um it's a good place to start so i i keep using 3857 which is a web mercator it's it's good for displaying maps it's not good for doing spatial analysis but it'll it it will create a google map style graphic that that you're familiar with so that that's why i continue to use this one and these codes are shorthand um for all of these different regions of the world eventually you get to know a handful of them by heart 4326 3857 that stuff and i see the gis folks in here give me the nods yeah [Laughter] all right so just uh quick let's plot our data make sure these are what we think they are here are home values in san diego okay now again we haven't given this a a color scheme so it's just doing a straight linear interpolation on our values here to give the color and that is like we talked about before often not ideal so let's add in a quick couple arguments here scheme quantiles okay eight oops that gives us a little bit better sense right it's not a the housing price distribution is not not not well depicted by the straight linear interpolation okay here's roughly what it looks like we've got a right sku here let's make a little bit better map okay so this time we've added a title we've changed the color scheme to green blue now it's clear that we are looking at home prices in quintiles where darker values are more expensive okay in general looks like we would expect the coastal regions tend to be more expensive but one of the things you notice when you're looking at this map is that your eyes are drawn to these these different patterns that emerge right so it looks like there is this big block of of blue here and it looks like there is this this much cheaper area in the southern part of the region right intuitively these these look like patterns that we can observe in this map um but as i'm sure i don't have to tell all of you right our our brains can deceive us quite often and we may be lighting up on patterns that look like they're here in this map that may not be borne out in the numbers so understanding spatial autocorrelation is a way of making sure or or testing whether the patterns that we think are obvious in this map are actually played out in the the underlying data so i i think i tried to summarize that that paragraph i'll let you read it in case i didn't do it perfectly so to understand start exploring these patterns we're going to rely on our spatial weights matrices and then we're going to rely on this this concept of spatial autocorrelation so the first thing that we want to understand is is how similar are attributes of these tracks in space right are does it does it tend to be that uh the the value of a home is similar in observations that are close to one another right if we expect that the first law of geography is true then we would expect that nearby observations are going to be more similar than than far ones we need some way of quantifying that so first we'll create a queen weights matrix from our san diego track data and then we're going to rely on this concept of a spatial lag which is basically the the average of your your neighbors values right this is a a pure sum but we talked about before in the um in the the weights matrix notebook that often you you row standardize your weights right so each um each observation is is instead of a sum of a row weighted sum this becomes a an average essentially all right so here we are mapping the spatial lag of home values right so this is the average of your neighbors because we've already row standardized our data and this map magnifies the patterns that we thought we saw in our first map right if i scroll up and look at the raw housing price map and then we scroll down and look at the lag right we see basically the same signal but a a stronger one and as we said before right this tends to enhance the impression value similarity right it's a local smoother we are instead of plotting the actual value we're plotting the average of the nearby values right so we're sort of smoothing this data out in space and space let's plot these next to each other so we can see what's going on right the map on the right looks like a exaggerated version of the map on the left okay so it looks like our our intuition of the map pattern here is is probably right but we want a a way of of testing that we want a way of of we want to put a metric to that to that pattern and there are a couple different ways we can do that and that depends on sort of the nature of our underlying data so the first the first method that we're going to look at is called a join count and join count is useful for for discrete data so in this case we imagine that every value every unit in our data set can take one of two values right this is again it uses we always use a chessboard analogy so you can either be a black square or a white square and then we want to ask how often are your neighbors the same as you or are they different so if you are a white square how often are your neighbors white squares how often are they black squares the same we want to ask for the black squares right what would we do these join counts say how many of your neighbors are of the same the same kind and so we do this with esda and we're gonna have to fake this a little bit right because our data are continuous they're not they're not discrete so we'll grab the median and then we're gonna binarize our our home price to be either above or below the median so you either get a you know a one or a zero to fake that that black white checkerboard so we create a new variable called yb binarized y variable and we'll plot that so now we've got high home values those above the median and low home values those below the median and again we want to ask questions about black versus white in this map how often are white observations grouped together how often are our black observations group together if we've got 308 black polygons how many you know under an assumption of a random spatial distribution how many times would we expect that a black unit has black neighbors and the same true of of the white units and we do this with a join count so we create a new variable called jc and that is from the esda module the function that the submodule join counts and then the class join counts we give it our attribute yb that we created and we give it our queen weights object wq and now we have this join count object jc and its attributes will tell us how many times we observe these these joins so bb is the black black count ww is the white white count black white is the black white count right obviously if we count all those up those should equal the number of of joins in our uh in our data set luckily enough it does and so what we want to ask is does our observed join count for the black black statistic uh is that significantly different from what we would expect at random if if these black and white observations are scattered at random through the map and the way that we do this is based on permutational inference so we shuffle basically the neighbors and we say you could be neighbors of anyone and we'll simulate a random process and we'll recalculate these join counts and we do that a bunch of times and we build up a reference distribution and then we test what we've observed in our data against that reference distribution all right so on average we've got 478 479 black black joins which is quite a bit less than what we observe so our observed measure is up here in the 700s but our average from the simulations is you know 430 438 and you can see how tight this distribution is around our focal measure so it's pretty strong evidence that we've got a a map pattern here right it's more common in our data to have black black joins than we would observe under a random process and we can give you the the pseudo p values the simulated p values and this just from uh from this test and the p sim bb attribute okay so that's useful um but often our data are not just binary right we don't operate on a checkerboard uh often we've got something like home values right that are continuous so there is another statistic that we want to use called moran's eye and as you would expect moran's eye quantifies the autocorrelation in continuous variables in space and we do this exactly the same way we did our join counts we're going to create a new variable called mi from esda in the moran module the moran class and we give it y which is just our home value values and again our spatial weights matrix w q and that i capital i stores our moran's i statistic now again like we that's we don't know what that value means this is you know 0.67 autocorrelation of this is the the correlation between a unit and its neighbor so 0.67 that that's quite a strong correlation in space but we don't know whether that correlation is meaningful right we don't know whether it's different from a random process and we can test this again using uh permutational inference so on the right you've got the the um the slope so the the slope of this line is the the coefficient of your your miran's eye the the correlation between a unit and its spatial lag um right the slope of that line gives you your moran's eye statistic so that's a 0.68 and on the left you've got our simulated reference distribution that's what happens if we shuffle everyone's neighbors we recalculate moran's eye and we do this a bunch of times this is the auto correlation that we observed none essentially right and it's pretty tightly distributed around none but our data have a moran's eye of 0.68 it's pretty easy to reject this null right we fall way way outside of this distribution so it's pretty clear we've got a strong spatial autocorrelation signal in these data um [Music] so i think the answer is yes you wanna you wanna do these analyses just about any time you want they're called exploratory spatial data analysis for a reason right this is getting a handle for what your data actually look like now i think what you're asking is what the before you go about using a lag or like a formal spatial model do you want to make sure that you need it first yeah i don't want to trick myself and that's exactly what these are for right you don't need a complicated spatial lag econometrics model if you can do an ols model just fine if your data don't have any spatial auto correlation then don't bother going the fancy route you're just going to confuse yourself and again the psim attribute stores our pseudo p values we can see that this is clearly not a random process okay so this is the the global map pattern this is the moran's eye for our entire region right this tells us the um the relationship between a focal unit and its neighbors for the entire study region but we can also decompose that into a local version right because every focal unit has a set of neighbors and we can also compute that correlation so here we're going to do we create our spatial lag again and now we're going to net look at something called a moran local and this embodies what's often called local indicators of spatial autocorrelation or lisa analysis so now instead of a single i that describes the entire region we've got an array of eye statistics one for every single unit in our observation in our every single observation in our data center right that tells you how correlated it is with its neighbors right that's our global eye okay and these are the quadrants that our observations fall into again we can test for whether our pattern is random and now we're going to do a different kind of moran scatter plot so now instead of simply showing the the price versus its spatial lag we color the observations by the quadrant and whether they're significant or not so these red ones up here are significant observations that are high high so these are high home values surrounded by other high home values all right the opposite side you've got the yellows here that are low low these are cold spots and the really interesting cases are in the diagonals here that's where you have either what we call diamonds and doughnuts right you either got a diamond in the rough high value surrounded by low values or you've got a donut right a hole surrounded by lots of of rich sugary stuff and those these get to be are are really interesting cases for uh for trying to parse out what's happening in our in our region so like i said sometimes i beat the narrative because i can't see it coming so as you can see here we we basically distinguish these these four different kinds based on which quadrant they fall in this plot again we can use our s plot visualization methods here like i said before a local indicator of spatial auto correlation in in esda we tend to call it local moran in s plot we often call it lisa these are basically analogous and interchangeable terms for the for the same thing we're doing uh a local analysis of of marine coefficients basically uh and this time we can plot it so just as we thought before scroll up when we did our first cut looking at home values in the region we picked out this looks like it's probably a concentration of high values this looks like it's probably a concentration of low values and our lisa analysis tells us that that is indeed the case right those those red areas are places that are contiguous and have high values these are hot spots our blue areas are cold spots we have one low high here i guess we've got two so if you are sure three so if you're looking for a a bargain and you want to flip a house this would suggest that these light blue places might be a steal right these are places that are prices are deflated relative to their uh local context so maybe those are places where something is either happening that that changes them from the prevailing pattern in the region or they just something just hasn't caught up yet so like i said sort of the off diagonals become the the really interesting places to look at i don't think we have any diamond we're gonna get there um sort of and with the plot local autocorrelation function from s plot we can put all these in the same graphic that makes it easier for us to intuit so now we've got the the simple plot of home values on the far right the uh the outliers in the center and the moran scatter plot on the left that shows where the the actual observations lie and this sort of becomes the core of a lot of spatial analysis workflows is understanding these relationships for lots of different variables and then trying to figure out whether we want to model these processes exactly which of these are spatial spillover which of them are some unobserved process that we just want to remove first we need to explore all these data first any questions all right okay so now we're going to move into the very applied stuff we've gotten through data structures we've gotten through exploratory spatial analysis and now it's well how do geographers and spatial social scientists at large use these things to think about neighborhoods or neighborhood change or the composition of a city and how it evolves over time so we're going to reuse our same data sets now we're in the 05 geodemographics notebook this one and the next one should run fine if you're on binder it's the two subsequent to that that you're gonna run into trouble with okay so we read in southern california again instead of dropping our nands we'll fill them with zero let's say we only want to look at los angeles county so we'll slice that one out we'll create a queen data frame and then to make things simple we're going to remove catalina and coronado islands because we know that they cause problems because they're not connected right we saw in the spatial weights notebooks how those got funky but here we can use that component label to grab only the ones that we need right we know the the the fully connected observations are all in component zero so we're going to throw away components one and two that leaves us with just the land mass that's connected to the rest of the country okay we'll just do a quick explore on that no our islands aren't included anymore we've got a lot of data and a lot of columns here okay so geodemographic analysis is basically the application of cluster analysis to spatial data and trying to understand how patterns emerge in space so our clusters are often based on demographic and socioeconomic inputs the idea in in geography and in sociology where this this tradition comes from 50 years ago was that you know american cities by and large are really segregated along lines of of race and class and they wanted to understand one were those patterns consistent across places does chicago look like los angeles is it a core versus periphery conversation or is it east versus west and then they wanted to understand well are are these dimensions that tend to divide cities do they change over time or do are we sort of segregating ourselves along the same lines of of race and class everywhere in overtime and so geo demographics help us get at that we put a bunch of demographic and socioeconomic data into a cluster analysis we map those clusters and we ask how those patterns evolve over space and time so these are used all the time in academic research but they're also used all the time in industry so um there are products put out by folks like esri i think esri if if you do gis work you're probably familiar with esri a big mapping company um and they have a product called tapestry i think it's called where they basically do this and they then they create a product and they'll sell it because these these geodemographic classifications can be really predictive of people's behavior inside each of these units right so this is a little bit like spatial feature engineering if you want to put this into some other predictive model okay so let's define some columns we'll use income home values and then a generic set of of uh racial classifiers let's standardize those with the scikit-learn z-score and then let's fit a k-means cluster object to it so we're going to look for six clusters on these inputs that have all been standardized in the labels attribute we get back our our neighborhood types right so this is this is a bit like doing a a typology right we're asking for groups of neighborhoods that are similar along these dimensions that where um internally they're homogeneous along raising class lines but they should be distinguished from from one another along those lines we'll store that that labels attribute as a column on our data frame so that we can plot it and then let's make a a web map so here's how los angeles breaks out la county along lines of race and class now i i i'm using class really coarsely right class is a much more complicated concept than income and home values but i'm just abbreviating here um now we're this is a categorical map um we don't know what any of these colors mean just yet but if you are familiar with the geography of of southern california or of los angeles county you might be able to intuit roughly what these clusters correspond with these places along the coast these are going to be really really wealthy rich areas with really expensive homes right these are some of the most expensive real estate in the country we expect that's probably going to look quite a bit different from from rural los angeles and we know that that looks very different from the way that segregation has shaped its core so the next thing we might want to ask is well what does each of these colors mean and we've got our clusters we see where they are in space but what characterizes them and you would do this the same way you do any other cluster analysis so let's take our input data let's group it by its cluster label and look at the average values for each of our inputs for each cluster and often right what you're trying to do here is look for the distinguishing factor in these in these clusters right what is the what is the variable that sets this group apart from the others a nice way of looking at this is with violin plots in seabourn this is relatively easy to set up we create a a new subplot object and then we're going to iterate over uh our our different axes here each one we put a violin plot using our our input columns and we want to see how those are distributed right so if you're not familiar with the violin plot this is like a kernel density estimate that's been rotated and then mirrored across its axis right so the places where it's fat that's where you have a lot of observations where it's skinny you have very few observations there's i don't have a good automated way of doing that right now so the best way would be to map them with like a dictionary or something and that's doable it just takes some coding yeah i i didn't know if there was like i wish in case anybody here wants to create a killer package that'll do this mapping uh color mapping between different packages if you notice the violin plots are actually different colors it seems silly but whenever you're displaying it to people that don't do it they're like yeah why does it the blue means wait no green is right there yeah why is this not that so that's that's that it's silly but yep that's really important so these these colors right don't correspond to the color on the map you have to look at the the label here so five what what distinguishes type 5 from from this graph well it's got a much larger share of asian population than than any of the other groups here so where is five five is gonna be so like i said if you know some of these if you know the geography you can sometimes intuit exactly what you expect these colors to be because unfortunately our cities haven't changed very much we still tend to divide along income and racial lines so first we want to create the probably the violence then we need to map the colors from the num the cluster to the color and then add that do a table join to our uh to our data frame that would that's a great idea next year i'll include that all right so this looks like a reasonable first cut we've we've had some some folks that can recognize some patterns in here this may not be the best model so a good way of interrogating this a bit more is to dig into you know cluster analysis a silhouette score is a measure a metric that we can use to ask whether this this cluster fits our data very well it's in the scikit metrics module and we can calculate a silhouette score on our data here okay that's interesting but can we do better than that right let's let's crank up our k let's look for uh for more groups now there's a tension here right because what we are trying to do is summarize this place right summarize this geography into a set of categories that make sense if we include 600 categories that's not useful right that's then we might as well just look at each individual tract we don't get any summary information out of that but if we have too few categories and we we oversimplify the problem then that's not not telling us anything either so it's sort of a balancing act in in determining the k parameter that gives us some insight into the region that we're interested in but isn't so so precise that it requires a bespoke analysis of every single unit right this is not unique to spatial analysis this is just like any other any other clustering analysis you might be doing okay so again we're all just at this point it's just it's just cluster analysis and then we're mapping the results we can play with different clusters so let's look at affinity propagation another cluster from scikit and we'll plot the results here we're using a little interactive plotting library called hollow views specifically geo views and hb plot um and what's nice about this is that the unlike explorer that we used before explores really fast and it uses the same signature as as plotting does in in geopandas it's really quick to get to give yourself a interactive map but hb plot will link these two together so that when we scroll in on one we also scroll in on the other and here we run into the color problem again even probably even more egregiously because the map patterns on these two maps are the same right the colors are different these were created by two different clusters so the label is different and the color is different but the patterns that the the clusterer is finding they're largely consistent so we need some way of a meta model to map all of this stuff all right so this is interesting this is a good way of summarizing a place is a good way of getting to know um sort of the socioeconomic geography of a place if you're unfamiliar with it but so far we haven't included anything spatial right this is just scikit that we're plotting we're just attaching the results to a geodata frame and plotting them and basically the all of the spatial autocorrelation just sort of falls out right we we see these groups of of discrete values that that cluster together in space so clearly there are some spatial patterns here if we think back to the other notebooks what kind of method might we use to interrogate whether these are random in space it's not in this notebook but because these are discrete data we could use something like a join count statistic for each of our input classes to figure out whether each class uniquely is different from a random process another way we can incorporate space is to force it as an input to the clustering process right so the way we've been looking at it now is create us some clusters and then we will analyze the results from a spatial framework alternatively we could say okay find me groups of observations that are the same but they can only be assigned to a cluster if they're contiguous so this is regionalization find me groups of observations that are the same but also are all touching one another and to do that we need a weights matrix right that's what defines the contiguity between our observations so we'll create a queen weights matrix we'll do another cluster solution this time we'll use ward this is cheating because [Music] the ward clustering in um [Music] in in scikit allows you to to specify a connectivity matrix right inside the the method so i'll show you this in just a second so first we're going to create the a spatial version this is the geodemographic typology here here are our ward labels our hierarchical clustering labels again we see lots of the same patterns but now we can fit essentially the same clustering method but we use the connectivity argument and we're going to pass it our spatial weights matrix so now find us groups and cut this tree but make sure that our connected observations have to stay connected and if we plot these we get we get very different cluster results if we just sort of scan across our values this is the a spatial version that doesn't force contiguity and this is the spatial version that does and if we compare those side by side we recover some of our same patterns but they they're different so we wipe away all of the variation in the northern part we that all gets grouped together maybe geography washes out those internal distinctions and then we get a much clearer signal about sort of homogeneous neighborhoods so to speak in in the denser part of the region and again if we scroll in on these because they're linked we can compare these now it just so happens that this color is largely the same is consistent right but these two methods are both picking up on this really strong boundary right here in inglewood there is something unique about inglewood that distinguishes it from its broader context i love england my best friends live there but we've also added a constraint here um our silhouette score for our spatially constrained model is in the negative which means often our observations can be put in a cluster where they will be more similar to one another but we did that by design right we we have forced a instead of putting you in a cluster with the best fitting other members we put you in the cluster with good fitting members that you're connected to right we're asking a different question of this right we're looking for homogeneous regions not just neighborhood types so our silhouette score is always going to be lower for spatially constrained clustering than it is for an unconstrained solution right so i've just given you the answer oops okay well the reason that that we think about this is that you know what is what is a neighborhood um this is a a problem that social scientists wrestle with all the time if you want to understand neighborhood effects right how how your neighborhood influences how much money you make later on in life we need some way of operationalizing what your neighborhood actually is we can take it as exogenous right it is the census tract that you live in but that might not be a very good definition of a neighborhood and that may lead to that that mis-specification of the appropriate spatial scope may lead to incorrect inferences later down the line and this is also important because like neighborhoods mean something in places right neighborhoods often have names they they were created through a a historical legacy of architecture and development in these cities and they often mean something this is chicago right chicago has has a very famous set of neighborhood names right people identify very strongly with the neighborhood that they live in in chicago the same is true of pittsburgh it has well-defined neighborhoods and so one of the things that we might ask is well how strong of a signal do we get from these neighborhoods today if if these were the relevant boundaries that that delineated pittsburgh a couple hundred years ago if we ask to find neighborhoods socially homogenous constrained clusters in in these cities do we recover a similar pattern well let's try it so first we'll create some rook weights in los angeles and this time we'll ask for 36 clusters instead of five and we'll let's let's get some neighborhoods and plot them and now we start to get you know a more refined spatial signal but now each one of these neighborhoods now we've got 36 of them now each one of them is going to take some actual looking into to understand what they mean right they can't be summarized at a high level very quickly but they tell us something very different about the economic and and ethnic neighborhood structure in los angeles is the same thing yeah so this is the interactive version you know what's what's interesting is that there are some you know neighborhoods that are entirely surrounded by others right so so torrance shows up as its own spot that's completely surrounded by this other orange type in in pv the same is true of el segundo we might just be picking up the tribe called quest legacy here i'm not sure but these are two different ways of using cluster analysis and spatial analysis to try and understand what's happening in in regions after this we're going to look at segregation and then after that we're going to look at how these these patterns evolve over time which i think is what you're asking about so i'll leave you a couple minutes to do these two exercises sure you could use just like with one variable i'm not sure if i follow the distinction instead of being okay to these smaller neighborhoods you create your own polygons based on your supervised model and then create polygons interesting so what you're saying is you so imagine your data are at a lower geographic resolution so instead of aggregate tracks we've got actual housing units and we might want to also divide those in space we could do that um ultimately what we would end up doing as i mentioned before you can technically create contiguity weights on points what it will do is build a set of intermediate polygons in between so it creates a voronoi diagram tyson polygons depending on what tradition you come from and that will create an adjacency structure between your points so we could use the input point data cluster on that include a contiguity constraint with our weights matrix we could also i mean this is a good example because we want well-defined neighborhoods so we it we use contiguity weights on purpose but we could also use say distance band weights um and in that case there would be no change whatsoever to the input data right if we want to use input if we want to use um observations at a housing unit level with a distance band weights we could do exactly this without changing a line of code basically supposed to be i think it only counts as a joke if you used to listen to laughs oops again if you want to jump to the solution you can uncomment that cell and run it all right i'm going to keep going because we've only got 50 minutes left and i got dense notebooks left so i'll move relatively quickly so thus far we have only used really pycelle and then bits of the other pi data ecosystem right so scipy's mixed in there second learn's mixed in there and now we're going to use another package called geosnap geosnap is something that we've been working on for about the last four years or so four or five years um it started with um an nsf grant to dig into some of these issues that we thought were important and now it's gotten some continued support from the bill and linda gates foundation and it's it's under heavy active development so some of the stuff that i'll walk through um today is is well documented in these notebooks but i still need to update a lot of the the tutorial documentation online so the api docs for all of this are current and they're fine if you go look up the api docs if you go look up the um the tutorial examples they use an an older api that will still work but it's not the one that we're going to go through today all right okay so instead of using pycell now we're going to use geosnap and the first thing that we will do is import its generic datastore class um we only really put this together so that when we're teaching workshops we can use a shared data set um if you are everyone's running on the shared cluster for example um we don't need 200 different copies of the data we just need the one that everybody can access um but here just bring the class in and then instantiate it and then we'll have access to lots and lots of built-in data sets that geosnap provides um we're going to use the get acs function to grab ourselves some data from the census american community survey and then it's got two functions cluster and regionalize that are basically analogous to what we just looked at from scikit okay and then it's got um two visualization methods plot time series and animate time series okay so the data store class is is just a thin basically wrapper around file paths that that fall back to s3 so we have basically compiled a bunch of commonly used data sets in spatial analysis especially for the spatial analysis of inequality which is is what my research team focuses on so we've got um block block group and tract geographies uh you know boundaries for several different time periods there's access to 200 or so variables from the american community survey there's stuff from the environmental protection agency on things like ozone concentration and stuff that that they collect from their sensors um and then there's there's data from the longitudinal employment household dynamics for looking at so where for instance uh regions cluster over time that sort of stuff so um it's it's a pretty simple data class and we're going to use that setup to grab data for chicago so we use the get acs function we pass it that data store that we just instantiated we ask for the county fips 17031 which is cook county we want the tract level we could there are two different spatial resolutions we could go tractor we could go block group block groups are a little smaller but they have fewer variables available right the census bureau won't tell you everything they know about block groups because it can be potentially identifiable so you lose columns when you go down to a finer spatial resolution we'll stick at the track level that gives us a broader set of variables to use and then we want uh all the years between between 13 and 17. all right great and this is going to return a long form geodata frame we've gone back we went back and forth several occasions on the best way to store this and i think this is the good compromise that we've found so because this is a long form that means technically the geometries are repeated for each time period that you have right that wastes a little bit of memory so this is why you may run into issues if you're on binder because we're storing all those vertices more than once we went back and forth on whether we should store one copy of the geometries and join on the fly that actually added more overhead this is more efficient but if you have thoughts on better ways to do this i'm all ears so instead of a single set of census tracts for chicago like we looked at in the last notebook now we've got 13 14 15 65 years of data all in one geodata frame together so if this doesn't work for you i think this chicago data is somewhere in the is it local okay geopandas did i did i not screw that import wrong at the top it should be import geopandas as gpd is that that's the fallback to reading the local version yeah okay so okay good it's commented there so that we can run these through if you have all the data sets available the way that this dataset class is designed to work is it will look to see if you've got the data locally on your machine if you don't then it will hit the s3 servers and download them it usually is pretty fast but if everybody were here doing that and nobody had the data on their machine it would slow things down a lot so that chicago data set i wrote that out there just in case we end up in that situation or if we don't have good internet where does it look globally i'm just confused it uses a little package called apters that tries to intuit the right place to put something on your machine given its architecture so on a mac it usually puts it in library application support geosnap in linux it puts it in the dot local directory i think in the user direct i'm not sure where it puts it on windows but it's not going to find this one right it's not looking for that chicago data set i put that there just just so that it's a fallback any reason why it might not work get acs is not working at all so it's trying to collect data from the web that's why it's taking forever so if i would just cancel that kill that cell and read the data in locally from gpd read file great question so uh in this example we're only going to look at american community survey data and those are all stuck in the 2010 census boundaries but my folks in the census bureau here know uh with every decennial census uh the census redraws its boundaries so there is no one-to-one map of 1990 to 2000 to 2010. so geosnap has a harmonized function or if you've got a long form and they have different variables you pick which one you want to represent your data set and it will squash them down into that same rip it'll use aerial interpolation basically to move you over under the hood that's using pysel's tobler package so that that'll give you more information on what that's doing but because geosnap is designed to look at time series data it's it's got that built in we don't have to use that for the data that we're using so well i uh if anybody knows better than i do please uh please answer this as far as i know the um the geometries for the 2020 census are available um the variables for the acs 20 what is it must be the 15 to 20 or 2016 to 20 period um they are available so i think somebody asked me this earlier about senpai and the the right way to get yes okay so geo snap takes a different strategy because it recognizes that querying the census servers takes too long and instead what we do is every time there is a new year available we go and grab all the data from their servers at once compute all these intermediate variables and then we send a backup to the cloud that that is having a hard time resolving now um so i i do that manually basically as a once a year process to make sure that these data are always in good shape and i just curate that manually myself because that's easier the census hasn't put out the 20 20 estimates in its demographic profile that conforms to the same way the rest of the prior releases have so all the tooling that i built to do that can't operate on it yet so i think the variables are available if you use something like senpai they won't be available in geosnap for a little while until i know for sure what what that looks like yes those are only five years again if you want um you can pull directly from the census servers using something like senpai um you know this i just realized who i'm talking to um but yeah right right now we're only using the five-year estimates because those give us the the best set of variables that we need okay so we've created our data set for chicago it's got four years of data and we want to plot this quickly um geosnap gives us the plot time series function which is useful because it's going to standardize these legends across space if anybody looked at the thread that i sent in the slack channel about user-defined classes what what brought that up was that somebody wanted to compare two maps next to one another uh and if you do a choropleth on one map you do a coropleth on the other map it's going to optimize the the binning schemes for both of those maps so when you try and compare across maps the color doesn't mean the same thing with plot time series it standardizes those bins over time so it's clear the colors always are consistent when we're looking across time now these this is still hard to see exactly what's happening here um because you have to look over and across at each map so we've also got a little function called animate time series and this will this takes your data frame it takes the same signature as a plot but then it it takes a file name argument it'll it'll write out a gif and this takes a little bit because it's got to plot them individually then squash them together but this makes it easier to see once this is done it's going to display each image in the notebook but then i just clear the outputs usually because you don't need that one we've written this file out to chicago income change and we can use the notebook machinery to display it so now it looks like this now it's a lot easier to see what's happening so it looks like all right this is this is income data looks like this region is getting a little poorer right starts dark and it lightens over time same looks like it's probably true up there now this is a toy example we're using overlapping acs estimates which you're not really supposed to do this is just to to mock up if you've got good time series examples so take this map with a grain of salt right part of what the change that we're observing can happen because of sampling right because these we've got overlapping five-year samples but this is still a good illustration of what the software is designed to do all right so we're going to do a geodemographic typology again just like we did in the prior notebook but now we're going to do it across the whole time series so we define our columns the same as we did before and let's introspect what this cluster function is going to do the signature is a little bit different than scikit so it takes a geodata frame it asks for the number of clusters that we want the the method that we want to use so whether affinity propagation or ward right under the hood this is either going to pass things off to scikit or if we're doing something regional it's going to pass it off to sp opt and then it needs to know what your unit index that identifies unique observations and it needs to know what your time index is that that identifies unique time periods all right so we'll create a another hierarchical clustering solution on these data but now it's over time okay fast enough and we've got this time we don't have to create the the label column ourselves right geosnap expects that you want to do spatial analysis right away so it attaches the columns to your data frame if you want the modeling object back then you can say return model right otherwise it assumes that you just want the data frame again we can plot this and animate it let's see if i can display this i think i wrote it out yeah i did okay good okay so now we've got this solution applied to all of our time periods and what's important about this is that we we have to pool all of our observations and fit the cluster model to all of them right we can't fit a cluster model to each time period and then look at changes because then the the meaning of those clusters is going to change for each solution right so we have to pull all the data together and then break it back apart by time so now we're looking at neighborhood change multi-dimensional neighborhood change because these clusters are composites of several different variables at once and how chicago has changed from 13 to 60. this may be surprising to folks or it may not be but neighborhood change despite the attention that it gets is is the exception it's not the rule neighborhoods by and large tend to be very stagnant as i said before if you want the model back instead of just the data frame then you pass return model that's if you want to dig into more of the diagnostics if you want to see exactly what was happening what were the input data that will give you back the scikit-learn clustering instance you have a question i was asking if it's going to turn on a second yep yep so what it returns is a model result class which is something that we created but one of those attributes if you if you call the instance attribute of the model results that gives you back the cluster that was used to fit so that's that's how you can get introspect your actual model results if you want to get some clarity but this this object that it gives back also has some useful properties itself so shy model is a a model results class that gives us the data frame the columns we used the spatial weights matrix that we used as to to create the solution if there was one um and then it lets you back out the the fitted model instance if you need it now we put that there because now you can quickly calculate silhouette scores on this solution we can take the average silhouette so this is the the aggregate cluster fit for this uh this data set over time it's about the same as we had before we could also ask whether the model is is good across every time period or whether it fits some time periods better right because we had to pull all of our data before we could fit the model instance if we group by year and look at the silhouette score you can see that it looks like that it's pretty consistent over time that's good if we had the model that didn't fit well in certain years then that would suggest our model is not very good at telling us how this place changes right if your cluster assignments are all all over the place in your final year you didn't very do a very good job of defining what those assignments were all right then we can also look at neighborhood change so we want a way of quantifying what we saw in that animated map of those uh those neighborhood types changing into and out of different colors over time um so to get at that we've got a the model results class has an attribute called links that's the local index of neighborhood change um and what the link quantifies is basically the the share of times that a unit is classified with the same neighbors so it's how often do you find yourself in the same neighborhood classification over time so it's the the intersection of your neighbors over the union of all possible neighbors that you've been assigned to over the time what this will show us if we plot these high values are places that have changed a lot low values that are places that have not have changed very little and this we recover largely what we saw in our animated map right that when change happens it's in these yellow places that's not it tends to be disperse but much more common is are these cold colors right where things are not changing especially in these large swaths okay and if we start building all these skills together right now we've got a new data set that we can calculate a say a moran's statistic on or or a local moran statistic so is this pattern random these these quantify neighborhood changes but is the the place of neighborhood change random so this is interesting most places don't change the places that do change tend to change a lot and let's do a lisa analysis let's do some local indicators of spatial autocorrelation so we'll create a queen weights object and we're going to fit a lisa analysis to the local index of neighborhood change scores the eyes attribute shows us what the i statistic is for every unit so if we want to plot the local autocorrelation we can assign that i attribute back to our data frame and plot it so these are hot spots of neighborhood change it's how often you are classed with this how often basically your neighbors are in the same group now that that could be because you're always in cluster five and your neighbors are always in cluster five or it could also be because your neighbors are all in cluster five and then those all of those neighbors also were classed in the neighborhood three with you if everybody moves at once then you've got no change because it's not based on the cluster label it's based on who's in that cluster label with you i have a question about link interpretation if it's one it means that that means its neighbor set was different in every time period yes okay so it has changed every single year like in our data set here yes and that's so yes probably um alternative it could be that its neighbors have changed and it has stayed in the same class and its neighbors have transitioned out of that class so there there are two dynamics that can happen here right it's it's a set membership statistic it has nothing to do with what cluster you're in and so if we do some local autocorrelation on these data we see exactly what we would have expected these places in in the south side of chicago almost never change they are these these cold spots of change they're unlikely to become anything different unless there is some external intervention by contrast there are some places of high high change but they're small and dispersed right they tend to be over in the gentrifying parts of the city if you're familiar with chicago okay to give us a little context we can drop this on a base map and so our blue areas are places that are really stagnant over time our red places are ones that appear to be changing and the orange places are ones that have themselves changed rapidly when their neighbors have not changed very much so this is when i talk about sometimes those off diagonals become really interesting orange becomes really interesting in these maps because these are places that are changing relative to their context that's not so this is a bit like identifying we don't know what direction this change is right one one answer is it could be gentrification this could be neighborhood downgrading we don't know what direction the change is happening but these are probably places that we should pay attention to all right so those tell us where change is happening but they don't tell us what kind of change right we all we know is that this unit has different member different neighbors than it has had in in prior time periods but what does that mean well to understand that we can build the full transition matrix how often does neighborhood type zero become neighborhood type one how often does neighborhood zero become neighborhood type two so we basically we model this as a as a markov process and then we condition it based on different neighbors so do your transition rates change if you're surrounded by type four do your transition rates change if you're surrounded by type five what if your neighbors are all type zero and so we end up with several different conditional transition matrices that are all different so a couple different things happening here at once one the fact that all of these transition matrices are different is pretty strong evidence that there is a spatial process of neighborhood change if there weren't everything would look like the global transition matrix but your rate of change and how you change is going to depend on what's happening around you the pysol package giddy is where we get a lot of the the underlying algorithms for this are built so this is a spatial markov analysis where we look at how these transition rates change under different conditions of spatial context these can also be used if we dig into the weeds here we can start looking at okay well what does neighborhood type 1 mean what would we classify that as maybe it's you know wealthy and diverse how often does that become type 2 which is low income and segregated probably not very often right but if we start looking and parsing these different transition matrices we can get a much better handle on when these kinds of change happens happen and in which directions and we don't have time to get into all of this in this example but that's the utility of of this this method here and then finally because we've got this this uh markov model of our process we can use it to predict what neighborhoods are going to look like in the future so the way that we do this is we say okay for every unit we know both your label in the prior time period and we know the label the modal label of your neighbors so we know which transition matrix to draw from if we think that this this model is going to continue into the future and we will basically make draws from those conditional probability matrices to simulate what your neighborhood label is going to look like in the future so here's chicago up through 2021. we can do if we wanted let's do oops let's do 10. now time steps and increment here are there there are two different arguments because one of them refers to how many time steps you want to simulate and the other is what your time step represents right here we're looking at acs data which are released annually so our time steps are like one year but if you had decennial census data you might want to simulate five time steps but that's going to be 50 years right because it's decennial data it's going to take a minute because now it's going to plot several more images how many did we do 10 yeah i should crank the resolution down all right while we wait on that i'm gonna move to oh i'm just kidding okay and here's what chicago looks like through 2026. we can use this to start thinking about how we would make investments in place and if we expect that some places are going to continue to be abandoned and those are good candidates for things like block grants if we know that some places are changing really rapidly then those are places that we want to invest affordable housing to make sure that people aren't priced out and they get to stay in the neighborhoods that they've lived in these these models are a way of helping us intuit what future conditions are like in the what things are like are likely to be how am i going to say this these are useful for helping us intuit what conditions are likely to be in the future and that way we can be better placed to make the right investment in the right place all right all right home stretch so now we're going to look at segregation and the unique thing about the segregation indices that are part of the pi cell segregation package is one that they are fast um i don't know if you are also our folks but there's a fantastic um there are several fantastic r packages for measuring segregation um they're nowhere near as fast as ours um but more importantly unlike any other package that we know of there's much better support for spatial segregation indices in um in the pysaw segregation package and i'll show you what that means in just a minute so we'll read in our california data again we're going to compute hispanic and latino segregation in southern california so we'll drop any observations where we don't have data all right and we're going to look at whether segregation is different in the coastal region versus the inland region so the first thing that i'm going to do here is create a set of county names that's just going to be a lot easier for us to deal with instead of these fips codes so create a new new column called county that's the five-digit slice like we've been doing before only now we're gonna replace those so here are the county fips codes here are the names of those we'll zip those up into a dictionary and we'll use that dictionary to rename the column to rename the values of that column so now we've got an actual set of column names a set of county names in a column and we can use that to split out our two regions so the coastal region is la or in san diego and ventura versus the inland region of riverside san bernadino and imperial if we plot these right they fit together like puzzle pieces the one on the bottom fits in on the east side there okay so we've got two two different data sets well let's calculate some segregation measures so the first thing is there are a lot of different segregation measures to choose from um the pysol segregation package implements over 40 by my last count and you know a segregation measure is designed to capture whether people of two different kinds where kind can be high income low income it can be people of different races it could be different ethnicities whatever what have you where they share the same space right and there's lots of different ways of quantifying whether they share the same space or not so three commonly used indices are the dissimilarity index the genie index and the entropy index we'll calculate two different versions we'll look at hispanic non-hispanic segregation and and black non-black segregation in these two different regions and then each class in segregation has a statistic attribute that holds the actually estimated segregation statistic it's very fast absent any context it's kind of hard to know what these numbers mean whether they're large or whether they're small often what you want to know is whether segregation has been increasing or decreasing in a certain place or whether one place is more segregated than another but according to these the dissimilarity indices show that the black population california is more segregated than the hispanic and latino population the reverse is true according to entropy all right like i said it can be useful often you want to get a quick handle on what's going on now this will definitely crash your binder notebook um so we can compute all of these segregation indices at once because you may not know which one you need and this is pretty fast some of these especially the density corrected dissimilarity index some of these are really computationally intensive they require like draws from a random population distribution and such um so the fact that we can compute 27 this quickly is a is a feat i'm not two to my own horn it's a i don't you know we didn't write much of this optimization code this is a an ode to the pi data stack i think this proximity profile takes a very long time i think you all probably have the results still in the notebook if because i didn't i didn't clear the one that i pushed up so you can look at those already is anything noted everybody notice anything interesting about these results anybody want to guess why we have a nan for boundary spatial dissim i left that in there on purpose because it's a bug the boundary spatial dissimilarity index is not well defined when you have multiple perimeters um our data include these islands out here so calculating the perimeter area ratio when you have multiple perimeters and areas is not something that the function can do just yet so i left that in there as an easter egg for myself to make sure that i go back and fix that we can do the same thing for multi-group indices so now let's calculate the the four group segregation measure for the region again we have lots of different measures we could use multi-group information and we can batch compute this one's much faster everything you could want to know about segregation in southern california this is where things get interesting so um the question is how does space factor into this so typically segregation indices take as given a census enumeration unit so what matters is whether you live in a census tract with other people that's the the boundary whether you live in the sense attract or not that's a questionable operationalization of space it may not be a census tract that matters sometimes the census tract is way too big like out in the rural areas sometimes it's way too small like in the dense new york city you know a census tract is a block or two and your neighborhood the way that you experience the city is quite a bit larger than that so we allow you to specify how big a neighborhood should be when you're calculating segregation statistics by using something like a spatial weights matrix so if we bring in a lib pycell weights module and we want to calculate a spatial dissimilarity index we could do two different versions um we can do one based on a queen matrix and one based on a distance band matrix so first let's look at spatial dissimilarity using queen neighbors and then let's look at it where your neighborhood is the 2.5 kilometer radius around the census tract don't do that right yep so if we compare the queen based version to the distance based version we can see that once you account for a larger amount of space your segregation index decreases because you can interact with a greater chair of the population if we increase your notion of neighborhood to be to be much larger the other thing that we can do is some segregation indices were formulated explicitly to account for space and others were or account were designed to account only for for group membership so any of the latter can be turned into a spatial version by incorporating a spatial weights matrix and a spatial lag basically we transform the input data to an accessibility surface instead of just the people in your neighborhood in your census track that you have can interact with you can interact with anyone inside your your weights band possibly subject to a distance decay so you're unlikely to interact with folks that live further away from you even though it's still possible and this is a much better sort of conceptually realistic notion of what segregation actually means how separated are you from someone discounted for distance so anytime we have a spatial implicit index like the dissimilarity index we can pass it a distance that defines our neighborhood unit and a function that defines the weight that we want to apply and recalculate it so want to compare these two so that segregation is quite a bit higher if we're limited to only the census tract versus if we include the ability to travel outside of your census tract right that that measure falls from 0.48 to 0.41 142. you do the same thing with multi-groups and what were what we just illustrated up here right this this falling once we account for space that's important because segregation can manifest at different spatial scales so this is a graphic i stole from a really great paper by some sociologists and some geographers so on the far left we've got a macro segregation pattern right big central tendency on the right hand side we've got like a checker board right still highly segregated environments just at a micro scale and on the right we've got sort of a blend between the two right and cities are on a continuum of this scale right sometimes there are cities that are people inhabit wildly different parts of the region and sometimes you have these little pockets of neighborhoods that are really homogeneous internally but they're all stuck together in you know a big urban fabric one way that you can quantify this is with a multi-scale or segregation profile the way that this works is you calculate an index based on the the groups that you have so census traps and then you move space out a little bit and you calculate it again and you move space out a little bit more and you recalculate and you move space out again you recalculate then you plot those each time you go out further your segregation statistic is going to fall because you're accounting for more of the population so if we define a range of distances say a half a kilometer and we pass those to the multi scalar profile function we tell it which index we want to calculate in this case we'll do the the multi-group information theory index we'll give it the same population groups that we've been looking at and then if you plot that line the slope of that line tells you the difference between macro and micro segregation if you've got a really flat line then the region is characterized by macro segregation if you've got a really steep line then the area is calculated is is characterized by micro segregation right your segregation falls really fast as soon as you get outside your your um your your highly local neighborhood and so we might want to ask whether these profiles are the same for the coastal region and the inland region so we'll compute a segregation profile for each of them and then plot them on the same graph so the blue line is the segregation profile for the combined region the yellow line is for the coastal region and the green line is for the inland region so it's pretty clear here that like we knew before segregation was is quite a bit higher along the coast than it is in the inland region but if we look at the difference between the region as a whole versus the the coastal region we can see that it's probably the case that that micro segregation in um [Music] in the coastal region is is driving this bump that we see at the at the larger scale all right and then finally we have local measures instead of an aggregate statistic that tells you how segregated a region is these tell you how segregated each unit is in that in that region and what's kind of useful about that is then you can plot them and this will give us a sense for not just how segregated los angeles is but which places in the region drive most of that segregation so blue colors mean less segregated red colors mean more segregated this is a brand new index this was just put in i think they published this not quite a year ago maybe a year ago um so these these numbers quantify the the distance that you would have to travel to experience all of the diversity that this region has to offer so what that's saying is here in this in the red part of sort of the egg yolk here people that live here experience the most segregation of anyone in los angeles because they have to travel quite a bit further than anyone else to actually encounter the diversity that the region has to offer so another way of thinking about that is much of the segregation in the region is driven by what's happening in south central los angeles we can take a look at that on slippy map as well all right finally sometimes you want to know are these patterns random um this is sort of an interesting question in the social science literature because sociologists will tell you it's not random we know it's not random people segregate into segregated communities so that any test of randomness is not useful and that's fair but we can do it anyway in the same way that we do uh tests for local spatial autocorrelation right we use random permutation to ask whether uh what the statistic that we observe differs from a process that's completely spatially random now that is of limited value when there is just one statistic right if you're asking is los angeles segregated or not the answer is almost always going to be yes p value of essentially zero here and we can plot our blue reference distribution really tightly centered on zero right this is simulating a null complete uh complete spatial randomness versus the segregation statistic that we observed right so it's pretty clear that these places are are significantly segregated where it gets quite interesting is when we're comparing two different measures right so if we want to know does is inland california more segregated than the coastal region or is california in 1990 more or less segregated than it is in 2000 or in 2010 and then we can basically apply sort of the same logic here where we pull the data um we use either bootstrap or random permutations to come up with a reference distribution and then we ask does this does the change in this process is that the difference in segregation that we observe between these two time periods is that different than what we would observe on these same data if they are reallocated to these two different groups at random so we pool all the data we re-partition them we recalculate the segregation we take the difference we build up a distribution of those differences and we test our observed difference against that so here we're asking is coastal segregation higher or lower than inland segregation and the answer is definitely yes statistically the coastal region is more segregated than the inland region right and we are just at time that leaves one notebook on the table but that one is largely batch computing and showing some interesting examples of uh other segregation analysis so it extends this but you've got all the fundamentals that you need to work through that notebook if you'd like to do that in your own time and if you have any questions i would be happy to take them so i i would say i'll put this book in the there we go i'll make sure this is in [Music] the chat here [Music] um that book i think would be the the prime place to start um part of the reason that we wanted to build this package is that this knowledge is spread pretty diffusely across several different disciplines right understanding segregation and spatial analysis is the domain of sociology and economics and geography and public policy and urban planning and each of those disciplines tends to have its own insular tendencies so part of the reason that we wanted to build this package was to bridge all of those different perspectives so there's there are not a lot of good resources that already pool all of that stuff the best place that i could point you to would be the geographic data science book that serge levi and danny are writing if you are also our folks in here kyle walker has a similar book um i think it's called analyzing census data with r it covers a lot of the same topics but in in r and so it's a slightly different treatment on a lot of the same subjects that would be another good resource geocomputation with r um alex and chris um i can drop some more resources in the slack as the week unfolds you bet all right thanks folks [Applause]
9,Interactive image analysis with napari,https://www.youtube.com/watch?v=vismuuc4y1I,uh hello everybody welcome to today's session um we put the link in the slack channel for uh the tutorial but if you didn't find it it's just here um so that contains the website with the instructions for the exercises we'll be going through today um if you haven't already set up your environments um there is a sort of pre-workshop setup instruction um definitely take a look at that but we'll start out today with sort of a short lecture just introducing napari as the project but then also some of the key components as well before getting started uh dragon i will introduce ourselves so i'm kevin i'm a postdoc at the eta store where i develop software for image processing also i am a core developer of nepari which is the tool we'll be using today hi i'm draga i'm also a co-developer on napari and i'm a phd student in melbourne australia and i'm working on cell tracking stuff hence my frequent usage of nepari so yeah just to get to know folks a little bit it would be great just to maybe a show of hands for if any of these things resonate with you so i am a data scientist some data scientists how about a biologist awesome okay also into biology um how about uh i do sort of uh geospatial uh geology stuff awesome some other type of engineering physics cool um excited to be at sci-fi all right cool well uh that's great so napari uh so a lot of the examples and sort of the discussion we'll have today um will be sort of with applications with an eye towards biology um because drug and i both do a lot of bio image analysis but we really hope that nepari can be a great tool for people doing image processing and all sorts of domains so we'd love to hear questions or comments from you um about things that you would like to be able to do at the party that maybe you're not sure if you can um or if things aren't clear because we're using jargon from biology so please feel free to stop us at any point and so today as i mentioned uh the the rough structure will be um we'll give an introduction to the project um to the nepari viewer um and then we'll have a series of hands-on exercises in which we'll ask you to pair up and then work through the notebooks and we'll sort of walk around and make sure folks are getting along just fine and then you can expect today to do a bit of interactive analysis combining nepari with jupiter notebooks segmenting nuclei and we'll learn what nuclei are in a moment using a plug-in and then also draga will be helping you make your first napari plugin so first a bit about the project the motivation for us is we really wanted to have in python sort of a fast tool that allowed us to interactively visualize our images and data sets while we're doing the image processing because there's so many great tools as i'm sure you're aware because you're here at scipy in the scientific python ecosystem we wanted to make sure that we could seamlessly interact with those tools and integrate them into our workflows we wanted to be able to support large data so arbitrary large in this case just means too large for memory um and possibly remote data so if you have the data and a bucket and a cloud somewhere we want to be able to access it interact with it analyze it because we're aware that lots of folks want to do lots of different things we want to make sure that nepari is extensible and customizable um so really we think of it as it's sort of this core viewer tool so that when you're developing your analysis you don't have to reinvent the wheel in terms of making sort of the core visualization components you can focus on bringing in the great analysis you do and share with the community and speaking of community sort of core value of the project is we want to be an inclusive community driven project and this is really only possible because we're building on top of the shoulders of many great scientific python libraries like vispi which we're using for the rendering and visualization numpy and das which support the arrays tsar which is some common format that's used for uh on disk storage and the cute library which provides a lot of the gui components so next we'll go through a few different ways that people interact with nepari um so one way is by loading images in and then using the plugins to actually do your analysis or exploration in the gui this is an example of a plugin from carson stringer at genelia and she wrote a segmentation algorithm that allows people to identify the individual cells in microscopy images and so this is bringing a deep learning neural network into nepari she's created a nice user interface that allows users to choose their settings choose models and the great thing about this is you can try a setting see what the effect is and then iterate another mode and this is more of what we'll be doing today is what i like to call interactive analysis um this is where you sort of side by side are doing something in an interactive computing environment like a jupiter notebook seeing what the result is in the viewer going back making a change and i find that's a really nice way to a gain some intuition for how the different operations we apply to the data affect the data but also as a way to sort of do human loop quality control to make sure the things you're doing are doing the things that you think they are um so this is just kind of a small toy example but you can fire up a notebook and we'll do this in a few moments here together and you can load in your images [Music] so you can load in your images and then pass them to the viewer and that allows you to then see the image in the viewer which is the window on the right on the right for you as well yeah um the cool thing is you can then take those images um in this case we're using psychic image um to blur the image a little bit with the gaussian filter and then you can add that image to the viewer as well it has a new layer so just like tools like photoshop and illustrator we allow you to layer your visualizations so that allows you to customize how they're viewed and then also turn them on and off so this case we put our blurred image on top of our original image and this would allow you to for example go back and forth and see what changed in another usage mode and this is really a sort of maybe more advanced application but because naparia is written in python you can write scripts that bring together lots of different tools so this is an example from juan nunes iglesias where he's doing some self-supervised denoising of some images and so this is an iterative process where the algorithm learns how to remove the noise from the image and he's able to do the training of the algorithm you can see that he's plotting the loss or sort of how well the algorithm is currently doing and then you get a preview of what the images currently would look like using the current settings um and so the idea here is uh by virtue of being written in python the part can act as sort of this hub that allows you to orchestrate and bring together a lot of different tools so now a bit of an introduction into the viewer itself so the viewer is sort of one of the main sort of top level concepts this is the sort of the graphical user interface window this is where you'll actually view the images interact with them there's a few different components to this window um so the first is the canvas and so this is where actually all the visuals are rendered and so this is where you see your images or if you have different types of annotations like points or shapes this is where they show up as i mentioned we have a concept called layers and so typically each data set will get its own layer and the layer list is where you can see and select how those are currently being viewed so this would for example allow you if you click the eye icon uh turn a layer on and off you can reorder um how they're visualizing the canvas and then each layer has a set of controls that allows you to choose how it's rendered so this current layer that's selected as an image and you'll be able to change parameters such as the opacity so how see-through that image is you can change the contrast limits so that affects how the color is applied to the image the visualization you can affect how that image is blended with with the adjacent layers so the layers on top and below it um and then as well as you can change the interpolation so how the image is actually rendered um i mentioned before that we aim to be multi-dimensional so oftentimes your images might have three spatial dimensions but you might have other aspects as well such as maybe you've acquired different wavelengths so that could be a fourth dimension different time points that might be a fifth dimension and so for the dimensions we you know we can only display two or three dimensions in the viewer um and so if you for all the dimensions that aren't currently being displayed a slider is generated at the bottom and that allows you to choose which slice or which time point or which channel you're currently doing you can also add and remove layers so if you you've added a visual you don't want to use it anymore we have a bunch of controls for doing that you can actually control how things are displayed so you can switch between two-dimensional and three-dimensional rendering you can select which dimensions are currently being rendered you could do that down in the lower left-hand corner in the canvas controls and finally we have a status bar which allows you to when you use your mouse in the canvas inspect what is underneath it so for example in an image you would know what the current intensity value is for the pixel that you're mousing over um if you're using polygons you would be able to see what shape you're selecting uh things like that and so we have many different layer types and the way that we think of this is the layers are a particular way to visualize a set of data so one of the layers types that we have our image layers and so this is probably our most commonly used layer so this is when you have some sort of intensity data maybe it's images from a satellite could be images from a microscope these can be multi-dimensional so again you can have two or three spatial dimensions but then you can also have other dimensions added on so that might represent color time different positions in a well plate things like that we also allow for dynamic loading and processing from disks so this is great when you're really large data so the jargon here is lazy loading but the idea is that you can just load the piece of the data set that you're currently viewing with with some constraints and we can talk more about that later but this is an example from tali lambert where they're loading in some images from a microscope and as they're being loaded doing a little bit of pre-process well it's actually quite a lot of pre-processing um it's deconvolving the image and doing the skewing which is basically trying to undo some of the aberrations that are introduced by the imaging system i'm doing that online as you load in a different slice we also allow rendering three-dimensional rendering um that allows you to selectively render planes so for example uh maybe if you've looked at a three-dimensional data set but what you want to see is actually in the middle um this allows you to sort of slice through or cut away different parts of the image and just see the parts that you're interested in a highly related layer is what we call the labels layer and so this is a special type of image that's most commonly used in the context of segmentation so for example if you were looking at this image of coins and you wanted to identify each individual coin the pixel values here each have an integer value that denotes which uh instance or class that pixel is a part of so for example all of these sort of light purple ones on the left hand side all would have the same value and that tells us that's one individual point and similarly the other coins would each have their own integer value and then of course we can view these in three dimensions or multiple multiple dimensions and so this is uh some data from i believe it's the allen cell brain atlas this is the mouse brain and the different regions are annotated by different colors another type of layer is the points layer so this is great when you're identifying objects so in biology this is most commonly used for techniques like single molecule localization or single molecule fish where you want to identify where individual molecules are in your image in astronomy this is often used to for example find different stars or show the position of different stars and we allow the user to sort of richly color them based on different properties so for example what kind of star it is or the size things like that we also have a layer for just depicting vector data so this is great for showing for example orientations of fields so if you do electromagnetism it's a great way to show the different field lines this is showing the orientation of different receptors on viral particles people often use this as well in mechanics so if you want to show a stress field or a strain field this is a great way to show that this is sort of a specialized layer but it's the layer that we call the tracks layer and so this is really useful when you are tracking particles or objects over time because it adds a little tail under the particle that becomes lighter and lighter showing where the position the position of that particle most recently in time so it sort of shows you where the history of where it's been and makes it a lot easier to see sort of the way that the different particles are moving together and this was contributed by one of their community members alan lowe at ucl and the turing institute in london and then they've actually built a full application that they call arboretum for tracking cells over time so these are both some this is a really cool example i think because this is something that a community member needed they built it they contributed it back to the project which means that other folks can now take advantage of it as well uh if you do work with surfaces um so for example this is um data from uh fmri so this is sort of a proxy for brain activity um and what it can do is it allows you to both define the surface as a polygon mesh but then also color it based on some measured value and these can be done both statically but this is also for example a time series in 3d and the final layer is what we call the shapes layer and this is for polygon annotations um so this is really useful um again sort of a biology example sorry um but if you have regions of interest and in maybe a large slide where you've identified the boundaries of tumor this is a great way to sort of make that annotation but also store it and share it with your collaborators hopefully you've already done this uh but uh you you can install napari in two different ways um one is via pip and the other is via condos these are kind of the two major uh ways that packages are distributed in python um if you do have issues with installation um feel free to well of course we'll get you sorted today but in the future feel free to reach out to us and we're totally happy to help and so what we've showed so far are sort of the core functionalities of nepari itself but of course many of you are likely interested in extending it and doing something custom for your own work um and so one of the ways you can do that is via our plugin system so our motivation here is we want users to be able to customize and extend napari without needing to actually modify sort of the core code but importantly we also want these components to be shareable by the community so people don't have to keep reinventing the wheel if you create for example a great segmentation algorithm we want your colleagues to also be able to use that segmentation algorithm if you wish to share with them some example use cases of why people in the past have created plugins so one is for example most fields have their custom file format and so this is a great way to add support for loading those data another is if you want to create a provide a graphical user interface for some analysis code that you've developed for people who want to use plugins if you want to advertise the existence of your plugin this chan zuckerberg initiative has been working on a project called the nepari hub which aims to be a centralized site to discover and explore napari plugins the url is here it says nepari.hub.org the idea here is you can if you have for example a topic or a sort of operation you want to be able to apply for your to your data you can search for it and it will show you sort of the existing plugins that do that and also some information about how you install it kind of uh so how recently it's been modified so that kind of helps you know is this being actively developed what are some of the other dependencies or requirements that you need to mind in order to be able to use it with you with your analysis um and so i use this both to find plugins to help me do my work but then also when i'm thinking of making a new tool this is always the first place i go to see has somebody already done this so i can maybe just contribute to their work rather than creating yet another you know something of something for installing plugins we do so you can install them via the command line but we also have a built-in plugin installer so that can be accessed via the plugin menu and so that allows you to search and then install plugins of interest just to give you a sense for some of the plugins that already exist we'll just go through a few examples um the idea here is not so much to show you how to use these plugins but to make you aware that they exist but also kind of maybe provide some inspiration for why or what types of plugins you might want to make with your own work so this first one is the pie clasperanto assistant from robert haza in dresden and this is a plugin that provides gpu accelerated um sort of common image processing operations like filtering counting measuring this is a great way to uh sort of create performant image processing pipelines um and the cool thing is it's written in opencl so it's cross-platform so you can do it on cpu and you can do it on gpus and then also he's written interoperability tools that allow you to export scripts that can also be used in fiji which is a really popular image processing toolbox used in bio image analysis this is one that we'll be using today it's called stardust so this is an object detection algorithm that's really useful for segmenting nuclei from microscopy images finder is another example of a segmentation algorithm this one's really focused on large images um so adam tyson who developed this uh would take images of old mouse brains um and then find all the individual neurons of interest within the brain um this is another useful one uh called napari animation from guillaume bits and baron and aleister bert nick zafraniev and this allows you to create animations from from your images by just defining keyframes so you can go to sort of interesting parts of your image and it will automatically create a video that interpolates between all those different uh places that creates these nice smooth renderings i find this to be a really nice way to share results with my collaborators generate movies for publications things of that sort um we'll be going through this today but we to make creating plugins easier we've created what's called a cookie cutter which is basically a template that sets up the project for you so you can only focus on creating the small bits which are generally your analysis code where your code to load load upload the data and drago will be giving a nice nice tutorial on that later today so as i mentioned nepari is a community driven project with contributions from a lot of folks the slide's a bit old it's well i guess it's still more than 80 but it's it's like way way more than 100 now which is great um and we have uh so sort of uh leading the ship uh juan nunez iglesias laura perrier nixon frania and tali lambert are on the steering council uh kira evans provided a lot of the really early contributions that got napari off the ground um and then we have sort of bold here a pretty vibrant core development team um and we also have a good contributions from many many more community members and if you want to reach out after the tutorial we'd love to hear from you we love contributions of all shapes and sizes you know we find it really helpful even if you just drop an issue telling us something that isn't working or something that you wish nepari could do um we also of course accept code contributions um jaga and i will be uh having a sprint on or hosting one of the sprints on this weekend so at the end of sci-fi um and we would be more than happy to so of course yeah we would be happy to help you make your first pull request but also if you just have something you want to try to do in nepari we're happy to use that time for that as well so please yeah don't hesitate to reach out to us and yeah great to see you oh last thing i want to mention is we do have community meetings and these are really meetings where folks come by and show what they're working on or maybe they have a question or maybe a complaint that's fine too that they want to communicate to the rest of the community and so these are at these times here we tried to spread it out so they can be accessible for people at all different time zones so usually there's two meetings two of these three meetings would be probably more or less during the work day for you um so hopefully we can see you there and we'll put links to all this stuff in the slack channel um so in conclusion um nepari is a python-based viewer for visualizing annotating and analyzing n-dimensional images and it can be used in both an interactive and scripted manner and we can use plug-ins to extend the functionality and provide guise for analysis workflows today what we'll be doing is performing some interactive analysis uh in a jupyter notebook we'll be using one of the plugins to call stardust to segment nuclei and also you will make your first um nepari plugin so are there any questions or comments before we sort of start jumping into the hands-on parts just out of curiosity how many folks have either used napari or heard of it before coming okay awesome cool mix um okay so this is not my computer um so i hope this part works so basically uh the way the way sort of the hands-on portion is structured uh we will have a part where we sort of show um some of the key concepts that you'll be doing and during that don't worry about following along in terms of like typing on your computer um just uh you can just watch us listen ask questions um and then you'll have an opportunity to sort of um in a self-directed manner with our assistants um work through those notebooks and then we'll sort of go around we would ask that you uh pair up if you're comfortable um we find that it helps to kind of have a buddy to go through this journey um and so uh to get started that's you um so this is just a demo notebook um so actually let me start from the beginning here just a full value experience so shout out to lucy over here she saved us none of our laptops were working with this projector um so thanks to her we have projection um so this is a terminal um and we have all relations let's actually start from the toilet so as suggested in the tutorial instructions lucy has set up her laptop with conda so this is a way to manage your environments in python and we like using environments because it allows you to sort of keep the packages that you're using for a given project together which helps reduce sort of dependency conflicts but also helps you when you go back to run that analysis some months later to make sure that it works more or less in the same way and so here we will use conda and then the command is activate which is will allow us to choose select one of our environments and this one is called napari tutorial if you follow this is how it was called in the instructions and when that works successfully in parentheses you should see the name of the environment that you've just entered and so there's this command in unix and linux which pwd which gives you your current directory so currently we are in the repository that you may have downloaded already i'm containing all of the workshop materials and what we're going to do is we're going to launch from this directory uh the jupiter notebook and all of this stuff is totally documented in the tutorial instructions just kind of want to show it to you live and again please feel free to stop me if you have any questions or something isn't working so oh sorry i didn't even explain it all that i did um so let's start over i'm sorry uh okay so the way i like to launch uh the jupiter notebook is i say python dash m which says we're going to use a module and then the notebook is is the name of the module so we say python-m notebook and what this should do is it will launch the jupyter notebook in your web browser so this will just open in your default web browser in this case chrome and it should open up to this home page which is it might look familiar this is it's kind of like a file browser so this shows you uh from the directory that you launched from so in this case this is the nebari intro tutorial and we will go into the notebooks directory by clicking the link and then we'll select a notebook i just added a quick notebook here just to do this this demo part but you'll start with part 0 which is the viewer intro and so if you click on the notebook it will launch and we will get started so this first thing so in python we want to use a module or a library we have to import it so we say import nebari and then next command that we'll do is napari.viewer and what this does is it launches the viewer so this is the actual graphical user interface and returns this viewer object which is your python representation of the viewer this is how you can interact via your script or the command line with with the viewer that you see so if the first time you launched without launched fairly quickly um the first time you launched an apartment it does take a bit more time to build some stuff that's normal just be patient um and so you should see this viewer window uh as you see we just launched the viewer with no arguments um so this is sort of the default window no data has been loaded um so what we're going to do is we're going to use scikit image to download and load some images and so this we're going to use the inbreed function can make this little big direction yeah then read function from scikit image and we're going to download the image from this this url okay and so uh what i snuck in at the bottom here is a little print statement that tells us something about the image that we just downloaded um so typically um in python uh images are stored as some sort of array um in this case it's a numpy array and this array has shape 60 by 256 by 256 and so what that means is that this is a three-dimensional image which has 60 z slices and in the xy plane is 256 by 256. um we can add that image to our viewer i'm using this viewer so again we have this viewer object which this is sort of our representation of the viewer in our notebook and we can add an image to it using the dot add underscore image method and passing it the the image that we just had or downloaded um and when you go to the viewer what you should see what we see now is uh the image loaded um and so as i mentioned in image is an array when we zoom in really far what we see is at each pixel it's a slightly different brightness and so what the viewer is doing is it's taking the value of the array and mapping it to a color so in this case the the values of the array can go from zero to one one is the brightest zero is the darkest and this is where you have to know a little bit about how the image was generated or acquired but that number generally tells us how bright the image is or how much of that quantity that we're measuring exists in that position and so in this case this is an image of nuclei and cells and so this tells us sort of roughly speaking how much dna is in that location and so as we mentioned just a moment ago this is actually a three-dimensional image and as you can see we're viewing in two dimensions um and so what napari did is created a slider down here that allows us to slice through that third dimension or in this case the z-axis so for example this is down here you see this is the 30th slice we can move to the right which increases the slice number now we're looking at the 37th slice um and so if you aren't doing bio image analysis regularly you might wonder what are these blobs um and so this is a cartoon of a cell you might recognize this from a like a high school biology textbook um and what we're looking at are the nuclei which is this sort of purplish region here and those are the uh where the dna is stored in the cell so all of your cells have a copy of your genome and this is for your dna and that's where it lives um the next image we look at will be the cell membrane which is the outside of the cell and this gives us the sort of the extent or the shape of the cell um and so the viewer um and uh from the viewer we're actually able to the viewer object were able to interact with the items that we were currently displaying um and so in viewer.layers this is the list of the layers that are currently being displayed and so if we go here this is the layer list and so when you say viewer.layers it's going to show you everything that's in this list and then we can access those items by their name and so this one is called nuclei and so this will give us the python representation of this image this image there and then what you'll see in the tutorial is we can use that object or python representation to change the way that we're currently viewing it but what i want to show you here is you can access the data so this is the actual underlying image that's being displayed by the data parameter and you can say shape just like we did before so it's a numpy array and it gives us the shape of the image and then you can also inspect the data itself so these are the actual values at each pixel in the array the last thing i wanted to show you is how you can overlay multiple uh images at once and this i hope sort of illustrates why layers are useful so we're going to load another image as i mentioned this is an image of the membranes so if we go back to our cartoon this is the outer boundary of the cell um and we can just like before we can add the image using bureau.add image to to the viewer so what you might notice here is we no longer see the nuclei because these images are on top of each other so if you use the little eye button here you can toggle the visibility of a given layer so in this case it's the membrane's layer we can turn it off when we see it below but oftentimes what you want to do is you actually want to see both things at once um and so the way you can do that is when you select the layer on the layer list the layer controls here are updated for that given layer and you can change some of the rendering parameters so in this case because what we want to do is we want to see through this top layer so we can see the nuclei underneath we just want to reduce the opacity a little bit so you should see the layer below come into view and then the other aspect is uh now it's it's still like because we've seen the images before we can uh as humans we can discriminate between sort of this nuclei and these cell boundaries you know there are different things but it can also be helpful to use color to separate these items so you can change the color math and so again when we zoom in each pixel has a value and the color map is what transforms that pixel value so this number between zero and one into a color and so this is a gray color map so what that means is uh zero pixels so the lowest pixels are black and pixels that have a value of one would have a color of white but we can choose a different color map so for example we can use this one called magma for the membranes and so what this color map does you can see in this little bar here so this is from the lowest value to the highest value um so it should make something that's sort of purplish black for the low values and something that's sort of yellow white for the uh for the high values um and then the last thing i wanted to show you about when you're sort of overlaying and changing the rendering parameters uh or explain rather are the contrast limits um so like i mentioned uh the color map transforms these numeric values in the pixels to colors and by default uh 0 is going to be set as the lowest value and 1 will be set as the highest value and so zero in this case would map to this sort of purple and then one would map to the yellow but sometimes our data doesn't actually span the full range right so a lot of times especially in microscopy most of your data will lie kind of in the bottom like 50 of that range so what we want to do is we want to compress the coloring range just to match that and that's what the contrast limits are for and so if you right click on the contrast limit slider you can select which value on the left is going to be the darkest value and which value on the right is going to be the lightest value and so if we slide it forward what you'll see is a lot of these pixels that had color before are now becoming darker because we're saying we're moving up sort of that dark point and similarly if you shift the right uh slider over you change the brightness so this is now 0.65 is now going to be mapped to the brightest color and so you can play with that to sort of get a visualization that allows you to observe all the features that you're interested in so that was sort of a quick overview of some of the concepts in the viewer i mean in this next tutorial or in this first tutorial you're actually going to do a lot of these things yourself and so i would encourage you to fire up your terminal and launch your notebook um does anybody have trouble installing or just hasn't had time to install yet great okay we'll come to you anybody else uh either way both so okay happy to come by um and so the first tutorial so if you go to the website that we had mentioned earlier the tutorial sort of order is here under the instructions um so currently we are doing the uh part zero which is just the introduction to the view so it's going to rehash a lot of the concepts that we just went over i encourage you to explore and play we tried to give plenty of time for these sessions for the different exercises so don't feel rushed um and yeah maybe just raise your hand if you have questions but i think the two of you have already plugged they've issues so we'll come back okay so we will um continue with the next part of the tutorial the last part of the tutorial um and i'm cheating here uh by loading up the solution notebook which you are all looks welcome to do um but what i wanted to show you is where we would have ended up right so when we initially loaded in the image and the spots we kind of were just seeing this so you have the nuclei in the background and then all these spots are kind of hanging around um the nuclei these spots by the way are rna molecules um we don't we're not sure what they're doing there but if a biologist knows they're welcome to to share with the class um but the goal was okay we know these you know we can physically see these spots there but how can we tell the computer to find these spots and then you know once they've found them maybe we can do some analysis on them and so the first step was implementing a gaussian high pass filter sorry and that is what the gaussian high pass looks like and once you add that filtered image once you fill you know run the filter on the image data and then add it you just see this right so if i turn off the spots in the nucleus so we can see a bit more clearly you can see that it's done some thresholding and compared to the original spots believe it or not it's actually less noisy yeah so you can see there's sort of like a little bit of noise in there uh once you get you know down to the contrast limits and this filter i should hopefully you know get rid of them and once you've got that filtered image the next step was to uh write the detect spots function uh which you know takes in these parameters uh and then does some computation and returns the coordinates of the points and the sizes of the points and so once you've once you've run this textbox function on your filtered image which we do down here um you can you have a points layer so we know this is the points layer it's got that little icon there and when you click on it you can see you can change the face color of the points of the edge color and so on but essentially what we're looking at is that the points are more or less lined up with the bright spots in the image um which they are so now that we've got this function we can use it anywhere in this notebook or we could put it in a script and like share it with our friends um but ideally we would share it more broadly with the world right and that's where the the plugins come in so i'm going to go through a um kind of dry potentially like child try not to make it dry but maybe it will give it right lecture on how you turn just this like python code living somewhere in a notebook how you hook that up to nepari and kind of tell napari hey this is code that should be a plugin and you should discover this code um and yeah so i'll tell you how to hook up all that machinery and i'll tell you how to turn just a plain python function into a widget right into something that maybe like you saw with the stardust has buttons and sliders and stuff like that um and then when it seems like all hope is lost and there's a lot to remember i'll introduce the cookie cutter template and that will hopefully brighten your day because you'll realize you don't have to remember any of this it's just helpful to know um but uh yeah so the cookie cutter template does kind of abstract all of this away from you so you don't really need to be a python packaging expert or anything like that to make a plugin but it's useful to know the the background and so uh let's let's go ahead and do that so i think i said pretty much all of this and anyway i'll say it again but yeah so nepari has different types of plugins each plugin can do many things but broadly we have readers that read custom data formats into nepari layers um so you know a common file format is tiff that one is read by the inbuilt reader but not all formats are read by the inbuilt reader so you can create your own custom one there's writers that save nepari layers to file um there's sample data that provides data that can be opened from the sample menu and actually we probably should have shown you this earlier but if you go file open sample there are a bunch of images that you can just open directly from the viewer without like installing anything additional and so it's really nice to have that there if you just want to quickly open up an image and you don't have something lying around the open sample menu is there and as you can see the stardust plugin actually provides its own sample images as well so lots of the plugins now provide sample images and that's really useful as well because without having used a plugin before you may not know what kinds of images it's it runs on or what it's good for and so plug-ins that provide samples are becoming more common and then there's widgets which a widget is just like a little bit of gui functionality uh and usually these widgets provide gui access to different analysis functions and different analysis code that you can run without running code um and then there's themes which like change the nampari theme like from light to dark or to pink to blue or whatever i don't actually think we have a pink theme um which is a gross negligence on our part but you know whatever and so today we're actually just going to focus on widgets because that's what we're going to be building but as i'll show you in the cookie cutter later there are like pre-built examples of readers and writers and everything so you can go in and play with them without having to um write your own from scratch and so widgets are just python functions more or less um there's actually three different ways to define napari widgets but in in in its plainest form it is just a python function that you decorate with magic factory uh and then all of the gui is built for you so you don't have to worry about adding labels or adding text boxes or making buttons or anything like that that's all done for you by magic gui and so i like to think of when i'm building plugins i like to think of the contract between me and napari and so your function signatures are that contract so if you give napari a function that accepts correct parameters as defined in the contract um and returns the right values napari will call that function for you at the appropriate time uh and so what does that kind of look like let's say you have a plug-in and this is a real plug-in called affinder and then you have napari right these are two kind of like separate python packages a finder returns a function to nepari that can build a widget and when the user goes to plugins and then clicks on the a finder you know button that function is created that function is called rather so napari in its code will be like oh call the i find a widget function and that will just build the the gui for you now how does it know what to put in the gui right is the main question because i've said that your function signature is enough so how does napari turn things into gui and the way it does that is using type annotations of your function signature now python many of you may know but if you don't this is the term is not a strongly typed language so what that means is that i can assign an integer and it is five and its type is integer uh but then i can just save a string into that same variable uh and then it changes type to a string right in most i don't know in most in many languages once you've defined the type of a variable that's it like if it's an integer it'll stay an integer for life and will be upon you if you try to assign a string into it but python's not like that um and so what python has provided as a kind of like middle ground if you do want type typed things um are type annotations and so python won't enforce thee that's not going to yell at you if you ignore a type annotation but for napari type annotations are super useful because if you have type annotated a function signature we will take that type annotation and we'll build a gui element for you so for example this is this is a function signature if you're not sure what a function signature is this is a function signature it is the name of the function which is string multiplier it is the arguments to the function which are input stir and multiplier in this case and both of them have been type annotated one is a string and one is an integer and then it's the return value and the return value the return value is also a type string so what this function signature tells me is that this function takes two parameters one a string one an integer and returns a string uh and all it does is it just multiplies that string uh by the multiplier so if i give it hello and five it prints hello five times um so the the type annotations are these bits after the colons um and they just tell python a guideline as to what you expect that parameter to be so how do we turn that into a widget we're going to focus like i said specifically on magicgui widgets so you won't be building buttons you won't be building labels or anything that's done for you and both the parameters and the return value are meaningful to nepari in this case uh so here's a bit of code we've just got two imports ooh too far we've just got two imports one is magic factory magic gui comes with nepari so you won't have seen that in your like install steps um and then we import napari that's for later uh and here you've got two things you've got a magic factory decorator that we've just nicely placed above the function signature and then you have a function signature like i was mentioning so the name of the function is intensify it takes two parameters one is image layer and one is intensity delta the image layer is type annotated with nepari.layers.image and that is the type of all images in nepari and if you looked at like viewer.layers nuclei or whatever you will have seen that it returns an image layer and the intensity delta is just labeled with an int so it's just an integer nothing special about it nothing nepari related about it and then the return value is another nepari type which is layer data tuple and we'll see once we build this function what that actually looks like but that's all magic factory needs whatever goes on in this function that's all magic factory needs to build a gui for us uh and because this image layer is annotated with napari.layes.image it'll be like ah you're trying to select an image layer so it'll build a drop down for you and it'll populate that dropdown with all of the image layers that are currently loaded into the viewer uh and the integer is an integer so it's like okay you're trying to select a number so it's just going to build a spin box for you into which you can either type a number or go like plus minus to select a number and finally the fact that you're returning a layered data tuple satisfies the rest of the contract with napari and tells it that whatever you're returning out of this function will be added as a layer into the viewer so you won't even need to add the layer yourself napari will do that just because you've told it that you're returning a layer data tuple and so all that this function is going to do is it's just going to add an integer to each pixel so it's a very toy function but you know it does the job all it does is it just makes things brighter and now we're going to build this layer data tuple so the layer data tuple needs a layer type and in this case it's just image it needs metadata which could just be an empty dictionary but in this case we've provided metadata that will be applied to the image so one is the name and the name is going to be intensified image and one is the color map which you've probably you're probably used to selecting from the layer controls but you can pass through encode as well and we're going to say that that's going to be magma just for the sake of it and the final thing that you need in the layer tuple is of course the data because without data it wouldn't be much of an image and so what we're returning here is a tuple it has three items in it and those items are the data the metadata and the layer type and that is that is how you satisfy the layer data tuple type because it's just a tuple tuple with three things the data the metadata and the type of layer as a string and that's kind of all you need to turn this into a widget but the next step is like okay outside of plugins and packaging and whatever how do i add this widget to an apari viewer so to make the widget you just call the function with no parameters you just call it uh and that returns a widget to you that is actually also callable with the parameters that you've uh displayed there from in the plugin that's abstracted but you can actually build widgets independently of plugins and so that's why i'm showing you this construct here so once you've built your widget you build a viewer the same way we've seen you add the dock widget using viewer.window.add.widget and then because we're in a script to start the qt event loop i've got an apari.run folder you haven't seen that in your notebooks because in the interactive python console the run is kind of i mean the qt loop starts as soon as you create the viewer so that you can interact with it both ways right in a script you can't do that and that's where the napari.one comes in so here's one i prepared earlier once i find my cursor so this is the exact same code that we just saw i've just copied it into a random python script uh there's nothing else really in this folder okay it's not like it's like a it's not a plugin or anything it's just a python script um and when i run this in theory um the viewer will open with my widget added over here right so it you know it's built labels for me and everything at the moment the drop down is empty but once i open an image and we'll just open uh any old image so here's one of the sample images as you can see it's populated here uh it's just got one thing if i add a labels layer that's not there because the labels layer is not an image layer right so only images are going to be added if i open a different image whatever camera that is there okay so again that's all handled for you you don't need to worry about it if i delete the layers again it goes away and then let's just say i put any integer in here if i try to add a float i can't i can't put like 20.5 in there because that's all guarded by the fact that i type annotated it an inch and then let's just add i don't know 11 and when i click run all it does is it makes the image brighter and returns it to you in magma color map okay not super exciting but i hope that you can see that it's named what we told it to be named is this big enough um so it's named what we told it to be name we told it to be named intensified image and that is what it's called the layer and we told it to be column at magma and it is indeed color map okay so that is all that you need to build the widget type annotations and your magic factory decorator now where am i going building a widget in a script is great and you know it gives you a lot of flexibility but you want to put it into a plug-in and the glue between your widget just living in a python file and napari is nepari.yaml which is a config file and in that config file you tell uh napari what your plugin can do you tell it whether it can read things or write things or add widgets or provide sample data and you tell it which functions in your code do what things uh so let's take a look at what an example the pi.yaml file might look like for this intensity widget that we just built so the name is intensify and actually this would be your pi pi your pip package name if you had released it to pip but if you haven't released it to fifth then um it doesn't have to be um and the display name is the name that you would see in the viewer so when you go plugins and you've got all those plugins listed there this display name can be something more descriptive and is less restrictive than the like pip package name which has to be you know a bunch of dashes and in python often just a bunch of like associated consonants that are uh you know not pronounceable but that's metadata that you need and then it has contributions so a command is you know gives your contribution a unique id and it points to your function so if i put my intensify uh function in a module named intensify in a file named underscore widget this is just a fully qualified path to that function and then it has a title that you could use to search in a command palette or whatever and so once you've told it that hey this is where you can find the function to run you also add a widget contribution uh and now for widgets all that is is the exact same command id as you typed up here but also a display name uh so that it's like you know human readable uh in the viewer this might seem kind of pointless like why am i defining it twice i'm afraid i don't have a satisfying reason for you that wouldn't involve us looking at readers and writers as well but for readers and writers and for sample data you actually provide more useful metadata over here in the contribution like for readers uh you can filter what file types um and what file extensions you accept and stuff like that but we won't cover that today come up to me obviously anytime and ask me if you want to learn more and i will show you what the cookie cutter builds for you um but this is all you would need in nepari.yaml to make our intensify widget a plugin but there's a bit more because okay now you've got nepari.yaml and it's a config file but how does nepari find this config file right that tells you where your functions are and so the way that works is nepari checks all of the installed packages in your python environment to find things that advertise themselves as plugins and a package makes itself discoverable to nepari by declaring an entry point in the package configuration file if this sounds like gibberish that's fine um we'll see you know i'll show you in the cookie cutter template how all of that ties together but this entry point that you write in your configuration file or if you use the cookie cutter you look at in your configuration file um points to nepari.yellow so the package config points to nepay.yaml apari.yaml points to your python functions and that is the the connection between just like a random old package that's installed in your environment and an apari viewer and so in setup.cfg which is your package configuration file you will see uh this exact code basically except that instead of intensify it'll potentially say other things and as you can see it directly points to the nepari.yaml config file and so speaking of packaging what is a python package what does that look like in real terms minimally it kind of looks like this okay so in home you have a directory and inside that directory you've got pi project dot tomlin setup.cfg and these two files tell the world and tell pip that this is a python package not just any old folder of like dog images or anything like that uh and inside that you've got a module which has an inner dot py file which could be blank and jonah.yellow okay so minimally this is what a python package could be and the benefit of having a package rather than just any old folder is that you can release that package and so make your plug-in pip installable right so like you know you probably could have installed a fair few things by now this is one way to configure your package to e-pip installable and allows you to distribute it right and potentially have it on the napari hub and configure other you know different options like author metadata and github repository where you can find your package and all of that stuff but you don't need to know any of that actually so maybe you took the opportunity to catch a nap during these last like 20 minutes or whatever um because the cookie cutter does all of that for you okay so it's useful to know how all the things connect together but you don't have to go ahead and write your own nepari.yaml or your own setup.cfg or whatever so the cookiecutter generates an example plugin for you and it comes apart from being pre-configured with code that runs as a plug-in that you can edit to do your own nefarious things or non-nefarious things um it also has a lot of utilities for testing so it has example tests um that you know you can run to make sure your plug-in does the things you want it to do packaging deploying your plugin to pipi so deploying is the step of getting it from your like local machine you know into the cloud um on pipei and it also has configuration options for the nepari hub so as we saw earlier then aparihub is where all of the napari plugins can be found and you can you know customize the way your plugin looks on the nepari hub all with stuff that comes with so let's see uh what it looks like to you know use the cookie cutter to build a plugin i'm just going to close down the notebooks okay so just to show you that i'm not like hiding anything behind the scenes all we have in this folder right here is that a little intensify widget um that i was playing with before there's nothing else uh and to start the cookie cutter uh i went and this is all in your instruction notebook um but this is the cookie cutter github and it might look a bit intimidating but all you actually need to do is just copy over this little bit and just paste it here uh and that's going to download the little cookie cutter template and then it's going to ask you some questions um and so i'm just going to fill in some of these but a lot of them i'll leave default just because i'm not planning to release this anywhere i can make it a little bigger is that better yeah um so yeah it asks your email it asks your github username if you're gonna actually host this on github um you can just leave it all blank if you want it asks for your plugin name and this is gonna be the pi pi name and i'm just gonna call mine intensify but you can call yours something meaningful i'm not actually going to put this on github so i'm just going to not provide a github url i'm happy with intensify as the module name in general i mean i don't know you can call your modules whatever you want but defaults for most of these are perfectly satisfactory the display name i'm not going to call that because that's not very descriptive and i'm gonna call it image intensification and in fact like i know that our uh cookie cutter gave it the napari prefix you don't have to do that uh so ideally you wouldn't do that don't don't use the party in your in your package names um basically because it makes it kind of hard to browse than a party hub but anyway that's a pet peeve of mine so so the display name is just image intensification uh i'm not going to change this short description but you know if you're planning to actually release this you probably should and then we get to the good stuff so now it's asking us what example plugin types you want to include i'm going to say yes to pretty much everything just so i can show you what that looks like in the cookie cutter but the only one that we really like care about today specifically is the widget right because what we're going to be building is a widget um these are kind of like in-depth things but if you want to use git tab if you if you're going to release this plug-in you might want to use git tags for versioning just because it um lowers the burden on you but i'm going to say no to these these are all by the way if you're like lost and not sure why you're being asked these things it's all like pretty well defined uh in in the readme so it isn't like what you can do with each of the options and then and what their point is it asks for the license these are all open source licenses i'm pretty sure we strongly encourage you to release your plugins as open source but once i click enter hit enter rather it creates the plugin for me and it gives me a bunch of instructions which again i suggest you read these instructions but i've read them a couple of times now so i'm just gonna show you what it built um so as you can see now in this in this folder we have a new folder called intensify uh and i'm going to cd into that folder and open it up in vs code yeah so just ls single so we were here inside the plugins directory and if i ls we've got that original file that i just had in there but we've now got a folder called intensified because that's what i call my plugin i'm going to see the internet and i'm going to open it up in code yes code that is just because it's easier to browse um so this all comes with your cookie cutter template much of it you probably will never touch but i will you know do a little sightseeing tour around which you've got here so you've got setup.cfg and pi project.tamil which you might remember from my rambling earlier before but if i open setup.cfg what i want to show you is apart from a description and a name and your author and stuff like that which is pre-populated you have the snippet of code here so it tells you that you can find the package in src and lo and behold there is an src folder right there and it tells you that the entry point for the nepari plugin is intensifying apart.yellow and so if i look in src and then i look in the intensify folder there is my nepari.yaml so it's all connected um and once i open the pirate.yaml you're going to scream because oh my god there's so much in here but um there's only a couple of things i guess i want to point out one is we have widget contributions right with commands and display names just as i um sort of mentioned before and then you can see it's already come like pre-configured with all of the writer commands and reader commands because i selected that right because i said yes give me a reader give me a writer give me a sample it comes pre-populated with all of this and so let's take a look at one of the widgets for example let's take a look at this one its name is make magic widget fine but the python name is intensify underscore widget example magic widget so i can use this i can go into intensify widget and then in here i should be able to find a thing called example magic widget and there it is okay and it's just a magic factory decorated function like we saw before and you don't have to be too sharp to notice that there's other things in this file and those are the other two ways of defining widgets that i was talking about before and the only one that i'm going to kind of briefly mention here is this example q widget so if you're trying to build super complex plugins if you're trying to build like big things that require you to pop dialogues and close dialogues and you know add tabs and and i don't know do all sorts of like weird and wonderful things you can use the full might of pi qt uh by subclassing q widget the downside of that is that as you can see you have to like build your own buttons and make a layout for your gui and add the button and add callbacks for the button right so all of the stuff that was being done for you in the background with the magic gui widget you have to do yourself if you're subclassing q widget but it gives you more flexibility right i mean if you are like a gooey guru um yeah i just came up with that one it's not bad right um then then you have full control right so you don't have you're not stuck in this like magic gui box you can do whatever you want and what i'm going to do is i'm just going to delete everything in here and i'm going to replace it with my intensify widget which i will find over here i'm just going to copy the widget in here and then i think the only thing we'll need to do is import magic factory as well so i'm just going to copy that import statement as well okay and so now that i've changed the name of this you know function that was in here i'm going to go into nepari.yaml and i'm going to delete the things that talk about example q widgets and the things that talk about function widgets because i deleted that code right i just replaced it with my own and the only thing i'm going to do for the magic widget is instead of pointing it to example magic widget i'm just going to point it to intensify which is the name of our function in here i'm going to save that and i'm going to save this uh bs code is complaining because i haven't selected my environment and that should stop now yeah so that's all that's in the widget file and in the pi.yaml i've kept just the command for that one widget and i've pointed it to the new function that we wrote and because i deleted the command i'm going to delete these contributions here as well and i'm going to rename the magic widget to intensify image um yeah so i made the changes to the widget file i made the change to nepari.yaml and then the final thing and this might catch you out because it always catches me out uh the functions are often imported in the init and so because they no longer exist you probably don't want to import them so i'm just deleting uh references to things that no longer exist you don't have to do this by the way i mean you can just keep all those widgets there and add your widget to the bottom but you know i'm just i guess trying to show you where are the different bits that you might want to change in your plugin and just before i run this to increase the suspense i want to show you these other files so these files were built for us by the cookie cutter right we haven't touched them but the cool thing about them is they're not just like empty files they're actually like fully documented descriptive examples of real readers or real writers or real real sample data plugins so if you're the kind of person who learns by doing you can just look at these files and start tweaking things and see what you know see how that changes your plugin functionality rather than building something from scratch but now that i have changed all of this i should be ready to install my plugin and the way i do that is with pip install dash e dot so dash e makes it editable so if i make changes to the code i don't have to reinstall the plugin to see those changes become active and i'm doing that i shouldn't need to because in theory my code's perfect but you know let's assume i made an error somewhere and then the dot just refers to the current directory so because we are in the directory where the plugin code lives and i'm just going to show you that again see so in this directory we have our project tamil setup.cfg we can just point it to this directory in theory that works and it does the installation and once it has i should be able to launch nepari and look inside the plugins menu and find my intensify uh plugin and when i click on that it errors because i forgot to i thought i changed that did i forget to save it ah yeah i didn't change it oops yeah so just to go back um it's complaining because it said fail to import command intensify intensify cannot import name example magic widget from intensified.widget and that's accurate because if we go into the widget function there's no such thing called example magic widget so all that i did here and in it is i forgot to make the change to what our function is actually called i did that on purpose to show you um now because we installed an editable mode let me just close down this window because i installed an editable mode i should be able to just relaunch nepari without reinstalling uh the plug-in and that change should have taken effect assuming i saved the file yeah so that builds the same widget that we saw before but now from a plug-in rather than just from a python script and if we open up a sample let's just do the clock again oh yeah you can see also the sample data that was provided by our cookie cutter right so you can see that sample data file there um and that's also available to you to play with straight away so you can just pip install a like naked cookie cutter plugin um and you'll see everything in the viewer and you'll be able to play with it but for now let's just open a clock and that's being pre-populated there so all is working as expected and if i just give it a number and run it adds the layer and it does the thing right so now our intensify widget is a full-fledged plug-in with the reader and sample data and so on and that would be ready to release to pi pi if if we wanted it up there but maybe maybe not so now it's uh you do bit and if you go into your notebooks there is a where is this actually don't make a sample plugin oh yeah it's on the website do i have that loaded anyway uh so here on the website if you go to creating an api plugin it will walk you through pretty much most of the things that i just talked about but for your detect spots function so that your plugin can detect spots in images any questions before we hit the road on that cool oh yeah you could yeah so if you wanted to write a custom data loader uh where you would go in your cookie cutter plugin uh is to this reader file now this one here uh reads numpy files into numpy arrays so potentially it's not super exciting um but this is where you would start editing the code to make a custom loader for different um the different data files and the different file formats any other questions uh stardust actually use magic gui um yeah so start so the question was what do stardust use to build their gui um and they're actually directly subclassing magikovi as well and so you know that plugin was pretty complex but it's not so complex that magical you can't handle it so it's really kind of a you know developer's choice as to whether you go the full-fledged queue widget route or whether you just decide to use magic gooey um it's really easy to spin out magic gui widgets though as we've seen and so often when you're just playing around with things that will be uh that'll be the way to go so from uh from a like philosophical well not just philosophical anything that can be loaded into python as a numpy like array can also be loaded into nepari and so that's not just numpy like arrays it can be a das array or like a a za array loaded from file any of that it can be loaded into nepari the what the readers do is given like a specific file format bring it into python right so if you can bring it into python you can bring it into nepari more or less there's definitely caveats there but but as as a rule that's that's the the um the requirement is that it be a numpy like array that supports uh numpy like slicing and indexing that's a good question how big can the data be that you load into nepari the if it's two if you're rendering into a d and you are backed by a lazy data type so like a desk array which if you just drag and drop something into nepari you will be backed by a task array then it can be as big as you want it doesn't have to be in in memory um because the fact that it's backed by a lazy data structure means we only load the specific bit that you're looking at volumes at the moment we don't yet have like async um like loading for and so what that means is that if you want to view it in 3d you are limited um by the amount of ram on your computer for now um and that won't stay that way for long basically but that's uh yeah so for now 3d rendering is limited by the amount of ram that you have available to you but if you're loading in 2d and you're backed by a lazy array it can be as big as you want of course though if it's like remote data or if your like chunks are really big or you're you know your slices are really big performance could be an issue and if you just have one giant 2d image that is like 150 gigabytes that would also uh cause trouble going a bit more into depth on that i guess if you have multi-scale images if you have pyramids of like successively lower resolutions we support that so even if your single slice is really really big if you've down sampled it that also like you know yeah is a way to sort of get around the data issue but longer term and by longer term i mean in the next sort of maybe year or so we should be able to load anything from anywhere uh regardless of the size because we're looking to implement async loading um so that so that you really only load the thing that is currently on screen yeah yeah the theory is good um yeah any more questions obviously come up to us put your hands up but we'll kick off with the rest of it what's my favorite plugin uh natari okay that's not a serious answer but it is well no it's kind of my favorite plugin just because um actually maybe because i just think it's really neat um so what it is i probably should just go on random and i'll show you because he usually has gifts um yeah so you can play like little games in the viewer so so one of them is like a slider puzzle um and then there's like brick breaker um yeah so so i don't know i mean this isn't my most useful plug-in maybe but it's definitely the cutest and i think you know i think one of the reasons i really love it is because it just exemplifies how like playful and and well i think fun the community is um and yeah we love seeing sort of you know cute stuff like this so if you you know if you want to yeah yeah yeah yeah exactly um but yeah enough about atari i suppose let's build serious plugins that detect spots yeah well yeah they are right yeah blast and there's yes snake as well i mean it's pretty cute uh and you know it kind of showcases um the many weird and wonderful things you can do with my keychain but yeah
10,The Jupyter Interactive Widget Ecosystem,https://www.youtube.com/watch?v=1vuI22MkkrY,good morning everybody and welcome to the jupiter interactive widget ecosystem tutorial my name's matt craig i teach physics and astronomy in um northwest minnesota so i hopped on a plane a couple days ago it was nice and 70 got off the plane walked to the bus stop and almost died um but you know in winter you folks from texas are welcome to come up and visit me in minnesota and you can have the exact opposite experience um it's uh we're fortunate to have a big group of people here presenting and so we're going to introduce ourselves as we go through the presentation but tutorial presenters if you could stand up and wave please so everybody knows who you are and jason your your tutorial helper so you can yeah right swave um before before i jump into the content um i want to check does anybody need help getting this open or give me a thumbs up if you if you're ready to run okay if you're not quite awake yeah just don't make eye contact and you're good so the way things are um organized here is that except for this index the notebooks that we're doing are in the file browser on the left and are arranged in order so telling you all of this so that when i get lost you can help me get found uh first questions please ask questions it's really helpful for us to have questions one of the interesting things about the couple of years that we're online is that online we got many many many more questions during the tutorial than we did when we were in person i'm sure that if those had been in person people would have had all those questions but it feels a little bit harder maybe to ask in person but it actually helps us if you ask questions helps us adjust the timing we know whether to speed up slow down so please do ask questions if you prefer to ask them electronically we are watching the slack channel as as we go on and feel free to um post and slack and we'll try to answer there [Music] uh so a little bit about jupiter lab um let me start with how many of you use jupiter lab all the time okay so i'm assuming by contrast the rest of you don't take a couple minutes and go through a couple bit of the mechanics so um there's file browser on the left and um double click on a file to open it one thing that may be handy as we're going through this so you can see things a little bit better you can either click on the file folder to collapse that or if you push either control v on a pc or command v on a mac that will also show and hide the file browser um to run code in a notebook click in the cell where the code is hold down the shift key push enter and that runs the that runs the code so go ahead and run to the point where you've got a slider showing everybody have a slider okay so one other handy trick that's going to be useful later in the tutorial is that um if you we can create a new view for any output and move it wherever you we want so on the slider if you how do you have right-click setup [Music] if you right-click or whenever you have it set up to be on your mac um then select create new view for output and you'll get a new view of the slider so it really is the same slider and you can drag that output view wherever you would like so if um there's gonna be a couple times we're working on a on a widget we make the widget then several cells later we're making some changes to the widget and it'll be handy to drag a view of that widget off to the side so that you can see what's going on as you're working [Music] um so a little bit about the widgets the uh widgets or controls that are displayed in the browser um under the hood the the widget itself is um written in javascript but the focus today is going to be on the python interface to that javascript so everything you're going to see today is going to be all in python um a widget and this first widget we've called slider has a value so let me add a cell here print out the value it's 8.2 i can add so all of the things i'm talking about now we're going to come back to in more detail but you can do operations on that um on that slider and observe changes in the slider so as the slider has changed this function handle change will get called and it'll calculate the square of the value of the slider and [Music] i have no idea what the square of 8.2 is that looks plausible let's try that right so notice it is updating the value of square as i move the slider sometimes all you really want to do is just figure out what's going on with the function um you don't want to have to write a lot of widgets code you've got some function you're interested in and you just want to get a sense of how that function behaves and there's an interact function which creates widgets for you without you having to type any widgets code so here we've defined a function that squares a variable um if you call interact with whoops with that function as its first argument um and you give it a you've got some options here that we'll get into in a little bit but you give it some some information about what the argument is and it will generate the widget for you and as you drag the slider it's calling the function and the functions updating my favorite example of this i taught a cosmology course several years ago and i was trying to explain that the opposite of observational data was consistent with a bunch of different possible parameters for the universe and they didn't really seem to be getting that so i said okay there's eight people in the room all of you take this widget and i want you to adjust the you know parameters density and double constant for the universe until you match the data and then i had them share their results and they were all over the place and then i could go back and say so that's what that error ellipse means that the values could be any of those and you'd be consistent with the data we do some of the um exercises in the tutorial we'll have solutions and loading those is fairly straightforward there's just one gotcha so you uncomment the load you execute that cell wow and then disappointment it ensues so we decided to that is a much easier solution than what i was gonna do thank you um okay so uh running that um loads the the code for the solution but it doesn't run the code for the solution so run it again and you get the solution uh so you've got links here to a bunch of other um widget libraries we'll be talking about some of these in the tutorial some of them we won't for many of them we have um notebooks in the tutorial that we're not planning to talk through but are there so that you can look through them um at your leisure and most of us are here most of the week and maybe over the weekend too and so if you are trying this out you have questions nothing makes us happier than when somebody walks up and says so i was trying your tutorial and i was like wait stop just let me enjoy that so um it's a little bit more uh framing i wanted to show an example of a um cova dashboard this link actually does not work right now but and i did not write this dashboard um but when covid started one of the faculty in my department was i decided to work on a computer science degree and took a was taking some dash i don't know what the course was statements of course so for his homework he made a covet dashboard um that showed a running count of cases um you could choose a couple of different locations to graph data for a bunch of different information you could choose to plot whether you want a logarithmic scale et cetera et cetera a map which you could hover over to see [Music] infection rates in a particular part of the country and um all of this is written in python the code is as far as i know is still here but the um dashboard itself is not running when i checked a little while ago so there's quite a bit you can do with just python so by leaving out the javascript we're not we're not omitting anything essential that you need to to write some interesting apps with widgets any questions before we move to the next section okay so up at the top here there's a breadcrumb thing so you can get back to the index and with that i will hand it off to our next presenter units looks like i left my mask somewhere oh it's in my bucket cool thank you well good to see everyone good morning um so my name is eunice there's a little bit of a little blurb about me somewhere um if you're interested in knowing more about me i'm happy to talk more about myself but like that's not the topic here so i'll just get right into it um so if you navigate back to oh i guess i already have the right folder open so if you go inside of the first folder in here i'm going to go through the first notebook at the top here um and so i'm going to be getting into more details um for what matt was talking about in terms of uh the mostly interact in in this this notebook so uh and like matt said it's way more interesting for everyone if you ask questions so feel free to interrupt i know it's early it's early for me too but feel free to just raise a hand and start talking because i don't especially like talking so there's someone else to do the talking but i'm here so um right so we're gonna start with some imports at the top here um and it'd be great if you ran through this with me and let me know if you run into any issues um this is mostly going to be repeating a lot of stuff that matt already said so i'm going to go over this pretty quickly but we're basically just auto-generating ui controls for function arguments using interact um and the way we do that is we basically just like create a function and then call interact pass the function uh and then we can pass several things like matt hinted at here we're just going to be passing a value an integer and what this does is that it creates some like defaults for the slider and you can slide this later around and you see that the value at the bottom here gets updated um so yeah another thing that you can do is instead of passing an integer you can pass a boolean and what this does is it creates a check checkbox and so here the value for x is a boolean but it translates into an integer because our function is multiplying things by three so there's that um and then if you pass in the string interact will understand that this is that you want a checkbox and it does the same thing it multiplies your string by three which in python translates into this thing here so uh and instead of creating things a little uh you know this is a little clunky to like write a function right interact you can also just use a decorator and this does the same thing where it creates like in this case it creates a checkbox and it creates a float slider and we're going to get into like how the float slider bounds are defined right after this section um so [Music] yeah any questions does that make sense cool um i also kind of want to know first of all i should have started with that but and i meant to but i forgot um does anyone use interact all the time a little bit nope okay um great well this is hopefully useful um great so another thing that you might want to do is you might have a function where you want to um explore one of the arguments but not the other one right and so you can in this this context you have a function where you're returning a tuple of the values that you pass in which isn't super useful but it illustrates the example pretty well um so instead of having two sliders like you'd expect here if i didn't have a fixed um keyword here you only have one and this slider controls the first value of the tuple which is expected so yeah uh we're going to move to widget applications now so um we are this is something that i sort of like talked about a little bit at this point but basically what happens for um in in interacts is that you pass in a value and interact interprets that value and understands like that what you want is an insulator when you pass it a uh an integer and what happens uh really is that the integer gets transformed into an insulator widget object with some default parameters but you don't have to worry about that if you don't want to the nice thing if you uh well it's good to understand that because for one thing you can pass in additional optional arguments in here and in this case you control the minimum of the slider the maximum of the slider and the step these are the default values so this would this actually creates the same widget that we've seen above but if you wanted you know a positive integer then you'd be able to do that by by controlling this um and there's an easier way to do that and we're gonna we're gonna talk about that in a second but you don't actually ever really need to declare the the widget explicitly this way so yeah i'm going to pause again any questions cool all right so this is just a little table that summarizes like what arguments do what so if you pass a boolean it creates a checkbox the string creates a text box etc etc and there's like a special case for drop down that we're going to get into in a second um so yeah great so this is what i was talking about a second ago when i said that you didn't actually ever need to create like uh that that widget object uh you could just pass in a tuple uh and this will understand that what you're talking about when you're passing in like these these two numbers is that you're talking about the minimum and the maximum of the widget and this is still like using that function that we had defined above that multiplies things by three so if you multiply zero by three you get zero if you multiply four by three you get twelve so it seems like it's working uh and a float slider is uh if you notice here these are actually integers but because the third argument that we're passing is a step uh it knows that what we want is actually a float float widget and we can move that around and the precision that we get here is 0.01 you'll notice that sometimes it goes a little crazy that's just like precision stuff so we're not going to get into that now but um i think what it's attempting to say 0.26 here that's close enough so here we're gonna stop for a few minutes and i'll just give you time to go over this exercise um you can be creative with it or you can just do the obvious here but basically we are giving you a function that reverses the string um and you know do something with interact that interacts with that string and turns into something something cool so i'll leave you to do that for like uh say two minutes maybe and pick up again on your computer uh or you just feel like chatting um is yeah we're gonna actually we're gonna talk a little bit about that later but we're not um it won't be interact directly but it'll be we'll be like observing a value change and then plugging that into a bq plot or something so we're gonna talk about like how widgets can interact a little bit yeah thanks for the question [Music] cool i see a lot of post-its um are we good to go and you anyone have any questions do you is anyone running into any issues i that's a no great um so i am just going to load the solution here and so what we did here which maybe isn't like the most creative thing to do but you could you know you could like maybe reverse the string by like jumping two at a time and like rotating around the string but whatever um here we're just like you know plugging in something hello world and it reverses it in real time it's wonderful great um so moving on um still no questions right awesome that's what i like to hear maybe um okay so this is yeah so this is talking about default arguments again um so if you wanted to have a slider that starts at a specific value the way to do that is that you well this is one way to do it you can use the decorator like we said above and then you pass the default argument inside of the function and this starts the the slider at 5.5 um and this is talking about drop downs you can create a drop down by passing in a list the list can contain strings or whatever you'd like and you create a drop down with those items and then you can also pass in tuples in this case what this does is it creates a drop down with values here that link to the other values so the function that is actually going to be using the second part of the tuple to compute this value and that's that magic and here we're going to get into something that's a little more involved um but we're going to talk about the old way to uh to plot things and then we're going to go over something that ian put together for like a better way to to create plots um and we uh we're gonna play with this for a little bit so i'll let you run this um and so this uh oh actually i'm gonna have to restart this nevermind what is that what you're gonna say yeah thank you oh no that's also not gonna well actually this would work okay fine that'll work um well actually it would work because i'm not running it again but um so oh no that's still not gonna work okay so just restart the kernel and run just that's all [Music] i'm sorry [Music] okay so so this is uh this is the old way of creating a plot um so i'll just go through the code really quickly so you create a plot um you clear it which doesn't do anything because the plot is uh right now well it the first time the function runs it doesn't do anything because the plot is empty it creates a grid um and then it creates this uh this like uh sort of like range object that's not really a range object uh like points that are spread between -10 and 10 and you're picking a thousand points between that that interval uh and then you're plotting that you're plotting plotting this function with these parameters that are being passed in here um and then you're specifying that you're you want your y-axis to be limited between minus five and five uh and then you just show the plot uh and so this is also passing in uh you know the ranges for um m and for b and this seems to be working fine um but as the text in here gets into we'll go over the the reasons why this is sub-optimal the first thing is that it's um actually recreating the plot every time you move the sliders around and you can't really tell right now because there might be some optimization doing uh happening in the background i'm not actually entirely sure how that works but um but for the fully interactive plot you'll see that things are a lot a lot more elegant the other thing about this is that there's no zooming or panning so you know it's just a pretty it's a pretty standard plot that doesn't offer a lot of features and the screen currently doesn't jump but it will in a second so a better solution is to use um i just jumped um is to use metplot if i pi mpl um feel free to correct me ian if i'm not pronouncing that right um and what this looks like so i was talking about panning and like moving around so you can you can select the the controls here and you can move around the plot and you can also just save the plot to something uh if you'd like the other nice thing about this is that uh we are actually only changing the the data that is plotted so if you look at here the what's happening is that you have this line object that's the plot and you're changing the the set of data that is being plotted as opposed to changing the whole the whole plot um so the fun thing to do here is if we go back here and i'm not sure this is entirely a good way to do it but you'll notice that now that if i change these parameters what happens what actually happened in the background here is that you create a new plot every time and you can see this this because the figure number keeps keeps increasing um and there's you know it's jittery uh because like again you're recreating the plot every time so great um pause again any questions for that yep uh did you did you like run all cells or uh-huh and the plot just doesn't show you die i think you're taking help yeah you can take a look is anyone else running into the same issue no okay does anyone um is everyone able to it sounds like is anyone not able to create the plot it's another person inside okay two people sorry the libraries are getting locked up oh okay did you go to through the installation instructions in the tutorial readme okay um hey jason thank you jason um anyone else okay cool well i'll keep going for now but uh let me know if you're uh the two of you are um you know running oh it's usually um great so i'm gonna keep going for now but um we'll uh come back to to this if you need to um and uh actually we're gonna have an exercise in a minute so we'll i'll give you plenty of time so that people that are having issues can catch up so uh this is gonna talk about mpl interactions um and we can automate here um interactions using matpotlib uh and so this is basically just like a more straightforward way of doing what we just did above um with a lot a lot fewer uh characters so this is this is you know this is creating uh two things the figure and the data uh basically and then creating a grid uh we're setting the limits again and then this is the same function that we've been using so far uh and you can do what we did above which is this command and it creates a very similar plot so um we are going to give you some time to go over this exercise and making a plot and um yeah we'll maybe give you five minutes for that so it's 8 31 right now uh if i see a lot of post-its i'll cut it short but let's go to 8 36. um yeah if uh if y'all are done with the exercise the post-it would be great um and we can keep moving if you need more time it's also fine you should say so anyone anyone needs more time no okay great cool so um this is just uh giving you more uh more information if you'd like to know more about interact and um interactive you can look at the example in the app rigid source repo and we will be going to um so this is this also there is there's this other notebook that you can go through um yep uh so if you create a float slider it'll be floats by default i see uh cool any other questions yep that's right i mean that's that's yeah i think that's probably most of it yep ah yes yes yeah well jupiter we'll be talking about jupiter light later which does does that so cool thank you for that question that was interesting anyone else okay cool so uh we are going to go back up one folder and then open the basics and open this simple widget introduction um so this is just going over like the concept of a widget what they can be used for um and yeah so it's maybe you know like at this point we've interacted with widgets um the sliders were widgets uh what they really are and this is sort of getting into the question that you're asking about the kernel um they have their own display um it allows them to display using python display framework um and actually this is not the part that i was thinking uh oh is this this is this is the text i was thinking about yes yes uh what are widgets which is eventful python objects that have a representation in the browser so there's there's a back and forth there's the fact that they have a kernel component and they have a front-end component and yeah so as we've seen we can use them for interactive guise and all sorts of things uh and so we are also just going to be going through uh a few things here so the the widget the insider um display uh by default if you just you know run this what happens here is that we're calling uh calling the display method and it just like shows the the widget object you can also just call that explicitly uh and so you in this case you're importing display and displaying the widget and you can do that several times so if you do that you actually you know it's the same widget object that you're controlling so when you change the value of one you change the value of the other one as well so this is a little diagram that explains like how that works and oops so basically the widgets are represented in the back end by a single object which is why when you modify one of them both of them change each time a widget is displayed and your representation of the same object is created in the front end and these representations are called views but they're actually pointing to the same the same widget in the back end any question no great so we're going to we're going to go over some properties of widget as we've seen this is you know this is our friendly widget it has a value of 28 right now uh you'll notice that if i change this this doesn't change but i can run it again and it'll update um you can also just change the the value manually we'll get into an exercise a little later um but you know if if this is like more than default this is not going to change anything um same thing if it's like minus 100 um you know our widget is not going to be able to handle that but we'll get into like how to circumvent those limitations um another property of the widget is the keys um this is you know it's just uh it's basically just a dictionary that has like a bunch of a bunch of keys and these are these are all the properties that you can access uh that belong to the the widget object um great that's i think that's that's it in terms of i guess this is also uh this is also a property of a widget you can set the text um manually by by you passing in passing in like some optional arguments like disabled in this case you're not able to change the text box which is maybe you know again like not the most useful but um there are if you also i don't know if you're familiar with this but apparently i think you mentioned earlier maybe not too many people use jupiter lab but you can uh i think it's like shift shift tab um and you can see you can look at the signature of this and in this case it doesn't tell you a whole lot of it doesn't tell you a lot because it just says arguments but um but sometimes it's useful to like know what arguments you can pass to in this case the text method uh great so that's that's it for the the properties any question on that cool um great so here we're going to go over linking two similar similar widgets so if like we saw earlier you can like you know you can display a widget twice and they're linked but they're only linked because it's the same object so this is like a way to like explicitly link um widgets or a different of different kind um and so in this case we're creating a slider and we're creating a um like an input value uh and so like if we change the value of this one we also change the value of this one similarly to earlier like if we go above you know 10 this doesn't work anymore but again we can we'll go into uh how to how to not run into this issue uh and in order to unlink the widgets you can just run this up [Music] i think this is [Music] yeah sorry about that there's uh so if if when you link an object it will return a link um object and then you can you can break that by unlinking it later um so right now they're linked uh and i think if you do this they're not linked anymore so um so yeah just just sorry i forgot to add this to this box but great uh any questions no okay cool um so this is this is going over observe which is another way to to control the way control the way a widget is the value of which it is set by by basically observing observing like a a property so in this case we're creating a slider and we're creating a html widget and then our function is taking in a change and picking up that new value and taking the square and this is also something that matt went over quickly in the intro but basically we're just um passing the function to observe and then we're telling observe what what property we're observing for so in this case we're observing for the value um and so the value here is going to be updated with uh with the change from the widget um so again like this is this isn't like the most uh momentous thing because because you feel like you've seen this before and you know we we sort of did like this is you know this is uh it's what something like that we ran earlier um i just added this for comparison but this this you know looks a lot like the same thing and it kind of is but in this case the the bottom part here is you know it's an html widget which is a thing of its own um and the the two are linked in in uh but by using observe which is which is different from from the uh from the linking that we did above so yeah does that make sense cool any questions this is the end of this notebook so we're going to be moving to the next one no questions great okay so if you sorry if you navigate one folder above and then go to widget list we're going to go over this first notebook first cool so i'm just giving you a minute to open it but um great so in the uh in this we're gonna go over the communication framework between the front and uh and the back end and um uh not explicitly but we're going to go over how that works and a set of fundamentals user interface elements like buttons and checkboxes so um the the main thing that i think this this um shows you like the power of widgets um i'm gonna let you explore this for um the exercise but there are lots of things that you're um that are you know at your fingertips when it comes to using widgets um and this is kind of a fun way to just use use some of them and see what you can do with widgets so um i well let's let's uh pick up again and uh let's do maybe 13 is that two 13 minutes is it too long matt so we y'all can get started with the exercises and just um you know exploring what you can do with widgets and um i think we'll we'll do let's do eight minutes instead and yeah pick up pick up at uh whatever that is 55 i guess um okay so well i hope you enjoyed playing with all the the widgets um we're gonna move to the last um section in this this main category the widget overview um and this will give you a sense of um some some of the some some of the stuff that you can do to like connect to an output um and then and then or connect to a widget that displays a bunch of sub widgets and then connect to it um and place it in different um different spots on your on your browser so um [Music] i'm gonna go through the code here so what you're creating here is that you're creating an output widget that has a cyan blue color and then after the widget is created you can you can still like add things to it so in this case what we're doing is that with this output widget we're going through range and printing hello world 10 times and i guess i should uh where is that so matt said earlier is that you should be able to where's that create new view for output yay thank you and uh i'm gonna put that on the side here and so i can add some more stuff and you see that this gets created and yeah so you can also direct rich output to the same area in this case we're displaying a jupiter interactive widget tutorial from the previous year and you can display complex mime types such as nested widgets and output widget so this is an insulator um and you can append the output to the widget directly with the convenience convenience methods depends stand out depends on their independence add display data um this is a different widget at this point because i guess this one's getting overloaded but let me just do that output and we're not currently able to display widgets what happens when you try to do that is that instead it refreshes the widget but it displays the uh the widget underneath it um there's a there's a an issue that has a lot of history but it seems like it's not i don't know if jason can maybe talk about that um it's not really a question there's well there's this issue that's that's been uh it was created in 2017 but um it looks like it's been closed but i think the issue is still there okay uh it's issue 1811 in case you want to look at it um and then you can clear the output uh in this case we're clearing it with weight just for fun um and uh so you can if you just clear the output obviously the approach is clear as if you pass weight what it does is that it waits for the next time you try and add something to the output widget and then it'll clear it before adding that something in this case i'm just appending something which still depends it it does not clear the widget but if you were to like add an insulator with this this syntax this this will clear the widget before adding the insulator and then you can also use this as a decorator um in this case uh if you run this and you're scared don't be this is intentional so what you're doing here is that you're capturing all the output from this function you're printing stuff and you're raising an exception and they're both being passed into the the output widget um and it looks beautiful um so yeah then you can just clear the output and that's that any questions set a question oh yay cool no questions great so um uh this uh so here we are uh i don't remember what this part is about but okay so here we're just like yeah uh interacting with uh with widgets in a way that's like um a little a little more i guess you know uh it's better better on the eye it's like a little nicer to to look at i guess um but basically you're uh you're creating a horizontal blocks um with these two widgets and inside of the first one you're going to have a vertical box with in sliders and then it just like you know shows things you know it's like pretty pretty to look at i guess um yeah uh the side car is basically like the output widget but it's on the side um it's i guess a faster easier way to do what i was doing earlier like clicking on the widget and placing on the uh on the right hand side and so we're creating here a site car with the name example that we can close and you could just recreate it if you create another one it'll have the same name and that's okay at this point you have like this this one is not going to be used anymore your site um sitecart is the second widget that you've created so notice how they they're they're sold different objects and you don't have access to the first cycle anymore um and you can do the same thing with the output widget and i use it as a decorator and then um capture the the output from a function and so in this case if you run the function it captures the the print which says greeting from the function and you can do the same thing with sliders yay and you can also clear the output uh and here we'll go into an exercise and after that we'll have a break so um i think the break is meant to be 10 minutes and this exercise should probably take you about five um so we can probably reconvene at like let's say 9 20 or 9 15. it's 9 15. so uh feel free to uh take a break uh go for a walk um after you've done the exercise and we'll reconvene here at 9 15. and it'll be someone else talking so if you're tired of hearing my voice everybody we're gonna go oh boy sorry um get started on the next section which is section three widgets events so if you go back to the notebooks and you go to the second number three that's here um and then open the first notebook in that that's what you want to do is this big enough can everyone see should i make it bigger in the back mostly all right um so in this uh sorry i'm ian uh i use widgets daily as a grad student my advisor is always like change this plot make it like that so they were very useful for that um and in this section we're going to talk a little bit more deeply about how to work with callbacks and how to set up sort of events manually which is a bunch of stuff that interact does for you automatically and this is a little bit more of the lower level how to use it how to interact with it how to debug some of it so let's get started let's hide some of that um so there's a library called traitlets which is where widgets inherit all of their sort of observe link follow along uh behavior from and so you can there's some links here where you can read about that in more depth but we're going to go over sort of the basic how-to of how to use it um and so this is sort of mostly information for reference and we're going to go over this sort of with via example um so we did we've seen a few examples of this which is in the kernel but in the kernel we mean in python and so registering callbacks to trait changes traitlets uh creates these things called traits on widgets which are like value is a trait on a slider and those are the things that we can follow so in this example we've seen this before we make a widget we make a display and we'll talk more about matt we'll talk more about this layout a little bit later and there's a key difference this is actually the last example from the basics section um and there's one big difference which is that in that section we had uh names equals value here and for the moment i'm just gonna comment that out because i think a really common thing that you can run into when you're working with widgets is you set up some callback and it doesn't work and you don't know why and you have to figure out why and so if we get rid of this names equals value and you run this it looks fine but if we move this around the square no longer works and so the big question is why clearly there's some error going on but where the heck did it go and this is sort of demonstrating the importance of that names argument and so to figure out what was going wrong there's actually two approaches we can do um one is the jupiter lab uh console log console um which for me sometimes shows up down here and sometimes does not and so you can also see it if you go to view show log console and when you read that you can see that we're getting a unsupported operand type star star for pow dictin in which is confusing because sliders should only have the number is not a dictionary a number should be an integer so in order to investigate a little bit more deeply what's going on a nice thing to do is to use the output widgets like we did before so uh in the next cell down i've copied a lot of the same text from before so we can mostly ignore that part and look at the new stuff here which is this output line so this is the way i really like to debug interact observe kind of things i make an output widget i display it right away so i don't forget to do that then i capture the function and then i just print out change and it will also capture the errors like we saw earlier so if you run this and then we move our slider around sometimes it works sometimes it doesn't but we can also look at what was printed out here so we can see that in addition to what we expect value and then there's a number here which is great sometimes we're also getting changes from property lock which is not something that we want to track and use to multiply or take the power of and so the key difference here is that when you do observe without any extra arguments the names argument becomes traitlets all so it listens to every single possible trait or anything that updates and has this sort of callback system on a widget um and so what you can do now is now that we've sort of figured out what the core error is is that we're listening to too many things um we can come back and this is again the same code just replicated and add back this names equals value because value is the part of the slider that we want to track we want to know what its value is and now we can see that we just get the change argument of this okay the value has changed so everything works because we can get change.new and it's not a dictionary it's just a number which is awesome an important thing to note is that it's not just when you're moving the mouse around here it's also when you change the value by code so anytime the value changes so if we do equals three here you can look at the last one last new is 80.6 slider value the new value is 0.3 um and then so a final thing we actually touched on this earlier with the dot keys argument which i have to admit i didn't know exist i've always done traits you can see all the things that are observable in this way or that might show up in uh observe or also all the valid arguments to names are can be found by anywidgets.traits function and it tells you not only what the name is but also what sort of thing it is so is it a list is it a boolean is it a float and you could you can you could play around with that print function and change the name and see what all these things look like um are there any questions about any of that i truly do not know um that might be in the traitless documentation um it's yeah it's like they they're like it's like a it's an object level like instance but they get defined as class variables which always was confusing to me as well um but each each instance will have its own thing so there's a those are objects themselves is my understanding good question anyone else all right um and this is something we sort of uh touched on earlier which is that one of the other things traitless provides is this sort of validation of values so in sometimes values will be coerced to be what traitless has like what the widget says it should be and sometimes it'll throw an error if you set it to the wrong thing so in this cell if we have a float float slider and we set its value to a float that works that makes a lot of sense um if we set an in-slider's value to a float and then we print it out we'll see what happens after we run this cell and finally if we set an insulator's value to a string we'll also see what happens so float slider was fine the insulator got squashed into being an int and then if we try to set it to a string we get a long error message that at the end tells you what's going wrong which is sort of a nice thing if you run into this error it's probably what it means and you have some sort of you're giving your object the wrong type all right so uh to sort of come back to using observe and name arguments to observe along with outputs um i've got a quick exercise hopefully quick i don't maybe it'll take a long time then we'll learn something um to make a text area widget which we've created here and then use an output widget and observe to print out the reverse of whatever's in the text widget which is an exercise we did earlier with interact and this is sort of how you would build it up from scratch and similarly put your post-it up when you're done i see not everybody is finished some of you have um if you're having trouble first of all be friendly there's people next to you with little post-it notes up so ask them what they're doing but also does anyone have any questions or is stuck on anything on this does anyone really want more time before we talk about the solution cool all right let's talk about the solution um so if we load the solution here um the trick is to do uh capture the output of the function that reverse prints out the reverse of a of uh an argument and so remember change is a dictionary and so we call the new argument of the dictionary which will give us the new value of whatever thing we're tracking and then we use this little python indexing slicing trick to reverse it and so for observe we just do reverse and names equals value because value of the text area is the thing that we want to track and the way you can remember that is that it's almost always value that you want to track and almost never anything else and then you could of course look at the traits that it has so if you run that hopefully this works it prints that a bunch of times um and so if you wanted to you could also clear the output in between each one but i didn't do that in the solution and i think this is also fine any questions about that solution now that we've sort of looked at it yeah so at success we're going to talk about sort of at the bottom of this notebook um uh if you want a sneak preview the continuous updates argument is what we're going to use for that um but we will get into that although we should use this example there and maybe in the future we will so that's a really great question thank you anyone else all right so um another thing we talked about earlier is uh using link and so there's a little bit of overlap between observe and link it would seem if we're linking two widgets together um why would we use observe or why why wouldn't we use link what's the difference and i think that they have overlapping but also slightly different use cases so observe is really nice if you want to have a side effect like printing something out or modifying a map lib plot or saving a file connecting to something that's not necessarily a widget but link if you just have two values of a widget and you want to two different widgets and you want to link their values together somehow link is really awesome and easy way to do that and it doesn't just directly link them you can also give a transform argument which is an arbitrary python function that takes an input in and transforms it to something else that will then become the value of the next thing so here for example we have two sliders and we can call widget.link and again if we hit shift tab inside of this it will tell you everything there is to know about using this function so it takes tuples of the widget and the trait you want to follow and then the other widget and the other trait that you want to link so if you do this you run this when i move one slider the other one also moves which is pretty sweet so these are two different sliders not two views of the same slider um and notably you are not limited in linking uh one trait name with the same trait name you can mix any two things that have compatible types so uh sort of a lot of code here but i think let's look at what it does first then we can talk about it i have a main slider which can just move around and i have another slider which controls the minimum of the first slider so if i increase the minimum you can see that now the minimum of this slider is eight and if i decrease it to minus 10 suddenly the main slider has a different minima and so you could do this with all of the different properties that these two sliders have you could do it with a range slider you could do you could use a range slider to control the max and min of the of the other slider if you wanted to um but again the critical line is this one this widgets.link line and here we have min here we have value um so any questions about that cool so then we have another exercise which is to use witch.link with the transform argument to link the two temperature degrees celsius and degrees fahrenheit so right now if you change one of these the other one doesn't change and 32 fahrenheit is not the same as 5c we should have changed the numbers to be more uh work with the austin temperatures but what can you do um it's more like 40 i guess um so in the next cell you can actually just do in the next you don't even need to put in the same one uh this should be a one-line solution there's you can use these two functions up here for the transform argument and then widget.link like we were doing above and so let's take three minutes or so to do that and again if you're stuck i encourage you to be friendly with the person next to you they might know something you don't and you might know something they don't [Music] all right um again reminder about the post-its if you have done it um seen some post-its and it is the sort of three to four minutes that i claimed originally um so are there any sort of outstanding questions for people who are working on this errors you're running into all right cool so let's look at the um solution real quick you can also load this yourself so just delete that load this and then you remember you got to run it again and so now if we change this they both change which is great and hopefully the temperature will go down [Music] so again looking at this we just have one widget oops one widget and the trait that we want to follow which is its value the second widget the trait we want to follow which is its value and then the transform is just a tuple of the two functions in the same order so degree c we do c to f so that's corresponds to the first one and degree f is the second one so we have f to c as the second function uh does that not make sense to anybody totally reasonable if it doesn't and please raise your hand right now all right um so we're gonna move on a little bit to how much that just rendered oh my goodness some more uh advanced widget linking so uh link is not the only function there's also d-link which is a one-way link so directional um and then there's things we're gonna talk about a little bit which are pretty nice jslink and jsd-link so jslink we were talking about the widgets exist both in python and in the browser and there's sort of this layer of communication from browser to python back to browser which can take some time and so js link what it does is it links the widgets directly in the front end in the browser then also gets synced back to python which is good but for the display it can actually be a little bit faster to do a js link which we will look at so this is an example mostly for reference if you just run this we have just create some sliders and we have a d-link and so if we change source target will change but if we change target source doesn't change and that's because this link is a one one directional uh similarly like other links this can be broken calling unlink now none of it connects um at any point i'm going to faster your question please just yell at me i will be happily just happily stop and so then this is the js link that i was just talking about so it works exactly the same sort of syntactically to using link you just add a js in front and now if i move these around they are synchronized and this happens first in the browser then makes its way back to python and similarly we can do the directional linking via javascript um so this is my understanding of the differences between [Music] linking in python or linking using javascript you can write a python function to transform your values if you link in python but it is slower and also doesn't necessarily work if there's no python kernel it's not going to work and you can have widgets outside of the content of a python kernel they can be in a website and so client linking is better sort of in that sense and there's a lot more details on this in the witch documentation like paragraphs of text going into great detail which is awesome and just as a quick example of that sort of drive this home here we have three sliders one which is the leader and then two followers which are either linked in python which is the client link uh sorry in python which is the one called python link or in the javascript which is one called client link and if you move these you can see that the javascript link ones stay in absolute lock step whereas the python one lags it by a little bit because the just because it has to go back and forth any questions about any of that that's a little bit of a whirlwind through a variety of functions but perfect um and then we're gonna talk about continuous versus delayed updates so this is what a person over here noted earlier is that sometimes you want to have some control over when when the sort of observe or the update fires and so for sliders it's normally continuous any time it changes at all as you move it along the value gets sent but you can also make it uh not continuous so you can make it so that every time a change happens at all it sends an event or you can make it so that it only happens sort of after you're done interacting with it so this is an example and you can i'd encourage you to play around with this sort of get a sense of what delayed versus continuous updates uh the difference between them and then here's a quick uh summary of what widget's default to continuous update equals true or versus continuous update equals false that's a typo and then the uh final part of this section is this idea of special events so things like buttons don't really have a value per se like if you just click a button it gets clicked and then it just is a button again so for for buttons there's an on click method where you can register a callback you can see what that looks like here so here's an example of using that on click um and that's everything i had to say about uh widget events any questions about any of that we can also talk about it in breaks later or anything but yeah cool yeah [Music] i don't know that there is it sounds like if there's not it would be a great thing to have is there jason do you know automatically what happens is so there's already there is a there is a page in the documentation about throttling and debouncing that also has some more but that was like maybe a little bit too in-depth for today all right cool so i think i am done and someone matt is next let's get our built-in fiber break here okay all right so if you go up one level to the notebooks we are going to go into folder four about widget styling um if you could please start with the notebook that's numbered0401 that would be great the zero zero notebook has some broken links in it at the moment that will get fixed later today but um i'd like to start us out in this notebook so we're going to spend a little bit of time talking about style which is something that a limited number of widgets have and you can change a limited number of the styles we'll spend most of our time talking about layout which is how the widgets are arranged on the screen or within each other um i believe that in ipad widgets eight there are more style attributes than at seven but as an example of a style i've created a button and actually let me check so folks in the back is this readable or do you want it large okay um right so the um some widgets have a style that style um in this case i've i've just set the button color uh there are several choices for colors not all of them are good choices um as with so the style attribute is itself a widget and so just like you can you can look at keys for other widgets you can look at keys um for the style so there's two things in the in the button style that i can change i can change the button color or i can change the font weight the nice thing about styles is that the style is say is a widget it's an object you can share it between different buttons so if i want to make all of my buttons this terrible yellow i can do that um i'm gonna change i don't remember why i changed it to red except i don't really don't like the yellow um [Music] sliders also have a property you can change the color of the handle um and uh if you click this link there is a list of all of the style keys that's for widget seven we'll update that when widgets eight is released buttons and progress bars have a couple of custom styles for them so i'm going to make a button and there's a warning style a info style and a few others um one thing to note about the styles is that if you well let me back up half a step so in in notice here i'm i'm using button style not just style so apologies the language here is confusing because there's a style for the button and there's a button style for the button and you want to make sure you style your button with the button style not with the style button um so uh if you set the style to a particular cover color that overrides button style so um if you find yourself spending an hour at the keyboard frustrated trying to figure out why you can't change the button underscore style that's why i would imagine that kind of thing could happen anyway um so there's a bunch of layout stuff that we could talk about but in the interest of of giving you a strong foundation for being able to put together lots of other interesting widgets that we're going to get to in a bit we're only going to hit some of the highlights many of the layout properties here are derived from css and so when i first started using widgets one of the things i had to learn was if i wanted to learn about to some extent about the layout or a little bit about the styling i had to google for css not for python um and there's a number of css properties that can be set we won't go through all of these but they are listed here for for completeness in many of the properties if there is a shortcut so let me yeah so overflow is a good example in css there's often a shortcut name for a property like overflow or a couple of specific names an overflow in the horizontal direction overflow x and an overflow value whenever we can in ipi widgets we just provide the shortcut property border is one exception that's changing in widgets eight um so let's look at an example so just like there are styles there are layouts so here i'm going to create a button and i'm going to give it this layout so it should take up width wise it should be 50 of the display area height should be 80 pixels and should have a 2 pixel dotted blue border um as you know as with the styles you can reuse the layout so if i want to make another button that's the same style i can do that and the nice thing is if i decide later that the um fifty percent is too big i want it to be thirty percent i'm using the same layout object for both of the widgets so i just change the layout and that updates all of that all of the buttons so quick exercise here say a minute or so um i'd like you to make the buttons border solid and green and make the width of the button 70 pixels and you're welcome to make the border 70 pixels if you want but i think that would probably be pretty terrible so the question is question from stock about whether button style is influenced by the jupiter lab theme um let's oh oh so it is effective yeah so the the text color changed on the button it's black now and it's um white when i go back to dark mode so actually yellow may not look so bad in light mode okay look just as bad uh so let's see we can do this in a couple pieces so i can set the width to 70 pixels [Music] uh so in the border we have to give it the thickness and what style it is and the color and i never remember what order those go in so i'm going to copy paste modify so one thing coming in in widgets eight is that you will be able to give different colors to different sides of the borders um you could have a little bit more flexibility here so now i've made the button smaller and change the border um so once you move beyond one widget and there's not a whole lot interesting you can do with a single widget you know interact is maybe an exception to that where you um you have several automatically generated well even there's not a single widget it's several widgets um once you move beyond one widget the question comes up how do you combine them and there's broadly there's a couple of different approaches to take so in a website layout you can use a flexbox layout which is great for laying things out in one direction whether that's vertically or horizontally there's also a grid layout which lets you lay things out in in two dimensions we're about to go through several different um approaches to laying things out i you know my recommendation would be rather than using the lowest level widgets the box widget and the vbox and hbox we've got some higher level layout widgets now some grid layout widget or 2x2 widget an app layout widget try starting with those first and if those do what you want great if not um you can dive into more detail and or you have more control um with particularly with flexbox layout and we do have um for both flexbox and for grid there's some very detailed guides about about using each of those so um there's a bunch there a bunch more there if you want to explore that further so we've seen a couple examples of boxes already i think we've seen v-box a few times um v-box and h-box do essentially what you would think they do hbox lays out widgets horizontally vbox lays them up vertically so here i'm creating a layout so i'm going to create a box i'm creating an item or a layout for each of the individual items in the box i'm creating a separate layout for the box itself so the items their width is auto so as the size of the box changes they'll expand to fill the box and the box itself will have a solid border and take up 50 it looks like i'm making a button oh i'm making several buttons uh let's see so if i change the width here so box dot layouts the buttons expand to fill that because the um width for the buttons is set to auto so in the next example um i'm not doing a ton of layouts so there's going to be a few different widgets are going to be arranged vertically and um the layout i'm setting is how i'm distributing any empty space in the widget so it's going to distribute that empties in the box it's going to distribute that space evenly between the the items in the box but notice that without me having to do anything fancy or or deliberate the size of the widgets looks nice in a column and the location of the labels looks good too so if you just straight up use a v-box you're going to add and you don't change any of the default properties of the layout the layout won't look bad there are issues sometimes with the lengths of captions that can be fixed and we've got a notebook one of the optional notebooks talks about that in more detail uh so i want to switch now to some of the more high level layouts and um we're going to take a look at a few of them there's a two by two layout which does what you might expect it plays which is a 2x2 grid an app layout and a grid spec layout for all of these it's going to be handy to have buttons which expand both vertically and horizontally so the cell you don't have to understand in detail what's happening in the cell but if if you want the overview there's a function up here that makes buttons which have um this particular layout so they'll expand to fill the available space and then i've made a bunch of buttons that we'll use in the later examples so make sure to run the cell but it's not going to do anything so here's you get what you get with the 2x2 layout um this does maybe it doesn't look very flexible at first because you it looks like you have to have four widgets if you leave one of the widgets out then the empty space will be occupied by one of the other widgets now in this case the reason that this bottom right button fills the whole space is that the um height was sent to auto you can leave an empty space if you want to so there's a merge argument to prevent widgets from expanding you can access the individual widgets so um the 2x2 grid widget has a bottom right bottom left top right top left and i can change the button style of the bottom right widget like that and i can fill in that hole if i want to by setting top right to a widget um there are a bunch of um or some style properties you can change about the grid so you can one of them is grid gap which affects the space between the widgets that make up the or that go into the grid so let me pause and see if there are questions my very first year teaching my students described me as a hyperactive gerbil on caffeine which was not wrong and the more excited i get about the stuff the faster i tend to talk so if i yes that's a fine question so one of the handy things to do if you're not sure what the allowed values are is just put in some value and traitless if if there's a list of allowed values it'll print out what the what the values are so um it looks like one of the allowed values is double quotes or you could probably use the python none as opposed to the string none so let's try that yep ah so in fact it you can't use python then it's got to be the double quotes um but like i said my preferred way to figure out what the values are as opposed to trying to look them up in the documentation is just put something there we go but something in it will tell you the values that are allowed so other questions okay uh so another example of this using a library plotting library called vq plot i'm i'm skipping all of the bq plot details here we do have a bq plot notebook you can take a look at but to keep us running close to schedule i'm going to skip over the details of setting up the bq plot um figure uh so i've got a min slider max slider the min slider effects the minimum value or minimum of the plot scale notice that i change the minimum slider the max slider is changing or appears to be changing what's happening there is that um let's see let me set the max down to negative point three um the range of values on the on the maximum slider is being set by the minimum value slider so if we look at the linking the maximum and wow i seem to have gotten it's interesting i've gotten um all of the tricky ones here so the minimum slider's maximum value is used to set the um value of the max slider um right so i'm going to crank the max up here to 9.9 and once i've done that the the range of values available to minimum has changed now minimum all the way up to 9.9 if i um the minimum down some and then drop the max down the highest the max slider can go or the highest the min slider can go is set by or is constrained by the max so once i get the max down to 5.7 we're done this was so this was actually about layout um so maybe sorry i got i just got distracted by the slider so here's the here's the layout so the top left is the minimum slider bottom left is the max slider the bottom right is the figure but it expands to fill both of the spots on the right side there's an app layout which is also very nice again i'm creating a bunch of buttons here to use in this widget that will automatically scale vertically and horizontally and in the app layout by default there's a header and then three center panes left center and right and a footer you can omit any of those so here the footer is none you can leave out the right sidebar and um like the um two by two if you leave something out everything else expands to fill the space and it's a little bit easier to see that in the case where he left out the right sidebar in the center expanded to fill so take a moment and make an app layout with no sidebars i'm going to give you like literally a minute on this so the approach here is straightforward if you get just set the left sidebar to none you should end up with no sidebars there are several different ways that you can affect the layout so i'm going to make this app and then make a new view for the output down there it's fine so um the first way is that the pain widths here so the pain is referring to the three things in the center the first one is a fixed width in pixels and the three and one beams of the remaining space give three quarters to the center and give one quarter to the right there's another way to say the same thing the fr is short for like free space or something um you can adjust the heights also so this made the header 200 pixels of the remaining space divided between the panes area and the footer with the size of the panes being three times the size of the footer so there's a couple different ways you can think about the three and one either as fractions but remember the total is four or is the ratio of the sizes you can change so app.left sidebar is a button and buttons have descriptions so we can change that and the sizing options that i've been giving changing after i've created the widget can also be done when you create the widget by providing um arguments to app layout questions okay can i get a quick thumbs up thumbs down do you want to do this exercise one two three show me thumbs up or thumbs down one two three if you're not awake you don't have to do anything that's okay so i'm seeing mostly thumbs up let's do the exercise gives me a chance to get a drink [Music] oh so when you're done with the exercise go ahead and put the post it on your laptop so i know and if you're running into issues or have questions feel free to raise your hand if one of us can come over and help you or ask in slack so the question was whether you can nest the app layouts uh you should be able to because each of them is a widget so they have so let's give it a try actually so app was a app widget so i'll just put that in [Music] to the center yes it's an illustration of the difference between can and should but write the new mandelbrot layout all right but we'll want you quick quick click through each of the mandelbrot widgets and when you're done with that we've got this great xeno widget so i'm going to load my solution for the app layout so a couple things here and this relates to the question that was asked about whether you can nest these layouts um to include the figure from above i'm not reproducing the code from above i'm just reusing the widget we created before so that looks terrible um so the problem if i remember right is that i've got my min and max the same way so let's change that oh oh oh i bet his size got modified when it got swished down into there or something that's unfortunate and it is and so if you rebuild the pop pq plot figure it works okay thanks uh so there's so another layout is i should pause other questions or comments thank you okay so there's a grid spec layout and uh this is like the grid spec layout in matplotlib if you use that and so um the idea is that you create a grid of a certain size here i'm making a four 4x3 grid and then grid behaves like a sort of like a numpy array you set each of the grid elements to a widget and you get the grid out so a couple nice things about this if you have um a button that you would like to have to you know take up several grid entries uh you don't have to use a bunch of loops to do that you can just slice and so when we run this button one was [Music] let's see rose ah right the rows up two but not including the last row and column one through the end um button two was all of the rows but only the first column and then three and four are set to specific grid entries um if you access one of the specific grid elements you can change its properties uh so so in particular this one um spans a bunch of of entries but the way to get at it is just to grab one of the one of the cells that's included in that range you can change the buttons you can re-slice in the next example here we're going to have a grid of um [Music] this is a random set of bar charts and scatter charts i think but illustrating the idea that you can um if you use either bq plot or if you use imi mpl which is the widget interface to map live then your matplotlib graphs are widgets and you can embed them into grids also or include them in your layouts and finally there's a grid box so what did i do here so the children of this box um are constructed here so in this case i've made seven buttons and the layout is um is a little bit interesting so with means what you'd expect it's gonna take up fifty percent of the of the available space um the grid template for the column tells you what size each of the columns should be and therefore implicitly how many columns there are the grid template for the rows is laying out what you want each of the rows to be um you're not actually so you look at this you might think okay the biggest i can do is nine um buttons because there's three columns and there's three rows in fact you can do as many buttons as you want i just put in 29. um what this does is the the column template really does specify how many columns you have you can have as many rows as you want the row template uses the template for as many rows as you give it so i gave it for three rows then beyond that it's using some default value so if i were to add say a 200 picks here then the first four rows are laid out according to this template and the rest according to the default i'm going to jump over to our outline and see where we are we're at see we started at 8 30 so that's 10 4 okay so we're okay um i think i just accidentally did the exercise i did part of the exercise so take a couple minutes look at look at this play around with the grid box layout a little bit this is one where when i first learned about this i had to do some reading and experiment a fair bit to like started to get comfortable with exactly how the template columns and rows works and this is one question i'll say please don't ask me a question about this ask google um or maybe jason or actually or itali or mariano or or you know ian but most of what i know about this particular thing is right there on the page already so go ahead and put a post-it on your computer when you're ready to move on so the exercises are primarily addressing how you handle rows that go beyond what you specified with the template there's an argument you can give to set the size of extra rows so this isn't the way you really lay it out but shows that um this is doing what what we wanted and i think that you can give multiple values like we did for the template so that the pattern that we chose repeats over and over again one last example of grid based layout is that you can do the layout in words so uh here's an example of that i've made four buttons i'm gonna make a grid box and um the way that the [Music] well yeah yeah the way that that's laid out is with this grid template areas so the header is going to take four columns main will take the left to the dot means there's going to be a open spot and then a sidebar and a footer across the bottom and so again what's nice about this is you don't have to spend a bunch of time not saying this in derogatory way you don't have to spend a bunch of time reading about css and html and figuring out how to do all of these details yourself um the over the last few years the the the base ipad widgets library has really progressed in terms of giving you some high level container objects to work with okay i'm just going to show you one thing from um notebook the next notebook which is 0.403 um there's a some interesting stuff in here to read through but i want to make sure that we get a break in a minute and so i'd like you to hop down to a widget for exploring layout options and a heads up about this this widget will work in ipi widget seven it needs some changes to work in ipad widgets eight um and run the cell that says from layout preview import layout and so on so uh what's nice about this widget is it lets you put things into a box i'm going to change the number of things in the box here to 10 and i'm going to shrink down the size of this a bit so we can so i i understand that you're not going to be able to read this from the back um if it's any consolation i can barely read it from here so if i hold my head at just the right angle the progressives are lined up right um so what's nice about this right is you you can change a bunch of properties so if you want to find out what would it look like if you gave your box a 30 pixel border you can do that this also exposes some of the nice properties of flexbox so just by changing a property i can affect whether things are laid out in rows and columns or rather i should yeah i'll leave it at that um you can decide what happens with overflow so let me add a few more buttons here and let me not wrap and because i've got overflow set to scroll i can scroll to see the extra buttons if i wanted to hide them i can do that all this is great i just want to make you aware that there's this tool for playing out with playing around with some of the from layout stuff you can experiment with what the um various uh size parameters mean and best of all this is this is all a python object and so if you click on the python code tab it'll show you the code to do the layout that you settle them so i think you know this this is maybe particularly helpful if um you know you need to do something and the the containers that are provided in the package don't quite do what you want to do this hopefully saves you some some um trial and error or at least makes trial and error faster so i think we'll wrap up the styling section here there are a few more optional notebooks in here that go through um some additional topics um about widget specific styling and widget labels and in the um if you go back up to the notebooks there are there's a folder for reference guides and in there there's a pretty long guide to the flexbox layout model and the grid box layout model i guess i i should be clear this layout widget was not written by me or any of the presenters it was written by an enthusiastic widgets user a few years ago so if you want more detail about about layout and styling it's there otherwise let's take our break for 10 minutes so we'll come back at 10 45. james jwst released one image yesterday and another four images uh today so one of the and i'm an astronomer so your captive audience and my job is to teach so um widget smidgets um oh i should use i should have read this in with the image widget so this is a picture of a group of galaxies called stefan's quintet it's um ford and racket galaxies and one that just happens to be in the same field of view this image is a hubble space telescope image and this is the jwst image so there's a couple differences to point out here um one is that the you don't see the spiral arm structures as much in the web image so you do see that here in the in the hubble image when we switch to the jwst image it's using infrared light and what you're seeing here is the dusty regions where there's lots of star formation and so you get a different read on what's going on inside of the inside of galaxies the other thing is if we zoom in on this right you see a bunch of dots in there that are something in web those dots are galaxies and you can like yeah i know i can't believe this um so the background of every web image that's released today probably has new physics or new astronomy in it um no it's it's um so honestly i don't know i think the ones that they're posting are roughly 4k by 4k uh it might not be i mean the the detectors 4k by 4k is pretty big for ccd you might have several of them in a camera though to make a bigger field okay so these are messages william works morning sorry i'm not seeing any technical help yet so okay if it's wrong i have both of them here right so it's the computer is detecting it it's fine what i think it might be it might be the refresh rate let's see it's called decimator yeah that's what we want that's what we're going through so the decimator is detected here as you can see 60 hertz refresh rate that has been tried many times i can get you another adapter that's all i can do before here so it works it works with other computers i suppose matt i think we will need to uh that's okay so you want the seven environment first yes i was okay you should be good now all right thank you um i mean i'm gonna have i'm gonna use this computer for this one but i have another decision after this if we can kind of resolve it during the break that would be nice all i can provide you is another remote or another laptop i don't know why it's not to communicate again it seems to be working with the others it is it is detected by the computer right yes all right everyone let's get uh let's get started on the next session so uh a quick quick introduction so my my apologies for the uh the technical issue here um apparently this uh this adapter doesn't like windows laptops seems to be consensus amongst not just adapters but the individuals too so i'm paying the price my name is italia i work for bloomberg a core widget maintainer and in this session we will talk about um you know what's some approaches for building more complex widget libraries so you will have seen some of the building blocks like sliders drop down boxes combo boxes buttons and sometimes you want to compose this into into some sort of application right that you can then create a dashboard from share with colleagues or just deploy on the web somewhere and feedback that we've gotten from previous years is that managing state when the application becomes you know more complex is is kind of a tricky topic so this this notebook here aims to kind of give you a very gentle um introduction as to potentially some of the approaches that could work um bear in mind that you know there isn't just a single solution or like the best way for doing things it really depends on your application and what you're trying to achieve um hopefully it's a good starting point at least to get you to think about how to go about and you know composing your widgets into something that is a bit easier to work with and deploy um so the way the way this is structured is we're gonna you're gonna see a lot of code here that you may have not seen before um so you know i'll briefly go through what each each line of code means but you know we'll have other sessions especially in the next section where we talk about additional widget libraries um where we will really explore a lot of these packages in depth so you know if there's something that is unclear specifically with some of the some of the lines of code like part of that question and then i can either come in the break or it is likely we will cover in the next session so let's start we're going to create a data frame with some sample like fake stock prices and in this in this cell here we are importing everything that we need to import we are using an ipad data grid which we will cover in the next session in the next section um to kind of take our data frame stocks df and create a data grid widget from it and in these lines here that's not what i wanted to do okay i don't know how to highlight this and let's see there you go keyboard always works and these lines here create a bq plot a big plot so bq plot based on grammar graphics kind of takes what a visualization is is supposed to look like it kind of breaks it down into some some you know principal components like the scales the lines any of the axes in the figure object and you can kind of compose them into a chart so this is what these lines are doing here and then we have a html widget that renders html okay still still nothing that you can kind of see um we're then defining a callback so using the observed pattern that you are now hopefully familiar with and we are going to observe any changes in the selections on in our in our data grid of stocks stock prices and whenever that whenever the selections change we're going to execute that callback here that essentially updates the the with the new data this is just a way of laying out the widget so you can see everything is kind of overbozed everything is in sales a lot of the code is kind of using global variables which is which is good you know when you're prototyping i'm just trying to close this bar here option b command b or control b there you go so the way this works is you will click on on a given column and then what will happen is the the chart will update with all of the values from that call this is the application not very useful but hopefully illustrates you know what what a complex application could then potentially look like just think about if you were to kind of add just more widgets to it and you kind of have to add um more callbacks manage more state right and clearly have a lot of these a lot of the definition of the widgets that you're using i'm kind of laid out in different cells that's not always easy to work with so let's try and kind of take a more structured approach and repackage this in a python class so the first thing we're going to do is actually just create a class for our big chart because as you can see it has a lot of different a lot of moving parts um you probably don't want to have all these variables kind of laying around you want to have a single class that kind of encompasses all of that state um in a single place right and then being able to interact you probably don't want to change the attributes of that class and directly you probably want to have sellers and getters and that's what we're kind of doing here right so we have a get figure which returns the figure get line which returns the the line that is currently is currently plotted um in set line that kind of takes the x and y coordinates labels and any colors if you want if you want your lines to to kind of change their color so that's the class chart chart class story we're going to have the same thing just for the application so the way this is structured here is you you have the data so it's going to take your your data frame not the data grid it's going to take your data frame um a raw data frame with the data and you can kind of specify the title and then everything else is kind of done for you so some of the properties here the data grid is going to be essentially whatever the data that you're passed in and there's a process function a process data function here doesn't really do anything other than return a data grid with the created from the data frame object that you passed but the idea here is that if you do need to do some pre-processing before you kind of ingest that data you would probably do it in this place a chart which is uh by composition we're just creating an instance of the chart class that we have defined above the application title and then you can see that under the at the end of this we're kind of calling this run application method and you can see that run application method the run application method itself has a method to cause a class method for the setting of the event handlers and then displaying the the layout right that's that's kind of what this thing does at high level and and of course we have we have all of these callbacks that are only a single one you could have multiple and also define as is like a class methods so if you can take this so you all you have to do is just create an instance of my application with the data frame of the stocks and the title and that thing is working exactly as the same as the application that you saw above now the nice thing about this is now these are all classes so you can you can you can use them as modules effectively and this is what we're going to do next the next notebook so we're going to recreate the same data frame that you saw just now and in here if you look under my application we have created essentially a python module right within it and in in here we kind of all the things that we want exported from that module there's only one um which is called my application we export and then the chart the chart class that we created is kind of encapsulated in this file here and the application logic is just a copy paste really there isn't anything that has changed between the two is in the application.hi and and then you just import it and it executes exactly in exactly the same way that's so think of this as like probably the next step once you have once you're more comfortable with which and you actually want to create an application out of it and potentially deploy using voila we will look at some of the options for you to deploy and share with with others later in this in this in the tutorial um but i guess that's it any questions comments complaints yes [Music] so just so i understand the question you you're asking if there is a way to declare the widgets in a react fashion or so you would probably need to create a custom widget from it there is i don't know if we're covering it in this tutorial but there is you know we can we can i can show you there are there are a few cookie cutters that you can use um that uh essentially generate a template for you um where you have the you know the javascript code and then you need to kind of take care of the bindings for python but the the cookie cutter will kind of sort that out for you um and then yeah you you can you can create an essentially a custom widget package from that but just using raw javascript um i don't think there is a way right there is you would need to go via the the jupyter communication protocol cool then i guess this is it from me and the next session i believe is sorry yes please so it's an excellent question and for the for ipad data grid you mean specifically the data grid like this this widget here so the the general gist is that in the current implementation the entire data set needs to be kind of synced between the back end and the front end so if you have a really large data set there isn't any like a data streaming model here so you would need at least initially to sync it with the front end and then the front end you know you can make some optimizations where only you know in the in the case of the grid if you were to change something then only the diff is kind of synced between the two so you don't have to kind of transport the whole data back and forth but for eq plot the data does need to come from from the back end at least for the lines object right um so yeah it's it's probably not the best uh solution for you know very very large data sets as for this the actual boundary when it kind of stops being optimal i'm not sure sure cool then i guess mariana over to you as simple as you can if it's not a windows laptop [Music] so hi everyone my name is mariana um i work at anaconda before i worked at a company that was doing several different jupiter things so i worked a little bit in like building widgets and also kernels um so i'm going to talk a little bit about the extended universe of like widgets because everything we saw until now it's um really the default library but then there are many different ways that you can expand on it so today we're going to look at ipad canvas ipad cytoscape ipod datagrid and ipagony and there is there's an extra notebook on other libraries that you can have a look at and there are also many other more libraries in the ecosystem that we're not covering here um so as ita was saying we have a few a few ways of getting started with building your own custom widget so there are the widgets cookie cutters there's the javascript one and the typescript one and then this will just generate all of the boilerplate code you need to create your own widget um and there's also documentation on it and a medium blog post that you can check it out um answering your question more or less i don't know who made the question but um so further on we're going to see a way of using view to create designs for your your notebooks so um starting with ipycongos um ipad compass is a is a widget that expands on the canvas the canvas tag from html and it allows you to draw draw all kinds of things so you can start with polygons and shapes so you you have to to import canvas and here we're importing other other kinds of canvas that this library supports and i'm going to show later what they do um so here so first we load a canvas and then you can you can add a color for the background and the color for the stroke that the lines you're gonna draw are gonna have here the line with and uh here we're filling the polygon and this is the stroke polygon the thing that's gonna go around so you can create like simple stuff like this with a few coordinates um you can also use bezier curves to create um round things so um yeah in this case we're just passing a bunch of coordinates and all and then so you create a line around it and then you use the fuel function to view the line [Music] you can also create lines with it here we're just setting a background for it but it's not really necessary and then we're generating some random lines and um here's the stroke lines function and then you get it lines um so this is um so the the point of this of this part of the session is more like show off like what are the kinds of things you want to do so i'm not going to get super much into details on what each one of these things are doing but here so here we're drawing a recursive tree so here we're drawing the leaves and then here we draw the actual tree we define the the style for it and and we also connect it through a button to generate random trees so this is a good thing you can do with it um so as you saw before like you can generate lines you can generate every everything anything you want so another thing you could generate is like your own very customized graph so in this case we're using this library called bronco and then it's a it's a lot of configuration around it but you get um anything you want you can generate any kinds of um of graphs you want um these are some other kinds of um canvas that ipad canvas offers so this is the rough canvas it works in the same way but it gives you a cute drawing style this is a game of life implemented just as a showing off like what else you can do with the library and yeah this is also another thing just to show off the library um here martin which is the creator of this library he made this notebook he implemented the perlin noise function and then you can generate beautiful things using epicons i don't i don't i don't know if anyone here used it yeah yeah but i i never did geometric stuff but yeah it should work yeah thank you for the question um so any other questions okay so let's move on to our by side escape so episitoscape is a library that's based on the cytoscape soft actually it's based on cytoscape.js which is an implementation of cytoscape which is a library used to render interactive um graphs so here yeah so there's a small walkthrough here so cytoscape support the many different kinds of um files to be to generate graphs you can use json you can use pandas network x or neo4j to feed your graph with data um we're going to have a look into how to add layouts how to add styling and then with network x the different kinds of graphs you can create and then by the end we have um a little bit we're going to talk a little bit how to connect this with the ipi widgets default library so you reported this is how you create a graph object and this is how data from cytoscape looks like and you can import it directly just as you would use in java or javascript at least it's the javascript port and to a json file and then it just generates a graph for you this is the default graph and it's interactive and we'll see many different things we can do with it so there are many different layouts that you can use this is the default i think it's the color one but you can have greed visualizations you can have circular visualizations you can also have um red first i forgot another word for the daggery visualizations which obey a hierarchy um yeah there's also this random layout which is gonna random your graph in like different positions in case you wanna you wanna show show it in different ways um so yeah any questions maybe we should stop more often sure i wrote this widget [Laughter] thank you for the question um yeah so so another way that you can load data into the graph is using pandas so here i'm creating a you know a small data frame and it's a data frame about robots and universes and each robot has a universe and a coolness level so i just create this universe and the way you load this inside um inside by cytoscape is by um so there are three three arguments first the data frame second is how you wanna group things and the third argument is how uh which which columns should go inside of each group so here i group stuff by universe and then i added the robot and coolness level column street so you get something like this which is not very informative but um we can do stuff with it we can for example add edges between things yeah so here for example i'm connecting c3po to r2d2 because they're connected i think this is a typo that i should fix this shouldn't just created something yeah this is something i should fix um but yeah so here here i'm connecting c3po to r2d2 for example and um i can also add some styling to my graph to make things more you know visual clear so for example i can paint c3po yellow and actually true blue so now this whoops this shouldn't be here well uh now uh c3po is yellow and actually choose blue and another thing i could do is adding two tips to my graph to be able to actually see what is the data that's inside of it and yeah okay so network x so for those of you who don't know network x is um i think it's the most used graph library the python ecosystem is the most famous one and then um so ipad cytoscape is completely like interchangeable with it you can just create graphs from x so this is one way of creating a graph you can create a complete graph and then i'm passing five nodes and then i can just pass the network x graph that i just created here to my cytoscape object and that's just going to generate generator for me and it's yeah it's just a cytoscape object um and then you can have stuff like direct and indirect um edges for your graph uh you can have for example this graph is only direct but you can also have a mix of both this is direct and indirect this means like if it is if it has edges that are um they have um a direction or not um you can also create custom elements so if you have for example a very specific um a very specific node that you want to create that you don't wanna you don't wanna use a json for it you can also just create it and then create a a network graph and add these nodes like so here for example i'm creating a custom node that has an id and has classes and then i'm passing the id and the classes here creating the objects and then i just add these these nodes to my to my network x graph and as you you would expect just like as we we had up here you can just create a an object um a cytoscape object and then pass through the network x graph to it and then it just works the neo4j integration i'm not going to show because for those of you who know neo4j it requires a lot of setup around so i just um but there's an example that can be found here you would take too much time to go over it so here um i wanted to show a little bit how to interact um how the widgets how the you know the complex widgets could interact with like the default library so here we're gonna have a look on how how you can interact with javascript events um so here i'm creating i'm getting my my old graph from up there and then i'm gonna set a style to it i'm gonna i'm gonna say that all of my edges that have the class highlighted are gonna be painted red that's that's what's gonna happen so this is the style that i'm setting into my graph and then so this is a this is a thing that every time you you set a new style the the previous style is set of true like none so this is the issue that's open on the ipad cytoscape it's a pr that's open and it's ready but um it's complicated and we're not sure if we're gonna merge it or not um so here i'm gonna create red edges and i'm gonna create a button that has a callback that goes through every every edge of my graph um and that's the class highlighted to it because as i as i said up here every edge that has the class highlight it's going to be painted red so um i have my graph up here and if i click the button then all the edges i painted red you can also so there's a you can use any javascript event in fact so under underneath here i'm using a different event that's the click event so on click i want to paint all of my nodes blue so every time i have the blue class on the node i'm adding this class here in this line i'm gonna i'm gonna paint my node blue so it's it's listening for the click event so if i click here here it doesn't work but if i click on my nodes then they're painted blue so this is the list of events you could use all of them and whoops and this is an exercise um not sure if you have time for it we don't know all right cool so we don't have time for it but it's just like a little exercise on like how you could do you know how could you exercise this idea of like um working with widgets you want to talk about ipad database you can look for that loud if you're having ipi data grid installation issues for whatever reason seems like safari doesn't play nice with it um i'll come see you after this um just gonna briefly because we are kind of pressed against time i'm just gonna briefly kind of breeze through this um ipad datagrid is is a library as the name suggests that is a data grid idea here is that you can take if you do a lot of eda and you want to kind of explore your data but you want to have a more interactive way of essentially working with your data frames ipad data is a great tool for that it essentially takes your data frames and it converts them here is here's an example data frame you just pass it as is you can kind of see that it created a data frame out of this for you let me just create an output for that what was the trick really okay figured it out this time um this is okay so yeah you can you can it has all sorts of parameters that you can kind of adjust like you can see the height of the the column header and the width of the row headers this these names might take a while getting used to hopefully they're intuitive once you do get used to them um you can you can change what this thing looks like right you can if you don't want to have any kind of headers or no header visibility at all you just want to look at the raw data you can do that hopefully you would want to do that in many many occasions um ipad data grid supports the idea is that each one of these cells that you see here is is rendered using a renderer object and we currently have this one says we have two we actually have by now we have three text renderer bar render and hyperlink renderer if you want to kind of render any links and all of the attributes of these renderers like the color the font the text the background color all these things can be controlled using vega expressions we didn't invent vega expressions at bloomberg it's just a part of vega the vega grammar open source project but we kind of embrace it as a way to give you a constrained way of specifying conditional formatting based on the cell value so this is this is a vega expression expressed as a python python function and that python function takes one parameter that parameter is cell in cell think of it as like a dictionary that has has a list of attributes that you can access they're listed right here so the cell position height and the cell value x y etc um so let me show you what this thing looks like so we applied you know some conditional formatting here and played around with different renderers so you can see the grid now is showing bars showing essentially different sizes of the text and different colors using color scales and yeah i'm just going to quickly go through these you can also style the headers independently as you can see here and you can apply what we call transformations so that can be any kind of filtering or sorting you can do it programmatically or you can do it from the ui um right here right you can do any kind of filtering like that um conditional forming based on another cell um this is so here we are essentially have we have a signal if the return is positive we want to buy if the return is negative we want to sell so kind of the signal column is conditionally formatted on the value of the return column it supports nested nested hierarchies right um so this is a nested hierarchy data frame and it kind of just works right you can just kind of pass it in we also added some conditional formatting because why not and two-way selections right you can i can kind of select something in the grid so i can select like this range and then go down here and that range is highlighted that's just the range think of it as like an iterator but i can also get the values if i want to um there's more selection stuff here kind of repeats the same thing and you can have an iterator if you don't want to have if you have if you're if it's a large grid and you're selected you've selected a lot of values but you don't want them kind of taking all your memory space you can have an iterator that kind of yields um the values as they can as they're kind of needed instead of having them all in a list um okay you can change the selection mode so you can have like a selection mode that is like a row based so instead of clicking on cell you can have the same for columns um this is this will select essentially all the uh all the rows that are this one i selected before even even even row number indices and yes let me see if there's anything else in here um you can edit two ways so i can kind of put this change this to one and you can see that this changes this will now reflect also back in the kernel so you have this two-way data binding which is which is awesome it's really really nice if you just want to change something you don't want to have to go through a number of pandas operations um yeah you can set cell values programmatically so not just from the ui and this this is an integration uh this is this shows you how to kind of integrate with ipi widgets with a broader ecosystem this example is actually the example that you saw in the building more complex libraries so you know we don't have to go through it in detail this is just an example of linking so we have the slider you can see the slider here controls the minimum property um for the color scale that we have and you can see that we're kind of playing around with the value of basically where we're restricting the range of the of the color scale and that in turn affects the the rendering of the bars and and yeah this is this is the stock df example that you saw previously some nothing nothing normal here and that is ipad data grid i think i fight ghanian and that's it right um that's so back to you yeah maybe we should skip this um because we're shouting time but this is just some because we try to show different things and just that this it's also possible to render 3d things uh the jupiter notebook and ipagony is um is um apart from the part part of you perfume so you can render really cool things and they're it's possible to do like some really cool interactions with ipad widgets um so yeah so another thing i wanted to talk about this um so we're now we're leaving kind of like the widgets side of things and going more to the um what are the tools that you can use to leverage your widgets um what's the word when you were talking about the surgery when you put it out there for people that are yeah publishing yeah this kind of stuff so um one very good tool is voila voila is used to create um dashboards and then someone was talking about using i think react but voila allows you to use allows you to use view so i don't really i'm not really a front-end developer but um so you can you can import ipi beautify which is um apart from view to python and then you can you can program uh the interface of your uh of your notebook just like your your programming view so you have tabs tab items layout and so the cool thing about uh about voila is that you can also in case you're also not a front-end developer you could also use the stuff that matt showed earlier so so this is just some code creating a dashboard and then i'm going to show you how to run voila so all you have to do is you use the word voila and then pass a template if you're using any template so in this case we're using view here if you want to enable np extensions and then the path to your notebook and the cool thing about voila is that it's not only a static dashboard this is running a kernel on the on the back end so everything here works just like a jupiter notebook this is basically a jupiter notebook but it looks it looks whatever is the way that you want it to look um and another thing i wanted to cover is jupiter light so oh sure sorry is maybe well i think i'm running this in different so what it's important so this uh this template that i'm using is actually um a library that you have to download so if you don't you know with conda or mambo or pip so if you don't if you don't have this library installed in your environment then it won't look the exact same because you need this library if you're using view and there are different templates yeah i don't i don't know where it's a good link to see the different templates that they have yeah but this for example is a different one so this is green and you know it's completely different so there are many different so maybe that's a problem i don't know because for me something else uh happens but we can we can look it up okay sorry do you have any other questions all right so yeah so i want to talk a little bit about jupiter light so jupiter light so this is a link to the web it's just a website and it runs a complete um jupiter environment and the cool thing about jupiter light is that it supports uh different languages so you can use not only like python notebooks like reuse true and do all the kinds of things that you would expect to happen except um accept some little things that are because this technology is leveraged i think i went too fast or i don't know this is just for me yeah yeah it's leveraged by biodiet and pi diet doesn't support a few things we're going to talk about this a bit later um yeah so you can also have a javascript [Music] a javascript kernel running or lua or p5.js or sqlite or ran which is um [Music] uh yeah it's a less known programming language but yeah so for example this is a sqlite kernel and you can load you can create database using sqlite and create tables and insert into them and show them with um with the jupiter's meta data and it's also possible to use vega to run to create graphs and this kind of stuff so so the cool thing about jupiter light is that it doesn't have a server you don't have to do anything there's no command line there's no need to install packages locally it's super fast to open superfast boost and um you can also create your own and it's super fast you just have to go to this repository and then do what this gif is showing here and create your forked creating your own repository yeah so jupiter light it uses many many different technologies that many people have been working for for a while so it connects to through [Music] through uh to jupiter using the zoos protocol and to leverage all of these and to have python in the browser is using piodide and of course all of the rest of the jupiter ecosystem that runs inside of it there there are few possibilities to have files in the running inside your jupiter light so you can use the local storage or the index db server um and you can also create shareable links to share just some specific files with other people so maybe you can show this this fast or here so i can select this and then okay it didn't work but it was working i don't know i don't understand why it didn't work um yeah maybe some bug i don't know um yeah and this doesn't have any access to your personal file system or anything like that everything that's why the file system thing is a bit complicated because it's hard to load things from your computer and stuff like this because everything is enclosed in the browser um so yeah so another thing this is new i don't i don't really know what's the state of this but uh people added um uh kevin added the possibility to collaborate so you can open two instances of um jupiter light and just have two people working on the same file and i don't really know the details on the on the set of this but uh as far as i understand it's working basically everything you would expect yeah and some of the limitations so for now you can only use for lighter workloads because of the limitations by piodide you can only use micropip and that means that you can only have fully python packages installed in your jupiter light you can only use python packages that are fully python or that are supported by piodie there are a few um [Music] yeah and i think that's it for it for this part i think we're gonna have jason talking about or is it italian sorry [Music] so um the upload widget should open to a folder this is this is great because i had i had an image of me which is like what i use for my green card application it's not what you want to see um so we're we're gonna ipod widgets eight is is a release um everything you've seen up to this point is iphone windows 7.6 um or 7.7 7.7 um apple we should say is coming very very soon it's in release candidate stage now um and it's it's a major version upgrade so there are some um some breaking changes we're gonna go through some of the changes that impact users mostly in you know try and kind of summarize them this is by no means an exhaustive list so i would still highly encourage you to kind of click on this link and look at the full change log if you're a widget developer is also a lot of relevant information for you there we're not going to cover any widget developer related breaking changes in this notebook okay so i suppose this is the correct version yes it is so the latest documentation is available here just make sure you have um hyper widget site selected there um so first you know right off the bat python python 2 any python 2 support and python versions less than or equal to 3.5 they're no longer supported um that is a big change right especially if you have if you are using python 3 but um you know some of the earlier python 3 versions just will not be supported so just something to bear in mind um there have been a number of additions to styling um so some of the some of the ones that you can see here are supported for core widgets and here is an example kind of taking all of these and generating an output widget with all of them it's not you know we're not we're not really good with front-end stuff so i know it looks hideous so forgive us um but that's you know that kind of illustrates some of these changes and and then kind of working across different uh different styles and so that's that's one thing and borders right there's been there's there's been some um there's been a few requests asking to set the border colors independently for each one of the sides previously you could just set the border and the border will kind of be uniformly kind of set across all different sides of the button but now you can kind of do them independently that was if that was an important feature to you it is now included in ipad widgets a there is an entirely new slider implementation and based on an open source project called no ui slider so this is what it looks like and some of the things that it supports is kind of dragging the range right um so there there are a few there's a new behavior attribute um that it supports you can kind of read about it in the documentation um but essentially just something to also to to be mindful of um the file upload widget has been revamped and there's been a lot of changes here some of the most important ones concerning users are here so the whenever you are uploading something the uh the value will actually give you um a bunch right it's called a bunch i think that's the object which essentially gives you metadata but the file that you uploaded together with with a memory view which is a binary representation of it so let's let's see if that kind of shows what this thing looks like so hopefully i'm this is just beautiful um so we're going to choose two files right one to open them and this is what it looks like it looks like this uh dictionary presentation so the first file that we uploaded is hello world it's you can see it's text or plane that's the size content and then the last time it was modified you can see that the content is this memory view object which python support so let's read some of the information so for the text if you wanted to read a text file this is what you would need to do so codex decode file upload we're looking at the first index and the content again pointing to this memview object here and there you go look at that play the computer of an astronomer uh and for an image [Laughter] it's a fantastic image look at that is that is that space-time warp what we're seeing here um okay um so this just you know a summary of the changes to how you interact with files that you uploaded that's different from the seven point x versions right so bear in mind um you have tooltips everywhere i kind of know why i chose the the um the file upload widget again but you can see a teeny tiny appearing here this should work for any other widget that was not the case before so you can kind of now put a tooltip on any widget that you that you render there's an error widget fallback i don't have any failing i suppose it for your safari it will fail right um i could use ipad data green now you can kind of see the the trace um the stack trace on at least on on the python but also on the javascript side kind of rendering nicely rather than this error where it says cannot display widget model which is not really informative so this this is something that you're going to get it's a much nicer way of or more graceful way of handling widget widget failures there's a new stacked widget and so what this thing does is essentially allows you to display one of the essentially the selected index here so we have two widgets button and in slider so if i change this here to there's no escape button here this no idea how to do this uh okay yeah that one so if i do stacked selected index equals one and then you can see the slider so that's that's a new addition there's a new dead time daytime picker so previously you would kind of only select the date but then the time will be like zero zero zero zero just the beginning of that date now you can be a bit more granular with what you select um and yeah that's it that's those those are the main changes and if again if you're a widget developer please do read the documentation because there are breaking changes you will need to if you're maintaining any custom widget packages uh you will want to support both seven point x and a point x you would need to make some changes and to make that happen any questions yes [Music]
11,Bayesian Data Science by Simulation,https://www.youtube.com/watch?v=2aa9V4zoDUc,hi everyone i'm um i'm i'm hugo uh we'll be hugo ban anderson we'll be talking about um bayesian statistics today um in particular starting off with a lot of simulation to get a sense for how to tell probabilistic stories um and in particular how to think about data generating processes um so the first half we won't be doing much base the first half will end with with bayes theorem in fact and the second half will jump into uh doing some bayesian inference and using the probabilistic programming language pi m c the language formerly known as pymc3 and formerly known as other things as well um could you put up your hand if you've had much experience with bayesian thinking before yeah i like that that's cool um well thank you all for for coming and um so i actually i work in data science and machine learning and developer advocacy and tool building i currently work for a company called out of bounds where we're thinking about machine learning infrastructure for data scientists in particular working on an open source framework called metaflow that was um open source at netflix a few a few years ago i'm also teaching a workshop on that tomorrow morning if anyone's interested in thinking about full stack machine learning uh my co-instructor eric maher wasn't able to be here but he'll be on the slack channel in a couple of hours uh answering questions in in in real time um but i just want to make clear that everything we're doing today um is collaborative with eric and uh you know a lot of input and inspiration from other people who i'll mention throughout as well um so it's just me up here so i can't really help run around and do individual troubleshooting throughout um let me ask could you put up your hand if you've not been able to open a notebook with a code that runs [Music] working on it that's what i like that's what i like to hear so i mentioned this maybe 10 minutes ago but for anyone who can't get any get the local installs working if you go um to the github repository which we've pasted we've put in matthew is matthew in the room actually hey how are you nice to finally meet you as well i mean at a distance but closer than our previous interactions um to bring continents apart um so matthew has kindly put the repo there and as i said uh around 10 minutes ago if you can't install um the required packages locally environment locally for whatever reason it may be you can click on this launch binder link and that will take you to um a wonderful creation of uh the project jupiter team team jupiter which allows us to uh spin up essentially containerized images of the repository to allow you to execute code wherever that is is happening and as you see here that's what i've done i'm not going to do this because i can do it locally and i trust local computation more than um other forms for some reasons but you'll see if i then go into the instructor notebook um i can execute that cell and so forth and can do similarly okay so um the first thing is once you have uh the repository open uh locally or on binder what i'm gonna get you to do is navigate to the notebooks directory and as you'll see there we've taught this at several conferences um and in fact been fortunate enough to teach this at the past well it looks like the past three sci-fi's but it's been more than that i think we first thought this in 2018 so we've actually taught this or related workshops at um this is the fifth scipy i'm very grateful to have been accepted again to to do so but what i'll get you to do is go into the scipy 2022 directory and i'll get you to click on the student notebook number one now the instructor notebook has all the code i'm going to go through the student notebook as well the instructor notebook has all the code um and so we're gonna write some code together and then we're gonna have some hands-on exercises so if at any point you're incredibly stuck feel free to check out the instructor notebook but i encourage you to chat with your neighbors ask me questions before before looking at the cheat sheet so to speak speak up sorry yeah i mean if you're on binder no but if you're running locally you just yeah you literally i mean you can hit command s or go to file and save notebook as well and that will save it locally exactly so if you've cloned the repo then you can save any any progress you make so um i'm just going to execute this first cell to do our imports you'll see we have some packages that you may be familiar with already numpy pandas cborn batplotlib um i'm doing some matplotlib inline so we get our figures in line within the notebook and styling with um seabourn sns dot set to make um figures uh maybe a bit a bit prettier to my eyes anyway but i'm red green colorblind so that may you know um so i just want to uh reiterate that um this tutorial is really appropriate for for most people um but i wonder if we have this here but if you know a bit of the pi data stack and the sci-fi stack um you'll get a bunch out of it on top of that though if you don't know too much you can pick it up along the way and i think a willingness and curiosity to learn is probably the most important prerequisite um for a workshop such as this so to think about this first part as i said the second part we're really getting into the nitty-gritty of bayes and how to do probabilistic programming and how to think about bayesian workflows and principled workflows um but this first part is really to get an understanding of what probability means in both bayesian and frequentest terms um to be able to simulate probability distributions uh that model real world phenomena um to understand how distributions and probability and statistics relate to stories and data generating stories for example what type of story gives rise to the binomial distribution or the poisson distribution the gaussian the exponential right then we'll move into joint probabilities and conditional probabilities which i presume some of you or most of you are at least somewhat familiar with um and then we'll jump into bayes theorem and see why it's why it's useful okay um we'll have a couple of breaks along the way i think there's an afternoon tea at 2 30 um and then there'll be hands-on exercises throughout as as well um so let's move on to talking about probability first um and actually just quickly if you've if you're using binder um execute the first cell um because binder will time out after 10 minutes and you need to restart it again i've been through that can't believe how many times i've made that mistake and i'm kind of surprised i didn't today um but um there's a wonderful book or one of my favorite bayesian books called data analysis of asian tutorial by sylvia and and skilling and they open talking about probability and they state that um to pioneers of probability and statistical thinking such as bernoulli bayes laplace probability represented a degree of belief or plausibility um how much they thought something was true okay based on evidence at hand um this for very good reason was considered vague and part of you know the scientific rationalist revolution part of the point there was to clarify a a lot of vague terms i mean they did it with the calculus as well right lebanese came along um and actually made calculus incredibly formal um as did koshi in in fact with the differential calculus and introduced the language of epsilons and and deltas statisticians and probability theorists at the time thought that this seemed to too subjective in a lot of ways it isn't um if we're considering degree of belief based on the same evidence and same principled workflows which we can discuss a bit a bit later um but what happened was the 19th century scholars uh redefined probability um basically in terms of how we relate to games of chance like flipping coins and decks of cards kind of the stuff that uh um firma and um a lot of other people were thinking about at the time so they redefined probability as the long run relative frequency with which an event occurred given infinitely many uh repeated experimental trials so this is kind of the philosophical underpinnings of differences between frequentism uh and bayesian thinking a lot of the time if you do them both correctly you will end up with the same results um the workflows of course are incredibly different um but what we're really thinking about is this idea of considering random phenomena or let's say uncertainty around different parameters so the question to ask is what type of random phenomena are we talking about here so one example um is click-through rate click-through rate that you may have seen in your jobs or you may know of uh particularly in in terms of how tech thinks about engagement and conversion but um what we're talking about is for example knowing that a website has a click-through rate of whatever 10 we can then calculate the probability of having 10 people nine people eight people and so on click through right upon drawing ten people randomly um but that isn't usually how science works right how science works is we're given the data of how many people click through then we want to calculate the click-through rate um and how certain can we be of this click-through rate or how likely is a particular click-through rate okay so science really asks questions of the second form given uh data what can we say about the real-world processes that generate that data and give rise to that data um given the data of um infection rate of uh clover 19 for example what can what can we say about the actual underlying rate of of infection right and how can we express uncertainty around this which actually if we're modeling things in a way that we'll see using the binomial distribution which you may have heard of is analogous to thinking about click-through rate at least in terms of the processes um yeah the abstraction of the processes so as we'll see one of the powers of bayes theorem is it gives us a way of moving from the probability of the data given the model um which we write like this and we'll get to that notation later to the probability of the model or underlying parameters given given the data okay so kind of switching between these two questions now if that seems a bit overloading so far that's chill um all of this will kind of come come out as as we move through these four hours of working together so we'll first explore questions of the first type using simulation so knowing the model what is the probability of seeing uh certain data okay so for example let's say a website has a click-through rate of 50 if we pick a thousand people at random from the population we'd like to ask how likely would it be to find a certain number of people click and we can do this using uh random number generation or actually it's pseudo random number generation they're not really random we can get to what that that means if you want to at some point or in the break but the the difference i think is inconsequential for this tutorial um so what we do first is as kind of a practice exercise we can use um numpy's random number generator to randomly select uh floats between zero and one okay and this is something known as the uniform distribution because we're picking we're randomly selecting them with equal probability whatever that means right but i think we have a sense of what what that means okay um so what i'm going to do and i invite you all to code along um is we're going to set up the random number generator and what i'm going to do is generate okay i've set a thousand samples so i should probably do that um i'll generate a thousand samples using rng dot uh uniform i think that's it we'll find out anyway thousand and then i've asked us to plot a histogram so let's just do that huh what has happened [Music] i actually don't think we have to do that but let me that's fascinating okay well we did do that but that's i'm actually just gonna that's fascinating that i didn't need to do that yesterday though i mean no seriously look here yesterday all right um i'm just trying to figure out why it's working here in binder like this can anyone see the so i'm actually going to we'll stick with this for the time being um thanks for that catch the things you don't see um great so we've drawn a thousand samples from uniform distribution okay um but that isn't really what we wanted to do right we wanted to say a website has a click-through rate of 50 um and then we pick a thousand people at random from the population and figure out how many is likely how likely would it be to find a certain number of people click okay so what we want to do is sample from this population okay because we're looking at 50 percent what we can do is take each sample from here and see how many were more than 50 more than 0.5 and how many were less the 0.5 so the number of clicks will be those that were greater than equal greater than 0.5 and the number of clicks will be the sum because we have our booleans returned right so trues will be one and zeros will be false and what we see then is that we have 484 people click through right given this um this data that we just just simulated okay um but we're not actually interested in the total number really we're interested in the proportion right so to figure that out what we essentially want is um the number of clicks over the total number which will be the length yeah sorry the total sample size um exactly because number of clicks will just be an actual um so what we see is that the proportion is 0.484 which we know the proportion is 0.5 from what we actually sampled sampled from and our assumptions um but we got point four eight four um well actually i got point four eight four can you put up your hand if you got point four eight four but everyone got something around that right or slightly over five like slightly less than 0.5 slightly over 0.5 right so why why do we get something different you have to speak up sampling right um because we're actually sampling in in different ways um is there a problem with this i mean one problem is what i've just done isn't reproducible i've written some and we've got different results which definitely isn't cool okay so what we're going to see is how we can do something called setting the seat at the random number generator to make sure that these things are reproducible um and we'll see that soon okay um so let's say that you have the way science usually works as i've said is that we don't know that as 50 we actually have the data at hand the data we've generated and then we want to calculate complete throughout or the incidence of disease right so if we have this data and we wanted to figure out the click-through rate what would your estimate be it's not a trick question so your estimate likely would be um the proportion that you've actually calculated right um and what this boils down to if you know this term if you don't it's fine but it is actually a maximum likelihood estimate of of the proportion um do you have a sense of how confident you can be in your estimate so so generally the more data you have the more confident you can be right um and you get like some plus or minus some like standard error of the mean or some something along those lines in frequent systems um what we'll see when we start developing our bayesian muscles um is that we can actually get which will give us this estimate but show us our uncertainty around this estimate which i personally i think is powerful um i'll also make a note that we've um described probability in two ways invasion terms using words um and in frequencies terms but we haven't described it mathematically that that's absolutely intentional i think at this point uh the maths or math will obscure kind of the intuition and then those muscles want to develop but if you're interested um so what i'd like you to do now is just take a few minutes to use how many people uh click through when the click-through rate is 0.7 okay so once again ask the questions how many click uh and what proportion of people click so let's just take five or to work through that [Music] with your neighbor about it and just put up your hand if you have any are we going to define uncertainty do you mean the term uncertainty or so at the moment i'm relatively agnostic with respect to which type of uncertainty we use um i mean you can think about standard error of the mean you can think about standard deviation you can think about variance all of these things um and the way i'd like to think of it is really the the spread of a distribution right is that is that helpful yep awesome yeah i mean in this case it doesn't matter so in the other case yeah you probably if you're doing this you probably and you're doing greater than or equal to you wouldn't use 0.7 is what i'd say yeah the pros there absolutely does um the pros i spoke does not but the pros in writing absolutely does so a great great point that was just made is that i've actually said we can check whether each float was ah no i said greater whoa uh yeah if less than or equal to we say the person clicked exactly um so that is precisely what we want thank you for for catching that um if i'll scroll up but all of this is in the instructor notebook as well so for anyone who wants to see other parts of the notebook it's actually all in the instructor notebook but where would you like me to scroll to so and if you're you're able to do this great okay um can someone tell me how to do this i know volunteering is can be scary how did you do it yep beautiful fantastic so this was the major difference right that was 0.5 before and now it's 0.7 and the rest of the code remains the same right cool so i'm gonna be classically lazy yeah so if you're doing greater than or equal to you you would make it three absolutely great cool and we actually see that it's somewhat close to 0.7 which is what we'd expect um and whether to take larger samples on average um we'd see it get closer and closer to 0.7 of course there will always be outliers right which is one of the reasons we want to want to talk probabilistically and think probabilistically um so this model is known as the bias coin flip um and i have it as a discussion point but maybe i'll just state this and i presume a lot of people in the room are already aware of this but the reason this is the case um is that a a fair coin or a normal coin has 0.5 probability of heads or tails were a coin to be biased uh you could change these probabilities around and have it being 0.7 for example now as far as i know there are no no bias coins maybe there are some magic weighted coins um uh having said that this is an incredibly useful model and we'll see why um as as we continue okay um so i'm also going to just introduce you to a data set quickly um that will will will be useful uh later on in this tutorial and it's the galapagos uh finch beaks so if you just execute this cell what we essentially have uh two species of finch um one called fortis one called scandans which we don't see in the data frame head here but we have bake length and big depth and one question which is a question charles darwin asked is is there actually a difference in the shape or size of these beaks okay and then thinking about how that may be a result of evolutionary pressures okay um and we'll see later on how to think about innovation sense whether there is a difference in in uh the length i i think of these beaks okay um so just as a little beginner to think about these lengths um i think what i want to do is i said store the lengths in uh panda series so we do that and we can think about what proportion of birds have a beak length greater than 10 okay so the way we're going to do that is look at the lengths and look at those that have greater than 10 and take a sum of that array once again this inequality gives us an array of booleans so that's actually not the proportion um that's the number so then we want to divide that by the length of lengths okay so we can see around 85 of them do okay um so i'll also say um this is a proportion okay in empirical data not uh the probability that any bird drawn from a population uh will have uh beaks of this length um so it's interesting to think about if we can kind of resample this population to get a sense of the actual probability and we actually can okay so the way we actually do that is i'm going to write this code and then start to explain it we um let me get this right so what we're going to do is we're going to sample from these lengths we're going to sample 10 000 of them but there aren't even 10 000 of them so how do we even even do that right so we're going to sample with replacement um i wonder is this yeah i think so this should be it and see how many uh greater than or equal to 10 and then uh divide by oh ah no i'm sorry so we actually want to divide by n samples because that's how many we have there okay so what we've seen now is that if we do this once we actually see that of this resampling um around 85 percent of them happen again um sorry have lengths greater than 10. now if we did this a lot of times we then start to get a distribution um and that's a way to get a feeling for what would happen if we were to draw from the probability distribution itself and um one i mean this is pretty analogous this is known as hacker statistics this is essentially an example of the the bootstrap if you've ever heard of that we'll go more into that later okay are there any questions about what we've gone through so far so i've actually made no no assumption here about about gaussian um right all i've done is i've resampled from the population with replacement so it could be any underlying distribution in fact yep um which is and so it's called non-parametric in that sense in that you're not making any assumptions about parameters or underlying distributions now what we've done above is we've simulated the binomial distribution um using the uniform distribution and that seems a bit a bit silly that was a nice way to introduce it i think um but numpy has um its own uh binomial distribution generator which we we can use um and it's it's quite powerful okay so to define the binomial distribution we've already seen this but it's a distribution that has two parameters n and p um where um and it's defined as a probability distribution of the number of heads seen uh or number of click-throughs um when flipping a coin n times where the probability of heads is equal to p okay so the reason i've stated it this way um is because it tells a story right and this tells the story of a general model what i mean by that is if we believe that whatever is generating our data has an underlying process that has a binary outcome that can be heads or not zero or one click through or not affected by disease or not and that one of the two outcomes occurs with a probability p then the probability of seeing a particular outcome is given by the binomial distribution with parameters n and p so any any data generating uh process that we can tell that story about will be binomially distributed okay and we'll see with other distributions the types of stories we can can tell about them um and i think this is in terms of considering data generating processes um and generative models this is incredibly important okay so any process that maps matches the coin flip story is uh binomial um and so you may have also seen the term uh bernoulli trial um which refers to a coin flip so you may also see we can formulate the story as the number of successes in n bernoulli trials where the probability p of success is binomially distributed okay so what we're going to do now is use the binomial distribution to answer the same question as above okay so first we're going to just make sure i've got this right so once again we set the random number generator and the first argument um is the seed we're setting okay um so i want you to set the same seed as me okay um and then what we're going to do is just simulate uh one run of flipping the bias coin ten times okay rng dot binomial and the first remember the binomial distribution has two parameters n and p so the first argument will be n and the second p right and so we see if we flip it ten times we've got six heads here which it's close to seven so that kind of accords with our intuition of what what we'd expect um put up your hand if you got six okay did anybody set the seed equal to 42 and not get six okay that's cool that's heartening that would i mean that that would be magic um right um so one of the things that's important though is we've just simulated this this once right but a lot of the time we don't want to simulate it just once we want to get out the whole district we know sometimes we'll get six heads sometimes we'll get seven sometimes we'll get eight sometimes we'll probably get one really rarely right but if we want to see that whole distribution of possible results with their probabilities uh we can now use the um the same random number generator to to do so and the api i'm just personally a huge fan of and we'll see why as we continue once again the first argument is n the second is p then the third is how many times i want to simulate it so now i'm simulating and i hope you're doing the same ten thousand runs are flipping the bias coin ten times okay okay so and then um we're plotting uh the histogram right cool right well i think this is this is cool um we can see that we have the density on the y-axis we don't have the total number um right so what we can see for example is that um 10 getting 10 occurred 15 of the time getting nine occurred you know just like 24 of the time getting seven which makes sense that would occur the most because click through sorry the prob p was seven we get over 25 and and so on and we get diminishing returns here i said you would get maybe one and two sometimes and you will but can anyone tell me why we haven't here the answer is we haven't simulated it enough times right to get these extreme events you'd probably need to simulate a hundred thousand or a million or 10 million times right then you'll start to see these these these extreme types types of events okay but congratulations everyone what you're now able to do is start simulating distributions and actually simulating experiments right we've just turned our laptop into a or our binder into a little lab of sorts where we can take some sort of data generating process and then run the experiment which we've done 10 000 times here which is which is pretty cool um if of course some of you may have seen this before if you have and um this is all uh a bit slow um just wait um and we'll we'll start to start to speed up soon okay so what i'd like you to do um say for five to ten minutes um is i'm just looking at the time i know my clock says um 310 that clearly is not the time that's eastern time which is where i flew in from yesterday but um what i'd like you to do in this hands-on is to flip a biased coin um with this bias 20 times and tell me what the probability of five or more heads is okay um and then if you flip a fair coin 20 times tell me the probability and then plot the normalized uh histograms um so yeah maybe take take five five ten minutes to do that hmm definitely and what i'll do is oh you wouldn't increase the number of bins you'd increase the number of samples i'd try 10 million or 100 million well you would if you did it enough i mean i don't know whether it's 10 million but if you did it if you did this experiment enough times you would even get zero actually yeah makes sense this is actually one of the challenges of frequentism um i mean it's fixable in the framework of frequentism but thinking about the uh likelihood of um extreme events and if you define the probability as the long run relative frequency the question then is like do you need longer than you know the time of or distance and space in the observable universe to simulate this particular thing and figure out the long run relative frequency so so maybe i'll just start talking through how i think about this right um so i'm flipping a bias coin 20 times so once again this is not necessarily the code all right in the end but i'm just trying to i definitely want to simulate the binomial distribution right um and i'm doing 20 flips and my probability my p is 0.3 right so maybe that's what i want to do well it just gives it to me once right so i really want to do this 10 000 times again and then look at how many of those ten thousand ended up um giving me five or more heads right so i'm gonna do ten thousand and i wanna know how many of those were greater than or equal to five okay so remember this gives me some some boolean array right which then i can i suppose i can sum it up again to get the number of those where um i got five or more heads right so this gives me the total number but i did ten thousand so i need to divide it by ten thousand okay so for those that were able to do this did you get something similar to this yeah now we all set the seed at the same time right so would we be getting the same results ideally yes but we're working in a notebook as well so if you executed a cell twice and i executed it once we may be at different points from our seed okay so that's why we might be getting different results so if that's the case um that's it's not magic yet um but we'll get there um so similarly if i flip a fair coin 20 times once again we can do exactly the same thing and we get like 99 and that makes sense right if you flip a fair coin 20 times you're almost certain of getting five or more heads right um very small likelihood of of anything else um now the last thing was plot the normalized histogram of flipping a fair coin ten times so once again we'll flip a fair coin ten times and i'll do a bunch of them maybe i'll just take exactly the same code as before um right for those that did this did you get something similar does this distribution look like anything to you yeah it looks kind of gaussian it's definitely symmetric um which is a hint at gaussian and maybe the you know the tails fall off pretty quickly um and we make it get to this but both the binomial the poisson distribution which we'll see soon and the gaussian are all related in in different limits okay um so we may we may get to that um i'll also um make one statement you may have noticed um that the binomial distribution takes on a finite number of values right so here it's 0 through 10 right like you can't get 1.5 heads yeah whereas the uniform distribution we had before um can take on a continuum of values between zero and one in in that case okay um now it's not particularly important for what a lot of what we'll be going through today but i'd i wouldn't be doing my job if i didn't mention that these are different enough um to state that they actually have two different names the former is called a probability uh mass function um when you have a discrete number uh sorry a finite number of values that are discrete and the latter probability distribution function um all good textbooks will cover this and i'll be happy to chat about these during during the break um in particular the existence of probability distribution functions these continuums of um of possible uh values is one of the reasons the calculus became so important in in statistics with these types of sampling techniques though you may actually have noticed that i'm doing my best to avoid calculus completely when talking about this stuff and i i do think in the end it's possibly unavoidable um but i think with random number generators and computation and these types of um computational labs we can get we can get so far without having to worry about calculus and actually learn a lot more than we would if we were to approach it like if i was showing formulae and doing calculus and um actually with this audience it it may be a bit a bit easier but generally you you lose a lot of people when attempting to do that so i think these types of computational resampling environments um give a lot of intuition as to the types of things we we want to be doing so let me just ask a question looking at this histogram can you tell me the probability of seeing four or more heads well can someone even talk me through how you'd do it looking at the histogram yeah yeah you would right so and in fact because i've done this before i think i've decided i'd do the other way i'd like count point one these two add up to .05 so it's like .15 and so the rest should all add up to one with that so like 85 percent should be um greater than equal to four or something like that now in this case that was doable and you had the right intuition like in like doing some eyeballing stuff um but it's not great to be honest um so i mean this is one histograms are fantastic for quickly having a look at what a distribution looks like there is a challenge in that the bin size can actually deeply impact what you think you see so consider if you had um some form of multi-modality so a distribution a histogram with two peaks but then you chose a bin size that was too large and so it actually only became one peak um so there are several challenges with with kind of developing intuition from histograms um on top of that if i wanted to let's say i did five experiments and had one control if i wanted to plot all those histograms together um good luck uh i mean you could do some sort of transparency and different colored stuff but um it gets pretty pretty gnarly pretty quickly um so that's why i'm actually incredibly excited to remind you of or introduce you to something uh called the empirical cumulative distribution function um has anybody seen this before one two yeah three four five six seven um and there's no no shame in not having having seen it this is something which um i feel perhaps i won't use the word should but i think as a community we'd be well served by introducing it to as many people as possible when introducing histograms so i mean that's i suppose that's the soapbox i'm i'm willing to get on today um and you can actually see a few other people including eric marr and and alan downey um talking and talking about it um but an ecdf is kind of like a histogram in some ways um the x-axis is the same the x-axis is the range of all possible values for the data um but the y-axis is different the y-axis uh what it gives you for any given x-value the corresponding y-value is the proportion of data points less than or equal to that x-value so it's going to be an increasing function right um if you think about calculus it is kind of um uh the the integral of the histogram right because you're kind of summing up the histogram as you go on okay um so what i'm going to do now is just define and we'll see why it's useful in in a second um and be obvious yeah but what we do is um we i've defined a function where we have the number of data points then i want to sort the data um from left to right essentially and then um what i do is i um on the y-axis i have all the proportions uh going up okay so um we'll see this in action now i've actually had this as a hands-on exercise but i'm too too excited to not do it myself now um so i will execute this line of code and then ecdf what was the input was the data and the data so i'm jumping around here was x so i suppose what i want is plt.plot x y now this ah x flips and y flips of course at least i'm naming my variables somewhat um cleverly okay so this shows us some form of step function and i actually want to avoid this i just want to check how i've done this previously um so i want to change i want to give each point a marker whoa and um remove oh nearly done okay great so to remind you what we're looking at here um these are the possible values on the x-axis and then on the y-axis it's uh the proportion that is less than or equal to um that particular value so we'll actually see here that given there's some form of thickness these markers aren't zero dimensional so the thickness we need to actually take into account when when eyeballing it but what we'll see is that the proportion that's less than or equal to uh four is around uh twenty percent similarly the proportion that's less than or equal to five is less than uh is is uh forty percent if you ask the proportion that's uh less than or equal to seven uh you'd see eighty percent yep great question so and you answered it yourself perfectly you gave a better answer than i could um i don't know if that's quite true but um i um you gave a more succinct answer than i than i could and if you well no as you can tell i'm relatively verbose and you you express it with precision and clarity um see i still haven't answered the question i'm still talking around it um so exactly the empirical in cdf means it's the real data that we've collected whereas a cdf um can be of an actual distribution a probability distribution as opposed to exactly yeah and we'll actually see that later on in this workshop we'll see when we model something we can plot the ecdf of the data against the cdf of the model and see how well they they coincide and eyeball that right um so i just want to give we've started simulating stuff and i it wouldn't be it wouldn't feel right to talk about simulations without briefly mentioning monte carlo simulations and showing a couple of nice examples of monte carlo simulations um right so put it behind if you've heard of monte carlo simulations and put it behind if you've performed monte carlo simulations great so i'm definitely um preaching to the choir here um so the two examples i'm going to give you may have seen the first one the second one i really like the first one i love because my background's in in in mathematics um but monte carlo methods are a broad class of computational algorithms that rely on what we've been doing random repeated random sampling right so we're going to estimate pi by uh using uh monte carlo simulations okay so how can we think about there are many ways to think about what pi is right um but one is to consider two shapes one being a circle with radius one and center the origin um and then the square that kind of goes around it and it's tangential to it at all the coordinate axes right so a square with the same diameter around the circle so the area of the circle is pi um pi r squared right and r is one and the area of the square is four so what this means is that pi equals 4 times the area of the circle oh 4 times what did i write here 4 times the area of the circle and that should be divided by the area of the square okay so if we randomly populate the square with points and then count what fraction of them are actually in the circle and then multiply the result by 4 will get um approximately pi so this is exactly what we're doing here um i'm generating the points to to drop into the square i'm generating the x and y coordinates and then here in here i'm seeing which lie in the circle multiplying that by four and dividing by the total number to see the proportion and that should give us pi and as we'll see we get 3.14 something something something which is we know that pi starts with 3.14 the great thing about this is if we increase the number of points we'll get closer and closer to the value of pi and just um to make sure that what we've done is correct i want to plot both the square which is what we've done here um and then the circle which is it looks like an ellipse um because the axes are not um on the same scale okay but it is actually a circle now i want to give one more example which i really like for a number of reasons and then we'll take take a break um and then we'll uh get more into probably distributions and their stories okay um so this is an example actually that i got from uh nassim taleb's book uh fooled by randomness um and although i'm not necessarily a fan of everything talib i think he provides um intuition around statistical challenges um via storytelling in a in a very nice way okay so what he does is he asks us to consider a population of investment managers okay um but each of them is making investments purely based around coin flips okay so each one each time um has a 50 probability of making 10 000 bucks in their investment at the end of the year and a 50 probability of losing 10 000 okay um now once a manager has a single bad year um he's thrown out of the sample now i don't quite do this what i wait is for their value to return to zero okay and then they're thrown out of the sample because they've gone broke in in our simulation okay um so what we're going to do is we're going to generate people making these investments um over and over again um and we'll see who who survives and so the reason i like this is that this is actually a wonderful example of survivorship bias if you have a starting population of 10 000 or 20 000 or 100 000 investors and they're all making investments based on random coin flips you're going to see people some people make a lot of money a lot of people are going to fall away but the people who are left um will be the ones who due to chance um made a lot of money and they may tell you all all the things they did to make make this money okay so that's one of the reasons this um book is titled fooled by randomness okay um because these people are actually acting randomly and yet we're seeing uh success okay so essentially what i'm doing here is i'm yep so i'm taking uh 10 000 time points and a thousand managers um and what i'm doing is taking the cumulative sum so i'm seeing the time series of each each manager um and here i'm only plotting those that um that never hit zero okay so let's i'm playing it with a certain transparency alpha uh in order um because there will be so many so many plots okay so here we go right out of a thousand managers over 10 000 time points with their coin flips these are the ones that have survived right so we see some have seemed to do really well some have done fine some have done not so well but they've still still survived what we don't see here are all the ones that have hit xero once okay and the reason i like this is because we've used our simulation skills um that we've just learned to um see an example of selection bias to see an example of essentially this is a process of diffusion uh without drift and we've seen the the survivors from that um are there any questions about either of those examples someone else may have a correlated question so could you repeat that yeah right i mean just like these investors um so i think yeah was was the challenge that you weren't in the sci-fi 2022 yep and that's that's on me just because we have so many so i'm actually i'm excited to do some yep exactly so yeah once you're in notebooks if that's happened to anyone else um just pop into scifi 2022 um so there's an afternoon break now 45 to 60 minutes um learning about the stories of other distributions then moving on to bayes theorem and then the rest the second half of the workshop will be focusing on on bayes theorem and bayesian inference okay um but this is i find this stuff a lot lots of fun we get to tell stories um and figure out as we saw for the binomial distribution um it has a story right the number r of successors in n bernoulli trials with probability p of success this is binomially distributed and any process as we've seen um where we can tell this story about it will end up being binomially distributed okay now remember when we're telling those stories these are kind of assumptions we're making about the data generating processes and then we can perform tests or use our intuition afterwards or look at um do do visual checks to see how how well that matches up with the with the real world data so i want us all to just develop together kind of a stronger intuition for for this type of thing and i i did mention you know this is from a lot of work with a lot of different people um but i want to shout out to justin boyce in particular at caltech who i've worked through a lot of this stuff with particularly with respect to telling these stories of data generating uh processes so we've seen uh the binomial story now i'd like to introduce you to the poisson story i presume a lot of people are familiar with the poisson distribution but this is actually from a book on me um by david mackay called information theory inference and learning algorithms makai tells the tale of a town called poissonville uh in which the buses have an odd schedule um standing at a bus stop in poissonville the amount of time you have to wait for a bus is totally independent of when the previous bus arrived okay so this means you could watch a bus drive off and uh another arrive almost into instantaneously um or you could be waiting for hours now of course there are no schedules like this um i have recently been in new york where i think the subway feels like a poisson process um quite often but arrival of buses in poissonville is what we call a plus on process okay so the timing of the next event is completely independent of when the previous one happened is the point okay and many pros the reason we talk about these processes is many processes behave this way um i've given an example of several here natural birds in a given hospital um landings on a website media strikes molecular collisions in a gas uh aviation incidents all of these processes and any process that matches the buses in poissonville story is a poisson process um can anyone tell me why i qualified births by saying natural birds why might all births not be poison distributed what's that yeah exactly so it has to do um if if they're induced for example that will be a function of uh certain people's time and when they're when their shifts are okay so you'll find correlation there immediately and lack of independence um great so the number of arrivals of a poisson process in a given amount of time is poisson distributed okay um the poisson distribution has one parameter the average uh number of arrivals in a given length of time um so um let's consider the number of hits on a website in an hour with an average of uh six hits per hour or any of these stories an average of six per unit of time okay so this is poisson uh distributed so we're going to generate such data so once again we're going to do random [Music] and set our seed and before we had rng dot binomial and now the only thing we change um at the start is we use poisson instead of binomial um similarly once again um we have one parameter uh previously we had had two sorry n and p now we have one which is um the average number of arrivals in a unit of time so we've said this will be six here and i want to generate how many do i want to generate you know let's do ten so 10 to the 6 okay and plot the histogram okay so a lot of the time we've got around six sometimes we've got less going all the way down to zero this is good that we don't get any negative results we wouldn't expect i mean that's some form of um check there and then we get up to around 15. okay so are there any questions about this so far hey lam um that should be lambda yeah so generally when we write the poisson distribution mathematically we'll use the greek letter lambo lambda um um what do you mean by a skewed gaussian well a gaussian is by definition symmetric so so in in that case i don't think that is i mean you can actually consider this a skewed gaussian of sorts but i don't think it's the skewed gaussian you're talking about yeah because it's a poisson distribution we can actually let's google it and skewed yeah they're related but they're no they're not the same this isn't a possum um as you can see you require three parameters for this right whereas the plus on only requires one so that'll tell us that straight away um oh brilliant lucky i wasn't working that actually working in that notebook um that's what happens in binder which for good reason will time out um after a certain amount of inactivity um so [Applause] but it does look slightly gaussian and it does look somewhat skewed does it look like anything else yeah it looks a bit like the binomial right um and in fact um the poisson distribution is the limit of the binomial for low probability of success and a large number of trials okay now i won't go through this particularly now but if you want to read that paragraph to see an example of why that's the case i encourage you to do so but what i'd like you to do now is uh plot the ecdf of the data that we just uh generated together and have have a look at that ecdf so just take a couple of minutes to do that so you put up your hand if you're able to do that okay so while the others are wrapping up i'm just going to do this so you'll recall we wrote our ecdr function earlier and you pass up one argument which is your data set now before i actually use plt dot plot um and a bunch of keyword arguments wim where's whim oh yeah we mentioned that i was essentially doing the same as a a scatter plot which i think is absolutely right so we'll see what that looks like here to do that um yeah there's something different with a marker size but that's that's fine um so this is the ecdf of the data we just just generated okay um so once again you can read off several things including all your the other yeah i didn't actually mention this at our last cdf one of the one of the things i really like about ecds is you can read off percentiles and immediately you can see the median is six you can see the 20th percent 25th percentile is four and and so on right um of course peaks are not as easy to see peaks on histograms correspond to points of inflection um any cdfs but you can get a sense of where where it will be right um and you can also see multi-modality uh multiple peaks although they're less pronounced in ecfs is worth worth mentioning as well um cool so now i just want to go through several several examples to get and get a feel so actually going to look at field goal attempts per game um of um lebron's field goals in the 2017 uh 2018 nba season okay so let's just remind ourselves of the story behind the poisson distribution it's the number of arrivals of a poisson process in a given set time interval and this is poisson distributed okay so something justin once said to me he said he didn't even write this he said this to me in a conversation which i recorded um we could model field goal attempts in a basketball game using poisson when a player takes a shot is largely a stochastic process being influenced by myriad ebbs and flows in a basketball game uh some players shoot more than others though so there is a well-defined rate of shooting um so these this is kind of a heuristic or a way of intuition behind why we're using um poisson process to model uh these field goal attempts um so let's just have a look at lebron's field goal attempts for this season um and um see whether they are poisson distributed so this is for each game and what we can do of course is we can plot the ecdf of his field goal attempts and then plot um the cdf of the model essentially um assuming it were uh plus on distributed and see how well um they're aligned okay so to show that the attempts are approximately poisson distributed uh what i'm going to get you to do is plot the ecdf and compare it with the uh ecdf of the poisson distribution that has the mean of the data okay um so this will be the maximum likelihood estimate as we discussed earlier so just talking you through a bit of this so you're going to generate the ecdf for the data here um which i've just done several times so i don't see this being too challenging but let me know if you if you need a hand then what we're going to do is we're going to simulate the model a thousand times okay so to do that what we do is we well we plot the ecdf of the data then we plot the ecdf of a thousand models okay so we're going to um iterate over this many repetitions in the model we're gonna build a model in this line so you're gonna do that using you know random number generation for a poisson distribution um then we're going to create the ecdf i've called it x there and y there because this is kind of our theoretical model and then then plot it okay now when generating um the distributions or the simulations uh you'll need to include um let's see what do i have here yeah you'll need to include as a um uh the parameter um which will be the main mean of the data so have a think about how to do that and we can chat about that if necessary but let's take five minutes or so to to do that are there any questions so far so i might just say a couple of things i think this is it's probably non-controversial that our data set isn't called samples it's called fga for field goal attempts right so um i'm creating a the cdf here for the actual data okay um so when thinking about the samples right um we want to we want to sample poisson distribution right so you'll recall that's rng dot poisson um now remember the first argument is the number of poisson arrivals which in this case will be uh the number of attempts um okay so we want to use the main of of that as i said you know the reason we're using the mean essentially is because that's because that's the best estimate we have it turns out to be the maximum likelihood estimate that's not particularly relevant right now but um and to get the mean just do not just do we do mp dot mean of field goal attempts now it's a good question what size we wanted to use and i don't think i was quite explicit enough about this we're trying to model we're trying to simulate the data again right okay so what we want to do is simulate the same number of games that lebron had field goal attempts in right so what we want to do is make it the length of this okay so the point is we're we're doing this a thousand times each time we're re-simulating under the assumption of the poisson model what the field goal attempts would have looked like and then we're plotting the real data that we have and then we're plotting all these simulations around it okay um now you'll notice when i'm plotting um the simulations i've made the transparency pretty pretty low because we have a thousand of them um and ideally i mean i've done this before so hopefully it works out um but ideally what we'll see is that all the simulated data kind of sits around the actual actual data okay so let's now execute this right and that's that's what what we see right we see in the middle is the ecdf of lebron's field goal attempts and then all around it are our simulations okay now i'm not telling you that this data is poisson distributed okay what i'm telling you is that at least intuitively there's a reason to think it may be and what we've done by plotting this is at least visually validated that it looks poisson distributed okay i'm not drawing any further conclusions from that um what i'm trying to get us to develop is a sense of understanding of of these stories and data generating processes and how they can lead us to think about statistics and probability um now there are you can do statistical tests and whatever to figure out whether something's poisson distributed or not um in the end i actually don't i think most of the time it's it's it's irrelevant um and i do think there are stronger stories to be told about statistics um than than p values and i think most of us increasingly agree agree with that um i do want to give firstly are there any questions about what we've just just done okay i want to give one more example um which is a fun example because this um [Music] is one as i've written here one of the more infamous um examples of a poisson dis distribution and if not the first one of the very early early ones right um which is the does anybody know this example the death by horse kick example oh cool okay well yeah i'm i was going to say this is fun but i don't want to minimize the the harm done by horses and the unconscionable loss of human life as as well um having said that it was in the french cavalry cause so we can expect there to be a certain number of deaths by horse kick we can also expect that unless there's like some dramatic event where all horses start kicking people to death we can imagine that perhaps they're independent so the idea of the number of horse kicks being poison distributed is at least reasonable to entertain as a hypothesis so um let's let's let's do that so this is um the data and you can find i mean where did i get the data this time let's see okay i got the data from somewhere called randomservices.org which i i don't like i mean that's total that seems totally untrustworthy to me i wonder what this website even is i don't want to go down this well no it is a okay it's about probability so maybe that's the joke um right it's about um i mean should i email kyle right now and tell him we're live streaming too okay so what we have here this is uh the data of cavalry core these are the different uh cavalry cause of deaths uh by year okay um and what we wanna do is we want to drop the year from the data frame and sum the rest to get the total number of deaths so that's essentially what i do here and now um what we're going to do is plot the ecdf of the data set so we see you know in all these years there was somewhere between 0 or 3 2 one and 18 uh deaths by horse kick in the in the yeah this is the wait what it's the french cavalry yep exactly so what we're actually going to do is do exactly the same thing that we just did i mean this code i mean maybe some of the very variable names are slightly different instead of fga we have kicks but the rest it's and the label access labels are different of course but it's exactly the same as we did earlier we have less data though as well okay um so what we see now and because we have um more sparse data we actually see we don't see the continuum we saw um earlier um but what we see is that this is the actual data and these are all the plus on distributions around it okay so this isn't what the original author did to show that what's undistributed but this is kind of the way we're introducing and by the way i mean myself and eric mar i wasn't speaking in the the royal the royal plural um i um so yeah as i've written here um i think this is evidence for our data being poisson distributed and then if if it's really important for you to figure out whether your data is poisson distributed there are other ways to go down that path um once again a lot of the time perhaps it isn't so so important but if it is there you can definitely do do other things um so that that's one of the one of the first first examples which i which i found earlier earlier this year before we move on to the exponential distribution are there any questions about the poisson yeah so i don't doesn't make the assumption that they're well they're independent that the events are independent of one another right so well you you don't right so i and that's a very good question because i think the logic kind of goes in a slightly different way it's that we have an intuition from the story of how these things are generated um that perhaps they're independent right um i mean you could as i said at the start it's it's reasonable to think that they may be independent in which case you would get a poisson distribution right and so by plotting by generating by simulating all of these um all of these distributions and plotting them and seeing that the actual distribution falls within them is increasing evidence that that's that's the case that's by no means um definitive evidence but it's increasing evidence and and what would happen and actually this is a great exercise that i don't think we have time for now but um if you were to take this data and do what we do next and try to plot it as an exponential distribution or try to generate um the data for an exponential distribution based on the data you would not see something like this right you'd see something very different um does that is that helpful yeah absolutely yeah exactly um and so once again i just i want to encourage us all to just think through where where the data comes from and the real world processes that that generated and when we move to a more basin context we'll see different ways to check check for this as well but the truth is whenever we're modeling real world phenomena we are baking such assumptions into it um and one of the things that i think is really cool about bayesian stuff is that it forces you to be explicit around your assumptions right um and you can do that in a frequent sense i think a lot of the time i mean due to a lot of the incentives of academic publishing in particular um and that isn't to to be dismissive i mean i um i i think it hmm now i'm not going to go down that but i think we all we all know where this is this is going um and it may have something to do with a reproducibility crisis but um so what i want to talk about now though is the exponential distribution um because we've encountered a bunch of discrete ones as i mentioned before there are continuous ones and the exponential is one and it's related to the poisson distribution right um so um let's think about what the story of the exponential distribution is okay so once again we're thinking through data generating stories um so let's go to back to poissonville um where the number of buses that arrive per hour are poisson distributed um but the waiting time between arrivals of a poisson distribution are exponentially distributed so that's the definition of that's the exponential story right so to be very clear the exponential distribution has the following story the waiting times between arrivals of a poisson process are exponentially distributed has a single parameter the mean waiting time um the distribution is not peaked as we can see from its pdf which i haven't included here but we i think um i mean i'm not going to label my axes but it looks something like that right and there's no there's no peak okay i'm going to label them x and y um um does anyone know the story of um of how descartes came up with the cartesian plane this would be like a three-minute tangent i just find it very interesting supposedly um he loved lying in bed i don't know if you know that about descartes he'd just lie in bed all morning and and philosophize and and think until like two or three in the afternoon um and he had this beautiful tiled bedroom um like with tiles on the on the floor on the walls on the ceiling and um there was a fly one morning buzzing around his room and he was trying to figure out its location he's like how do i even think about this and he realized he could use the projection onto the different tiles on the wall the ceiling and the floor to define its location at any point in time and realized that this then gave him coordinate axes so it was due to him being incredibly lazy essentially and having insects in his room so that's the story of the creation of the cartesian plane um okay i've no idea if that's true by the way but i heard it in a bar once and um from an opera singer actually um so um what we're going to do um let's look at an example of a potentially exponential distribution okay um so the time between all incidents involving nuclear power since uh 1974 okay um so it's reasonable to to think perhaps um that these incidents could be modeled by um a poisson process um that they'll be independent of one another on on average um so if this is the case um the time between incidents should be exponentially distributed okay um so i'm just here reading in all the data um and doing just a bunch of um just foo to get it in the right formats okay so then i'm going to calculate the mean and now what i want to do is generate a bunch of samples so once again i want to generate samples and just notice how i mean i don't i mean we i love this api right because it's rng dot whatever then you have your parameter values and so on then you have your size and it's it's just very nice to work with i find um and then i'm gonna have the size being 10 to the 6 now i'm going to compute the ecdf for the sample the data which is intertimes and it's intertimes because it's times between right these nuclear events um and then i'm going to so that's the ecdf now this is the cdf um although because we're doing it via simulation it's really an ecdf of the simulated data um if that makes sense okay so i'm going to do that and now i've plotted the ecdf of both okay so [Music] that's pretty cool i mean there's some sort of sort of bump there but we can see at least to a first approximation that the data is close to being exponentially distributed um which means that we can model the incident times as a as a poisson process um and back to your question earlier which i think was such a fantastic question is um let's say you thought it was poisson distributed not exponential then you could try to model that and see that these just wouldn't wouldn't align um and what you can also do is look at the actual incidents and see that they're uh poisson distributed as well now i want to talk about the gaussian which is perhaps the most overused um distribution uh in the world i do think gauss is probably you know worthy of the name i mean i feel like having something like that serious named after you but she's got a friend who um when she first worked in it was telecommunications there was a report at work that was named after her and then she moved to a different team and the report still had her name on it so whenever it failed or was wrong she got the flack for it so never the the lesson there is unless you're totally sure of something never have anything named after you um but gauss clearly doesn't care anymore um so the normal distribution gaussian or bell curve can can anyone tell me i mean you can read this if you haven't read it can anyone tell me why the why the gaussian is everywhere why why we use it so much because of the central limit theorem exactly so really what that what that means is i'm actually going to read read my own words here right um one formulation of the central limit theorem is that any quantity that emerges as the sum of a large number of sub-processes tends to be normally distributed um provided none of the sub-processes are really broadly distributed themselves right so when we're doing a lot of repeated measurements and then we take the main for example um then that will tend to be um gaussian or normal right because of all the different sub processes that contribute to to measurement in general yeah now it's entirely overused so i would also i mean this is one of the i suppose challenges about something like students t-test right um which is when we have assumptions baked into it where we don't have to be formal necessarily around those assumptions um so we get results that uh may give us both false negatives and and false positives um that we're not aware of so um but what i really this is an example i really like because it's one of the great as far as i'm like in my opinion my humble opinion one of the great scientific experiments the mickelson moorley experiment um where they measure the the speed of light and what we'll hopefully see is that all their measurements um are normally distributed okay so once again i'm doing something bad here actually um what i'm doing is i'm loading the data doing a bunch of stuff um to just um get our axes axes correct and labels correct uh then i'm fitting a normal distribution to it and plotting the histogram and the normal distribution over it okay now of course this is bad because of binning bias right but we're going to do it and then we're going to do the ecdf okay so look this is the data and this is the gaussian fit so that's kind of chill i mean maybe cool thing is we can do the ecdf which will uh tell us a lot more right so i'd like you to take a few minutes to uh plot the ecdf um and the ecdf of a simulation and it's very i'll give you a hint it's very similar to the exponential distribution i i just did so take five minutes to do that and then we'll we'll look at it do put up your hand if you've been somewhat successful just a bit higher i can't wait nice just wait one more minute so maybe there's a hint if i don't know whether this is where people are slightly blocked but um we're going to use you know when we did poisson we did iron g dot poisson so now we do rng dot normal and there are two parameters which are the mean and the standard deviation and i've calculated the mean and the standard deviation of the data itself um so if you plug those in you should be good so maybe i'll do that now remember the last thing we want is size number of samples and then i've generated the ecdfs here using ecdf of the data and then cdf of the model okay so i'm going to plot this now and look we see it kind of kind of fits right kind of looks normally distributed um so once again the story is that we had reason to believe because we know that measurement error a lot of the time is normally distributed we have reason to leave this data maybe and then we plotted the ecdf and modeled it and saw that they kind of look similar so this is more evidence for this being normally distributed and that's the only takeaway i want you to get from that i mean the story we've told is the most important part okay i do want to point you to uh what i think is a beautiful blog post by alan downey who i mentioned earlier um he was at olin college for many years um and he's been a big influence for a lot of us in the python space um he has a lot of beautiful books that he self-published under green tea press and then were published by o'reilly as well think baze is a wonderful book but there are many others he's recently just joined um really cool startup called driven driven data and i think um but the reason i mentioned this post wow okay i remember when this post was two years older now it's nearly 10 years old that's that's wild um but the title is important are your data normal hint no um the point is that your data will probably never actually be normally distributed but you're not part of the point is that you're not actually usually interested in whether it is or not and as alan makes very clear there are so many tests to do this such as anderson darling kolmogorov smirnoff shapiro wilke ryan joyner i like how he chose all the double-barrelled names because i've got a double-barreled name myself um but he makes the very important point that these tests are almost never what you want um what we want to know is whether a particular distribution is a good model for a data set whether it's a good model for what we think generated the data in in the real world um and as he makes clear that a statistical test will not tell you that because that's a modeling decision about assumptions you make and you can see whether that holds and what that leads to but a statistical test will never tell you whether that's true or not because it is a modelling decision a lot of these times in a lot of cases i'm sorry um and so as he goes on the normal distribution is a good model for many physical quantities because of the story of error measurement and that that due to the central limit theorem that gives rise to normally distributed data right yeah i'll i'll try to use alan's words again because i think he said it so what we want to know is whether a particular distribution is a good model for a data set right and a statistical test won't tell you that whether it's a good model it'll tell you how close it looks like it to um it won't tell you whether it's a good dis modeling decision to make um whereas thinking about through the data generating process can inform that decision um so i mean that's that's the the goodest question you could ask right now um and the answer is quite challenging but good i think what it really means is does it actually correspond to the processes in the world a lot of time physical or material that generate the data you observe and a statistical test will not tell you that it will not tell you about the data generating process because all you're doing with a statistical test is comparing the data and nothing nothing upstream if that makes sense was that somewhat helpful that's why i said somewhat yeah i set the bar low with the question um okay great but i do appreciate the question um look i'm going to skip joint probability and conditional probability with resampling techniques i think we all may have a sense of this i want to move on to some of some of the media stuff um i think what i'll say and this maybe just reiterating what we all know presumably um joint probability um is the probability of two things occurring okay um if those things a and b are independent then the joint probability will be the product of the individual probabilities this is not often the case um a lot of time it's it's it's not the case at all um okay that's all i want to say about that and you can go through all of this um just checking the time good um then conditional probability and we'll see this will become increasingly important and in fact some of you depending on how much bays you've done may know that some of the key bayesians would tell you that all probability is conditional um and we can we can talk about that later if you'd like um but conditional means it's a probability of something knowing that something else is true right so and we use the notation p of a uh vertical bar uh b to denote this so we looked at probability of beak lengths before um we can condition that on the species right so we can look at the probability of a finch beak having depth less than 10 can knowing that the finch is of species fortis as opposed to scandins which are the species we saw earlier right so that's what conditional probability is um can anyone has anybody heard the statement before that all probability is conditional yeah a couple of people um i think the point the point of that really is to is to state that whenever we're talking about probability we are conditioning it on some knowledge we have of the world okay so let's say that we say the probability of heads is 0.5 um and we haven't experimented with a particular coin right let's say i've got a coin i say i think the probability of heads is 0.5 um and that statement is already conditioned on what i know about how coins are made uh that the mint or the reserve bank or whoever does it here i know in australia it's the mint um will um not create bias coins what we know about the physics of flipping coins what we know about the physics of coin spinning um all of these things are baked in to we're conditioning our notion of probability on all this external knowledge we have whether we're doing it explicitly or not okay so that's one one example um so in order to really introduce bayes and move on to bays i want to give an example um i the example i want to give is um drug testing we can let's consider this to make this less controversial let's consider it as um uh breath testing a driver as to um their levels of alcohol consumption okay um whether that's less controversial or not i'm not i'm not sure um but so when i say you uh using a particular drug this is what i'm referring to that they're drunk beyond a certain they have an alcohol uh blood alcohol percentage beyond a certain threshold okay so suppose that a test um for this has 99 uh sensitivity so that's true positive rate right a 99 specificity true negative rate um so this is one percent false positive one percent false negative now we suppose that uh five and a thousand of people uh use the drug now what is the probability that a randomly selected individual with a positive test is a drug user okay so why this is interesting is essentially what we know already is the probability of a positive result if they're a user um but what we really want to know is that they're the probability of them being a user given a positive result okay um so this will be some sort of mvp for moving sorry for using the term mvp i've worked in tech for too long clearly um but this will help us to think about moving from the probability of the data giving them given the model uh which we're good at doing um to the probability of the model or parameters given given the data okay so what i'd like you to do now um well firstly i mean if you i presume most of you are actually quite statistically sophisticated so you may be aware of the base rate fallacy but um let me ask what would what would a naive approach to this question suggest that if we know it has one percent false positive and one percent false negative what's the probability of a randomly selected individual with with a positive test being a drug user does anyone have a sense yeah exactly um and that i mean we know that isn't right now right but naively you would you would say that and and so the the reason that's not not correct as some of you may may be aware is that we're not actually taking into account that so few people are actually users in the population right so the base rate of usage is incredibly low um so that really changes these one percents um to a lot of different things as we'll see and we're going to tell this story via simulation now okay so what i'm going to get you to do is take 10 minutes to simulate this okay so what what we're going to do is we're going to take 10 000 people randomly from the population sample uh for the number of users and non-users and we know that the rate of usage is five in a thousand um so we can sample um there's a certain probability so maybe we use the binomial i don't know um and then we sample each of those to see how many will test positive how many will test negative and then we look at all the positive tests and see how many were actual users okay um and of course this is actually totally applicable to and we would have seen in the past two years with um coveted infection rates as well [Music] do you put up your hand if you were able to do the first the first cell nice um is someone willing to talk me through it perfect so what we're doing here we want to sample for the number of users we know i actually wrote 10 000 and then set n is equal to 100 000. so i'm gonna make it a hundred thousand um but yeah um our colleague up the back is absolutely right that um we know that point five percent our users so we sample with p is equal to zero point zero five or zero zero five yeah exactly thank you what's an order of magnitude between friends um my postdoc advisor actually used to say what's three orders of magnitude between friends um but we were working in biology so what's that um yeah yeah it does um oh the reason it took me a second i was thinking which n um and what i mean by that this last one is its size and i think of psi yeah we've got like yeah i mean yes the short answer is absolutely yes um great and we're and we're simulating at once okay so then the number of non-users will be n minus users right so how can we think about how many tested positive and how many tested negative another binomial right because we know that um there's a false positive rate of one percent and false negative rate of one percent right so these are our probabilities um so this will be r and g dot binomial and what is n now users and what is p 0.99 we'll test positive and similarly here we'll have another so we're we're literally just like using the binomial sampling three times essentially so here then we have non-users and how many of those will test positive exactly 0.01 right and so to figure out how many of those positive tests were for users we do upos over the sum of upos and non-pos okay and what we'll see is that it was 31 which is significantly significantly significantly less than what we may have thought naively okay hey yeah so i suppose essentially you're talking about propagation of uncertainty through the modelling process um you're absolutely right and i in in this case it isn't actually particularly you'll get this result most of the time and the distribution won't be wired but to convince yourself of that i would encourage you and anyone else to um what you can do you see where my cursor is here you can actually simulate this 10 000 times right and then see then you get the uncertainty built in into the model essentially um and see how that propagates through um great so this is the base rate fallacy and essentially what we've been doing is and this is something in the past two and a half years we've actually seen i mean selection bias has been rampant um lack of knowledge around measurement techniques and then base rate fallacy as well we've seen in in the media time and time again um the probability of a condition on b being mixed up with the probability of b con conditioned on on a even when reporting uh covert incidences as a result of tests um so another reason why this is so i mean this is important across the board but particularly in the past past several years um so essentially what we really want to do and i'll make this as explicit as possible um a lot of the time we can figure out the probability of data given the model but what we the scientific question is we get the data and we want to figure out the probability of the model and the relative probability of different models as well the relative probability of different parameter values uh these these types of things and it is bayes theorem that allows us to do that um bayes theorem um that the probability of b given a is equal to the probability of a given b times the probability of b over the probability of a um and what we will see is that when this when we use this for example um for the drug testing problem we just went through we can do this analytically and by that i mean like using the mathematics and not simulation what we see is that the probability of someone being a user um given a positive test is equal to this and we can go through um a bit of algebra and essentially to figure out exactly what's what's happening here and that it is uh equal to 0.332 and i encourage you to go through the math using bayes theorem if you're if you're interested um are there any questions about anything we've done so far because we're about to really move into bayesian inference okay cool right so did you install the environment from that's interesting because just sick whoa definitely remind me later yeah look in the environment i've specified pimc so yeah when did you do it requirements here um so i think there's several buttons yeah yeah so this is definitely on on me whatever the issue was but the first thing to note is that i did request in the readme that pull the repository at 9am today i think that may have been the issue i'm not sure but um then did you can you did you do this you know i'd just love to know i mean i want this to work for as many people as possible so any way to make sure that it runs on everyone's systems but installs are tough right dependency hell is a real thing all right um now it's time to this is even more fun what we're doing next so um if you think we've had fun so far wow you're in for a treat um so let's import we're in notebook 2. can we put our packages um some learning objectives to to think through don't worry about that warning um it's a non-issue for us um we're gonna start to understand about the building blocks of bayesian inference we're gonna understand what priors likelihoods posteriors are we're gonna use random sampling for parameter estimation um in order to appreciate the relationship between sample size and the posterior distribution along with the impact of the prior uh we're going to use probabilistic programming for parameter estimation and for hypothesis testing and maybe for regression i added the regression example recently so didn't even make it into the learning objectives that's how like cutting edge today is um good i may have asked this at the start can you put your hand if you've used probabilistic programming languages before oh this is fun okay so and so put up your hand if you've used pyemc or pimc3 okay so that that is a probabilistic programming language um i'd be hard-pressed to give you a a precise definition of what probabilistic programming languages actually actually are but one thing that's very important with respect to them is that so in numpy uh matrices or arrays of first-class citizens right um in pandas data frames of first-class citizens um and psychic learn models and pipelines are first-class citizens right fit and predict and pipelines um in probabilistic programming languages distributions are the first-class citizens so that's that's one way to think think about these we want to use distributions to build statistical models right so that's why it's important that these are the first class citizens um so the first thing i want to do is to move from bayes theorem which we've just discussed the bayesian inference now one thing the terms probability and likelihood the lowercase ps all of these things are entirely overloaded so i want to disambiguate some of that but i want to make clear that we do live in a totally overloaded world with respect to this as as we'll see okay so why is this important once again so let's say we flip a bias coin several times we want to estimate the probability of heads right so as we discussed earlier statistical intuition tells us our best estimate is the number of heads divided by the total number of flips however it doesn't tell us how certain we can be of that estimate and this type of intuition doesn't lend itself to even slightly more complicated examples right and this is one of the places where bayesian inference helps so what we can do is we can calculate the probability of a particular p equals probability of head so think of this if the probability of heads is 0.7 that's what p lowercase p is here okay so we can calculate the probability of a particular p given data d by setting a in bayes theorem equal to lowercase p and b equal to d so i understand notationally we're entering um a slightly annoying or frustrating space and notation is i think notation is probably one of the big reasons a lot of people think they're not good at mathematics to be honest um but um that's for for another round um so what we have here then is uppercase p p the probability of p of heads being a particular value given the data d is equal to the probability of the data given p which we know how to calculate that's what we've been doing in the past um for the past couple of hours we've been given giving probabilities and calculating the probability of seeing particular data right then the probability of times the probability of p over the probability of the data whatever whatever these terms actually mean right but this is what bayes theorem tells us when we replace a with lowercase p when we replace b with d okay so in this equation we call the probability of p the pr right i mean this is ridiculous we call the probability of this probability uh the prior distribution we call p of d given p the likelihood uh and p of p given d the posterior distribution um and if this is slightly confusing still we're gonna be going through this for the next hour and a half okay so it's chill um so the the intuition behind these names is the prior is the distribution containing our knowledge about p prior to the introduction of the data d right the posterior is the distribution containing our knowledge about p after incorporating the data into our modelling okay so posterior is after post pm post meridian right um i can't believe i just mansplained posterior um so i just want to make clear that we are overloading the term probability once again we have three distinct usages of the word here so probability p of seeing heads when flipping a coin that's a probability okay the resulting binomial probability distribution um p of d given p of seeing the data given p we've also called this probability then the prior and posterior probability distributions of p encoding our uncertainty around the the value of of p okay so all of these things we're referring to as uh as probabilities in in different ways so i just want to make clear that i understand the suffering involved in this this discipline um i i know this is a lot of talking and text to go through but it'll i was gonna say it's worth it i don't even know if that's true but it's it is important okay so um the other thing to make clear is that we really only need to know this uh posterior up to multiplication by a constant because we only really want to know um we only care about the values of the probability of p relative to each other uh for example we might be interested in what is the most likely value of p on top of that um once we have all the values of p of p given d we can normalize it to actually get the probability distribution right um so we don't need to worry about the term the probability d probability of the data um and i'm glad because that's something i don't even really want to talk about um okay so the probability of p given d is proportional to um these two terms so essentially what this is saying is that the posterior is proportional to the likelihood multiplied by the prior okay so if we're talking about a coin flip what's the prior what do we know about the probability of getting heads and this is a big this is one of the big critiques of bayesian inference right what's the prior right so what we can say about it we can say that it's going to be between zero and one right because it's a probability okay um so maybe then if that's all we know if we know nothing else about the coin maybe we just give it a uniform distribution for for the prior okay maybe you know when i was the australian mint that that made the coin so we can actually probably assume with high likelihood that it's 0.5 with a very narrow distribution around it okay um maybe if it's a magician's coin we actually assume it has a high probability of zero and one and it goes down in in in the middle all of these things are possible right now is this concerning i honestly don't think so for several reasons i mean part of the critique is that if you have a different prior to me um then we may end up with different different results um essentially we should only have different price if we have different evidence okay so if we have the same evidence and incorporate the same thing we should come up with similar or analogous priors on top of that um what we'll see is that as we generate more and more data um the prior becomes less and less important okay so just looking at at this if we don't have much data then the posterior is influenced by the prior if we generate more and more data the likelihood dominates um and the prior becomes less important and that's what we're what we're going to going to see now okay so what i'm doing here and i'm going to go through this somewhat quickly um what i'm doing is i'm generating the posterior um trying to estimate the probability of heads okay for the example just stated um i'm i've told you i'm telling us what the likelihood is we can go into that in question time if you'd like i don't think it's particularly uh relevant now but what i'm doing is i'm generating the posterior and i'm giving it two potential arguments one is what the actual probability is then um the other is the number of coin flips okay so i'm creating that function now and what what do i actually want to do next i want to plot this um so the default um argument is 0.6 so all i'm going to choose is the number of coin flips and so notice i'm sampling within this as well okay so i'm going to do 10 coin flips and look at the posterior given probability of six okay right and so what we see when we sample this is that it has center 0.5 um and that probably means that we've actually had five heads in in flipping this ten times and not six um and then we see you know we have some sort of variance there right if i do this again it's probably i wonder if it is deterministic yeah because i set the random number generator within there um now if i set it equal to 50 what should we expect to see or what would you expect us to see with everything i've just babbled at you yeah we should see it narrow right um as we add more and more data we should be more and more certain of our value and that's exactly what we see now if i make it 150 and narrows even further and closer to 0.6 okay um so in order to actually do this um what i want to do i wonder if this will actually work this should not work in in binder unfortunately i don't think but i'm going to use interact i don't know if you have much experience with ipi widgets um i think there's another session on jupiter widgets currently um so this i think this is right so let's see right so this has given me the posterior distribution for n is equal to 0 and p is equal to 0.6 okay which when when we have no data we just get the prior distribution which is uniform right and so what we can do now is use this slider to increase the sample size so we see as add more and more data we get a narrower distribution like that right and we can also adjust p and see how that changes which is what what we'd expect right so i encourage you all to play play around with that and actually um we're going to play around with with a couple of different prizes very soon okay so i think the two the two points worth mentioning are as you generate more and more data as i've said your posterior gets narrower you get more and more certain of your estimate and once again i'm sorry i didn't really emphasize the power of we actually have the the entire posterior the the probability of all possible values um for the parameter of interest right we don't only get um you know the maximum likelihood estimate plus some standard error of the mean or something like that we get the entire distribution which is which is incredibly powerful um now the other thing worth mentioning is you need more data to be certain of your estimate when p is equal to 0.5 um so the closer um it is to symmetry the the more data you need so imagine if p is equal to one you flip it ten times and you get ten heads then you're pretty certain that it's going to be close and close to one right um so that's really all the only point that's being made made there um i do want to spend a few minutes talking about the choice of the prior again and then i'm going to get you to do a similar code up um on what we've just done um so firstly you would have noticed that we have to choose a prior and when we have small to medium data sizes the choice can impact the posterior so they are very much worth talking about um probably three types of priors that you know it's worth classifying them into um informative priors which express definitive uh definite information around a variable um so the example i gave before if we get a coin from a min and we think that is probably 0.5 um with a very narrow probability of anything else that that already contains information so we call it informative um there's weekly informative um which expresses partial information so we'd have p is equal to 0.5 with uh larger variance and then there's uninformative priors which express no information except what we know for sure such as knowing that p is strictly between um zero and one okay um so this this does get weird now um i think it is important to mention that you know i would have thought and you may think that the uniform distribution is uninformative right um but it actually it isn't quite um because i've been thinking about this question in terms of the probability p right and eric who is in isn't here at the moment um or any any one of you could actually be thinking about the question in terms of the odds ratio which is p over one minus p right which is a totally legitimate way to think about this question right um so eric rightly feels he has no prior knowledge about the odds ratio and so he chooses the uniform prior on r uh which is the odds ratio i rightly think i have no information about probability p so i choose the uniform prior on that and they are not the same prior and you can do the math to figure that out it's not particularly interesting um i mean unless you find algebra and transformation of variables interesting which um i'd be concerned about um so because we've chosen different priors we actually need to think more in a more principled fashion about this and there is a prior called the jeffries prior which is an which is an uninformative prior that actually solves this problem um if you are interested um severe and skilling gives a nice treatment about it um i mean what link have i included here yeah it's wikipedia there's a link about it the work of how jefferies is is is very beautiful um as is that i don't know if anyone has read any edwin james but all of their work on thinking through kind of the principled nature of these types of probabilistic arguments uh is is super cool um in the binomial coin flip uh the jeffries prior is given by this um and what i'm going to get you to do now in five to ten minutes is to yeah i haven't given you any scaffolding for this um that's cool most of it's there what i'd like you to do or attempt to do is create a plot like the one i have above which has the jeffries prior and the uniform prior so i'm gonna if you do what i do i go to this cell i'm gonna copy this cell i'm going to paste it here and how do i want to do this i'm going to call this prior prior one and you're going to want to create a prior two and then this will be posterior one yeah this is a bit gnarly and i have normalized so that the peak is at always one which isn't actually quite normalizing um but and then we'll have posterior two is equal to something and then something like that so all i really want you to do is i've given you the equation above for the jeffries prior so use that i've spoke jeffrey's in two different ways as well um that's not one of my common inconsistencies but having lived in the us for seven years and being australian i don't like my use of my spelling has broken down completely in terms of consistency [Music] [Music] i suppose the first thing the likelihood which is this thing is gonna stay the same right so i'm just gonna the posterior two is gonna be that times prior to that's safe to say can anyone tell me what the prior the jeffries prior is so i want something to the power of minus one and so what what what that will be is like the square root of something right so i want np dot square root then yeah and i'm i must apologize i mean the the variable name of p the x-axis i've labeled x um so it's going to be x star or times multiplied by 1 minus x and all of this to the power of minus one i think um i'm just gonna interact again let's oh yeah there's a divided by zero error which is because let me actually just go to the instructor notebook because hmm oh that should be okay let's see no actually hmm reach into the stump yes that's the issue um if you change x starting from 0 to 0.01 because we don't want to be dividing by zero and actually maybe we want to take it up to 0.99 we'll see great um so in in the spirit of time um i'd like to move on and you can have a look in the instructor notebook to catch up on any code you weren't able to do then but essentially here is i don't know why the labels didn't turn up but i'm not going to worry about that for the moment this is the jeffries prior and this is the uniform prior okay so i just want to see what happens as we generate more and more data okay so you can even see as soon as we do six coin flips right we're getting the posteriors are looking pretty pretty close to be honest okay um so this is actually and as we get to 20 we're we're good um why am i showing you this um for several reasons one is to show something we already know that um as you generate more and more data the less and less the prior matters but it also gives you a sense you can do these types of computational experiments based on a few assumptions before you even go out there and collect real world data it also gives you a sense of how much data you may need to collect uh in order to say something robust about about your system okay um so thinking about this in a principled fashion and allows allows for that um so now it's time to move on to probabilistic programming and the bayesian workflow but um are there any questions before we do that all right so we have around an hour left and it's going to be the most exciting hour of summer um so what we get to do now um is to build on all the skills we've learned by learning the basics of probabilistic programming using using bmc okay so in terms of building bayesian models let's remind ourselves of what the steps are very explicitly um okay so firstly we want to completely specify the model in terms of probability distributions so this includes specifying what form uh what the form of the sampling distribution of the data is so this is the likelihood what generates the data right um so we'll see this time and time again now but previously we're like what generates the data oh we think it's a binomial uh generating function so that we use the binomial oh we think it's plus on and then we check that right um or we think it's normal or we think it's exponential right so we and we want to explicitly encode these assumptions and write them down and then perhaps iterate on them when it turns out that they didn't turn out to be right okay so we want to completely specify um the model in terms of uh probability distributions first the sampling distribution which is the likelihood then this likelihood will have unknown parameters binomial has n and and p right so we want to um uh think about what uh form describes uncertainty in the unknown parameters and we use priors for this right these are what give us the uncertainty okay then we calculate the posterior distribution right so oh look i've written that below in the coin flipping example the form of the sampling distribution of the data was binomial that was the likelihood uncertainty around the unknown parameter p captured the prior okay now i said remember the binomial has an n um which is the number of flips there's no uncertainty around that because we've flipped it n times we know how many times we've flipped it it's the size of the data set right so we know exactly what that is so now what we get to do um is to do all of this what we've done above using a probabilistic programming language and as i said one of the cool things about these types of things including pi mc are that probability distribution to first class citizens what i mean by that is we can assign them to variables and use them intuitively to mirror how we think about priors likelihoods and posteriors and we'll see that in a second and then pi m c calculates uh the posterior for us okay um and under the hood it uses um well there are several things it can use but um what we'll be using is a sampling based approach called mcmc marco chain uh monte carlo it also uses variational in inference um but let's let's do it let's let's jump in um i don't know why i've okay let's just execute these imports again because there are a few more and let's get this ecdf so the first thing we're going to do is just basic not basic is parameter estimation which i think is a good first thing before doing like comparison between groups and linear regression and these types of things so we're going to try to estimate a click-through rate remember we knew the click-through rate before and we generated the data so we knew the probability of getting data given that click-through rate so now we're going to do the opposite we're going to use pi mc to given data um figure out what the click-through rate is now i'm going to be really cheeky and i'm actually going to generate this data uh already um but that's actually cool right because the reason i'm doing this as opposed to just giving it to us is that we know what we'll know what the click-through rate is and then see what happens at the end right whether our pi mc and and uh probabilistic programming has given us something that is is sensible okay so i'm generating we've got 150 um people hitting a website we know the probability of conversion is 0.15 number of successes will be rng binomial um whoa and pa this one okay looks about right okay so what do i want to say now now it's time to build our probability model so recognize that this click-through rate question once again is a biased coin flip um so the sampling distribution is binomial and we need to incorporate this in the likelihood um there is a single parameter p that we need to describe the uncertainty around using a prior and we're going to use a uniform prior for this okay so i'm actually going to start we build these models bottom up right so i'm going to start with a likelihood and then think about what we need in terms of the prior as just described so we have y is equal to pm so actually i want to make clear um we're using some sort of pythonic context manager doesn't really matter um for the time being why we're doing this but this is what we do um and then inside this context we define the model essentially okay um and if you use context managers a lot you may notice that it's um slightly unorthodox in in some ways but it's not a big deal okay so we right so we have our binomial likelihood now we need to specify uncertainty around the parameters okay now remember n there's no uncertainty around it we we know how many coin flips there were that's capital n okay but there is uncertainty around p so we're going to call that prob which we're going to define as a prior in a second now remember the what we also need to do is put into the likelihood the observed data okay and this will be the number of successes right now we also want to specify our prior so just as we did pm.binomial we now do pm.uniform because we're choosing a uniform prior and we want to give this variable a unique symbol which we're using p i should use prob and i would suggest in future that um the python variable mimics what we're using as a as a variable name um but um because i've done this previously and it may impact later cells i'm going to leave it like this for the time being okay so the takeaway the real takeaway is though we want to specify the likelihood and then uncertainty around all the parameters in that likelihood above that okay so that's essentially what we've done here okay and oh yeah i can't spell observe properly that's cool okay that worked great um now something i mean as we're building a model like this maybe it's chill that you know we kind of know what it is but as we build more and more complex models it's pretty useful to be able to visualize them um so let's just visualize this using graphiz um which you may know if you've worked with psychic learner visualization that type of stuff what we see here is that we have this uniform p um and it we're using it among other things to calculate the binomial or using it with the likelihood to calculate the posterior okay um so there'll be time for questions in a minute but i want to get through so we've built the model okay now what we want to do is and actually i wonder what we want to do is sample from the model so we do pm samples is pm sample um and we'll do 2000 to make sure that we have a sufficient because we're sampling we want to make sure we have some sort of burn-in period where it goes around the probability space and funds finds the correct correct things um then actually i'll execute this then what we're doing in here um is i've created this object called idata for inference data which um it gives us a few more things it gives us a prior predictive and posterior predictive which i'll i'll tell you what they are in in a minute um so we don't need to be concerned too much about these warnings that's fine same as before this is cool um so what this is doing essentially is it's sampling the space to help us figure out what the posterior is okay um and when that's done we're actually going to want to plot the posterior of our p um and we've imported a package called arvis i wonder if this is samples [Music] okay so you can see that we've finished sampling our posterior sampling from it and now i'm going to plot it within this context manager of the model okay great so we've got our posterior distribution which tells us several things it tells us the mean of the posterior is 0.18 okay now remember we set it equal to 0.15 um so it's pretty close and we wouldn't expect it to hit with 150 samples we wouldn't necessarily expect it to hit 0.15 on on the nose right um and we also get um the 94 highest uh density in interval which goes from 0.12 to 0.24 okay and what that helps us do is to let's say we need to speak with our product manager or head of growth or whoever we're running this experiment for and they're like oh um you know what what's the conversion rate and you can go and say 0.18 but you can say with 94 confidence i know it's between 0.12 and 0.24 you can also say if you need more precision we need to go you can do some sort of um you know power analysis for example and say we need we need to go and do more um get more data with respect to this this experiment um but essentially once again we get uh the entire distribution here i i think which is well i don't think i i know it's incredibly useful and important to get that um so what we're going to do actually before we move on i want to talk about predictive checks and trace plots in terms of diagnosing this type of stuff but i think that's that's probably depending on you know your level of expertise with this type of thing that that's a lot to take in so um perhaps there are questions yeah so i actually think to do that it will actually be useful i actually go through the idata stuff in in a second so i can show you what i draw out of it um in the next section um but can we come back to that because i do go through that next awesome other questions okay cool um so that that's a great question and to in to introduce the inference data object i just want to kind of talk about what i've what i've put in there essentially um so we've put in a prior predictive check um posterior predictive check and trace plots okay um so the prior predictive check uh generates samples of the data based on the specified prior without looking at the data okay so they will help you check whether your priors are sensible distributions um or if if they're totally unreasonable okay so what we're going to do here is plot the prior predictive check for our model okay and as you can see what we're doing is we're taking the prior predictive and selecting y and then flat flattening it out and you may recognize here that essentially we know that we have a uniform distribution and that's what we've done here and i think what's clear is that it's bounded by the total number of um visitors which was 150 at the top and bounded by zero below so as a prior predictive check if if we're getting negative values for our prior predictive check we need to be very concerned right um but this kind of accords with our intuition of what we what we said it to be and what we want it to be um oh yeah that's exactly what i wrote here so it doesn't look sensible it is a qualitative and quantitative question so qualitatively it does look extremely similar to a uniform distribution quantitatively its lower bound is zero up about 150 um so it looks it looks pretty good um and as i said we would be concerned if there are negative values or values over 150 or yeah because that would violate the assumptions we have about the data generating processes um and we'll see how red flags in our prior predictive check can actually arise very soon okay um and why we may want to change our priors in fact um the posterior predictive check samples our posterior distribution um uh with respect to the data so we would actually sorry of data so it takes the posterior distribution and generates the type of data we'd see from that posterior distribution so we would expect it to look uh something like our actual data okay so let's now plot that and see what it looks like um so let me get this right um this text below is slightly incorrect it shouldn't be 21 but this is our the data generated by our posterior over successive simulations and we'll see the median is around sorry the mean oh it is the median of course is around 26 now remember our data actually has is just one number okay it was which is the number of clicks so let's just have a look to see so the number of clicks was 26 which is pretty much the center of this uh posterior distribution which once again um this isn't a silver bullet but these are all things that validate the modeling we've done the choices we've made and the assumptions we've made okay and as i said in the next example we'll see how this may break down i also want to plot trace plot which what this does is remember what we're doing is we're like we're sampling the space and what i've just shown you is kind of at the end of the sampling procedure what the posterior looks like right um but we want to see it throughout and see whether if we see it like drifts a lot um and doesn't stabilize at any point then maybe that's that's a red flag right essentially what we want to see is um diffusion around a particular value and and no drift because that's that that's what the mean will be that's what what the actual value will be right so what we want to avoid are things that shoot off like these right so the way we do that i need to look at my cheat sheet now um oh yeah there's i mean the simplest cheat sheet possible um what we do is we use obvious again to plot the trace of the samples um and essentially to get this kind of drift without diffusion we want to see what looks like a hairy caterpillar right we want to see it kind of oscillating around a particular value um what we have on the left what we've done with this sampling procedure essentially is um in parallel we've done four chains of of sampling um and this uh on the left it shows us uh all the different chains and then to calculate the actual posterior we concatenate these chains and get the posterior out of that um so i think that helped think about the inference data object with respect to what we wanted to get out of it is was there other stuff you wanted to know about the it's a highest density in interval yeah which we can think of as so someone may want to correct me but i i think i've got this right the 94 percent hdi is the interval in the distribution [Music] which captures i don't think it's quite the third to the 97th percent percentile but it's the it's the interval which is 94 of the range which captures the highest density but remember that wasn't in the in the eye data that was in my samples actually yeah so in in fact there is i wish so let me actually i've overloaded something here the truth is that um these samples are actually all contained in part of i data as well so um right yeah i need i think it's in sample stats but i need to need to dig into that a bit more um yep yep absolutely um so what is going to happen next is that you're going to do something similar you're going to do parameter parameter estimation just like i have um but on the mean of the beak length population of galapagos finches um but once again i feel like that that was a lot to to take in so um i just like to open the space for any any questions before we before we move on it's either a really good sign or a bad sign yeah hey yeah the funny thing is in this case is that we don't have from the actual data all we have is a single number right so we just want to make sure that that single number lies somewhere credible with respect to the posterior so if the data consisted of something distributed then we could compare distributions but in this case all we know is that we had 26 conversions or heads right so then we just need to so let's say in this case that um the counterfactual would be let's say we actually had 40 heads then this would not be convincing at all but the fact that the the experimental data gave us 26 and it lies in the center here if that makes sense um yeah so in that case if your prior predictive check is looking good then it's probably not the prior and the other thing that you've decided on or made an assumption upon is the likelihood so then you delve into your likelihood um and check your assumptions there so it may not be binomial in this case right yeah cool um so if you all would like to oh i wonder how much of yeah i'm gonna i think maybe take 15 minutes or so to jump into this um i'll make clear we'll import the data of course um remember the things we need are a likelihood function for the data um what i do here is i plot the data and it looks kind of normal there's a slightly long tail that's fine looks kind of normal so um we're good we're going to use a normal likelihood for the beak measurements and once again because it's measurements we can you know think perhaps this is normally distributed as well um what are the unknowns that we need to express uncertainty around with priors we have the mean mu and standard deviation sigma okay um and so we're going to use a normal prior for mu with mean 10 and standard deviation five and a log normal prior for mu um with these these values um i think this was a note to self um there are biological reasons for these um i mean how much i want to go into this let me just say the reason we use a log normal prior for sigma um so sigma is is variance right um standard deviation and it's strictly positive um so we need a prior that's strictly positive um we don't think the variance will be like incredibly large so we want something strictly positive that maybe doesn't fall off doesn't extend too too far and log normally is something which satisfies both of these so this is why we're using a log normal we'll see perhaps that it doesn't quite work out but those are a few bits of intuition around that um so having said all that um why don't you jump in and you get to build the model similarly to how we did above then visualize the model and then sample it so take 15 minutes or so for for this and i'm able to answer any any questions along the way and for those leaving early this is not an intention to to call anyone out at all um but what i would love in the readme i have um uh just a feedback form we want to make this workshop as good as possible so if you if you're able to that would be so so awesome to just give us feedback and please be as critical as possible but constructively so or not [Music] thanks um okay please so okay once you yeah so the challenges we've got two probabilities one is the probability of x which we're uncertain okay because we don't know if the coin is yeah exactly right so because there's an uncertainty we can express that as a probability absolutely so what i'm going to do is just very briefly talk you through my solution to the to the first cell or the first two let's let's say okay um so remember what we need to do is encapsulate what we know about or our assumptions around the sampling just distribution of the data through the likelihood okay remember i said that's going to be a normal distribution right that's the assumption we're making which is based upon you know looking at the data and then thinking about what generated the data um we call it y1 and then it has um two variables which are mu and sigma um which we're assigning to mu one and sigma one remember we'll need priors around those so that's what we're doing above but we also need to give it um the observe data so that's what we're doing there okay now mu1 we said we're going to assume is normally distributed so that's what we do here with mean ten and sigma five that's exactly the assumption i i made there and then sigma one being lognormal um with um yep mean zero and sigma 10 okay so essentially that's that's what we're doing there and you'll notice with respect to the api we're doing the same thing for each of these first class citizen distributions it's pm dot whatever distribution we we have there so i'm going to execute that um and while we're executing that i just want to get that oh where are we i'm sorry okay i just want to remind myself what that is and so let's just we'll visualize that right where we have a log normal sigma a normal mu coming into a likelihood which is normal um gaussian distributed so i just wanted to visualize that um we can move on in a second but are there any questions about what i've done there because i understand this can be nuanced it does build on intuitively from from the last lesson so okay good so we'll take a couple of more minutes and then come down and sample it so maybe moving on slightly um the only line or the outline i asked you to complete was the sampling line which we do as above um p.m sample 2000 um so i've done this sampling and now i'm going to do the same as above and do my prior predictive check okay now one may want to zoom in just to see exactly what's happening here but and this may not be quite obvious for people who've just been introduced to the ecdf which is totally understandable but it looks like we have some incredibly large values for our for our prior right which doesn't doesn't sit well okay um so we'll come back to that in a second but it looks like the log normal prior um for mu gave us some some really large values which which isn't which isn't cool i mean this is the mean of a bake length right so when we're not going to get something that that large so maybe we want something which is greater than zero but it it falls far more far more quickly okay um to think about the actual posterior predictive check remember it generates data and this comes back to the question we had from our colleague there who asked us how we can actually robustly compare the posterior predictive check and data so this is the actual data the ecdf and this is the posterior predictive check with the data on it okay so remember previous case the data was a single number and it represented the mean and it looked like it was cool now we have a data set which isn't just a single number and we're comparing that with the posterior predictive check and once again there are quantitative ways to think about this but that is not my intention or my goal here my goal here is to give an intuition um so what we've seen in this particular case is that the posterior predictive check looks chilled to me chill is not a technical term um although i mean you know why not um it's generational right so um but the prior is off okay because we're getting these these means that are like wait wait way too large and i'm i'm not into that so um and that's pretty much everything i've written there so our priors aren't cut aren't quite right is what i've written so looking at them um i'm sorry the log normal prior was for our variance and that that was that what shot off and we don't want a variance that that large um so we're going to try a prior that drops off more quickly you may know the term shorter tails so essentially that's what we're looking for um and we'll try an exponential prior um there's a whole family of um prize and distributions which are strictly positive lognormal is one of them exponential is another that has a shorter tail so that's the intuition behind behind that um i don't expect any of you to have been aware of that um but i'm telling you this in order to help build intuition around these around these things um so this was left as an exercise to the reader but in the um spirit of time because we have under 20 minutes left what i want to do is just i'll copy and paste this and so that's from the instructor notebook um so we're using an exponential distribution lam remember um was the variable of interest in the exponent and we'll plot this as well so it's exactly the same as before except now we have um an exponential instead of a lognormal and i'm going to do the same pm.sample and so that will take a second and then the other thing of course i should have mentioned hmm wait no that's fine and then i'm gonna get this stuff to just so we can plot it all when we're done okay so okay so what we see here at least is that we're doing um a lot better right with respect to the the large values the posterior predictive um let me plot the data there as well i'm sorry to be jumping around this no notebook um so much but but it's a notebook right um oh oh no great what okay i've messed this up completely okay so that still looks good um what we see here are our posterior samples for mu and sigma i'm sorry posterior distributions for both our variables um of interest in the sampling distribution um and once again we can plot the trade spots for sigma and mu and they both look like hairy caterpillars and i must say i i'm sorry that we're rushing slightly now and um but i did want to get to some bayesian hypothesis testing to kind of round round this out we won't make it to regression um but i did want to get to hypothesis hypothesis testing so i'd like to take us through this example in the next 10 minutes or so and then give you a few minutes to to um complete the feedback form um because the data's in that you get more completions of feedback forms when it happens in the room um and we don't need an a b test for that okay so remember we had the click-through rate example before now we're going to have our click-through rate for our normal website and our click-through i can't i'm going to change this whole thing to have no click-through rates at some point i think it's really um doing my head in um but it's better than coin flips um [Music] so we're going to have some form of website change or product change and have a click-through rate there as well so one of our click-through rates is 0.15 we're setting them as we did before the other is point two uh we're doing a thousand samples of each um and then calculating the number of successes with each okay and actually let's just see what these are okay so i mean this makes sense that number of successes of b would be higher than a because it was 0.15 and 0.2 um so this this is where things start to get slightly gnarly right because we're building bigger models now we have two different things we're building models for there are no interaction effects so essentially we're building two models um that are very similar or actually analogous to the one we built before right um so we want binomial likelihoods like before and uniform priors on pa and whoa well it's pb i'm not going to edit that now um and but we also want to calculate the posterior of the difference between pa and pb to figure out if there was an actual change in click-through rates we want to know the difference okay so we do so using um pm.deterministic now the reason we do that is i mean look i've written it specifies a deterministic random variable it doesn't even matter what what that means it's a it's an uncertain quantity right um this difference but it's deterministic because it deterministically depends on p a and p p b right there's no extra randomness inserted in um when we're calculating it that's all that's all i mean by deterministic here um i've written that one that it's interesting that i wrote this and then i say the things that you know i've written um but yeah essentially it's one that's completely determined by the values it references okay so once again remember what we did earlier inside our context manage manager we um created our likelihood this is exactly the same as before and we have two likelihoods now right um but we require both of them have two uncertain parameters prop a prop b so we give them both priors yeah so let's execute that um oh of course then we have the pm deterministic right which is diff clicks right calculating the difference between clicks and our posteria our sampling will give us we'll figure out that distribution sample that distribution for us so here we go we have our random variable pa random variable pb that influences the binomial on both and the difference of these is the difference of clicks which is what we want okay so i have not done any um posterior or prior predictive checks here that can be an exercise to to you um that was not my intention to leave it as an exercise but you know what an exercise it will be um while that's computing um are there any questions around what we've what we've just done um absolutely and i didn't i'd to to go into that i'd look at the pi mc docs but i'd also i'd also probably encourage skepticism of such thresholds as well it's a good question where we want humans in the loop with respect to this this this type of thing um i don't think we necessarily want to eyeball things constantly but if something is generating so the prior that we thought was a bit weird was because it was generating potential standard deviations or variances of like 10 to the 10 to the 12. how do you even know to look for that right um whereas when you plot it and see it it's it's evidence straight away um so there are a thousand things you could test for i mean you know you can rank order the most important ones right um but i do think these types of checks particularly when you have humans with domain expertise um to be able to do it that that seems like pretty serious utility there um if that if that makes sense and also i don't mean this in a facetious way i i mean it in look look where the threshold of p is equal to 0.05 got us right um with statistical tests so i think there's a lot more to be said for qualitative in interpretation of of things based on quantitation of course um than using potentially arbitrary thresholds that don't involve domain expertise um okay cool this is really nice right i double clicked on it to expand it um so what we see here is that um the mean for probability of a wow is 0.15 which is actually you know what what it was um probability of b is 0.21 fine um now this is this is interesting right the difference between clicks the mean is well the absolute value of the mean well it's minus 0.06 right what what is even more important here i think is that this range that you can tell your product manager or head of growth or whoever it may be who's interested in this that it's nowhere near zero i mean the the upper bound is minus point zero three and depending what type of um changing click-through rate they're looking for that could be super relevant but what's what's key here is that we actually have the entire distribution for the diff the difference between click-through rates which allows you to encapsulate all the knowledge you have about it in order to talk to once again your domain experts who need to make decisions uh around this around this type of thing um i'm gonna plot the trace plot and once again we see some pretty some hairy caterpillars which um once again and of course i mean to to your question as well of of course you can uh quantify uh drift and diffusion so see whether this is actually too much drift or not enough but the real question is do you throw something out because it exhibits a bit of drift or not enough or and what these thresholds actually mean for your scientific workflow um and i would say they're important but what's more important is validating your underlying assumptions and making sure that you get you know the data generating processes as is as robust as possible um so what i i'd like to wrap up now um what i'd like to say is that there's a hands-on for hypothesis testing with the beak length between different species i'd encourage you to to go through this and the solution is in the instructor notebook of course and then there's an example on bayesian regression which is think about regression is slightly different because we kind of have two variables but we're really talking about the sampling distribution of dependent variable um so going through that i think will give a bit more uh in intuition um but yeah to kind of wrap up what we've done in the in this kind of second part of the workshop we've introduced um the fundamentals of a of the bayesian workflow um and looked at relatively common scientific questions such as parameter estimation first um and hypothesis testing and i really hope you've got a feel not just for pmc but for kind of the power of beige bays and the the power of these probabilistic programming languages which can take and they can get funky and take a while to get get used to but once you kind of get in the in in the zen of thinking about probability distributions as first-class citizens i think there's a certain um huge amount of utility that that comes with that um so i would love to answer any more questions i would i'm going to post this link in just a sec yep i'm going to post this link in the slack um oh this is great thank you all for putting stuff in the chat as well oh yeah and also i do agree a huge plus one for mamba over conda i've had issues with mamba and tensorflow recently um well i seem to have just copied the term feed whoa what it really is it's time for some kaizo um yeah so if if you're up for providing feedback that'll be that'd be super cool um and while you all are doing that i'd love to answer any any questions about anything we've gone through today what's up oh that's a that's a good question i've never used it with extremely large large data um but i actually think eric might might know so i'm going to ping him in slack now is he still around you know or might be a bit late and if if he doesn't answer i'll i can i can figure it out by tomorrow what's up yeah yeah i actually don't have a have a strong sense of that unfortunately but we'll we'll figure it out and just um i've just asked the question in the channel if you want to tag yourself in it as well or you can just come back and for sure [Music] yeah i've used it for a bunch of stuff most recently on not incredibly large large data sets but genomic data in terms of um yeah yeah exactly yeah a lot of parameter estimation and comparison between groups yeah but it's used a lot in in finance these days all types of stuff yep confidence intervals incredible approach which gives you a the hdi definitely is used a lot more invasion inference um the way i would frame it is that it gives you i don't know if this is the language people would use but it it doesn't tell you about the credibility of your model but it tells you about the credibility of the range for the parameter of interest right because it is about a particular parameter not necessarily the model yeah exactly yep absolutely [Music] and the farm any other questions pop up definitely post them on slack also if after the conference the stuff in the repo you're interested in um please raise issues on the github repository as well it's working for did did anyone else have that issue oh yeah okay i'm sorry i'm i'm going to create a new form and post it again tomorrow for anyone who wasn't able to i think um eric created the form and perhaps um so many people responded that you know it it closed um which is great but i'll post another one tomorrow oh yeah i don't think so though because we've actually been using it for years so thanks everyone
12,Developing Extensions for JupyterLab,https://www.youtube.com/watch?v=9_-siU-_XoI,welcome to the tutorial for developing extensions for Jupiter lab I'm Martha cryan um I work for IBM contributing to Jupiter lab and some extensions for Jupiter lab called elyra I'm based here in Austin and I'm going to be talking about some context for um what we're learning about today uh talking about what Jupiter lab is what some extension points are and then giving some examples of what popular extensions are and different ways that they've interacted with Jupiter lab as extensions [Music] um and then after that Alex here is gonna be um talking about how to write a very simple extension and walking you through that and everyone should get it running on their own machine and then piyush is going to go through uh how to set up debugging in vs code so that you can debug your code really easily and at the end we're gonna have a section that's more um time for everyone to just write their own extension and so if you came with your own idea for an extension that's great and we'll help you with it uh but also as you're watching if you think of any ideas that's great and just keep that keep that in mind but we'll also have a list of suggested extensions and in the back is Luciano he's also going to be helping if you guys have any issues with like setting up your environment um in Alex's section um yeah and then uh one piece of vocabulary before I get into like showing you Jupiter lab is uh I'm going to be talking about core extensions versus like third party or just regular extensions and uh what I'm talking about is just that Jupiter lab itself is kind of a set of extensions and I know that sounds abstract but basically um things like the notebook editor that's its own extension that is built into Jupiter lab when you install it or like the file browser or the code console those are all their own extensions but they're called core extensions because you don't as a user install them as an extension um and you'll see kind of what I mean more as I show you Jupiter Labs interface so this is what you see if you haven't used Jupiter lab before um when you open it up um I'm also keeping in mind that you want to know some terminology that is more useful as an extension author so um some of this might seem obvious but it's kind of just to say what we call it this is the main area and you would add something called a main area widget into it so that's what I mean um and then there's the side panels where you can have the file browser and there's a panel on the left and right and then there's a status bar at the bottom and a set of main menus at the top that are drop downs with commands that are context specific within Jupiter lab um yeah and so in this main area you can see this is called the launcher and generally what that's going to be doing is launching something into the main area uh so it could be a file editor like this Jupiter notebook file editor but it can also be a code console or a terminal and these are all core extensions and then we also have a text file marked on file in Python file as built-in launcher tiles um but they're all using the same code editor and I bring that up because as an extension author you can use that code editor as well and then in the side panel we have a file browser there's a way of tracking open tabs kernels and terminals a table of contents for notebooks or any file type and uh an extension Explorer which shows you your installed extensions and helps you install new extensions um yeah so that's kind of what I wanted to show you about just core Jupiter lab and now I'm going to get into the extension examples so the first extension that I wanted to start with was draw IO and you might have seen that actually it's it's its own thing but they made a Jupiter lab extension and it's just a simple way of making a diagram we have um so it opens this main men this main area widget with a file editor and a DOT Dio file so this is a good example of an extension making a file type accessible to Jupiter lab users and the other thing I wanted to show about this extension is that it's adding a main menu up to the top so you can add as an extension author more options into this main menu that are specific to your extension like this is allowing you to change the format of the editor or just do different kinds of edits the next extension that I wanted to show is lot Tech and I'm sure everyone sees logic here but um if you haven't it's just a way a language that helps you write um mathematical and scientific notation um but the interesting thing about this is it's also a file editor but when you open it this is actually the built-in code editor for Jupiter lab um and I'm I'm going to open a sample latex file but what the extension is doing is actually adding a context menu option which opens the rendered PDF of your latex in the same view and yeah again if you've used that Tech you know how useful it is to see the rendered lattec side by side um but the other thing that I wanted to show with this extension is actually getting at that there are different kinds of extensions uh there are extensions that are called lab extensions which kind of more so interact with the front end and rely on apis that already exist for the back end but this one needed to run the law Tech to PDF converter on the back end so they had to add an API to the server using a server extension so there's lab extensions and server extensions and this is an example of an extension that uses both so the the server the lab extension is adding this button into the context menu and opening the PDF another example of that is the git extension and um so they they wanted to have this built-in UI for different either like looking at get information or committing your current git changes um but they had to add an API to the back end so that they could um actually access this information from the server so that's a different angle of how you might use a lab extension and a server extension together uh the next extension example that I wanted to show is called spell checker for markdown and so if you make a markdown cell in a notebook and you start typing it will automatically highlight a word that it doesn't recognize and then it'll add this context menu with different suggestions of similar words and then you can click on it and it'll automatically update and I just wanted to show this to show that you can make an extension that doesn't add necessarily this big renderer like the git or the draw.io extensions but it is still using commands in a way that's really useful for users and then finally I wanted to go over the third type of extension from Lab extension server extensions is the theme extensions and I'm sure everyone's used theme extensions there's built-in light and dark mode and Jupiter lab but you can customize those colors the fonts to your liking and this is an example of like an alternate dark Mode called darkula that has been um popular with Jupiter lab users so yeah those are the extension examples that I wanted to show and then one last thing um this is about publishing extensions so and also kind of as a user um how you would interact with extensions has changed because um So currently we're on Jupiter Lab 3 we're about to release Jupiter lab for this year but starting with Jupiter Lab 3 we changed the way that extensions publish their extensions um we called it the earlier version source and you had to install them using npm and you had to rebuild Jupiter lab every time you installed an extension to be able to use it and we've shifted to something called pre-built which you can install using pip which most people using Jupiter lab will install Jupiter lab using pip and you don't have to rebuild when you install the extension so that's what we're recommending for new extensions to be pre-built and the difference as an extension author is mostly just in configuration like code mostly stays the same but that's just something to keep in mind if you're looking at the different ways to configure your extension we're gonna build it in a pre-built extension configuration by default so yeah that's just a note and uh with that I'm going to pass it off to Alex to go over the how to actually write an extension I I am shorter so that needs to go down now everybody hear me awesome so thank you Martha for kind of giving us the walk through the different types so I'm actually going to be walking through uh we're calling this Anatomy extension the idea is I'm actually going to walk you guys through some example code show you the different pieces and chunks of what makes an extension in code and the end goal of this is for you guys to get your fingers into like actual writing of code so after I kind of walk through this we're actually going to give you guys a like a 20 30 minute exercise where you guys are going to take the code that I'm showing you and you're going to edit it and actually try to extend it and do a little bit more so while I'm talking if anybody has not already get cloned uh the git repo that we had listed on the Sci-Fi website it's also listed here uh not there which one is it I didn't actually have it in the docks uh it's under Martha's uh I can share oh I remember I shared it on slack so anybody who's on the slack channel for this tutorial I shared the link to it there uh you can find that link it's under Martha's uh user and that has all the slides and examples and stuff that we're using for this today including the example code that I'm actually going to walk through so again uh this example code is built using the cookie cutter the cookie cutter is a Jupiter Lab Cookie Cutter anybody ever used cookie cutter before the tool cookie cutter okay a few of you so cookie cutter is pretty cool um you just install pip install cookie cutter and then you just say cookie cutter and you give it like a git repo and it clones that git repo and then creates an instance of the cookie cutter for you in this case it gives you the bare minimum Bare Bones extension uh in order to actually start so you don't have to figure out oh what are all my imports what are all my build files like uh what's my index you don't have to deal with all those nitty-gritties it gives you a starting point so when we start coding today we're going to be doing that cookie cutter step uh also this example I'm going to walk through is Loosely based on the example in the Jupiter lab docs so anybody who's actually looked at the Jupiter lab docs before uh and looked at the example tutorial in there it's a very very long detailed dock uh it's very frustrating and difficult to read sometimes because it is so detailed uh so we're going to be basing that off of this um this off of that so oh Martha pasted that into the uh example there so Let's uh dig into the code so I'm going to come over here to vs code I personally don't use vs code every all the time uh zoom in yep what's the quick code for that again command plus there we go there we go uh is that big enough what do you guys want to pick that look good to everybody other people who need glasses like myself a little bit more yeah I'm gonna minimize the file browser here a little bit there we go okay so if you come look at this file browser uh I've already opened up where I cookie cuttered so you can see all of these codes all of this this was Auto populated by the cookie cutter that's how much the cookie cutter does for you it's very useful it gives you all of these build files it sets up linting it sets up tests recently committed or opened up some code that will also set up uh being able to uh test yeah um so yeah so I'm going to walk through what I'm in right now is the index.ts file I you can write in pure JavaScript but most people choose to write in typescript when creating extensions anybody in here never touch JavaScript or typescript before okay so we have a couple people who've never touched uh JavaScript nowadays is not the JavaScript of the 90s it is uh practically a full programming language on its own not just some script thing you type in your in your browser anymore um so uh the front end of Jupiter lab is written in JavaScript and typescript uh typescript is a typed JavaScript um and when you build it it just compiles down into pure JavaScript that code can look really ugly though so sometimes if you look at the code in your browser it's not always pretty so yeah I'm just going to start walking through the anatomy so in the index file this is where we kind of start um an extension consists of lots of different parts in this case we start with the plugin the plug-in is what is actually uh is the is the bulk of the extension it's the core so the idea is you write a plug-in in your index file and at the bottom of the file you export it as the default so that's but uh this says hey this plug-in is going to initialize and start up my extension fun fact you can have multiple plugins in your index file and you can export them all so if you want to create a couple different extensions and just Jam pack them all into one like install you can actually do that you don't have to have completely separate repos for every single extension so uh popping back up to what is this plugin if you give it an ID so you can identify it is auto start true because you want jupyter lab to use it if it's not true then it won't auto start and then what's cool about these optionals These are various Registries or other extensions that are that uh usually come from core so your extension can export or return like a registry or some sort of uh interface that then can be leveraged and used so if you want to use those interfaces from either core or other extensions they come here in this case we're using the settings registry which allows you to add more settings to the Jupiter lab settings and we're going to use the launcher which allows us to add new items to the launcher and then this is the key the activate function activate is the function that is actually called when Jupiter lab starts and it initializes your extension when it's installed so I uh the cookie cutter adds this console log I always like leaving this console log because then when you go into your console on your browser you can immediately know if it's successfully installed your extension or if you have a problem so yeah uh starting off this is the settings registry I'm not actually doing anything with the settings registry in this example but I have it here and all set up uh in order for you guys to extend it further later if you choose to in this case what it's doing is it's going to be taking this schema over here called plugin.json in the schema directory and it's just adding these settings to Jupiter lab none of these are super important it's title description type there's no additional properties so it's essentially adding nothing but if you wanted to add new settings or customizable settings you can add them in this Json and then this will actually add them to the settings registry I'm not going to go into much detail on that this is more of a explaining what this is uh but when we get to the end and you guys are wanting to like extend it and try yourself this is a useful base tool for you this is the midi this is the grid of this so Jupiter will have this Jupiter lab has this concept of commands so you can create commands you give the command a name you give it a function to execute when the command is called you can give it a label and an icon and commands can be used in many different ways you can just straight up add a command to the launcher in which case it will use the icon and the label and it will put that icon and label on the launcher and when you click the launcher button it runs the command you can add the command in a drop down in the menu like file open that runs the file open command commands can be added anywhere you want commands can be run from other commands so the command registry which is right here is one of the core tenets of extensions so typically when you're writing extensions you typically start with writing a command and then you surface that command to the user somewhere in the code in this case we're writing a command called tutorial open we're opening our tutorial widget so in the execute we're creating a tutorial widget we're adding it to a main area widget which is important because that's what allows you to then plug it into the main area it gives it the all the main area widget functionality like toolbars opening close uh signals all the Nitty Gritty stuff that you don't want to have to deal with when you're writing your own extension then we give it a label and an icon and a caption is the like Mouse over text um and then we add it to the Shell the shell in this case is not some like terminal shell this is literally the shell of the web application so you're adding it into Jupiter lab so when this command is run it's going to open up a main area widget containing our tutorial widget and then down here you can see I add this command to the launcher which then puts it in the launcher just so you guys can see how this works I'm going to quickly just start up Jupiter lab you guys can see that there's now an open tutorial widget here with the icon that I added and when I click it it runs the command which opens up the tutorial widget in a main area widget so uh anybody have any questions on that concept of commands so far no okay so we're gonna pop back over to the code so this is all done in the activate function technically I could have just made a new widget like completely empty doesn't really do anything but that wouldn't have necessarily been any useful to you guys trying to figure out how widgets work so I created this new tutorial widget which can which I put in the widget.ts file put it in a completely separate file technically everything that's in here could actually be defined within that activate function but that would have just been putting all your eggs in one basket would make the code very messy so in this case I defined a new tutorial widget and it extends widget widget is just the bare bones everything you need to know in a widget like uh saving State uh the uh the node that it is in the Dom in your web browser so in this case in the Constructor I'm creating an element an image element so I'm just creating a Dom element in HTML that's an image and then I'm setting the source on that to this picture I'm using this site called pixum it's like lower mipsum but they give you pictures pretty straightforward the example in the uh documentation for Jupiter lab gets is like super long two pages long on how you can pull NASA to get cool space images and it's like 100 lines of code just to get an image I thought Lauren mipsum was faster and then you'd say this dot node so this in this case is this parent widget and node is actually just we're going to follow through using the browser it literally is the Dom node owned by the widget so when you create a widget it just happens to have an HTML element if you never add this to the Shell it's just going to be floating in nothing that's why adding it to the Shell in that activate function is so important so I'm just going to close that real quick and so I'm appending that child the image so now I've put this image in the in the thing so that's what puts this image into the main area it's just HTML you can do a lot of really cool things in there other than that but I wanted to make it simple so coming back over here you'll notice I have this cop this commented out right now it says this dot load image this is because I hard-coded image source here we've also wrote this helper function called load image which leverages a server extension which creates an API that I'm calling in this case the api's image this would be localhost slash I think it's just this slash image if I remember correctly might have still been lab slash image I got it wrong I'll remember that API eventually um but the idea is we've created an API that you can call and it's a server extension I'll walk into the server extension in a second but this is making API requests to that getting that image and setting the image source to that to that new image so this is a pretty cool function we're not using it in this place right here this is actually going to be part of the example that exercise you guys are going to walk through is leveraging this function which refreshes the image yes there we go you'll notice here I have this little button here called it looks like a little refresh button in the tab can everybody see that it's up here when I click it it refreshes it this is going to be your guys's exercise in a few minutes um to add this button it's one of the reasons I didn't rebuild before I started demoing because this code is not actually in the IDE right now I built this and ran this yesterday and so the example is already written and committed uh so yeah so I'm gonna have you guys add this button which then calls that function but first a quick walk through of what a server extension looks like so you'll see in here that we have this Source directory which has all these typescript files similarly we have the tutorial extension directory tutorial extension is the name of this extension and in here is all the python files so by default like the init inversion those are all going to be added because you need those python files in order to create a pre-built extension but in addition to that we added handlers this is a server extension so Jupiter lab uses Jupiter server Jupiter server itself is the the back end it does all the apis all the back end work for those that know you can't do everything in your client's browser in this case we are adding a new extension point so we set up the handlers this is all filled in by the cookie cutter again and but what we did we changed this route pattern and we change this here image oh this is why because I needed a tutorial extension then image and what it is is if you call this so I'm just going to come over here if I go to localhost slash tutorial extension slash image it Returns the image URL so you can see it's just the URL and a Json return this is an API that anybody who has access to your jupyter lab instance can access and so what we did is we wrote a normal get function that just returns a random image from this list these are just hard-coded specific images from that lorem pixum so again this is just a simple example of a server extension we just added an API to the Jupiter lab server you can do lots of stuff on the server it's just a server extension if anybody's ever used tornado before that's the uh tornado server is the server that Jupiter server uses so anything you can use in a tornado application you can use in Jupiter server Jupiter server can be run on its own if you want it to uh but it is the back end for Jupiter lab yeah so that's the server again we just tweaked the cookie cutter a little bit by changing this this used to be test because there's a cookie cutter and then we just changed the return to be this image URL yeah uh I believe yes uh Luciano did you pick up his question so is there a way to interact with like what is our user exception or just something that's specific to a user session or their state of the server yeah for those that couldn't quite hear Luciano and for the recording uh a quick summary of what he said uh if you're talking specifically about kernels you can access kernel information and kernel sessions through the API and there are many examples of that online uh Luciano pitched uh our team's example of elyra Lyra AI which uh who has a python editor which leverages those apis uh yeah so popping back over to the index here so you can see right here there's a nice to do add a button to refresh the image so um has everybody managed to get clone this repo so it's called This is the repo that we've linked should look like this I'll refresh it probably have new things pushed there we go has anybody had trouble get cloning this or not like get cloned it yet okay so if anybody's having trouble get cloning this uh you can have some of us come around and help but the idea is in this repo in examples there is tutorial extension you will want to open up either the whole git repo or specifically this folder in your IDE I've been doing everything in vs code because that is the open source uh standard most people use it I personally use pycharm in my day-to-day but that's because as a Apache committer I have access to free pycharm not everybody can afford the pie charm uh uh price so I would recommend opening up this extension example in your IDE a we're going to be editing right here where this to-do is in a second I will teach you guys how to build but coming back over to our setup so this is going to be our code exercise so as I said we're going to be opening up that example in your IDE and the goal is to add a toolbar button that refreshes the image using these are the hints I gave you guys some hints so you guys know where to start so the toolbar can be accessed by accessing the toolbar object inside the main area widget a toolbar button is a class that creates a toolbar button that you can then add to the toolbar and that class can be found in Jupiter lab app utils if you want to find if you want to know the answer the answer is on the next slide after this which is available in the readme through a link and you could cheat if you really really want to but I suggest you guys take the next 20 to 30 minutes to try to add this button just to show you guys again this is what it does look like when you open it it has this button on this toolbar when you click it it changes the image again all of the code that you would need to do this can go right in here in this to do I'll make a note that when you want to import for instance the toolbar button so when I say there's a toolbar button class you can just add toolbar button right here like that and then you can create a new toolbar button so that's in app utils as an example and so the idea is here in this to do you want to create a toolbar button and then you want to put that toolbar button into the toolbar using the hint that I'm going to leave on the screen now so that's going to be our 20 to 30 minute exercise it is 205 I say I will kind of reconvene at 2 30 to see how everybody's doing I'm going to check our read me just a sec we have our kind of our agenda here in this repo as well if you ever look at the readme we are going to be following this with like a 10 minute break so how about uh we will reconvene at 2 45 so that will be including the break so you can take your break early if you really want to just blow off the exercise but why would you be here um but yeah that is going to be the break so the four of us uh will be available to walk around you just need to raise your hand and we'll be able to come answer questions they don't necessarily have to be around this exercise but um adding a button to the toolbar used to be the whole exercise like when I did this tutorial two years ago uh adding a button to the uh to the uh which one the notebook widget was actually the whole thing that was that was the end goal but most people don't need to add a button to a notebook anymore they want something more which is why we're making this bigger so yeah uh you guys have time just feel free to raise your hands if you have questions um already got caught I forgot to teach you guys how to build the extension so you can even run it uh this is actually all in the readme so if you did the cookie cutter in the tutorial all the steps are in the readme but I'm just going to open up this readme real quick and show you guys it so here in it here is the readme file and what's really cool about the vs code is you can do this and it opens up the markdown so when you publish it you would just do this pip install but we're doing Dev so you actually need to do these steps down here in this code block so you'd want to do a pip install Dash e dot so that's going to install all of your dependencies which includes Jupiter lab server all the Jupiter lab build utils if I remember correctly you probably will actually have to pip install Jupiter lab itself before you do any of this I don't know when it happened but uh the last time I checked out the cookie cutter it did not install Jupiter lab anymore it used to install Jupiter lab with the cookie cutter but it doesn't so yeah I want to pip install Jupiter lab then inside of this directory you pip install Dash e Dot this actually installs your uh dependencies then you do Jupiter lab Jupiter lab extension develop Dot this will actually Sim link your directory that you're currently in the build files into Jupiter lab this is one of the keys with pre-built extensions that you're developing because normally pre-built extensions just put all the built files into a Jupiter lab directory and say it's there well when you're building those pre-built files that don't exist so what we do is we Sim link our build directory into there that's what this jupyter lab extension develop command does and then the third step is Jupiter's server extension enable uh this simply says oh there is a server extension turn it on the server extension was already installed in the first two steps and then the very last step is jlpm build jlpm is just Jupiter Labs like wrapper around yarn um if anybody's used yarn or npm before uh what this is going to do is this is going to actually build out the files into that build directory that we've already simulinked so if you do these four steps that will install your extension and then you can just run Jupiter lab uh yeah and then if you ever need to rebuild you can just run this again and then and it will immediately update in jupyter lab with just a refresh of your browser so um we did mention in the prereqs for this course that we do suggest doing all of this in a Content environment uh for those that uh have used conda or python environment if you try to do a lot of these steps in what we would refer to as a dirty environment you can run into a lot of build and install issues if with like conflicting versions conflicting dependencies and weird such so if you know anything about python environments I would suggest creating a fresh new clean environment if you do not you don't have to worry we can always help you or you can try to do it without the python environment and we can debug with you if it ever causes problems okay now you guys are ready to go um given I took another five minutes of you we'll go till 10 till now hey everyone uh my name is piyush Jan um I work in AWS and I make open source contributions to Jupiter lab um I'm gonna go over um how you would uh debug Jupiter lab extensions um so this is the agenda that will go over um I'm gonna talk about a few use cases when debugging is useful and then we'll actually walk through how to set up um debugging for Jupiter lab within vs code I specifically picked vs code because a lot of different encoders are actually using to vs code for their front-end development and vs code has a really rich interface to actually debug both the front end and the back end code um um after setting up we'll actually walk through how do you launch Jupiter lab how do you launch a debugging session for Jupiter lab within a vs code so I'll show that and then how um you would set different breakpoints I'll show how do you do that both in the front-end extension as well as the server extension um and if you are not using vs code and you want to do debugging in the command line I'll show some other ways to set up that um you know so that you can if somebody's not using vs code you can you can do that from the command line as well um and it's a it's a very Hands-On um part of the tutorial so I'll actually walk through how to set up so there's another cookie cutter as a part of this repository um and and we'll be using that to set up debugging so when is debugging really useful right um so debugging might be useful if you have actual errors in the code and you might want to go through the code and see what's happening actually inside the code so a common issue that you might see when you're working with extensions is that um you you your the UI that you're trying to create is missing or you have the content which um you did not expect to see um and so to investigate those issues um having a debugging setup within the code is very useful um it's also very useful if you want to see how code Jupiter lab extensions work like if you are trying to um see how the toolbar is used within Jupiter lab you can actually debug the code for the Jupiter lab code repository and and see how it's set up how the code path is working it's also very useful if you are just trying to set up a new extension that you're working with and and don't know how the you know how the extension is working you can actually go into um the debugging session and and look at and inspect different objects and variables to see what's going on um yeah so these um you know these are the common scenarios when uh you know uh something goes wrong in a Jupiter lab extension most common of them is you know you'll see that the UI elements are missing so that means that either there's something that failed on the server side to load which you are dependent on or there was an actual um JavaScript error so that UI actually failed to load um so there are um two places that you can actually look for these errors the first thing you can do is look at the actual Jupiter server log and I'll show that when when I set up the debugging session and I'll go through that um so Jupiter server log is um the log when you launch Jupiter lab in the console um there is a log that prints out all the server-side failures on that log and you can actually look through that log to see if there is any server side problems um another thing that you can do is you can actually look for any JavaScript errors inside the browser console so these are usually linked with any JavaScript errors in the code or any UI related problems in your extension so browser console is really helpful for that uh so the next part of the um the presentation is going to be focused on actually setting up um the uh debugging uh debugging within the code that you just worked on um within vs code and um there's another um file that is there in in the repo that you see it's it should be in examples uh debugging dot MD and if you open that it has step-by-step instructions on how to set up you know the debugging session within the vs code so you have you don't have to do change any directories the the place that you were working with before um for Alex's uh part you can start from there and and you can you can work through those instructions to um to set this up uh but I'm actually going to walk through that um here and and show you how to do that uh so let me share that so I already have um the extension and the exercise part of the um the code setup here so so let me just um show you that that works yeah so it it already has this tutorial widget and um it has this refresh button also that that refreshes the image um so I'm going to close that I'm going to go back to the terminal and stop the server and what I'm going to do next is I'm going to actually so there's another cookie cutter that is uh part of this repository which actually sets up all the configuration for the debugging session so I'm gonna say uh let me just make sure I have cookie cutter installed so if you don't have quicker just make sure that you have cookie cutter installed so I already have that installed here and the next thing I'm going to run is Cookie Cutter let me just make it bigger and the bug configure so um this directory should be in the Parent Directory of where you are running the tutorial extension so once I run that it's going to give me some prompts and you can basically um just select all the defaults you can just click enter on all of these and and and run through so I'm just going to do that Okay so it what this does is um it adds um some configuration within your vs code and your total extension directory um and I'll go through that so you see this new directory called dot vs code um vs code uses this to set up all the settings launch configurations which are used for running and debugging so that's what you see here and if I open launch file here you'll see that there are two Targets here two configurations the first one here actually launches Jupiter server or Jupiter lab but it doesn't open the browser on its own um the second one is what opens the browser and it'll basically attach session with the vs code so that you can debug interactively within vs code okay it also adds this webpap config file which is needed because um this extension is written in typescript and if you if you build it without adding Source map so Source map is a way of um adding references to typescript files the source files because when this extension compiles and runs in the browser it's running as JavaScript files and the browser doesn't know how to go back to the typescript source file so that's that's what the source Maps do and some of this configuration basically enables that [Music] um and so once you have once you run the cookie cutter um you can come back to this run and debug menu this on the left here I'm going to click that and you'll see that it has this target called jlab debug and if I if I click on this right now it's actually going to run the Jupiter lab and it's going to launch a new interpreter lab session here and so this should refresh in a minute once the server starts so there's um sometimes um the the session that vs code starts it it fails to refresh the Jupiter lab session so what you can do is so what happened here you can just click on refresh and then that should load Jupiter lab it it happens occasionally I've seen it happening I'm not sure why so we have the same extension um we can open that it loads an image I can refresh it from here but what this provides is it also gives you this debugger interface so you have all these commands here so this is used to step into or step over code and then you have all the call stack here you can add new variables into the watch list to see the state of different variables and objects you can do that from here and I'll I'll show you um actually I'll add a breakpoint inside the extension and show you how that is working uh let me do that so I'm going to go to index.ts here and I'm going to put a breakpoint here so I'm just going to click on this line here and it's going to put this big Point here um and now if I go back to this session and and refresh um the controls should go back to the my vs code editor now a second oh actually I I failed to do one thing um so remember how I was telling you that for in order for the browser to know where the source files are it needs the source Maps um so after we added the computer we we didn't build the project again so we need to build it so that the source maps are referenced correctly so I'm gonna I'm gonna stop this and go back to the command line and I'm going to build again I'm going to say glb and build okay um so there's another missing step here um I need to run glpm install because this cookie cutter adds some new dependencies so we need to run that first okay so once that's done I'm gonna build so all these instructions should be in the debugging um tutorial file that I was talking about um so once this step finishes um it should it should correctly link The Source Maps into the generated source code so now if I launch from here again so I'm still in the run and debug menu here and I'm going to click on this and let's see if it refreshes this time I did refresh it okay okay so now you see how it came back inside vs code automatically because I had a breakpoint and you can see I can inspect any object here so I'm for example I'm looking at app I can actually look at all the commands that are there so I can say app dot commands here and I can look at all the commands that are registered here um and so this is useful this breakpoint that I have this is part of the front-end extension um so this is for all the UI code the JavaScript code that you have or typescript code so it can help with that um similarly we can also add a breakpoint on the the server extension side on so let me just do that so let me go back to here and let's go in the handlers file and I'm going to put a breakpoint here and I actually don't need to reload this because once I hit this and click on refresh it should go back here so this is um gone back to the the python Handler so this is the actual API that Returns the image URL so you can inspect things here as well so you can see that this is the image URL it's going to return um and so this is useful for actually looking at the python code that works on the server extension side I'm gonna let it go um so at this point um if you have been falling through and you have the the cookie cutter set up done you should be ready to debug both the front-end and server side extension um in in your vs code um instance um um I can show some examples of common errors that can happen and you know how to go about debugging them because some of these errors are not obvious so I'm going to go through that a little bit so what I'm going to do here is I'm going to stop this session and so stopping so you have to click this twice because it has two Targets one is actually launches the Jupiter server and the other ones actually launched the the the Chrome browser so I'm going to just stop both of them here and I'm gonna just say remove this part here the this import statement and so what would happen is that um this API will will will fail to load when when I refresh the image from the front-end extension and we'll see um an error on the UI side because there will not be any image to load on that end and we'll also see an error in the server log here so I'll just show that uh so I'm going to run it again here let me remove the breakpoint k all right so this loaded fine because this is on the UI side but if I try to refresh this part um you can see that it's not actually changing the image and if you go back here this was the server log I was referring to earlier so this is um the this will have all the errors from the server side extension and you can see that it's talking about that this this module is missing because we never imported that so it's it's basically throwing that exception so so you can the first thing you can do when you see that something is not working on the server extension is you can come to the server log and see if there are any errors um so I'm going to put that back let me go back here Handler start pi and I'm going to say import random okay um there are some other types of errors that can happen that doesn't actually throw a lot of errors on the on the server log because the extension the server extension itself fails to load um and those happens um I I have seen that happening when you are refactoring code and you failed um either to do the right import or the import that doesn't exist so for example if I try to do something like import test this module doesn't exist so what this will do is basically it'll fail this whole file to load and so the extension the server side extension will never load so if I actually start the Jupiter session now and I actually want you to look at here so it's saying here this line extension failed loading with message none type right so these types of error are hard to see because once you start working with Jupiter lab after after you know you have actually loaded basically you are at the bottom of the log here and so you usually don't know that this extension didn't load and so there's a better way to see if you have actually encoding error in server extension and I'll show that how to do that um but I want to just show here that if we start to refresh oh how is that working I I'm not sure how this should be working but it it should be failing this module but let me show you how to see how to actually see um these errors let me know so what you can do is you can just say Jupiter server extension list and if you have an error on any of these extensions like for example the tutorial extension um it will actually show you the stack trace for that extension whatever the error is because um because of the extension is not loaded it's not going to actually throw the error in the server log when you are actually working with the extension so it's it's useful in this case when the actual extension is not loaded um so there are some other ways uh to debug Jupiter lab extensions um if you're not working with vs code and if you want to work in the command line you can you can so for front-end extensions you can directly debug within the browser so for example I can um actually go here and if I just start my Jupiter lab session from here and I'm going to go into sources tab and look at page and all the tutorial extension code should be here now this will only work um and it will correctly link the typescript files if you have the correctly generated Source Maps otherwise it will show JavaScript files which might be um minimize sometimes and so you're not able to actually look at the code but you know since this has the correct Source maps you can you can actually link the typescript files so I can for example go here and I can I can put a breakpoint here itself if I reload that it stops there so this is another way to do that if you don't want to do it from BS code um I'm gonna stop that and run it ah so this is for the front-end extensions um if you want to debug something for a python extension um there are two choices um One is using the the python command line debugger um so you can add this statement import pdb and then pdb set Trace in the code where you want to put the breakpoint and it'll actually stop at that line um but a better option is to use the IPython debugger so this has a more interactive and it has colored output so you can use iPath and debugger as well it has the same interface as pdb the only difference is you you will use basically you will import ipdb and and set trace on that I can actually show this um for the python debugger how this looks like and so I'm gonna go here in my python code and I'm gonna put a breakpoint here so I'm going to say import pdb PDP dot set Trace and now if I run Jupiter lab close that right and I'm going to open that and I'm gonna try to refresh so now you can see that it actually um stopped the the debugger here so I can list my code here so it shows me that this is the next line that's gonna I can print out different variables it's showing me that this is the image URL it's gonna send back um so this is useful if you have um a code that's not part of your um server extension and something that you cannot access within vs code you can you can do this um so I'm going to get out of this and close my extension um let's try it with ipdb so ipdb you have to install that so I'm gonna install that and very similar to pdb I'm just going to add bdb and iptb Dot set Trace okay and I'm gonna run Jupiter lab once again okay open that refresh okay so this is a little bit better because you have colored output um you can look at more code it has the same interface as pdb um but it is just a little nicer than the default python debugger I can look at image URL close that um so um does did anyone so far set up the debugging part and were they successful I I see only one nod to um uh so if you if you use the cookie cutter it should already add the configuration and once that is added then you will see once you click on this run and debug on the the left you will have a g lag debug Target there if it if you have the launch configuration added to vs code and I can I can look through that um if anything is missing anybody else had any success with this I can I can walk you through if you are having any trouble setting this up I I never had success with that I I don't know if there is anything needed to set that I I'm not I'm not sure but but that's true yeah no I know it it I have seen people using it but when I tried it um I have Python 3 8 here it it doesn't stop there so maybe there's a flag needed to on it so I think at this point I can I can help you set up debugging if you want or if anybody was not successful doing the previous exercise I can help with that but this is the end of the debugging part of the tutorial um and if you want to work on any of the extensions we can help with that also um is there anything else you want to add Alex or Martha cutter extension um so if anybody's interested in just not starting from our little example that's starting from scratch I can walk through the steps to use uh but there should be a link in the slides I think earlier in the slides to the actual cookie cutter into one of my slots yeah right there so that first link there if you go to that that's the cookie cutter we use it actually has instructions in its own reading on how to use it and you just follow those instructions and it will create a cookie cutter extension that you can then start filling in the example extension is just an edited version of that but we've added a lot of extra dependency Imports and uh so if you did the two there's actually lots of little tapering that I did to make it simple for you guys to run uh without having to worry about those little nitty gritty gritty class so yeah if you came here with an idea for an extension and want to start with that cookie cutter and start Tinker with yourself we are available if you just want to try to keep adding that button just so you get a feel of it that's also a good too I'm going to leave this open so if you want to refer that yeah over the detail and way more precise than the example I did but it is where I got the inspiration of the idea of putting an image in an extension in a widget and making that and putting that in the main area it also has way more details about server extensions if you remember my server extension detail it was like this is an API done they actually go into like doing in-depth API calls to other services making a more detailed server essentially and a server extension is where peuge is debugging really comes in handy yeah debugging on the on the front end it's really simple you just go into your password and Jupiter lab has an examples repository and um I will link that later on on the on the repository that has a bunch of examples how to do different things in Jupiter lab for example if you want to add something to the menu how you do that if you wanna remember the state when you refresh something that's also there so there are lots of examples that you can follow um in learning how to do different things in Jupiter lab so I'll add that later um onto the slide as well yeah definitely all right there's a list of extension ideas also in the slideshow oh yeah so these are all the extension ideas that people have suggested so if there is anything here that jumps out at you you want to try you can try these though these are specifically open issues against Jupiter where after somebody opens it she's like oh I want to add this the developers said that's a great idea that's a good third-party extension and so throughout the extension pack on it natural filter the slides are very useful we have lots of 20 slides you can find them uh you can find PDF of them inside the repo we had to clone or in the readme uh there's a link to a Google .com yeah you can open PRS on this repo you can open PRS on the cookie cutter itself you can open issues with notes the slack channel for this is also available for posting I've been trying to keep up on it while we've been in here but uh so yeah what is it the shy of an hour and a half three times yep how about we say one hour from now so 4 30 we'll come back together we can do some like q a and show and tell if anybody like created something or I could just walk through how I did the button every time I get everybody you got an hour of us just being here oh there are five of us because there Okay so as you can see here this is all the code I added it is a full two lines except one of them got lint wrapped so um as I said I created a toolbar button toolbar button can take props those two props that I leveraged was the icon and the on click I set the icon to be the refresh icon which I if you come up here just imported from UI components similarly I imported toolbar from apputils like I showed earlier coming back down here the on click you pass it a function so I said the function is you take you can define a function like this in typescript and I just called on the widget the load image so when you click it it runs widget load image and then I went into the main area widget toolbar add item which is a function on the toolbar class I named it refresh and I gave it the button in this case Button had to be a toolbar button and that was how I added the button in two lines so widget.load image was that function in the tutorial widget called load image which calls the API that was the important part about having this function was so we could call it on that on click if it wasn't in its own function it wouldn't have been able to be called like that so yeah that was how I defined it in two lines so yeah now that I showed you guys that uh did anybody uh start working on anything else like had any other ideas that they wanted to share or even demo or if you guys want to just keep working um I we will be here till five because that's when we said we'd be here unless everybody leaves this room no not a wink wink I'm I am very much available until five to answer questions I know that in when you're getting started on working on extensions there are lots of pit uh pitfalls that you can fall down and spiral for hours to days um a good example is we mentioned the idea of pre-built versus Source extensions uh early on in this I work on a project called the Lyra elyra is a suite of extensions uh which is it's its own uh ball of yarn because unlike a normal extension where you just have the one extension and everything's streamlined Suite of extensions is you have multiple extensions each defined all-in-one repository and you have to deal with the extra step of uh doing that install step for multiple of them so when we decided to update to using the new pre-built system of build I had to completely change all of our build tools I thought it would take me a week or two it took me two months and that was like all I worked on for two months uh that is a great example of like knowing exactly what you need to do having all the documentation there and yet stepping in pitfalls not knowing oh I'm stuck here I should go ask this question to this person because like I can see Jason like nodding in the background because like there was probably quite a few places there if I just like Ping Jason or ping one of the other committers they're like oh yeah I know you just do this but I didn't take that necessarily take that step to ask that question or know who to Ping and when to Ping them so it's very easy to step into those pitfalls that was two years after I started working on Jupiter lab I hit that so this this was literally February that I that I finally figured that out so yeah uh easy to hit those pitfalls I we are always available to answer questions um Jupiter lab itself actually has a getter for the Jupiter lab repo for those that know how to use getter where you can ask questions people always are following that um and for those that are interested Jupiter lab also has Dev calls every Wednesday the links to and info on that is available on the Jupiter lab repo that's a great place to get questions answered both for developing Jupiter lab itself but also developing your own extensions um in fact just a couple weeks ago I wrote an extension that uh allowed you to put a streamlit app inside of Jupiter lab and after I finished that I did a five minute demo at our Dev call saying hey check out this extension I wrote and everybody's like oh yeah cool and some people are like oh I have a question about that and people like oh how'd you do that I was like iframes and then like three people like oh how the heck do you use iframes in Jupiter lab extensions um and so like that's you get those answers um when you go to those Dev calls I Central Time the dev calls at 11AM uh you can convert that to whatever time you are locally so yeah uh did anybody work on anything else they wanted to share ideas or just take the next 45 minutes to just keep hanging out foreign they're just going and digging into it's great documentation he's also yeah as I said super detailed so it can be overwhelming one of the things that make sure brushed away the unnecessary details for that individual start but as you build more in depth and start dig into your own stuff it can tell you there's the examples repo it literally says how to make a button and that's all it does button doesn't need to do anything it's just show you how to make a feed he doesn't do anything but it shows you like how to define it and they're very concise and they're all they're all that's a great example and foreign the idea of saying oh if it's called Jupiter then Jupiter left build then run Jupiter like nobody wants to do that any other ideas people want to share or places they got to stop or you can just keep checking on them it's like a very nice whichever you want you want so um a good example it is uh so in the library we have something called we literally took the file editor and we said okay when it's a python file also wrap like essentially wrap that file editor and add more stuff to it so we added a separate launcher item we added a new thing to the file new so if you say file new there's also one there all of this was done in the one activate we also wanted a theme well we want that human absolutely so we put a team in a completely separate extension now inside of core there's some great examples of oh well I want a button and I want because that does something but I also want this main area which that does something they created two plugins that each have their own activate but they're both in the same index file and it just X so you can put it all in one activate it's all one extension or you can put multiple plugins but they're still all in one extension or you can make completely and you can do all of them all of these are validated the one of the coolest things about how to figure out is written is how modular it is you can make one giant monolith or you are either one monitor is is okay and how modular you want to get quality what are the advantages of having separate plugins that are in my extension is you can install that full extension but then if somebody else comes along you can actually disable plugin if somebody else comes along you're like oh I installed this extension but I don't want this to plug in from the extension they can disable that which allows it to not so it's models that you use you don't have to install those extensions modular disabled it's an if that's a good example of why you might have and that's also something that uh I think we put in the slides but we might not have very detailed discuss the difference plug-in is that that those individual things yeah that's fair I just speak loud all the time but people aren't recording oh that's right there's a recording yeah that helps so yeah uh what I was saying was distinguishing between a widget a plug-in and an extension so when you're talking about an extension you're talking about like uh uh specifically like a library so you can install it so in in our case the extension is the whole thing that we made from the cookie cutter you can do pip install extension and that installs it extensions can be as big or as small as you want the plug-in is specifically that Jupiter lab plugin that we defined in the index file that had its own activate function you can have as as many as you want in there or just one and you just export them and they can all be in one extension widgets in particular are just a class so you can have a widget class and it can be extended in lots of different ways you can have a main area widget you can have a button widget you can have a sidebar widget you can not even have a widget you can just make a command um and commands don't necessarily have to create widgets they can just do an action on widgets that already exist widgets are very versatile if you looked at the Imports in in the code you'll notice that widget is actually not from Jupiter lab it's from a different Library called lumino lumino is part of the Jupiter lab org it's it's specifically all the UI bits that create Jupiter Labs like the the main area the sidebar all of those things are defined in lumino Jupiter lab leverages lumino as it's uh what would be the best term um renderer maybe would be the best term how would you describe lumino as like Jupiter lab uses it it would be a yeah it's it's the UI building blocks for Jupiter yeah oh yeah signals are a great example I we didn't even talk about signals today um if you have two completely unrelated widgets that need to talk to each other sometimes if they're both installed signals is a great way of doing it uh when you you can uh create a signal and when you do something you can say signal dot emit and that signal can just be part of your class and then some other application can come in and depend on you and say okay grab that signal object when this signal and then I'm going to connect to it and say when this signal emits do this and so uh for instance if I made uh we let's say we're using the button that we have now but rather than tying in that like load image what if we wanted to have that load image called whenever somebody like clicked right clicked on the the right toolbar you could have a signal set up that's whenever somebody right clicks on that right toolbar it emits and then you can connect that function that reload function to that signal and it will trigger so signals are uh very complicated but also very useful way of having two completely unrelated widgets communicate together yeah and there's lots of signals that get emitted by core Jupiter lab like you edit a a notebook signals are just popping out saying oh I edited this or this this got changed um spell check like we uh we there was the spell check uh uh widget that Martha showed that I I don't I haven't even looked at the code I can almost guarantee it's using a signal to know that something changed check it so just mind dumping for you guys no we didn't do you want to come up and give a you can just talk about them um oh good my toolbar is working again I can actually get out of this yeah um here's a have a have a web browser tab okay thanks I'll just show two things that might be interesting to people um first thing is uh Jupiter light so we've talked a lot about okay you guys have talked a lot about Jupiter lab and how uh how Jupiter lab extensions aren't uh distributed and everything an even easier way to distribute a Jupiter lab extension and Jupiter lab itself is Jupiter light so you'll probably hear a lot about this there's a lot of hype uh not like not unfounded hype but there's a lot of excitement around Jupiter light these days uh and for example you'll see uh if you go to the numpy homepage the numpy documentation you'll see uh in the numpy documentation let me where is it see yeah here we are nope uh where is it okay how about let's go to the iPad with this documentation I'm not sure exactly where it is in the numpy documentation um and the IPI widgets documentation we just added this like five minutes ago um if you go to the latest documentation you'll notice that you can try ipy widgets directly on your web browser and it's even better than trying it directly in your web browser it takes a few seconds to download a few megabytes so you can see I can try it and it takes a few more seconds to download a few more Megabytes uh but basically what's going to happen is it's going to download the entire python uh ipy widgets python plus a couple other packages and run them in your browser and so this doesn't require you to start at jupyter lab doesn't require you to start up any server but what it gives you is full working jupyter notebook totally indirect browser with the server also running your web browser it gets better than this this is kind of a little embedded image if you just click on one of these links you have a full Jupiter lab running in your web browser where the server is also running in your web browser and so in that sense what you can do is you can install your extension in fact we installed the Jupiter lab GitHub extension right here you can install it into this Jupiter lab that people can go to and access just by going to a web page no installation no like anything just go to use your browser go to this webpage and you have a full running jupyter server with python matplotlib numpy scipy pandas on and on and on um so this is sort of a really exciting way to have people access your Jupiter lab instance your jupyter lab you know your python Library plus your Jupiter lab extensions all together just go to this webpage okay quick questions about this it's it's appearing on people's documentation like somewhere in the numpy documentation I'm not sure exactly which page there's a just like an iPad which is try a numpy right now in your browser I pay leaflet Hazard as well you go to the ipy leaflet documentation and hey you can just try it right here in your web browser okay questions about Jupiter light this is built on the piadite project which is providing python in the browser which itself came out of the iodide project from Mike drupum mozella and it's a really exciting development in Python and python in the web browser you'll probably hear a lot about Pi script at the conference Pi script is also built on piadite and both of these again provide python plus a number of scientific libraries running directly in your web browser no Server Connection needed just I keep emphasizing it because I still am in amazement that you can just go to a web page and all your python scientific stack just works and Jupiter lab just works and everything just works just like you would expect okay okay the other one I'll point out to you that might be interesting to you is ah yes but we felt it was better as instructors to not teach you guys using a tool that we absolutely yeah so if you would like to play around with it you could install the Jupiter lab plugin playground uh extension essentially what this does is it provides a way to write an extension just the Core Body of the extension just for example this this plug-in thing inside a code editor inside Jupiter lab and then immediately have Jupiter lab run that as an extension so you can do your at least the core of an extension building experience for simple extensions right in Jupiter lab you paste this code into your editor you ask Jupiter to run it as an extension and your button appears up on your toolbar menu or your command appears in the commands uh Etc and so it provides a very quick way to quickly iterate on an extension or to provide just sort of like a little Macro Short snippet extension that you can have Jupiter lab just pull up every time so you can add your favorite button to the toolbar without having to create a whole python package and distribute it on Pi Pi and then install it in your environment and everything if you just want some lightweight way to add a button to your toolbar or to add a new command or to script go three cells down insert this text go two cells back up you know put a header or whatever like whatever sort of thing that you want to script this provides a very simple way to write it and execute it right inside of jupyter lab or save it in your settings so that it automatically comes up every time you launch Jupiter lab so very lightweight to add lightweight way to add simple Automation in jupyter lab interface from from within and jupiterab itself so there's a there's an example here of the gibra plug and playground I don't think we have it installed so I can't demo it but uh yeah too bad we don't have Jupiter light for Jupiter plug-in program maybe I'll go do that um so you can click and it'll pop up peers uh yeah that's an excellent question um so I am just getting familiar with Jupiter light so I'm going to give you an answer that's the best of my understanding and I hope that there's a better answer um so if we launch Jupiter light here let's just launch the default Jupiter light with default sort of packages configured wait oh back back try it in your browser now right here yeah there's a bunch of confusing buttons to press okay so you can install a python package see this is using the piadite kernel which we see over in the corner here you can install a python package doing uh import pip Lite which is a special version of pip that works in your with the browser operating system my weight piplite dot install uh I don't know uh iPad Widgets or whatever this just installs the python package though so so I could import it but it doesn't install the the the jupyter lab extension my understanding is in order to install the Jupiter lab extension you actually have to configure Jupiter light to be built with that particular extension installed and we can check that here the Jupiter lab extension is not installed oh ah it's installed by default here but if I did a different yeah so so it's already installed on this particular uh Jupiter light um but you have to there's a config file that you essentially uh we could I just put in a PR to to a pip light Dot install I just put in a PR to do to to install this GitHub is the one I'm looking for yeah this adds a little GitHub browser might be an underscore oh right yeah so it's having some issues um yeah so so my understanding is you have to configure Jupiter light in order to install the Jupiter lab extension though you can install the python packages most python packages yeah pure python packages that don't have dependencies that uh that are not pure python which is I think what you're seeing here um trying to go back and yeah anyway we can find where you can figure it you basically in drupalight config you just say okay here's my Federated extensions and you just give it a list of all the python packages that can contain Federated extensions and then it pulls out of that the JavaScript and bundles it in Jupiter light yeah I mean we can try it right now if you want um but I think it won't work in fact I was just thinking a few minutes ago wow is it would it be easy uh to enable that I don't know now I have a new question for the Jupiter light folks anyway two two things Jupiter light which is getting a lot of excitement these days for Distributing Jupiter lab plus a bunch of plugins both Python and JavaScript and the plug-in playground which you might find very useful for doing very lightweight uh extensions to Jupiter lab should we try importing it uh is this the python package something happened right but but the question is did the JavaScript actually get loaded in Jupiter lab uh yeah so we'll be able to easily see by right clicking yep there's no there's no menu entry for Jupiter it's a Jupiter plugin playground no it makes me wonder if you would run that in the terminal I don't think there's a terminal in Jupiter night yeah because it's not renting a full Linux in your browser it's just running jupyter lab plus python I would I would assume that if you uh included it in the config that's right then it should work yep yep it should work then that's right but uh what's really what I what I found really interesting listening to this though is uh all of this enables uh how I originally did this demo um when I go back two and a half years like before this was even a tutorial and I was literally just like walking through in like a top like a half hour talk I think this is before you guys were even part of the Elyria team I did this whole this whole extension develop add a button within Jupiter lab I literally spun up Jupiter lab and in Jupiter lab created the file said add the button and then opened up a terminal in Jupiter lab said install this file then I opened up that I opened up the little sidebar where it lists all the extensions and I said install extension and it was all within Jupiter lab no restarts uh so a lot of this is enabling that functionality in a more broad sense and especially with premium build because now that you we use pre-built those wouldn't those steps wouldn't work anymore so this is all really cool stuff so anybody uh have any other questions or discussion points they want to talk about yeah we have we have no wait 15 minutes of time for everybody so how much of the following okay so for example of a regular Jupiter lab it takes away how much does that like yes um so a great example of this is um the launcher actually uh I know this because elyra does this a Lyra has its own launcher so uh what we did is we wanted um our own little class our own bar so if you just uh open up here I'm just going to run Jupiter lab real quick so you can see this I had to reboot my computer since I was last up here so everything's kind of rebooted yeah had a feeling um yeah now uh so in the launcher you'll that you'll remember that there was multiple like bars like oh this section this section this section we wanted our own section we wanted it not to be at the bottom so we extended the launcher and said okay this is how we want it all to be arranged and just change that stuff but of course the launch already existed so what you can actually do is any core extension or any third-party extension you can just disable it and it just won't be started you can disable it using uh settings like you can just give there's a specific Json file where you can just list disabled extensions or you can actually just in the command lines a Jupiter lab extension disable and then list the extension and then that extension won't start in the case of the launcher if you just turn it off and you open up Jupiter lab the launcher just won't be there um and in our case we disable that in that settings file and then we start our own launcher and we actually give it the same name as the original launcher so if anybody else wants to use the launcher when they go ask for the launcher they get our launcher because we gave it the same name and we disabled the one it came with similarly you can do the same for the menu you could do the same for sidebars you can just disable one and insert your own that's a very common thing actually uh is actually just the launcher disabling the launcher and putting it in your own one of my personal projects that I've tabled for months now that I plan on doing in the second half of the Year hopefully is to completely overhaul and refactor the launcher so you don't have to just completely disable the launcher and replace it if you want it to be customized because that is what you have to do right now for the launcher we want to make it so rather than disabling the launcher you can just say oh I want different stuff in it and make it customizable like that that's a big project that I personally plan on working on later that keeps getting tabled it's probably not going to come in 4.0 unless 4.0 keeps getting delayed for like months and months which I hope not um but yeah uh yes you can disable extensions uh Lyra is in the elyra AI org um so it's the org is a Lyra Dash Ai and then a Lyra is a repo in that uh e e l y so yeah it's just like the moon elyra of Jupiter except with the Y just like Jupiter yeah you can Luciano left but you can thank him Luciano name to that project yeah no elyra was Luciano's brainchild when it started so but since then a lot of the goals of elyra have actually just been subsumed by Jupiter LabCorp in fact there's a lot of functionality in elyra that the allyra teams like well core would love this like why is this why don't we just give this to core or such as the python editor uh so but and that happens all the time is uh projects will donate chunks of their code to core or uh projects will uh be just donated to Jupiter lab in general good example is the uh uh what extension did we recently put in the Jupiter lab org that used to be outside toolbar one I always think of the git extension but I think the git extension actually always lived inside Jupiter lab table of contexts is a great example of a third-party extension that actually got it uh submitted to core itself so table of context is now part of Jupiter Lab Core it wasn't um you can thank Martha for that Martha's the the one who uh moved that into core uh I don't know who actually wrote the original though uh yeah yeah no disabling extensions very common very useful so any other questions people want to share General q a
13,Introduction to the Qiskit Runtime,https://www.youtube.com/watch?v=iZsHzVvAGLM,so maybe what i'll do since it's 12 after is i'll just get started on my intro and then we'll do an exercise at the end of that if anyone needs help troubleshooting installation just let jessie or i know um we can help you or caleb now you i'm volunteering to help with installation if needed okay so should we get started you guys good all right okay so i'll spend a few minutes on this um just to kind of set the stage a little bit and make sure that everyone's familiar with some basic concepts before we dive into some of the code stuff okay so i was going to start with this question which is why would we want to use a quantum computer there's quite a bit of hype around quantum computing and some people think uh you know quantum computers will replace classical computers um that they can solve you know any problem faster that there's some kind of really great all-purpose machine that you can just solve any problem with in reality there you would only want to use a quantum computer for very specific kinds of problems where you know you can get a computational advantage because there's a lot of overhead associated with using quantum computers and very often we already have efficient classical solutions so here's kind of a little bit of an example to illustrate so whether or not quantum computers are always faster so if you wanted to ask if you have two numbers how long does it take to multiply them we have pretty we're pretty good at doing that classically already in fraction of a second if you wanted to do this with a quantum computer given all the overhead this would take many orders of magnitude longer to do so you probably wouldn't want to perform multiplication basic multiplication tasks with a quantum computer if you flip this around and you say can i find the factors of a number it turns out that classically this takes a really really long time but there is an efficient quantum algorithm that does this which is shore's algorithm so this kind of illustrates that there are some problems why where you would definitely not want to use a quantum computer and there are some where you might want to and this is a really sim overly simplified picture of kind of the complexity landscape of problems that we might want to address with quantum or classical computing so in the middle the white blob is are problems that we can solve efficiently already with classical machines the light blue outside of that are problems that we can't necessarily solve efficiently today with classical machines and then this sort of elongated blue darker blue are problems where we think there are efficient classical solutions so some examples of things that are in the cat this light blue category of things we can't adequately address today are things like factorizing large numbers and simulating quantum systems and but we we we know that there are efficient classical or quantum algorithms for those kind of problems and then there are known examples where this dark blue oval extends out beyond the light blue so that's kind of interesting as well but this is a lot of this hasn't been settled um and this is a very highly simplified picture but just to show that you know um you don't want to throw a quantum computer at any problem so there's um in the sort of complexity theory world there's a name for the kind of problems that quantum computers can solve efficiently which is bqp or bounded error quantum polynomial time problems there's a few problem types that live in that class so solving large linear systems maybe you've heard of the hhl algorithm that has an efficient classical algorithm simulating quantum systems so if you wanted to find the ground state of a molecule or simulate dynamics and quantum walks so search problems things like that so also factorizing large numbers discrete log problem things like that so these are the kinds of at a very high level problems where there are efficient quantum algorithms so we might want to look for applications that are in these kind of buckets okay so then the other part is if you're trying to think about when to use a quantum computer sort of one of the first things you might want to start with is seeing if the problem has any sort of interesting structure to it that you could use quantum entanglement to represent and this sort of boils down like any sort of want to do with a quantum computer boils down to the underlying quantum circuits that you're running they have to be hard to simulate classically because if they weren't then you could just simulate them with the classical machine and solve the problem classically so this is kind of the foundation of any sort of application or algorithm is the circuits need to be hard to estimate classically and that kind of gives you a necessary condition but not a sufficient condition for a quantum advantage so what makes a circuit hard to simulate i think there's not really a really simple answer to this but there's a few things we do know which is you need a lot of entanglement you need to be able to create interference and i'll talk a little bit about what that means but essentially you want to be able to create constructive and destructive interference of the amplitudes of your wave function and then the circuit needs to be algebraically complicated and what that means is there there are known um sets of gates called clifford gates that if you build a circuit out of those gates there are known classical algorithms that can simulate those efficiently so just having a lot of entanglement or a lot of gates is not alone sufficient you kind of need at least these three things then the other thing is if we wanted to run stuff on today's hardware it's noisy and so you want to kind of look at what are some short depth circuits that we can run on today's hardware that are still interesting and hard to simulate if you scale them up and there's been some work showing that there do exist these kind of short depth circuits that are hard to simulate so then you know what kind of circuits are hard and how might you use them there's a few kind of different ways of thinking about it so one on the left i'm showing if you want to you can create quantum circuits where if you wanted to classically sample from the distribution that was produced by that circuit it would be hard to do that classically and there's a category of quantum circuits called iqp that where this is known to be the case so if you um the distribution created by these circuits is is hard to sample from classically and those can be used in different kind of applications there's on the right hand side there's an example it's called a forelation circuit it's connected to some more like theory stuff that i don't want to talk about here but this circuit is is conjectured to be hard to simulate classically if you scale it up and so these are kind of two general types of circuits that you might encounter but then you might be wanting to know like what can you do with these are they useful for anything um i think we we're starting to answer these questions so they're this example the foralation circuit we do know a way to use this in a kind of meaningful way which is in machine learning so it turns out that that foralation circuit i showed you it's actually a more general version of a circuit that you can use to calculate quantum kernels so you can use that to if you guys are familiar with with kernel methods you can use it to compute similarity measures between data and so that circuit has a you know a hard to simulate sort of condition and it has some kind of practical application and then a couple let's see last year 2021 that paper was taken a step further and it was shown that there's actually there there exists a case of this circuit that can provide a quantum advantage but it's not very practically relevant um in the sense that it's doing machine learning that you probably wouldn't want to the problem it was solving was not particularly useful um and then there's work that was building on that that was showing how to generalize that framework um to data with group structure which might have more practical immediate practical applications so that's one example of where these hard circuits can find them their way into a meaningful application okay and then so that was kind of my little motivation for why using why one might want to use a quantum computer um i was going to since everyone here said pretty much they don't have too much familiarity with basic concepts i was going to spend a few minutes going over some kind of fundamental things and also please stop me if you have any questions this can be as interactive as you guys want it to be all right so well maybe i'll stop there do you guys have any questions so far all good okay um all right so i'm gonna do a little um sort of analogy um which is always kind of problematic but i'll do it anyways um so like you know in classical computing we have um bits which are zero and one they take on binary values and we have logic circuits that are composed of gates and these are sort of what together form the basis of classical computing in the quantum world we replace those things with quantum bits or qubits and they can be represented by this sphere which is called a bloch sphere and the state of a qubit can live anywhere on or within the surface of the sphere and essentially what that means is it can be a linear combination of the states zero and one and then instead of classical logic circuits we have quantum circuits and they're also composed of gates and uh measurements so gates and measurements and together these are what form the basis of quantum computing all right so i'm going to go through a few kind of basic ideas that we use and then but please stop me if you have any questions so the first is this idea of superposition which i kind of alluded to so with quantum computing we we always talk about these basis vectors called zero and one and they live in this two dimensional um complex vector space you can write down any state in that vector space as a linear combination of zero and one and we in the quantum computing field we usually refer to 0 and 1 as the computational basis because that is the basis in which we can natively measure the qubits in on the hardware then we also can perform measurements on quantum states so i'm illustrating the block sphere here again because what happens when you measure is you can think of if you have a vector in this sphere if you make a measurement you can kind of think of it as like projecting what is the projection of that vector along some axis and that will give you the probability that you'll see an outcome zero or one so if you know if we measure this can you guys see this cursor yeah if you measure the state you'll get um you'll see the the state zero with probability a absolute value a squared and probab and state one is probability absolute value b squared and so that's kind of the at a very basic level how we measure quantum states and then there's also this idea of entanglement so if you have more than one qubit in general you can write down a two-qubit state like i'm showing here so you have four complex amplitudes and when when at least two of these amplitudes are non-zero you can get entanglement but not necessarily so let me give you an example so the first state in purple this is a linear combination of two two qubit states but it turns out that is not an entangled state because you can just pull out the first zero and then you have the state zero that's uh multiplying the linear combination of zero and one for the other qubit so those are are in what's called a tensor product state so they're not actually entangled on the other hand the two at the bottom in blue there's no way that you can kind of pull those out or factorize them so we would call those entangled what that means is if you were to measure the state of one of those entangled qubits then you know the state of the other one right away all right and then this idea of circuits that i mentioned um before which is and this is kind of the like building block of the quantum algorithm applications that we're running so i'll kind of give you the anatomy of a of a circuit and how we how we think of these so you have these different horizontal lines those represent the qubits so we have a set of four qubits on the left we initialize them to the zero state or the ground state and then we apply a sequence of gates running from left to right in time so we have some single cubic gates in red and some entangling gates in blue and at the end we make a measurement on one or more qubits and at the output of that we get a bit string and so you can write down what is the probability that i see any given bit string if i make a measurement and it's just the projection of your state onto that bit string and then you square it and so the basic at a basic level what is happening when you're running quantum algorithms you're initializing qubits you're applying a set of gates and you're measuring things the other thing that makes quantum computing interesting is that when you run these circuits you would in principle get a bunch of different bit strings out at the end but what you can do is for certain algorithms the combination of gates that you use let you suppress or enhance the probability of getting certain bit strings on the output and what's nice about that is if you can do this in a clever way then you can enhance the probability shown here in red of getting the correct answer and so this is kind of what a lot of algorithms are doing like if you're familiar with grover's search that's what it's doing it's kind of flipping amplitudes and amplifying them and so at the end if you measure you should get the right answer with high probability all right any questions on that so far any takers okay all right then there's a couple two really important um tasks that i want to cover and these will um show up again in jessie's section when she talks about runtime and this idea of primitives so um there's if you kind of look across all of quantum algorithms and applications there's in most cases two basic things that you're really doing so one is sampling which just means i'm gonna prepare some qubits i'm gonna run a set of gates on them and then i'm going to make a measurement and get a bit string out and so that's called sampling like i'm trying to sample from the distribution created by that quantum circuit there's some examples of where you would use this that you might be familiar with so quantum phase estimation if you're trying to estimate eigenvalues of a unitary operator grover's algorithm recommendation systems things like that the other basic task is computing expectation values so if you have some observable that you want to measure maybe it's the energy of a system you do that by computing expectation values of operators and this is also very very foundational so algorithms like vqe which you might have heard of where you're wanting to minimize the energy of a quantum system like find the ground state of a molecule you would calculate expectation values there if you want to compute quantum kernels if you want to do combinatorial optimization all that stuff requires these kind of basic tasks and kisket runtime which jesse will introduce and talk about a little bit it provides these two tasks as primitives so it makes it available as kind of a nice function that you can use and kind of insert into your applications or algorithms and perform these basic tasks okay and then i wanted to spend a few minutes on how do we calculate expectation values because this will be part of the first exercise so i'm going to give a very simple example for this slide and then you guys will try to see if you can do the same thing but for a different operator okay so let's the question is if we have an operator which is actually let me pause do you guys are you familiar with what the poly matrices are okay maybe i should have had a slide on that um so the poly matrices are single qubit matrices or operators that we use to they're single cubic gates that we can use to apply to qubits so they you can think of them as if you remember that block sphere if you have a vector in the blocks here that represents a qubit state you can think of the poly matrices as like rotating the state of the qubit on the sphere so it's a way of performing single qubit operations um so like for example the x poly x gate is kind of like a bit flip if the qubit is in the state zero it flips it to the state one and vice versa um and so these the poly matrices are really important they show up like everywhere and um oh question question what's the difference between a circuit and an operator in an operator a gate an operator so they're kind of the same a gate is um [Music] a gate is a unitary operator but we call it a gate because it's um it's in a circuit so it's like an analogy to classical logic gates in a classical circuit good question um okay so in this so the question is if we want to calculate expectation values um how do we actually do that on a quantum computer um and then this very simple example we'll just take the poly z matrix or operator which is just a diagonal operator with diagonal entries one and minus one um so if if you if you apply that operator it just um does like a phase flip essentially so the expectation value you can define it um here you you have some wave function or some state psi and you're trying to compute the expected value of the operator z so you have the state on each side and the operator in the middle then what you can do is you can rewrite the operator z in terms of the computational basis which are these zero and one states so this operator is a diagonal operator and it has eigenvalues 1 which is the coefficient from the 0 and minus 1 which is the coefficient here then if you just plug this expression into the definition of expectation value you get this set of equations over on the right hand side okay so we'll go through this one line at a time so here we get this is just plugging it in i didn't do anything fancy um this term here you can sort of play a trick and flip one of them around and add a little star on it and you can write it as the projection of the state psi on the state 0 absolute value squared and then you can do the same with the state 1 and it turns out this these projections and the amplitude squared these are just probabilities of measuring the state zero and the state one so you're just projecting the state onto one of the computational basis states and that's a a probability that you can measure by sampling from the quantum computer so what you would do to calculate this expectation value is you would initialize a qubit to the state 0 you would apply the poly z matrix or polyz gate and then you would measure it a whole bunch of times and you get a histogram of the probability of the counts of measuring zero and one and that'll let you estimate probability of zero and probability of one and then you just subtract those two and that gives you the expectation value of z poly z in the state psi um does that make sense should i go over any of that again because this will be your your first exercise so please ask questions if it doesn't if it's not clear all right this these equations will be in the notebook that you guys will have access to as well so you can look back at it again um but so then um we can do that we can play kind of the same tricks for the other poly operators um there's a little bit something extra that you have to put into it but um that'll be for you guys to figure out i'll give you a hint though in the notebook okay and then i wanted to give a quick preview for some of the things that jesse will touch on and that i'll touch on in part three which is that and i've kind of said this multiple times which is this these two tasks of sampling from quantum circuits and computing expectation values these show up in like every application or algorithm for quantum computing and one of the most well-known instances of this are called variational quantum algorithms maybe you guys have heard of them one really kind of well-known one it's kind of the workhorse of quantum computing near-term quantum computing it's called the variational quantum eigensolver and people use it for computing ground states of molecular systems or things like that like in chemistry or material science there's also if you want to do combinatorial optimization there's an algorithm called qaoa it's the same idea though you're computing expectation values of operators and you can also this shows up in machine learning in the context of kernel methods so if you want to calculate kernel matrices for your data points this is essentially an expectation value okay sorry for the blinding white so okay and then since you guys said you were not super familiar with kiskit this is just a high-level picture of the of the architecture of the stack so at the bottom let's start here so here is what's called kisket tara and this is kind of the the foundation of kiskit it lets you build basic quantum circuits transpile things to hardware um you know kind of all like the sort of basic tasks that i was mentioning sampling expectation values all that kind of stuff underneath here is kiskit air which is a package for simulating classically the quantum circuits that you build there's also some hardware that's down here that you can run the quantum circuits on instead of simulators ibm provides a bunch of different backends what's the largest number of qubits that we have available 127 so far and then here at the top left are the application modules so there's kiskit nature which is for chemistry material science kind of problems um we have kisket optimization which is for solving combinatorial optimization problems like max cut although don't expect quantum speed up kiskat machine learning these kernel methods i mentioned are available through there and then there's a finance package as well and then on the right are some other kind of more like experimental list tools of sorts if you're interested in that and then i wanted to point out a few extra things in case you guys hadn't heard of them so there's the kisket textbook which is pretty great it has a lot of really good kind of intro material i think exercises too that are built into the textbook if you haven't looked at that i would recommend checking it out there's a bunch of other stuff too there's a youtube channel there's medium posts lots and lots of stuff so if you're interested and then also plug this i think the registration is already over for this but maybe for next summer if you guys are interested there are summer schools so last summer was machine learning and then this summer is um what was the summer quantum simulation so these are really cool i participated in one last summer and it was pretty fun so you guys can check that out if you want okay um questions before we move to the exercise any anyone need help with installation are you guys all good oh yes um here let me go back to the beginning yeah i just i went to the repo yeah i just climbed it using the ssd i timed out first because i got oh i see what you got yeah um i'll probably yeah i think i can i think i i can post in there um yeah i'll i'll see if i can do it a little later i think jesse's slides it should be in there ibm quantum account do you want to help and this is the file name of the exercise for part one it should be in the tutorial or doc slash tutorial uh folder so then i can actually go through some of that how am i on time jessie okay okay did everyone find that notebook good okay all right so i was going to walk through some of this in in the notebook itself and then inside of this notebook there's two places where there's exercises that you guys can try um but let me go through this first so i was going to show using kisket how to do these two tasks sampling and expectation values in a very basic way there's better ways to do it that are in kisket itself but i'm just going to show you a sort of manual way to do it okay so first uh sampling from quantum circuits all right so since you guys um we're not too familiar with kisket i'll kind of show you some of this like basic stuff which is um so from kisket we want a few things we want a quantum circuit which is this object here and this is what we use to build these circuit diagrams that i was showing you so we can add gates to them and build circuit objects and then measure from them we're going to use basic air which is the classical simulator that comes as part of kiskit tarot when you install it for kiskit and then a transpile class which lets us take any given quantum circuit and um map it so to speak to the topology of the quantum hardware that we're running it on um jesse can talk more about what that means in practice okay so what we first start with is defining a quantum circuit and here we're just telling it that we want one qubit is this big enough by the way can you guys read that text yeah um so we want one uh qubit in the quantum circuit and then the second line here just means i'm going to add a poly x gate to that qubit and then i'm going to measure i'm saying measure all but there's only one qubit so i'm going to add a measurement to the end of the circuit and so when you run this it looks uh you see this kind of diagram here where this horizontal line is the qubit and i put an x gate on it and then at the bottom is like a classical register kind of where um i'm measuring with this little m symbol and then the output of that measurement which is either 0 1 is stored in the classical register so then so once we've defined a circuit then we want to tell kisket where to run the circuit which means picking a back end and the back end can either be a simulator which is just classical or a real quantum machine so here i'm going to use a simulator that's called chasm simulator and what this does is it kind of mimics the what what you actually do on hardware where you sample from a circuit and you get a distribution at the end and so as part of that we specify the number of shots and shots is just the number of times i sample the circuit i run i run the circuit i measure it and i repeat that shots number of times and then so once i have my back end i can call the run method and i pass it the circuit and the number of shots i want to sample and then i can get the result out of that so the results object contains a bunch of stuff but what we care about are the counts so we get this counts dictionary from the results and i'm printing it here so it just has one entry which is good because that's what you would expect in this case so what i did is i started a qubit in the ground state which is the zero state and then i applied an x gate which just is a bit flip so it goes from zero to one and when i measure i should only see one because that's all that it did there's no superposition or anything like that so i i should only see one and indeed in the dictionary i get i observe the state 1 and i see it 8 192 times and that's just the number of shots that i measured the circuit with and so if you look at this dictionary you can say okay what was the probability i measured 0 and it's zero because there's no entry in the dictionary and then it the probability is to see one is a hundred percent so that is all good then um if we wanted to create a superposition we can use lots of different quantum gates for that but here we'll use what's called a hadamard gate and the hadamard gate if you initialize the qubit to the zero state or the ground state and you apply hadamard what it does is it creates an equal superposition of zero and one and so if we apply the hadamard gate here and then measure we get the following circuit so it looks the same except i'm replacing x with h if i do the same thing as before i i pass to the back end the circuits um i get the results uh the counts dictionary out of it and now if i look here at the output i get the state 1 and the state 0 with roughly equal probability and the reason why they're not exactly the same is because we're sampling the circuit a finite number of times so there's a little bit of sampling noise there finite sampling noise but they're roughly equal which is what you would expect for the skate okay and then i'll just show you this quickly but you guys can come back to it this is the exercise so first part of the exercise would be kind of repeating what i just showed you but use a combination of single qubit and two qubit entangling gates and there's a circuit library that you can kind of decide which ones you want to use two suggestions you can do whatever you want but two suggestions is building a bell state or a ghc state those are pretty simple and interesting and so just do the same thing build the the circuit for that measure them run it with the chasm simulator and then plot a histogram of the output and see if it is what you would think it should be okay so we'll come back back to that in a second but the second part is this expectation values so this is this is the same stuff i showed you on the slide for how to calculate expectation value of the poly z operator so you guys can look back at this if you want but essentially what you do is you create a circuit that represents the the state of interest psi that you care about and then you sample from it a bunch of times you get the probability to see the zero state the probability you see the one state and you subtract them and that gives you the expectation value of this operator with respect to this wave function or the state so to do that in kiskit i'll show you let's let's suppose we want to calculate the expectation value of poly z in the state one so this is um all i'm going to do here is just define essentially an array that represents the state one so it's has two entries zero and one this is a computational base of state 1 that we already saw and i want to calculate with respect to the state what is the expectation value of the operator z um let me zoom in a little for you okay so what we first do is we um initialize a quantum circuit to the state one because that's the state that we're interested in and there's um kind of this nifty thing in kiskit where you can use this initialize method and pass it a list of amplitudes and it will prepare that state for you so it's kind of a nice little shortcut and then we want to measure this circuit because we're interested in the probability of zero and one so we do the same thing i showed you in the first part with sampling you pick a back end you pass it on the circuits you get the counts dictionary and here our dictionary only contains one entry which we'd expect because we're initializing it just to the state one so we only see the answer one and if we take these two probabilities that we get from the counts dictionary and we subtract them which is what i'm doing here um then that should give us the expectation value of this operator and the answer we get is -1 and the reason why that makes sense is because the state 1 is one of the eigen states of this operator z and the eigenvalue associated to that eigenvector is minus 1. so this is exactly what you'd expect okay then i'll go through this locally so this is the similar thing i'll go through this kind of fast in the interest of time but if you wanted to do the same exact thing but you wanted to ask now what's the expectation value with respect to the state plus and the state plus is just um the equal superposition of the zero state and the one state so it's uh one over square root 2 0 plus 1. so we can define that state as a array again we can initialize a quantum circuit for that state using this initialize method we measure it and then we will pass it to a back end and get the counts dictionary again and here we see that um we get zero and one with equal probability so now if i was to subtract these two probabilities i would get roughly zero for the expectation value and that makes sense because the plus state is it turns out it's um you can write it as like an equal combination of the zero and the one state and so the the two like the expectation value for that operator then just like exactly vanishes in that case if you think of the bloch sphere the with three axes the plus state is the plus x direction and you're asking what's the expectation value of the z operator which has eigenstates pointing straight up and straight down so that it kind of makes sense it's halfway between it that expectation value should vanish okay this is what i kind of just said so the reason why you can do this is because when we measure quantum circuits on the hardware we natively measure in the computational basis state which is the eigen basis of the poly z operator and so if you wanted to measure in a different measure a different operator that doesn't have that eigen basis there's kind of a little something that you have to do to do that so yeah this is just uh this is the block sphere that i was kind of just mentioning um so this is one of the eigenvectors of the polyz operator and then i think um yeah so your so the exercise is the second part um time willing is to try to do the same thing that i showed here but for poly x and poly y operators and the hint is that you can write the poly x operator in terms of the poly z but with kind of a change of basis using the hadamard gate and you can do the same thing for the poly y operator but with this s gate and the hadamard gate and so if you were to work out the math for that expectation value i showed and plug in these kind of transformations what you should see is that to measure those two operators x and y you have to perform what's called a post rotation to the circuit before you measure so that's kind of the hint there okay so i think that's where i was going to stop how much time do we have yeah so i guess if you guys want to take a crack at this for a few minutes um please ask questions and then i think we'll have a 10-minute break also before jesse starts so should we say how long should we say for this 45 uh maybe the is the bottom of the hour too long 9 30. should we should we say um break at 9 30 and then come back after a few minutes good all right i will leave you guys this uh this again too if you want to look at it um that's kind of a hint for the exercise okay first do so you have to do this transpile thing that i was mentioning so um the back end doesn't automatically recognize the hadamard gate so you have to transpile it to the basis set that it knows um so if you look at let's see where do i show this here so see how i'm taking um the circuit and i have to transpile first to the back end and then i can pass it to the run method oh okay so it takes the hadamard and it kind of rewrites it in a way that it understands and then it runs it sure i'll leave this up i might go get more coffee experience this one so um concept finished is um um yeah it just tells you oh i have no internet beauty do circuit library they have this which is like the standard gates that they list but to actually apply them to the quantum circuit you you don't have to do it like this like you can say um quantum circuit dot x right like so where is the where do they show that like those forms you know what i mean yes is it in the quantum circuit class maybe it's not important okay in the what okay oh it kind of shows it here right like it's just yeah okay so one thing you could look at is um if you look in the kisket docks for the quantum circuit class here um ins probably should be a better place so this is listed but there's you can look at its list of methods and it will show you some of these will be the gates that you can add some of them are other methods so for example zoom in here like here's the c naught gate so you would say like quantum circuit dot c naught um yeah i think you can do either one c x and c naught are the same um let's see if they have that here yeah oh here's some better ones so this is the format that you can use to call it the like the method of the quantum circuit object um the link that i had in the in the uh notebook you can use that too but it's a little bit less obvious how to go from those two um applying them as gates to the circuit this is a little bit more intuitive so maybe just um look at these methods and see if you want to like any anywhere that says um apply a gate here like for example this uh the cx right here this applies this object capital cx gate but you can just call the method lowercase cx on the quantum circuit object instead and it's a little easier okay so i'll just do the expectation value one so that's probably the slightly more interesting one so can you guys read that it's a little small okay so i'll just show you the one for the poly x gate and the one for poly y is kind of the same um but so what you do is a little bigger so if you if you think about this um transformation that i wrote here i'm trying to highlight but it won't highlight um you can rewrite if you do a like a change of basis um you can transform the poly x gate into the poly z gate according to this and so what you can do is just go back to the original math that i showed you for computing expectation values which i kind of repeat here and just plug in like you know we want to calculate expectation value of x but let's plug in that transformation between x and z instead and so you get this expression and then you kind of play the same trick where you say the paulie's the operator i can i can write it in terms of um the computational basis state zero minus one and so then you get these two terms here on the middle line um and then you can rewrite these uh kind of two symmetric expressions in terms of just one with the pro with the amplitude squared absolute value squared on the bottom line so what you can see is that to measure x what you do is you you initialize the circuit for psi and then you apply a post rotation which is the hadamard gate to that circuit and then you just do exactly what you did for the poly z operator you measure it a bunch of times you get the probability to see zero and one and then you subtract them so it's the same exact process you just have to figure out what is the right post rotation to transform your operator into a diagonal operator essentially and this is all because we can only measure in the computational basis which is the eigen basis of the z operator so the tricks really like for measuring any operators is just to figure out what's the right post rotations and kiskit will like handle this all for you normally i'm just kind of showing you how to do it explicitly here but there are you know built-in things for doing expectation values and then the estimator that jesse will show you handles this all automatically so it's similar for poly y you do this you do the same trick you plug in this change of basis transformation work it out and in this case you would apply the post rotation which is s dagger followed by h and then you you measure subtract the probabilities and you get the expectation value so i i kind of go through the how to set this up with kisket here maybe we'll look at one of them so let's say i wanted to measure expectation value of x in the one state so the one state on the block sphere is the one that's pointing directly down and we want to calculate the expectation value of an operator whose eigen basis is on the plus and minus x axes so it's kind of like halfway between the computational basis state of one and zero so you might just looking at this expect that the answer should be zero and if you prepare this the state um one and then you uh um uh run this at the or let me actually here's where i do the post rotation so you initialize the state 1 which is the state that we're doing the expectation about expectation value in and then this is where we apply a post rotation so that's all you do is you just apply hadamard and then you measure and rinse and repeat and so what we get is that the expectation value is approximately zero and you can kind of explicitly work that out with the math and you'll see that you end up subtracting one half from one half which is zero and then you can do the same thing for all the other states plus the plus state and then for the y operator um but yeah at the end of the day you just have to figure out what the right post rotation is and that's pretty much it um shall we go on a break yeah coffee oh yes so that if you look at uh if you go to the solutions branch here um it's in the same directory as the tutorials but just with the little solution tag appended or word appended to the file so no peeking at the other ones yet all right i'm just going to unplug this a little bit um yeah should we do five minute break so come back at 9 30 eight oh everybody there we go oh is how do you do your cursor um my cursor is just gone oh you see it oh okay i think um because i wanted to do this i guess it doesn't work oh it does work but it doesn't show on the screen it's really annoying like i can't see yeah like i have to look at this i have to look at this big screen maybe yeah if you could mirror this somehow it's been a long time since i had to use a real projector usually it's just webex which one there's a yeah starts on sunday um you shot yourself are you guys it was um i need to get my passports all right let's get started um so i will be talking about part two of this workshop which is about cascade runtime but before i jump into the tuskegee wrong time i want to first of all variational quantum algorithms these are a type of algorithms that use a classical optimizer such as the ones you would find in scipy to trend a uh trend a parametrized quantum circuit um to to approximate the solution of a given problem and normally you would um run your circuits calculate the expectation values plug this expectation values into the optimizer get a new value back and that's how then you would use that as the parameter for your next set of uh circuits so vqueries are very popular today because they typically use fewer gates and qubits and therefore they are more suitable for they're more resistant to noise and today's quantum computers are quite noisy um well i guess i already say that so they're for they're suitable for near-term quantum computers uh i feel like i jump ahead of myself but vqra is not typically iterative you start with a parameter circuit and i will go more into that later you run your circuit you get the output such as your expectation values and then you put that into a classical optimizer then the value from the optimizer will then be used to generate the next set of inputs into your parameter circuits so what does this have to do with cascade runtime vqueries are very popular especially in recent years when because layout they are typically higher level algorithms that people use to solve like chemistry or machine learning problems um there are higher levels so you don't really a lot of people don't have to play with gates and gates and qubits that's why they are popular and before kiss kit runtime typically when you run a vqa let me see if i can find the laser thing [Music] so here's your laptop you start with your parameters circuits and then you send your circuit into some kind of remote content computer oh well unless you are basing a lab otherwise you are probably going to send your job to a remote quantum computer and because computers are still very scarce and expensive resources typically there is a cue you have to wait your term before your searches will get wrong so once it gets wrong you get the output back which gets fed uh all the way back to your laptop gets fed into a classical optimizer which updates your next set of uh circuits and then you have to go to low q again send your your circuits to some remote uh qpu that can take a long time so this whole uh loop is quite inefficient because for every execution you have go through the queue and go to the remote you have to all your data over and go through the queue and go through the the remote uh quantum processor so and that was the main reason that uh we introduced kiskit wrong time so kiskus runtime is a common computer computing service and programming model that would efficiently efficiently execute the workloads at scale there are two parts of case skill runtime one is what we call kickstand runtime session and this is similar to the classical session where you you know open a session with some kind of remote server and then you can do interactive processing with a remote server without having to go through the queue and and you can also cache data on the server side so that's uh basically what the runtime session the idea of learning session is and the second part of case runtime is what we call the primitives as jim mentioned earlier there are two basic tasks that a lot of people do which is sampling uh the distribution as well as calculating the the expectation values and of course it will be annoying having to do that by hand every time and that's why we have these primitives so let me talk a little bit more about uh kiski rongham session so the goal of it is to provide an interactive communication that would minimize artificial latencies so he accomplishes that with a number of steps one is that so when you are once your session is open every time you send your circuits in um those searches are prioritized by the scheduler so you don't have to wait in the queue for others to run another is data that's used within the session such as these parameters circuit is cached on the server and also that's um i will also talk a little bit more about this is the transportation can also only happen just once you know so you don't have to do transportation every time and the third point is let's um we the classical cloud technology has advanced quite a bit and there is the server-less technology where you can upload your program into a cloud and that can very easily scale the classical capabilities and use special hardware such as if you want to use the gpu to do your optimization and kisk runtime makes it easier to do it so the primitives are pre-built programs that gives you an interface to perform those basic quantum computing tasks and much like the other cloud-based services you enjoy today such as your gmail or your google doc because they are based in the cloud there can be enhancements down to these programs that require zero interaction on your part you write your program and then if there are any kind of updates it just gets you just get the benefit of it and some of the updates uh can be such as the any advancement in radar mitigation oh so what are the primitives today we have two the first one is called sampler and as the name implies a simple the distribution of your circuit outputs and it's usually it's useful for search algorithms like grovers and we also have estimator which estimates the expectation values it takes circuits and observables as inputs and it's typically used in uh chemistry simulation like in the in vqe or cause optimization problems all right and that's all the slides i have now if i can just figure a way to switch oh that is not the one i want so um this is the one of the notebooks on the repo it is the um runtime exercise it's called exercise but it also has the introduction part thank you so um the first because we will be using resources on ibm quantum you do need to sign up for an account uh does is anyone does anyone not have an account yet oh well so so um it's just uh you can sign up for an account at the quantum computing.ibm.com and so here is my account and let me just show real quick so to find your api token which you so we need it's under on the there's this little person icon and there is account settings and this is where you can copy your api token all right so because um we are using a remote resource the first step is to authenticate with the server and the way to do that from kiskit is first you should need to import the runtime service so it's from uh underscore i mean runtime imported runtime service and typically if it's your personal laptop um i would just save the uh the api token so i don't have to copy and paste it every time so there is a save account option that you can use and the channel here is because we chrisky runtime started out on the ibm quantum website there is also it's also available on ibm cloud now so you you know if you are using an ibm cloud account link your channel will be ibm cloud if you're just using the ibm quantum which is what i posted here because it's easier to sign up than ibm cloud then that's the ibm content channel and if you have previously saved so in the future if you after you've saved it you can just do initialize it with and specify your channel and if you only have one channel uh you don't even have to specify the channel it'll just find it but if you have not saved your account on disk your api token on disk then you specify the channel and you specify the token all right so here i'm going to now with authenticated i'm going to show you an example of calling the sampler primitive inside a runtime session so there are a few steps involved in this the first is to prepare the initial input data and this is typically your parametrized circuits and then you have to select a quantum computer to run on what we call the back end then you can open a session and then submit your request so the first step is preparing a quantum inputs the simpler program takes a number of inputs so the circuit is your quantum circuit it can be parametrized or not and if it is parametrized then you can specify the parameters you want to use you can choose to skip transpilation uh i will talk about a little bit more about transportation in a minute um but typically if you peop like normally we would just let the the the program do transpilation for us but if there are specific thing you want to do like if you are studying you know different transformation methods then maybe you can say okay i want to try different things then you can you would just say oh don't do it for me i'll do it so service is the service instance that we just initialized in our previous step and there are other options like the number of shots and usually we run a lot a large number of shots because um the quantum computers today are noisy so you tend to so each shot is basically a repeat of the the same circuit the execution of the same circuit um so you want to run a large number just so you can get a more accurate results you can also see find the inputs and outputs of these primitives on the website so so on the quantumcomputing.ibm.com there is the programs and under programs you can see the sampler and the estimator i probably should delete all these but these are my own programs that i uh wrote for different things um let's see if we'll load i guess networking is a little bit slow today oh there we go so you know it shows you all the inputs and all the undone written values and he also tells you know when was the last time it was updated let's see so step one prepare parameters circuits oh i promise to talk about transpilation so what transportation so transportation is a term defined in kits kit and that is the process of rewriting a given input circuit to a different circuit and why does it need to do that a number of reasons the first is to convert high-level gates to basis gates um so kind of similar to classical computers where you write higher level functions and the compiler just converts la to lower level stuff that in the quantum computers tend to have a small number of basis gates um such like the header market for example it is typically not a basis gate it's a higher level gate and the transpiler will convert that into one of the basis gates for you another thing is to map the input circuits to match the topology of a quantum device so let me just oh i think one of these is no so let me just show you one of the devices uh so here is a one of the our quantum real quantum machine so this is what we call a coupling map and that is the layout of the the qubits so this is a five cubic machine and qubit zero is connected to one one is connected to zero and two and so on so forth but if so if you are trying to write a two cubic gates like the c not g we talked about earlier the two qubits like you want to entangle they have to be right next to each other so there has to be connectivity among them but when you're writing your circuit it's already difficult enough you don't want to have to look at the map and figure out which qubits are connected with which and then figure out if you can apply those those those skates so what we normally do is you kiss kit would present to you as if all qubits are connected with each other so you can write a c not gate between 0 and 4 for example even though they're not connected and the transpire will just insert what's called swap gate to make sure that so that you make the your change your circuit enough so that uh the zero and you can do uh intent command on zero and four so that's another big piece of the the transpire and the last thing is just like classical compiler it does uh optimization of your your circuit for execution now you may ask why is the code transpiler and not the compiler if he does a lot of things as a classical compiler and the reason being we actually have another compilation process which is take your logical your your high level representation of a quantum circuits your you know your qubits and your gates and actually translate lads into um something our control electronics would understand so the way sorry if this is too much tourist information but i find it fascinating um so the way we control our qubits is we have a bunch of uh microwave pulse generators so they generate microwave pulses and to stimulate the qubits and so your circuit your gates need to be trans uh compiled into something these uh waveform generators can understand and that's what we call the compilation step so back to our regularly scheduled topic parameter circuits so transpiler it makes things a lot easier for developers so you don't have to worry about basis gates and and coupling map but it can be time consuming especially if you are trying to do a lot of optimization and with variational quantum algorithms typically the circuit layout doesn't change what what changes is the parameter values so for example you know your circuit can say i can do like it in rz gates but it's really the rotation angle that changes it's still the same gates so we have this notion of parametrized circuit which means you can create a circuit with parameters not yet defined you can transpile this circuit once and then just bind it with different parameters for different execution so this is an example of creating a parametrized circuit and so kisky has a library of commonly used circuits and here i'm just i just randomly grabbed one called real empty tools and you can go to the cascade documentation for all these circuits and their descriptions so here i just say i want to use two qubits and i want the repetition to be two and of course i want to measure all my qubits and this is a parameter circuit with six parameters and you can see here it had this uh six roi gates and each one has its own parameter and these parameters are undefined right now because i haven't really given it any values yet all right finally step two selecting a back end so i kind of um showed you a little bit that this is the page the quantum compute resources is the page where you can find all the back-ends and it gives you a lot of information about these back-ends can i go back to oh by the way so all our back-ends are named after cities but they are not in said cities they are only in new york otherwise we would have way more traveling opportunities so yeah manila is not actually manila it's it's in new york um so this page tells you like it's a 5-qubit circuit it's online it has 12 jobs waiting to run and more importantly it tells you like the average uh c not errors or the real error and you know t1 t2 a bunch of values uh let's see so you can uh look at the page to find a a backing you want to run on or you can do it programmatically so one large one major selection criteria a lot of people use is to find the least busy back-end because leads are shared resources so sometimes the queue can be really long when i say really long like you can wait days to before your job would run lead speech is one of the common criteria and like you can say you don't want any simulators you just want real devices and you want at least five qubits so and the api doc has will tell you about the more different selections but here is just an example uh and then you can say so each back-end has two major methods that would give you provide information about the back-end the configuration and the properties the configuration contains static information about the backend such as the number of qubits and the properties is the dynamic information about the backend such as its error rates or the t1 t2 values in this workshop we will be using the simulator because uh although i because otherwise you would have we would have to wait quite a while for everyone's jobs to finish so the simulator is the sim simulator you just use locally it's but this is a cloud-based simulator and the reason to use a cloud-based is so that it doesn't lock up your cpu or your cpu resources while you're running your job all right so step three opening a session um opening a session with a one of the primitives is relatively easy um so here in the simple example i'm using the sampler so i just import the simpler class and the typical way of opening a session is to use a context manager in python so here i can say with sampler and then i passing the parameters quantum circuit that i prepared earlier i have to give them the service which contains my credential the end i want to use and it would return me a simpler object i'm not doing anything in this session because i just first want to show how you open the session so now i want to actually submit requests to this session um so this is somewhat same syntaxes before where i open the session given my circuits and and my selection of back-end and i can say oh now i'm ready to call the simpler primitive using uh the the first circuit that i provided and these are the parameter values that i want to pass to my parametrized circuits and of course i can print the results so because this is a simpler it gives me a distribution of the measurements so you know the different probabilities of measuring one zero zero zero one one and zero one oh let me pause for a minute does anyone have any question sure uh your sampler context with sampler and you pass in your service you said something about a session does it open a session yes the when you do a with simpler contacts manager it opens a session for you oh you can do all the so you can do all of the code it just so the free account restricts the uh the quantum computers you can use so i think you can only do five qubits is the biggest that you can do there used to be a 16 qubit but i think they took a lot away oh i guess seven okay sorry yeah i i haven't used my free account in a long time so i don't actually remember but yeah so yeah i guess seven is the largest that you can use all right any other questions yeah so the simulator will simulate up to 32 cubits right now it's so it is a simulator but it's um maybe jin can talk more but it's a no maybe not so it's it uses a different simulate simulation method so it's not a general purpose simulator it only it would only take a certain type of circuit and i cannot tell you which type of circuit because i'm not an expert in simulation you should be able to use like the chasm simulator yeah like all of that so if with a free account you can run with that chasm simulator right right so so the uh the ibmq castle simulator is a all-purpose simulator so you would take any kind of circuit while you're passing and it simulates up to 32 cubits okay any other questions all right so i'm now going to show you an example of submitting multiple requests because uh well let me first show so i'm preparing another another parameter circuit again i just pick a random one from the circuit library and this one is called efficient u2 it has eight parameters uh this is probably a little bit hard to read but just ignore the output so as i mentioned the whole point of having a session is like you can submit different circuits and different parameters within a session without having to wait for the queue so here um i said i want to cache these two circuits pqc and pqc2 probably should have landed on pqc one and two um so so these are going to be transpiled and cached on the server side so i don't have to send them these two every time and i don't have to re-transpile them every time and then i can say oh i want to you know use the first circuit in the list which is pqc and i want to use lease parameter values and i can also say i want to run 2000 shots instead of the default of 1024 and similarly i can say oh now my second request within the same session i want to use i want to call this in the pqc the first circuit twice but with different parameter values and the third example is i want to use the circuits 0 and 1. so pqc and pqc2 and here are the values parameter values i want to use for them so this is how you can you know open a session with a number of different primary circuits and then just repeatedly call the simpler with the same circuits but different parameter values is there a limit how many calls you can make within a session there is a time limit and that is a very long time i want to say four or five hours um there is documentation on um ibm quantum uh sorry kids giving runtime that will tell you what the time limit is um so next i want to show because we talked about how variational quantum algorithms are tend to be iterative so i want to show an example of an iterative processing it is a bit of a contrived example but i tried i wanted to keep it small so in this example i'm going to prepare a parameter circuit that only takes one parameter and the this parameter would determine the degree of rotation around the x-axis so it would be somewhere between straight up or straight down and starting from a quarter pi we would increment the value of theta by the probability of getting a one from the previous job times pi so for example if i run a job i run a simpler job i get back the probability of getting a 1 is 0.35 then the parameter to my next job will be 0.35 times pi and this iteration loop it uh would end when the probability of getting a 1 reaches uh greater or equal to fifty percent so just a bit of a visual demonstration this is a block sphere so we start with uh the zero state and here is the x-axis that i want to rotate from so i basically just kind of like going this way rotate around x axis until i hit the probability of getting well i guess you can really see one here the bottom is one uh hit the probability of getting a one becomes greater or equal to fifty percent so this is my little sample program so again step one is to create a parametrized circuit and here while kids can search circuit library doesn't have a predefined one cube uh one qubit circuit so i have to create my own so i define my parameter i create this one qubit circuit and i said i want to do an rx which just rotates around the x-axis with my parameter value theta for qubit 0 and i want to do my measurement and this is what my circuit looks like so then here my uh my little program or call iterative processing i start out with my parameter value being a quarter pi then i open my simpler session give it my the circular i just built and the back end which we're just using the simulator and i can call simpler with my circuits and my parameter value and because well i guess i was kind of lazy when i did this i only did 100 shots so here is my uh my loop my while loop saying if the distribution of getting uh the probability of getting a one is less than point five so fifty percent i'm going to continue updating calling simpler with the newly updated value so here my value would be you know getting the percent no percentage of getting a one uh just in case i get a um zero percent which would means this will become infinite loop so i give a default value so whatever percentage of getting a one is i'm going to time pi pines pi and then add it to my theta value so you can just see and once i reach 50 greater than 50 then i exit my loop so it's just in a little demonstration of how you would take the the results of a previous execution and plug it into your next one all right now is the exercise time before i go into the exercise any any questions all right so the exercise is to do basically what i just showed you except with the estimator program so you would open a kisky runtime session with the estimator context manager and then you can make multiple requests to the estimator primitive within the session and uh ideally the parameter value of the request would be based on the output of a previous one and the loop has to end at some point when you know some set target is achieved so there is a link to the estimator api and there is also a tutorial on the estimator estimator is slightly more involved in this lens simpler because it takes more parameters than than simpler but the idea is pretty much similar so how are we doing on if we time all right um i'm going to go over a solution which is again another slightly contrived solution but in the part 3 we will be talking about how estimators are usually used in a real vqa program so here and the solution is kind of based on the previous exercise during which we talk about how we would compute expectation values for the poly x and y operators in the one and plus states so in in my solution i'm gonna do the same thing but with a slight small twist and that is i was um we would do a circuit in the one state and use the poly x operator if the expectation value the calculate expectation value is greater than zero and what probably y operator if it's less than zero and because the expectation values should uh vanish which means it should be approximately zero we are going to set a target of 0.01 and we are not setting a target of zero because it's the simulator simulation is not gonna give us a perfect 50 50 chance which means we are most likely never going to get a zero it'll just become an infinite loop which is not what we want all right let me zoom in a little bit so the first step uh again is to authenticate with the server and uh we do that by creating this initializing this runtime service class which would return as a service instance so here i am preparing circuit one qubit circuit and it just does an x-gate on qubit zero and the measurement then i am going to prepare my two operators so there is the poly x and the poly y operators um and here i'm using the smart poly up class from kiskit in which i can just do a front list uh this is not gonna fit on the screen all right i should have all right so here i am using calling the estimator um primitive so i am first opening my my runtime session using the the width which gives me the is the how i define the context manager so i do a width estimator um passing my circuits and my possibly possible observables i want to use my x and y operators the services instance that i got after authenticating with the server and the backend again i'm just going to use the casing simulator so here i'm setting my target as 0.01 and i call my initial value is from calling the estimator primitive with my circuit and the x operator so that's my expression value so as long as the expectation value is or the absolute of it is greater than my target of 0.01 i'm gonna do this loop and the loop just says oh if it's greater than 0 i'm going to use the x operator and if it's y if it's smaller than 0 i will use the y operator and i just call my estimator repeatedly passing in a different observable each time so uh well i guess i can't really scroll down but so this is a again very contrived example of calling the estimator repeatedly within a kiskatron time session all right does anyone have any questions about kids quarantine or in general if not i think we can take 10 minute 10 minute break and we'll come back at 11. oh well i guess i should mention again that the solution is in the solutions branch inside the repo you guys ready for the third and final part all right okay so i'm going to spend a few minutes talking about how to build on what jesse talked about but specifically for applications and in particular i'll show how you might use this for chemistry problems combinatorial optimization problems you can also use this for machine learning but i did not include that specifically in this or in the exercises but those also in principle can leverage this too all right so kind of a recapping a little bit um and i think jesse like pointed this out pretty well which is that pretty much anything that you want to do with quantum computing requires an interplay between a classical machine and the quantum machine in some form or another whether you're doing an iterative variational quantum algorithm or not at the end of the day that's usually what you're doing you have some kind of back and forth so the the variational quantum algorithm is a it's kind of like the main example that everyone points to for these and these are kind of considered like the workhorse of near-term algorithms because we can we can run these on machines today as opposed to what are known as fault tolerant algorithms which require quantum error correction um and have massive overhead require you know of order millions or tens of millions of qubits and things like that so those are clearly out of reach right now but we can run what are called variational quantum algorithms and the reason why we can run those is because they usually let us get away with running shorter depth circuits and then outsourcing some of the computation to classical optimizers so you kind of pay the price and that now you have to usually optimize a very complex landscape with a classical optimizer but the benefit is that you can run the quantum part on today's machines usually not always so some common examples of where you would use these type of variational algorithms are with chemistry so the variational quantum eigensolver which is used for finding ground states of quantum systems you could use it if you have a binary optimization problems you can use an algorithm called qaoa for finding good solutions like max cut for example for machine learning you can use this for training quantum circuits to be classifiers you can use it for training quantum circuits to be used as quantum kernels in kernel methods for machine learning and so in most of these cases you can kind of think of what you're doing is you have some system represented by a hamiltonian and you want to minimize the energy of that hamiltonian and so the way you do that is you calculate expectation values and then you can use a classical optimizer to find an approximate ground state and so the reason why i kind of touched this already but the reason why these are useful is because it's usually the case that we can run these algorithms with short depth circuits that are within reach of our hardware today and they kind of because we're doing this sort of classical optimization we can kind of handle some of the inherent noise on the machines a little bit so here's a little bit more detail about what this might what these algorithms kind of look like so suppose um you want to find the ground state of some system which you represent with hamiltonian so on the left here would be a molecule this is lithium hydride so maybe you want to find the ground state energy of that molecule or maybe you have a graph like here like a combinatorial optimization problem and you want to find some whatever this graph represents maybe it's max cut or something you want to find a good approximate ground state solution so what you can do is if if you can represent your problem in terms of a hamiltonian then you want to calculate expectation values of that hamiltonian which gives you an energy and then you want to minimize the energy using some kind of classical optimization routine so you can use this idea of what's called the variational principle variational principle which um if you if you take a good guess for what the ground state wave function or solution is which is the psi here and maybe it has some parameters that you inject into it then you can use that approximate ground state or a guess or an onsite sometimes it's called to calculate expectation values of your system or your hamiltonian and that is guaranteed to be at least greater than or equal to the exact uh ground state energy and then what you can do is you can use the quantum computer to calculate those expectation values which we saw before and then pass to a classical optimization routine that as a cost function and use it to update the parameters in your circuit using a classical optimizer and you repeat this back and forth until you decide it's good enough or you reach some kind of stopping condition and that'll give you an approximate ground state solution so for the case of chemistry suppose you wanted to calculate ground state energy so we have a simple little molecule it has some energy spectrum maybe you want to find what is the ground state so what you do is you um so i'm not a chemist disclaimer but there's a bunch of stuff that you have to do to start with the representation of a molecule and transform it from you know the sort of chemist view of the molecule to a form that can be mapped onto quantum hardware so you have to um if you're familiar with like second quantization you have to write down your system in second quantized form and then you have to map to qubit operators so that you can run it on a quantum computer so there's a bunch of chemistry specific stuff that has to happen for that transformation but once you get to a hamiltonian that's written in terms of qubits then you can kind of just think of this as like a black box optimization thing where you have a cost function which is expectation value of the hamiltonian and then you you call a classical optimizer to update the parameters so this little circuit here is just indicating this is my guess for what the ground state wave function is and i'm going to add parameters into it and then use some kind of classical optimizer to update those parameters at each iteration and hopefully at the end this circuit will be a good representation of the ground state wave function for the casa if you want to do combinatorial optimization it's kind of a similar thing except kind of the up front stuff is a little bit different so you have some cost function that you can write in terms of binary variables maybe it's a max cut or a traveling salesman or something you have some kind of cost function that is a function of zeros and ones and you want to find a good solution to that problem so what you have to do is you have to transform bits which are zero and one to qubits and there's a trick for doing that and what that means is that you go from your kind of classical cost function in its normal form um to an ising what's called an ising hamiltonian which is just a hamiltonian that's a function of poly z operator so it's diagonal then so now you have a hamiltonian you kind of you do exactly the same thing as you did with the chemistry problem where you want to come up with a good guess for the ground state wave function you compute these expectation values so it's kind of giving you the energy of that original problem and you plug it into your iterative algorithm and you try to find a good ground state solution and this is just to kind of go back again to what jesse was talking about that this back and forth iteration is really well suited to the kisket runtime framework because instead of having to like every single iteration of the of the algorithm you would normally have to get back into the line of the queue of the hardware and wait until something opens up and then send your net next batch of circuits get an answer repeat and that can be extremely slow there's other latencies as well so one nice thing here is that you can kind of open up a session and you can have access to that hardware during this iteration and so it's a much more natural and nice way of running these algorithms uh and then i was just going to point out there's currently there's a couple of um so-called pre-built runtime programs that are available one is for vqe and the other is qaoa and so these are kind of um they're more like ready to go kind of so to speak and you can just plug in your inputs and run them um but i think these are sort of these are like the original versions of what jesse was showing you now with these like estimator and sampler primitives and this idea of sessions these were kind of like the original versions of this and i think um jesse you're saying these might be moved out somewhere later um yeah so so um initially we built these pre-built programs because they're easy for people to use you can just plug in your routers but the issue with these is that they do not scale well classically um so i think we the plans to keep them but move them into like a proper cloud environment um so now they can scale better scale out better yep okay so then i will switch over to some slides here or some a notebook slide uh i can find my cursor so i'm going to show you guys how to um build it's kind of like what jesse showed with this opening up a sampler or an estimator and doing some kind of iterative procedure but here i'll show you how to do that using classical optimizers so kind of a little bit more of the standard framework of a variational quantum algorithm so i'll show you the mechanics of how to set that up and then the exercise will be to take this as a starting point and see if you can input either a chemistry problem as the input or a combinatorial optimization problem you guys can choose which one you want to do but i'll show you this is kind of the mechanics of how you set it up and then you kind of just change what problem you want to put into it so the the sort of flow of this is you want to define a hamiltonian that represents your system so maybe it represents a molecule maybe it represents a material maybe some kind of max cut graph something else and then you want to pick a quantum circuit that you think will be a good guess for the ground state and that circuit will have parameters in it that you will train to to try and find the ground state and so you set up this optimization loop between the energy of that hamiltonian and your classical optimizer that updates the parameters in your circuit and then at the end you can retrieve the result which is an approximation to the ground state all right okay so in this it's really really simple example i'm going to build a four qubit operator that's diagonal so it just has two poly z operators on the first two qubits and then the identity on the on the second two so this will represent our hamiltonian it's very simple and for for um to to pass this into the kisket runtime framework we have to represent this as a sparse poly operator um jesse showed this a little bit before too but um and then we have to turn it into a list and so it when it's in that format then we can pass it to um kiskee runtime and this coefficient here just means that i have one term like the coefficient in front of my zzii operator is just one okay so since it's a really simple hamiltonian there's one term and they're polyz operators you can just look at it and know that the minimum energy is minus one because of the eigenvalues of the polyz operator so this is what we expect to get when we run this iterative procedure so the next thing is to pick a a good guess for the ground state wave function or onsots i'm you can pick anything you want but here i picked what's called efficient su-2 and it's just called that because it's good for the noisy quantum hardware it gives you short depth circuits so that's why it's called efficient su-2 but um so it's composed of two layers of single cubic gates that are parametrized we have theta zero to theta seven and then we have a little ladder of c naught gates and then we have another set of single cubic gates that are also parameterized and so what this algorithm will do is we'll initialize these thetas to some random value or whatever value you care about and then we'll use our classical optimizer to tune the values of those thetas at each iteration and then at the end this circuit should be a good representation of the ground state all right so because we're interested in the energy of this system we'll use the estimator primitive because estimator lets us do expectation values of operators and then we want to pick a classical optimizer to adjust the circuit parameters and then we'll define a callback function which just lets us store all the information that we get as part of the optimization process hopefully you guys can see that all right so we um we set up a kisket runtime service oh so a lot of us you already saw from jesse but um we started a kisket runtime service we want to tell it what back end we want to use we're going to use the chasm simulator and then i'm going to use an optimizer that requires the following callback function structure so it'll have and if you're interested in this i can link to the documentation but it'll tell you the number of function evaluations optimizer makes points is the set of circuit parameters that it gets at each iteration fvals is the value of the cost function at each iteration updates is something step size i think it's yeah how large the step size is at each iteration of the optimization and accepted was whether you're essentially whether your cost went down or not and if it went down that's good okay now we set up the estimator primitive so this should look pretty similar from before we um set up an estimator we pass it the circuit that we just built that is our guess for the ground state we pass in our operator which is just that diagonal hamiltonian we pass at the options which was the back end and then the service and then what we do is we can define a function that explicitly calculates the expectation uh value given um an input x which is our list of parameter values so all that we do is we this is really just like a simple wrapper where you're you're um you're making a call to the estimator you're telling it uh this is my ground state onsot this is my hamiltonian and then these are the parameters that i'm going to pass to my circuit um for this iteration and then it will return the results of that for you and then this is just the callback function which i kind of went through before that has the five different inputs um that and it will store these in the history dictionary during the optimization okay and then the other thing we can do is we can tell the optimizer to initialize the parameters at some value so i'm just making them random here so there's you can get from the circuit object the number of parameters in your circuit with this method num parameters and so i'm just saying like initialize a an array of random values for this circuit and then i'm going to use an optimizer called spsa which is available in kiskit if you want to know about the details of that we can chat about it but maybe for now i'll skip over it but this the inputs to this is oh right so spsa the first argument is to tell it how many iterations you want it to take so i am telling it to do this for 20 iterations and then i'm passing it this callback function so that it stores the information for me okay once you have the optimizer set up then i guess if you're familiar with sci-pi optimizers this would look familiar to you we just call optimizer.minimize you pass it the function that you're trying to minimize which is our evaluate expectation and then you can pass it this initial point this x0 and then you run this cell and it will optimize that expectation value for you okay just zoom out a little but so i'm not going to run this because it was a little slow but um i ran it for 20 iterations if i zoom out it got down to minus one so if you remember that was what we expected the answer to be um so that's good then we can just check from our callback our history dictionary we can pull out the the objective function values and i'm just grabbing the last one and comparing it to the target energy so it did what it should which is great and then you can also from the callback get other stuff actually this is not from the callback this is from the results object itself so i can say result.x and x will give me what the optimal set of circuit parameters were for this optimization so these are all the parameters that are encoded into the circuit okay so that was kind of all i was going to say about this for the exercise essentially what you would do is you would start with this kind of basic framework where you have an estimator and you tell it you set up a classical optimizer you pass it your objective function and you and you optimize the only difference is that instead of passing it just some really simple hamiltonian that i did here you can see if you can set up um an operator corresponding to a chemistry problem or an optimization problem so i will show you those quickly so there's two files in the part three um in the docs tutorials folder there's several files that start with part three um the two exercises are the ones that say chemistry exercise and optimization exercise so for the chemistry one um i give you some hints here so the the problem is suppose you're given the h2 molecule find and then the two hydrogen atoms or some distance apart try to find the ground state energy of that and so this is the link to the generic vqa that i just walked you through so you can see how to use the estimator primitive and then if you're interested in how to set up the h2 molecule problem there's a tutorial here from kiskit nature that shows you how to do it and then i give you some suggestions for different kind of functionality that you might want to try to use and then for the optimization problem it's the same thing but instead of a chemistry problem you can set up a binary optimization problem so i give you a few examples of things you could try so there is a the weight matrix here of a four node max cut so you could start with that if you want or you could just start directly with this cubo that i defined with three binary variables and in either case um there i linked to a couple of tutorials on how to set up if you're starting from the weight matrix or from the cubo how you set up those problems and transform them into hamiltonians and then i give you a few other hints about maybe good optimizers to try and things like that this is probably a pretty challenging exercise i'll admit so um definitely feel free to like look at it later too um and there this i guess you merge the solutions right so there's solution or there's a solution um for both of these in in the tutorial folder so you can they're just ways that i solved them but i'm sure you could do other ways of doing it so you can take a look at that as well but yeah any questions on that all right godspeed i guess we have what 35 minutes left roughly um so yeah thanks for making it all the way through almost okay so this is the the file part 3 optimization exercise solution so and you can definitely take a look at these later that they're pretty long so um i'm not going to go through all this in a ton of detail but i'll just kind of skim through some of it so i'll show you the one where we're given this cubo and we want to minimize this function so what i will do this one was maybe a little trickier because you have to use two primitives use the estimator to to do the minimization and then at the end what you want to do is use the sampler to sample bit strings from that solution to get the actual answer to your optimization problem okay so i'll skim down here all right so if you kind of scroll like halfway down the solution notebook there's one for this cubo specifically if you guys can see that um but okay so within the kisket optimization uh module there's a bunch of functionality for dealing with these cubos there's something a class called the quadratic program and that lets you so if if anyone here is familiar with doc plex it lets you build optimization programs in the same sort of way that you would do that with doc plex so here we're going to build a quadratic program and then what we can do is we add our binary variables to that program so here i just have three of them x y and z and then i can tell it that i want to minimize this program and then i tell it what linear and quadratic terms i have and so i can pass it the list of the coefficients in that original cubo minimization expression that i had at the beginning and then i can pass it this dictionary for the quadratic uh like the cross terms with their coefficients and then you can print it out and look at it so this is the original cubo that we that i specified at the beginning and then here we're just regenerating that but in this quadratic program format so then once you have this what you want to do is you want to take [Music] these binary variables and this kind of expression here and you want to convert it into an ising type hamiltonian so what that means is you take these binary variables that are 0 1 you map them to kind of like spins like plus and minus 1 which you can do with the poly z operator and then you should get an expression that is a function of terms linear and quadratic in poly z operators so the way that you do that is you can just call this two ising method of the quadratic program and it will give you the qubit operator now in qubit form and then if you have sort of like a constant shift of your hamiltonian it will give you that so you can print it out the operator here is on three qubits and here's the coefficients it's a completely diagonal operator which it should be since we're doing a classical optimization problem in the standard way and then once so now we you can kind of just think of this like forget that it was even an optimization problem all you have is an operator and you want to minimize its expectation value so here i'll skip over this but here i'm just solving using a numpy minimum eigensolver like i'm solving exactly for the ground state so we know what we should get and so it's saying that um the minimum objective function value is minus two and then the solution for the the binary variables is 0 1 0. so that's kind of what we would expect to see with our quantum approach so then what we do is we have an izing hamiltonian but we have to convert it into this poly sum up form for kisket runtime so there's just a little like kind of pre-processing step here where we take our qubit operator and we pull out the coefficients and we plug it into this polysome op object you can look up all the details this later but it's just a little pre-processing and it puts it in the right format for kisskit runtime so this is just our original hamiltonian but slightly different format and then all this stuff is kind of the same as what you saw before we can set up a kisket runtime service uh pick a simulator or maybe later you want to try real hardware um if you have lots of time and we can pick a circuit for our ground state i'm going to pick real amplitudes um there's good reasons why you might want to use this but you could try other things too and this is what our circuit looks like so it has nine parameters in it and then this is the same block of code you guys have seen several times now so we this is our callback dictionary but we we set up an estimator pass it the circuit the operators all the other good stuff we tell it we make this little wrapper function that just does the expectation value call for us and then we set up a classical optimizer and then minimize that function and so if you run this it'll take just a little bit but you can run it and at the end you get a results object so i can pull out here the final set of circuit parameters that it found and so at this point what you've done essentially is you've minimized the energy of that hamiltonian and you're given what were the set of parameters in my circuit that that produces that minimum energy but what you really want is the solution to your original cubo problem so these binary variables that represent your classical optimization problem so what you can do is since you know what these optimized circuit parameters are you can plug them into your circuit that you set up and then you just sample from it and you what you should find is that those samples that you get you can say okay like say i measure the bit string 0 0 1 what is the objective function of my original problem for that set of binary variables and you can kind of sweep over all the possible bit strings that you sampled and you can pull out which is the one that gives you the best objective function value and hopefully it's also the most probable so that's where the sampler comes in is we want to sample those bit strings so here you have to remember to add in a measurement to your circuit otherwise sampler won't know what to do so you pass it your circuit with the measurement attached to the end and like how jesse showed you call the sampler with your circuits these are the optimized parameter values that our estimator found and at the end we get this dictionary of probabilities and here i'm showing the convergence of the algorithm and then on the right is a histogram of the different bit strings i sampled so pretty much there's like a little bit on this bit string and a little bit on this one but mostly you get 0 1 0 which as you might recall was the correct answer to this problem but then what we can do to actually verify that is a little post-processing here you can look at it later but essentially all i'm doing is i'm i'm iterating over the samples those bit strings and i'm plugging them into our original quadratic program and saying what is the value of my cost function for those bit strings and then at the end i get this list that i've sorted according to best objective function value so you'll see the bit string 0 1 0 corresponds to objective function value minus 2 with probability 99 and so all you want to do is you want to pick what was the best objective function value which is this top one so 0 1 0 and i can just kind of print that out and i get it with nearly 100 probability which is uh ideal so that's kind of the like main flow of it it's the same for the weight matrix one which i also show in this notebook and then the chemistry one is a little bit more chemistry heavy so if you're a chemist by chance maybe you'll enjoy setting up that part of it but the flow is kind of it's pretty much the same except no sampler at the end all right well with two minutes to spare should we call it a wrap all right um i guess there's a slack channel so if you guys have other questions later feel free to ping us i don't know how long that will be up for but 600.
14,Dask Tutorial: The PyData Ecosystem,https://www.youtube.com/watch?v=J0NcbvkYPoE,right i think we're probably kind of coming towards the end of this feel free to like keep if you're still going through these uh you know keep keep playing around while i'm talking if you like um i'm going to run through the rest of this notebook and then i might go off on a couple of tangents just talk a bit more about the dashboard and then when we get to the bottom of this notebook we'll probably have like a 15 minute like rest break you know go and there's coffee and stuff outside um so let's go through the like answers to these exercises if you've been running through filling these in directly so the first one was how many rows are there in our data set so this is effectively doing a length of our our data frame um and you can do lend edf on that oh my candle's dead i won't run those cells no i will let's restart this sorry i probably just need that no need my imports [Music] okay sorry i think i'm about that so yeah the first one was doing a length of the data frame see how many rows there were not quite sure why that is turned there we go so we'll talk a bit about the dashboard in a second i think because it feels like a good time to talk about that so the next one was uh how many non-canceled flights were taken so you can do this with like a standard selector in python you see square brackets and then we want uh the inverse of the cancelled column and again taking a length of that so if you are familiar with pandas this kind of stuff should feel really familiar right but you can just see the dusk is breaking this down into all these chunks underneath next one in total how many non-cancer flights were taken from each airport so again we've got like a much larger panda style operation it's using that kind of documentation chaining of operations here so you're taking this not cancelled uh data frame grouping it by the origin taking that origin uh series doing a count and then uh you know this all of this up to this point would be this kind of lazy operation that would create a task graph and then sit and do nothing it doesn't know if you want to do it so we've got this dot compute on the end just to kind of make that make that happen for you next is what was the average departure delay from each airport so again it's it's doing a group by taking a column and taking a mean um my flight was very delayed yesterday so maybe it's in this data set next one is what day of the week was the worst average departure delay so you can use the idx max we have like really strong coverage of the pandas api there are like a few things missing i should try and find one of my colleagues in in wrappers slight tangent we have like i'm doing a tutorial tomorrow about like gpu accelerated compute and we have like a gpu pandasy kind of thing it plays really nicely with all of this so if you're interested in that you should come to that um but we that library is implementing the pandas api and dasc is implementing the pandas api and we started creating some nice visualizations recently of like api overlap to see kind of how much of the coverage we have so we have like these kind of status boards that we can keep on top of and kind of see you know how much are we moving towards like 100 api coverage so i'll when i swap over and somebody else talks maybe i'll dig out those links you might find that interesting and then the last one is let's say the distance column is erroneous and you need to add one to all the values how would you do that so we can use oh my candle died again i don't know why my candle keeps dying it's not doing anything okay i'll go through and rerun that again in a sec but um we're using dot apply here right so we can pass our data frame functions there's a little lambda function in here we're passing it a function to map across our code and again das can kind of handle just breaking that out i'm running that across so i'm gonna let me restart my notebook and kind of come back from the beginning and then we'll move on to the next the next bit actually no before we go into the next bit let's do some more dashboard stuff because i think folks i've seen a lot of you kind of setting up the yeah sorry my camera's completely gone oh dear oh sad oh really okay okay the whole bind is gone yeah let me restart my binder um yeah that's a good idea is it it's undistributed right is it dark gosh yeah okay so let's talk about the dashboard a bit so i think you've probably seen up until this point that you've got kind of standard python code that you're writing that's following these apis that you use and then you've got this kind of slightly more abstract dash cluster thing right that we're creating in various ways we're going to create it in a bunch of different ways but we have like this client we have the scheduler we have these workers and we're kind of giving it code and data and it's doing stuff and then when it's done it's giving us results back and so that can be strange especially when you're in a situation where you run some code you might have found it in the first notebook actually you run one of those cells and it just hangs right and you get a little star in jupiter and you're like what's happening what is my cluster doing is it okay is it happy is it healthy like you've got this added layer of complexity and this abstraction on top and so i think it's important to have a good overview of what's going on so we both have a dashboard which we'll kind of dig into like i think i think i'll go through like a whole bunch of the plots so we can see all sorts about what's going on but we also have other like ui elements where we're trying to expose information to the user so the first one kind of showing up here is we have these like uh html records so if you're using das in jupiter if you create a client object if you create cluster objects they'll give you a bunch of information about the cluster they'll give you information about the workers you can click where it says cluster info and it like drops down and expands you can see the scheduler you can see the workers i don't know if there's not more of that but you can get like a bunch of information about what's going like what kind of cluster it gave you which is really helpful but one of the most helpful things that gives you the link to the dashboard now when you run the dash scheduler it listens on port 8786 for like desk related things it's like a tcp port and then it listens on port 8787 with this web server that's running the dashboard so das always starts these two ports and a lot of deployment around ask is just like how do i make these how do i get to these two ports because i care a lot about these so it'll try and do its best to like figure out where it is if you're just running locally it'll just give you localhost if you're deploying on like a more complex environment often this link will reflect the like current location apologies the binder one was a bit broken um when you click that link it opens up the dashboard well i'm back into jupiter labs let me just run a bit in here and get i'll get the actual dashboard alive again yeah yeah okay so i said you can click this cluster info and you can see all the information so the the client connects to a cluster which connects to a scheduler which connects to workers and you can kind of drill all the way down and see everything you want to know about the cluster um but the main thing i want to show you here is the the scheduler has a dashboard and we'll talk a bunch about that the client object tells you where that is the cluster object tells you where that is the scheduler object tells you where that is and then also the workers have a dashboard as well so you can see on like a worker by worker level what is this worker doing what tasks does it have what is the state and you can click these links and and view those now if i pop open the dashboard again in here i'm having to change things because you know binders is proxying things for us and i have to go through that proxy to view stuff but we basically you know we have this website that you can go into for your dash cluster and poke around and see what's going on there's also some routes on here that kind of describe these all the graphs i have these are all the different plots that are available and this is what jupiter lab and the das lab extension is kind of plugging into here so if you know if we if we give this url here it's gone and got the list of what are all my plots and i can kind of grab those and rearrange those and do whatever i want um but rather than opening a million plots in in jupiter lab i'm just going to go through the web page but just so you know this is there is like a one to one you can get all of the plots uh either way the website also gives you like these ad some aggregated view pages so this is like the main status page it's got the task stream it's got the progress bars it's got a number of tasks and memory and kind of like a high level view onto what is my cluster doing often this is like a first point of call of i'm gonna go and run some das code and i'm gonna look at this to make sure that that i am asking right my code is actually running on the cluster stuff is happening but then once you've kind of got that reassurance that things haven't hung and you are just waiting for your computation um then you can start drilling around in this dashboard so let's have a look let's run a let's run a thing did i miss then right there we go so we can see the dashboard doing some stuff now we can see up at the top we've got the total memory being tracked by the cluster um and the amount of bytes being stored per worker it's not super clear here but you can see we're tracking like a various memory statistics so we're tracking the data that das knows about right these are the kind of inputs and outputs of those functions das keeps track of that memory if it starts running out of memory on that machine it will start swapping that to disk automatically like das will do that before the operating system even kicks in and starts doing that and you'll start seeing these bars go orange and that means that oh i'm shuffling stuff onto disk we're not going to have out of memory errors things are probably just going to slow down depending on the speed of the disks that it's swapping out to but you can also see we're tracking unmanaged memory so this is kind of stuff that's happening outside of desks view that maybe you've submitted a function that function is allocating some data by creating an object that's maybe not immediately returned or you know it's causing some kind of side effect that's creating some data outside of das that can mean that we start running into issues we're trying to track all of this memory so that we can still intelligently swap but occasionally like you can have like leaky functions still that start creating memory that we can't clean up or we can't keep track of them so being able to just come here if you see out of memory areas or if you see anything along those lines this is a good place to come and see when i run my code do i get this oh it's gone again on my thing you get this little faint blue bar that starts to grow on the end of this which is unmanaged memory that's one of the like my favorite uses of this plot that's really helpful to see what is happening in terms of ram usage um you can you know you got all your bars about how much cpu utilization and how many tasks are processing and everything which is which is useful and then you've got the larger task stream here so there's like a few different colors that are like worth knowing about within this so the plot for the tasks is just using like a standard color scheme so i think it's just using various underneath um and so your tasks would just be assigned a different color you can kind of get a visual overview of oh this is the read csv task this is the select you know count this is whatever but also when the workers move data across the network between each other you'll see red blocks on there so you can see um i did a length so it would have done like a length on each partition and then an aggregate length and in order to do that aggregate length all the workers had to pass that memory to one worker to aggregate them you can see the little red stripe at the end which is just this is network access we're moving some memory around if you run some work and it starts going really really red that's bad the workers are getting really chatty they're moving a lot of memory around um it's a it's a point where you need to start thinking about like can i do this in a way where i don't need as much communication is there a bug in dusk is there something wrong with the collection is that you know you shouldn't be seeing too much red there we also see when you start moving on using um more high performance networking that these kind of shrink down which is which is great and i'll talk about that in the deployment bit a bit later the other color you'll see orange in here you'll see orange blocks as it's swapping to disk that's kind of an operation the worker has to wait for stuff to move to disk has to wait for stuff to come back so you'll see orange chunks coming in there if you are reaching your memory limits of each worker and then the big scary one is when you get a big black bar and that means that the worker had an exception that it couldn't progress from and it's probably stopped and you probably got like a worker killed exception or something more sinister coming up in your notebook but you'll get like a big blank bar of the work tried to do this task and it couldn't and i think let me just um i might just do client so all right [Music] yeah big black bar i tried to import a library that didn't exist on the worker there is no way for the worker to progress at this point um so this is the kind of thing where you need to come back and have a look um i think if i probably get the result of this future i'll get the rest of the trace back from that uh no i didn't no oh i didn't even look at the store anyway i'm kind of flitting about a bit but this is this is a good place to come see okay something's gone really wrong i've got a big light bar i need to go and have a look at why has my worker died we have quite a useful documentation page on um to ask worker why killed something along those lines um this one might have made workers die yeah so this page kind of talks through standard situations um the most typical one is is that your condo environment or your python environment is not consistent between where your client is and where your workers are and so you're probably importing packages wrong versions and missing packages those kind of things but there's also um you know various reasons that that can happen this is a useful page if you start seeing black bars and worker code exceptions the other thing we can see is we can see the workers themselves we can click this workers tab we get a list of our workers we can see what their cpu utilization is we can see what their memory usage is this is really useful especially when you're deploying onto like remote clusters where you don't necessarily have on your local machine you can open up the task manager or do top or whatever on remote clusters this is a great place to come and see your overall cpu utilization memory utilization etc and we also have a tasks page this is just the task stream uh from before is this can you get the task groups on this one or is that oh yeah i'm jumping ahead okay i'll come to that we also have a system page this is showing us the system utilization of the scheduler itself um so if you want to know if the scheduler is becoming overwhelmed um the scheduler works well with graphs that kind of up towards like a million tasks and when you start reaching beyond that that might just be too much for the scheduler itself to handle so you can start to come in here and see how the schedule is overloaded it's kind of backing up it's not giving enough work to the workers that kind of thing the profile is i would argue the most useful page in the dashboard in that it gives you kind of some diagnostic information into the code that you run this is a flame plot of the code that we ran this is probably the length that i did and it's kind of whatever the last graph that it executed it's got all the timings because the workers are executing those functions it has the timings of all of those function calls it will feed all of that back to the scheduler so you can come in here and see what is taking the time when i run this graph it takes 40 seconds to run through the graph where is time being spent what is going on and you can kind of hover over and see oh this is our read csv here um you can see we're doing an open file uh you know this is happening within uh fs spec and then here we've got our this would be oh no this is the read oh yeah so this this first one is to open the file and i think because we're reading from like a remote file system this took a little while and then we have our pandas read uh here which is doing our vcsp it should be read csv yeah so you can see this is kind of what's taking the time and as you kind of drill up you can drill in and see what is what causes this function making um you kind of get a good view of what's going on there's probably our length at the end here yeah safe size of this is really useful to come in and see i've got this really long block here what is this what's taking the time can i tune my code um to kind of work better in this situation so i you know i think when you're really trying to squeeze performance out of this and trying to how to speed things up this is a good place to come to see what's taking the time so we also have uh this page which i should probably do a more interesting graph to make it do something better okay shall i persist like one of these we haven't we haven't talked about persist yet haven't we okay right i won't i won't preamp to persist um but yeah okay so we can see here we have our um this is a high level view of the graph that we made right so if i call dot visualize on that object we would see that kind of inline in the notebook view this is our our task groups this is a high-level view of what's going on so these are the things that i've created um should there be text on these yeah okay okay okay we should fix that but this is this is giving us like a high-level view so you know in the graph well let's look at the actual graph right so we have this graph page where we can see the graph itself this is the same kind of thing that we saw plotted out with graph biz but just kind of on its side in bokeh kind of got our btsb tasks down the sides and then our kind of you know our aggregation steps as that graph comes together but then this groups page shows you still each one of those layers right you've got our vcsv layer and our length layer and our you know final result um i think it's had a group buy in it as well right this one that i was doing and so this is just showing us the individual blocks it's showing you like the workflow as a diagram um we'll have a look into why this isn't quite displaying properly but um this is a good place to come in and just see what kind of graphs have i created what is the scheduler working on at the moment um you can see we've got uh the statuses of each of these tasks so this one is you know some stuff's being held in read this one's aired for some reason but um yeah these are these are useful graphs you're getting an overview of what you're running and then we also have like this other info page now that i this i find it's most useful to like jump to the worker dashboards from here um i don't think that if i click them they're probably not gonna work in bokeh in in binder um but it is a very similar visualization to this you can see what's going on in the worker you can get like a whole bunch of stats about the worker and then we have this more list now this is kind of all of the individual plots that you find in jupiter um that you can kind of click and drag and move around you can also get them in this dashboard and open them as like full pages if you want to just open a bunch of windows and rearrange those right you're not limited to just using jupiter you can use this however however you see fit um but you know we if we want to just open the graph graph in one panel we can do we've got these kind of individual pages here yeah so like work and memory we can open up a bunch of panes and view these if if we're not in a jupyter environment so hopefully that's kind of giving you a little bit more insight into into what the dashboard can kind of give you and the powers of it and this is why through this whole tutorial we're kind of doing das distributed um which is not always necessary das does work you don't have to create the client you can just use task it will use like a little threaded scheduler but you don't get the benefit of this dashboard so i tend to advise to everybody use distributed either way the performance is is roughly the same and you get these kind of added benefits here so let's dive back into the more data frame type stuff we'll run through in this notebook and then as i said we'll do like a rest break at the end go and get some coffee so when we is this the persister that i preempted no okay okay um as we were running through this notebook we kept running through the same chunks of the graph over and over so sorry i'm just trying to remember where i am in this notebook but yeah no that's fine we're running the same things over and over as we're running through this notebook um but really we kind of want to be storing the outputs of these things um before feeding into the inputs so when we've been like chaining stuff together let's find one of the big chaining things together we've got this data frame that we're calling a group by on and then selecting a column and then doing a mean each one of these intermediate steps will give you this like lazy dash object that's not been computed yet it's not been evaluated like yeah and we can store those as we're going through so here if we just take the um not cancelled flights and we can assign that to a variable not cancelled and then we can use that later on and build more on top of that so then if we're going to select the delay column and do a mean on that we can assign that operation to mean delay and then we could do you know standard deviation as well and if we run this that cell ran almost instantly because these are all kind of lazy objects these are all lazy variables that i've got um stored and then you know i can i can see what it's like to actually run through and compute those go to the graph again i think i've maybe not run all the cells on my notebook and that's why it's not happy yeah there we go right so this you know actually running through and doing that calculation took the time in that operation um the other thing we can do is if we've got these kind of lazy objects we can call dot compute as we as we have done but we can also use the das client um to compute that we actually use the dash api here but instead of saying dot compute of this thing we say compute please can you compute these things for us right so we have a dash dot compute call here and it's kind of a way that we don't have to wait for each one sequentially we have to call compute wait for it to finish and then call compute on the next one wait for it to finish we can just say das compute please compute the things um and it will run that so if we run this one you know that ran faster it was able to do more concurrency because it's trying to evaluate two um delayed uh two lazy dash objects at the same time so it takes kind of roughly half the time um and this is because the graphs were actually merged together at the point of execution and because there's kind of common operations in there we can reuse those as well we can uh visualize all of that together or this is using the new visualization engine so we do have uh this is like using an experimental we have this uh cytoscape engine that is not using graphics it's using a different engine and this is showing us as merging those two together um oh yeah yeah yeah so i have to like do two finger drags and stuff but oh you can oh nice i've not played with this much yet actually because the the graphs that we've seen so far just using used graphics and they just spit out a png or an svg right and we were kind of doing two levels of zoom you just click on it to make it full size or not whereas this is using the the new cytoscape engine where you can actually get the graph and kind of zoom and plot and drag and move stuff around so ah yeah okay so persist so as well as running through you know so far we've made some lazy objects we've created variables that point to those lazy objects and then we've told das to evaluate those and that gives us the result right but then the cluster kind of forgets everything that it did right there's kind of run through all these operations the operations have got these intermediate steps and all that intermediate data kind of evaporates as it moves on it doesn't need to keep track of any of that but sometimes it is valuable to you to say i want you to run up to this point and then store that result and because i'm going to build on top of that but i'm going to you know i'm going to tweak a cell and run it and i'm going to tweak a cell and run it and we can sell and run it and i don't want to do the load every single time and you know i've maybe done a filter or something at that point you can you might want to just say okay get all the way to here and then pause keep this stuff and then i'm going to kind of iterate on top of that so you're almost like extending the graph on in various ways and to do that we use dot persist this is kind of the counterpart to dot compute instead of saying run all the way through this and give me the answer back you're saying run all the way through this and remember the answer for me so if i take uh you know again we'll take our not cancelled flights um and filter by flights from jfk we could do like a mean and a sum on each of these and this is going to run all the way through that graph i should reopen actually there maybe i should open the graph one here so you can kind of see oh is that my that's my random future isn't it that i had hanging around sorry let me just try and delete that they call it foo hey that's it there okay what was that that was fruit ah jupiter holds on to references to things sometimes i think i can't get rid of that one but if i run uh through these operations that was very fast but it made two separate graphs right each one of these this mean and this sum is built on top of this operation has already happened right we've already got this inversion we've got a filter you know some filtering and then we've got a selector of you know like a where here and so doing mean and sums when start to finish on that graph both times and it took you know half a second to do so but what we could do is we could take um just the results this jfk results and we can say persist right so instead of instead of having a pointer to a graph that would run this thing we have a pointer to a graph that would run this thing plus the intermediate result of that so if i run this cell you can see the graph has popped up it's done this first operation and now it's sitting and waiting the workers have all of this data in memory now obviously this does eat into your memory of your cluster you have to be cautious about not running out of memory if you have like lots of different steps persisted but now if i do my mean and standard deviation it goes much faster because it hasn't had to recompute this chunk of the graph right it's just building on top of that graph each time which is much faster so yeah okay so we'll move on a bit now into um kind of running your code with these data frames and how to like kind of this is starting to peak under the hood of how these things work and and how to kind of access these these objects underneath this abstraction layer that we're providing um das doesn't cover the full api like i said i'll try and find those pages that show how much api we do cover but there are just things that will be never covered they don't necessarily make sense in terms of how we can implement them they're not necessarily distributable and that api is also just huge and it moves fast and like pandas is this vibrant amazing big project and it's hard for us to kind of keep everything up to date so sometimes we just say this is not implemented and it's not going to be implemented and if you want that feature you can kind of do it yourself and these are the building blocks the bits and pieces you need to add um yourself or you may have some specific nuance to how you want something to be done and so you can you can do that yourself um and so by doing that we have a few different methods that we can call on the data frame which allow you to effectively map a function over the like layer below so you know this task data frame is many pandas data frames um and our operations are kind of running on top of all of those but we do have this map partitions function where you can basically just pass the ask a function and say on each partition please run this function and so you're kind of operating on all of your chunks if you've got 10 pounds of data frames that make up your overall data frame that would just be run you know on on each of those pieces and that can be a really nice way for you to kind of express your own custom logic passing things through um into here so here's an example let me shrink my graph down a little bit so we might have uh here where we have this custom converter where we're going to take our df this is our pandas partition and we're going to multiply it by a specific number so if we have a series here just making the matter here is that this matter is because we have to tell it what the d type is isn't it yeah so because all of this is happening lazily it may not know what our d type is um so we have this kind of meta option that you can pass in a data frame that looks like your data frame um and just can give it this this metadata so um we're set in the d type here so we're going to take our distance series and we're going to call map partitions and map our custom converter over that and we're going to say i want you to multiply everything in in here by 0.6 here's the meta object so you know what it looks like and we'll get back this uh you know distance km object if we look at the visualization for that uh does that go bigger yes we can see in the graph we've got our vtsb we've got our get item which is selecting our column and then we have our function right my custom converter we're passing this um this function straight through this is kind of how you append the graph yourself if you want to do stuff manually at this point and we can call head on on that which will show us um what the result is i preempted that so yeah there's more information about what that what that meta does at the end there there are also some other functions uh up here that are kind of mentioned so as well as doing map partitions you can do map overlap where you kind of have uh overlapping rows from neighboring partitions if you you know if the boundaries cause you a problem in what you want to do you need to read a few either side and we also have a reduction if you want to do custom reductions where you're doing those aggregations yourself so with all of this you can load in your big distributed data frame and then you can write really customer bespoke stuff if we don't have the pandas api compatibility or you want to do something um very specific there so yep that's the end of the data frame section so thank you all for being awake still which is great it's yeah any questions go for it wow uh yeah yeah sure so the question was there's a lot of similarities here with pi spark what are the kind of differences and what are the pros and cons to using each one so yeah sure yeah yeah absolutely so yes does do that that's kind of does all the same kind of things that you would you would get in that okay i think the deployment section might kind of clear some of that up you can also run disc um you know if you have a hadoop cluster with yarn installed that you might be using for spark or whatever das can also leverage that same hardware so if you've got that you can try out you can try out that um there's a great documentation page here comparing das to spark there's like a whole lot of stuff in here that you can kind of read in your own time but the few things i would say is dasc is like this low-level building block for building out distributed python it's written in python for doing python stuff so we have all this data frame stuff which has a lot of overlap with pi spark um but also we have this whole like array implementation and we have all like all these other things that have kind of been built out from this kind of ground point so there's a lot of stuff that you can do in dusk that you can't do in spark um but there's also you know spark is very well tuned and very you know optimized for its use cases and we kind of get it going in that direction i think one of the reasons i when i was working in weather and climate i evaluated dark and spark at the same time and one of the big things for me was just when something goes wrong in my python code i do not want a java error message i don't write java i don't like java i do not want javascript traces i and i'm gonna have to hand this to somebody who is you know i i have a software development background i'm kind of comfortable with these things but i'm now gonna hand this technology over to somebody who their job is is you know geosciences and maybe don't necessarily think of themselves as a programmer programming is just a tool to do the thing that they want to do and the last thing they want to do is be like what is this java stuff pi spark has moved on a lot since then i think you get much more nice error messages now but it's the ecosystem of of das is built within the pi data ecosystem for the pi data ecosystem and it plugs in natively whereas pi spark is kind of this this compatibility layer almost um there's like a whole other bits and pieces in here about these these kind of things but they're both they're both good options um and you know they both got pros and cons i think dasc is kind of going for this more integrated more flexible package um but it does mean that we're trying to add features that have been in spark uh for a long time right like dusk sequel is a relatively young project like spark has had a sequel engine for a long time whereas dusk has only kind of more recently got see like powerful sequel i think like charles when when did the predicate push down stuff go in yeah like a month or two ago das sequel got pretty good push down capabilities so that it could kind of optimize graphs and squash things down that's not been there you know that's been in spark for a long time so we're kind of catching up with some of those things um but it's it's kind of the flexibility of if i want to start with das i can grab my laptop i can install das and that's it i don't need a cluster you know i think going from zero to spark is like a big step and often you know involves getting it folks involved and like there's like a huge amount of setup around getting like a functioning production ready spark cluster and it's hardest to dip your toes into whereas das lets you go from like my laptop to a bigger machine to a couple of machines to a super computer to the cloud to wherever right you can kind of scale from zero up from there so that kind of answers it but please do go and check out this documentation page because it's got like a ton of ton of stuff in there um of course there's any more questions before we take a break sure so that each task worker creates like a local file system space that it can use many for the swapping that i was talking about right when it runs out of memory it will start swapping stuff to disk and the worker has um a space that it does that by default that is just the current working directory it will create some worker directories and it'll start swapping into them i don't know if i can yeah i can see it here um in my little browser thing this is the das worker directory you can see there's some lock files in there um commonly when you when you set up desk again we'll talk about something like the deployment but if you're running on like an hpc you might have some scratch space and temp space that is for that kind of writing and so you would just configure and say workers you should put your stuff over here um that's kind of mostly what that what that is for in terms of you know other than in other than operating on the data on the file system that you're telling it to do like read csv and you know two csv and things it's all kind of for doing this memory management swapping in and out stuff so you can you can use like unix sockets um to communicate between the processors rather than by default all the workers use tcp to communicate and kind of go through the network stack you can use unix sockets if you prefer obviously that restricts you to the machine that you're on but that is a thing as well so you can use the file system for that i mean yeah in in the deployment bit i think we're going to talk about multiple clusters and how things kind of work but the way that we've set things up in the notebook so far is it's launched four desk workers they're communicating with each other in the standard way we're kind of you know we're limited by the binder environment you're all limited by the laptops that are in front of you for within this tutorial but using das feels the same whether you've started one desk worker on your laptop or a dashboard here and a dash worker there everything will operate the same the dashboards are the same it's just the actual deployment of how do i run these things and we'll go through that in the deployment notebook cool okay well let's give it like maybe 10 minutes everybody go and get a coffee and and have a rest break and then we'll come back and and go through the array notebooks this is the like numpy flavored bit of desk off that's well that's loud okay uh so going back to the analogy so imagine that you're building a house and you have to put like on a wall like bunch of bricks and if you have every person bringing one brick and then the the back and forth of bringing that break is going to take a long time while if the foreman says hey why don't you all get like um how you call those stuff that you put things a wheel something will borrow yeah a wheelbarrow so you get a wheelbarrow and then you can bring like more bricks and then you can put them on the wall and then you can go back and forth and so that's kind of like the idea of if you have like a too small of a problem and then what every worker or every person like bringing one break and this is going to take a long time we're going to get it down but it's going to take a long time now 2-bit is also a problem uh if your chunks are too big this is also a problem because you likely will run out of memory the way i want to i like to think about this like maybe it's just like too heavy for that wheelbarrow to occur and then like you're gonna kind of get stuck in there and you're going to kind of have to rest or then like you're not going to be able to to push it all the way um so you will start seeing that the dashboard data has been spilled to disk i was going to start seeing gray on the dashboard on the memory and then uh then it it's kind of like going to take longer time to process that um so in general like there are a couple of rule of thumbs uh like that you can take a look and then see depending on your application what to do but users in general have reported that chunk size smaller than one megabyte it just tends to be too bad it's just too small and it goes a good number is something between 100 and a gigabyte so that's also it's like it's good but if we go over that it tends to be like just overloads and memory and we don't we don't want to be there the other part to take into consideration is like as an upper bound we want to avoid very large task graphs so when we were looking at the number here on the amount of task we don't want that number to be too big so something between ten thousand and hundred thousand it took okay but like after that it might start like being bad um and then as a lower one you want just to get the number of chunks to at least equal the number of workers that you have so if you have like four workers let's just get at least four tasks and then ideally multiple supports that might be good uh if you get way too many then that might be a problem uh and if you have only two then trying to kind of like if you have let's say four and you choose three chunks then there's gonna be one worker that is gonna be sitting there doing nothing so you kind of wanna like take into consideration that and there are like other um considerations that go into trying to align the data and the with with the chunks and how you read that data depending on the format so in general in multiple applications you're not going to be creating your your task arrays right so you will have you will read your data from some format uh usually there are like good formats that are already chunk data uh like czar net cdf tiff and you want to like kind of like check okay how those chunks look like and how my my resources look like and then trying to accommodate for that there's a lot of readings on how to choose a chunks and arrays i will consider you to like if you are in this in a field that deals with arrays go to the documentation um for now we're going to do like a simple example using sar which is a common format for chunk data so well imports are this data should be should be created so if we get here you'll see that i i am reading this data this data is already chunked uh and i'm seeing that okay the chunks are about five megabytes uh and then i'm seeing that i have 33 tasks 32 chunks it's not too big so in this case yeah the chunks are a little small but we're like also working with smaller data because we're in binder i believe like if you're in your local machine this should be a little bit bigger uh you'll see different numbers but notice that yeah this this is the array is like already chunk and we didn't do anything when we read it so let's try to compute the mean and then see how long this takes hopefully won't be too long so it's like 1.46 seconds uh for those in your local machine this should be a bigger and then maybe it might take a little longer um so now let's load a different example oops where on purpose we made this array to to have smaller chunks so i'm having like on the kilobyte kind of size now let's see what happen when i try to compute something that it has super small chunks and now we wait hopefully this won't take too long but the point being here is when we see this kind of problem it's also here we have like 20 000 tasks i had like 20 000 chunks this is kind of like saying like hey this might be too tiny it's like this graph is like way too big uh might not be great actually a good thing would be to have the dashboard looking at this while we do this um i think this is still computing so we can take a look oops can we do it in here oh we need to then what was the okay so this is proxy slash that should be and this might even still computing stuff all right we're doing stuff but see like they're like super tiny and they're like a lot of space in here a lot of red going on this is something that like if you see a dashboard it's red then you might want to think like hey what can we do better this is like a lot of white space and you see like these tests are taking like so small amount of time but then like and then we're doing a new one a new new one and let's just run again the one i want to see can we yeah that one might be sure but let's see how it looks the dashboard on the one that wasn't that small so let's do this one okay so if you see like still some some red over there but it's like this took kind of like it was it was pretty decent um so as a quick exercise let's try to make this array the one that has like a super tiny stuff um let's just provide a chunk size when we read it so something like chunk size equals and then come up with something that will make the computer of this mean look a little bit better on the dashboard and then take a look and then look at the dashboard look at um look at the time this like took one minute and 30 seconds so it's like it's a big big amount of time compared to two seconds so let's do i don't know three four minutes try to come up with something play around a little bit see how how low you can get that number from one minute point 30 seconds to something that is better than that right so the question is are there any public attributes to get the number of tasks do we have it's there right but like can we maybe something like uh let's do it on a dot you mean yeah what if we do length of that 33 yeah that's that's it great question actually you know what all right was anyone able to get it like run faster than if you're in binder that one minute 30 seconds uh but if you're like your local machine it should have been probably around 20 or a little bit less right i there are multiple ways of going about this and then you can you can optimize and then see and then play around one way of doing it is we saw before that the original case had 33 tasks 32 chunks and we sort of kind of have a hint over here on at least how to get it to the time that this first case run so if we the same chunks than the original one we can see what happens so reading this why god okay good fortunes this might be something weird here something happened oh it's because that one the solution i believe it's for the case that you're on your local machine so something to fix i think there we go so if we have our chunk size to be the same size that we have before then let's see how long it takes should be ideally faster uh all right five six seconds and then now we are reading that and then like re-chunking things so then it's not entirely uh the same time um but hopefully with this yeah you have a question yes you so yeah you can use like you can read it and then use rechunk or then when you are reading it you can provide a chunk size and then that will kind of like accommodate things but yeah either way yep fair oh yes absolutely so there was a question uh from over here that uh if it's there's any way of getting the amount of tasks uh but but instead of like just running this and then reading it from it's like by executing basically some code so a.desk will give you the the task graph and then if we do the length on that we'll get in this case was 33 because i was running on a so does that this is this number yeah absolutely yeah is that the number of tasks that the scheduler will be like sending to the workers um so now we play around with a lot of arrays and in in general like we we won't be creating our own arrays and and we're going to be reading from somewhere and in some cases like we will be reading data that is data that has like many dimensions and for for that case there is a good um there is a good library called x-ray that allow us to kind of do this mix between arrays and pandas data frames so recover we can label those dimensions and then work with that uh yeah that would be great i think you know more about it i can go through this but there's gonna be a whole tutorial about this but this is like a quick peek on yeah this is like a little sneak peek at x-ray i'm like i've used x-ray a bunch but i'm not like a super expert but anderson is there who can like interrupt me and tell me that i'm wrong in things that i'm saying which is helpful so yeah but if you're interested in um all the arrays you've seen so far are like just bare arrays of numbers right um but if you have data which has other arrays that describe each of those dimensions like you might have air temperature data like we're going to open in this example where you have latitude and longitude and time and those dimensions have that associated metadata is important right the values of the time access is something so you have an array with other arrays that describe the axes of that and x-ray is a really great tool for working with data in that kind of format i've used a bunch of weather and climate i've seen it used all across geosciences and in other places too so let's import xray we can use the open data central tutorial to open up some some air temperature data um now xray uses das under the hood for its array representation you can use just numpy as well right um but i think by default it will do x-ray on it will use dusk on on data like this so if we open this data we'll pass it some information about how we want it chunked or how the chunks are and we have a look at this x-ray has also got nice representations in jupiter i'll try and make it a bit easier so you can see we have this array with these dimensions latitude longitude and time um we can kind of see all the information we can see the values from these these descriptive arrays and we can see the kind of data variables so this is just a has one variable of air temperature but say you're loading like the output of a weather model you probably have a whole load of variables right wind speeds and temperatures and humidity and all that kind of stuff so you you can have like a whole load of variables here you've got this like high-level description of this data set and what's inside it but kind of as you start digging into it we can see if we just have a look at this air variable it is a dash array underneath right so is leveraging das to do the the numpy like operations um but x-ray gives us like this nice api on top we can see the shape and size of this uh we can see the dash chunks and all that kind of stuff and the tasks this is just a desk object underneath um it's wrapped up in one of x-rays x-rays adding a bit more descriptive stuff but it's just like plopped the dusk representation in the middle there which is neat and we can have a look at you know chunks as you would with a normal array you can use the same operations to kind of inspect down into those but then we can do some kind of more domain aware operations whereas we can say i want to take the mean of the air temperature across the time dimension and because we've got these labels because we've got this descriptive information um we can calculate that mean but again this is a lazy dash operation that's calling down into das and it will give you back a lazy dash object and so this ran very quickly it didn't actually take our our mean and it's showing us in order to calculate the mean um we would have to calculate this this task array so if i the das dashboard still live here let me just open the dashboard plot so we can just see something so if i just call load on this mean oh that was not the most exciting of graphs but you can see it did a task graph operation here and it gives us back the result and this you might recognize is a you know a numpy representation now now that we've computed the result it's kind of filtered it all the way back up but it's still wrapped in this x-ray context right it's still got all this rich metadata um from x-rays perspective now x-ray is built on top of numpy and ask array and gives you you know this view onto multi-dimensional array data sets but i think one of its core goals is to give you a pandas-like experience of working with your data right so you can do things like group buys and selecting and all that kind of filtering stuff that you would do um in pandas but you can now do it on like multi-dimensional data sets you're not working in um you know tabular data here so let's just pull out the a the air data array all right so you have like a data set that contains all of our things and then we have our data array which is each thing within that so just pull out a reference to that there we can do you know we want to group our data by month and then take a mean across time and then we can subtract the uh from the original array and again we build up this task operation we can see like the final result is this one-dimensional uh array and then we can call dot load which is kind of you know calling dot load in x-ray we'll call dot compute um underneath in das if we run that again we can see x-rays doing things there and we get our our 1d array out from there you can see there was a bit of chit chat between workers as they were loading data around moving and doing tasks um but these are all kind of array operations under the hood doing our group by um we could also do like a resample of our data you know the thing we've just calculated we could so we could re-sample it onto like a one-week sample size and we take a mean across time and a standard deviation across time and then we could plot that right x-ray also one of the things i used to find particularly useful when i use this day-to-day is this dot plot function so if you've used dot plot in pandas right it just generates sensible plots for you based on the data you give it obviously you can tweak and configure it and however you like but x-ray has that same ability so if you have an array with scripted metadata you can call dot plot and das will be get busy doing some stuff um and then you'll get out your lip lib okay or however you're configured so that's just like a small taste into how we've started with this like low level desk thing that's doing these kind of out of corporations these lazy operations it allows you to scale and then you're building up to domain specific tooling on top of that if this is interesting if you like the look of this please do go to the x-ray tutorial this afternoon and they'll cover this in in much more detail so yeah that's the end of the array notebook do we want to go straight into the delayed one thanks or okay i double checked too and the the len of the dot desk is is truly the number of tasks for sure for sure um so we just wanted to make sure that it wasn't just one just the top level or something okay all right so moving on to dash delayed is one of the lower level interfaces into desk so we had this analogy of like gasker ray and das data frame being like ready to wear clothes like a pants or a shirt and dastawade is more like the cloth that you can use to like sew your own clothes so it's one of the really powerful but like um a little less like um no i wouldn't it's just like a powerful building block that you can use to construct more advanced functionality in desk um so in that we don't really need the yeah this isn't going to be very intense intensive compute it's more showing how it's set up so if you don't have data if you don't have or if you don't have an array or a data frame maybe your data some other structure that's a little weirder that doesn't naturally fit into one of those paradigms delayed can be really good for you because the rate is much more about the functions rather than about the data so you can use delayed to to decorate functions um to so that you create the task graph as you as you chain together functions or as you um construct these different operations so it's it can be if you have a pre-existing code base that's very function heavy it can be that using das delayed can be a really straightforward way to just parallelize your code base it's a bunch of documentation and let's start running through so here's a typical workflow kind of sketched out that might be good for delayed um so the whole point of it is to process a bunch of files and there's this there's like you read a file um you do a transformation maybe that's like changing the units or whatever twist flipping it or something if it's image data normalizing it in some way and then maybe you write it back out to a different to a different file path so that can all be summarized in one function and then you have this for loop where you're iterating over all your files and you're you're doing this processing to them so this is this if you see if you have this kind of pattern in your code base this can be like one of the things that's really good for dash delayed because you don't have to worry about what the what the data really looks like if you already have the functions that operate on your data um you can just just pop maybe this is even hard to notice but you just pop this dastilade decorator on that wraps your whole function in das basically it's like a desk cloak or something um so here we're going to set up these super simple little functions to just show how this works the mechanics a little bit so this um and if you look at the desktops we do this all the time to show how dastalid works so this is functions that are kind of that are total toys there's an increase function and add function and they both have they both have some sleep in them to just make them take some time so we can see how the timings change on these things but that's not that's just a way of demonstrating um how delayed works so we'll set up these little functions so far these are just normal functions they don't have any dashiness to them they're just regular python functions what's this one restart the kernel refresh the page [Music] all right so we start by let me just get my client okay so we've created these das function or these these simple functions node ask um so to do so now we're going to take these functions we're going to we're going to do use the increase function twice to get an x and a y and then we're going to add them together so and we're going to time this the important part is timing this so that we can see how the timing changes so if you think about this function uh you can ex you can we can develop an expectation right it's going to take three seconds because we're doing an increase it's going to take one second another increase another second and then we're going to add them so it'll be three seconds because the actual operation takes no time um and then in this one we're going to add this das delayed um this decorator and we're going to so that'll make the functions lazy um so it's just constructing a task graph now so we're we created we declared those functions and then here we are um we're setting up the task graph so takes no time basically because we haven't done anything um and then once we called dot compute um we should have set our expectation before so we didn't get the answer revealed but um basically the increase can happen at the exact same time in parallel and then the add happens afterwards so it only is going to take two seconds basically because it has one of the seconds is happening at the same time so we can look at this z and we can see that it's a delayed object so before we've been looking at um data frame objects array objects delayed objects have a lot less metadata that we know about them so we can't have a pretty wrapper like we can't have a pretty representation um because we don't know that much about them um they're just holders basically for the graph um so we can visualize them though and this is one of our little past graphs so you can see the two increases that happen at the same time taking one minute and then the add so we talked about how we went from three to two seconds um you might have noticed that we didn't actually go to two seconds we went to like 2.2 there's some overhead that desk is always going to take to do this setup and scheduling and we can't go down to one second because the ad needs to wait for the increases to be done so you can't parallelize the increase increase at all at exactly the same time because it needs to wait for the inputs okay so this is um just an example of paralyzing a for loop so we have this data and we're going to be running this increase function over all the all of the items of our list and we're going to be using these dash delayed increase and sum functions to parallelize i'm realizing now that i've redeclared the increase in add function so this isn't actually a serial but uh sequential code so this is actually um already um uh uh decorated so it's it's oh no i'm read aquarium here okay okay so this is good so this is a sequential code so it's going to take um whatever 10 some num some 10 plus 2 or something seconds um and then we can get the result okay so we're gonna i guess i'll let you guys um work on setting trying to make this into a dash delayed type thing um so i'll give you maybe two minutes to work on that you probably will that's a hint you're probably going to redeclare the function and if you have questions too that's okay all right i'm gonna go ahead and reveal the solution so all you have to oh the only thing to change really is to add dust away to the increase function um and that should allow your data all your increases to run in parallel um and i should have put a time on this so we could see the difference so you might notice that you don't have to parallelize the sum because all of the inputs to the sum are delayed objects so that'll just work correctly like once you only have to add delayed to functions that are taking like non-das inputs basically um so that takes two seconds down from our eight seconds before all right so this um exercise is to paralyze a for loop where some things are delayed some things are nasty objects and some things are sometimes are necessary to know what to do next so some things you want to happen immediately and some things you want to be lazy um it says so there's some so some functions are really fast to run and it can be useful to do those immediately to determine which ones are to determine which functions to run next that are potentially slower so this is like part of the part of something that you're always kind of thinking about with ask of what do you need to know immediately versus what can wait and that's part of the part of the intuition that you'll develop as you work with task um so uh i can leave you to i'm gonna skip that one because it's less interesting and go right to panda's group by reduction um so you can go back to that one later if you want um but in this one we're going to read some csvs this is like similar to the das data frame section um but we are going to be using dasolaid to use pin as directly so we can look at these data frame these csvs that we've collected and we can use pandas to read one of them so this is all pure panda stuff so in this case uh what we're trying to demonstrate is that in in this case we're using pandas but it doesn't have to be any type of object that has a special relationship with das so you don't we're not using that data frame so this could be any type of object that you might have like some special um objects that you create in your content area or your area of expertise um so we can look at the data types of the pandas object uh when we can sort of we can start stepping through the different things that you can look at about this data like the origin um and you can look at the these are the types of things that we've been doing on the pandas data on the das data frame notebook we can also just do raw on the pandas data frame um and we can so in this example we're going to be going using the pandas data frame iterating over each of the files and getting the group by i'm getting the mean departure delay per airport so this is pure pandas code that we are running and you can see how long it takes so we're going to read in the file we're going to do the group by do all the panda stuff collect up our results add them all together to get the delays per airport and this takes some time for for a desk uh for delayed um there's you could you can use delay to paralyze that code um without using that data frame so that so you can use to ask um so um i guess i'll let you work on this one so we're going to be some things to keep in mind are that you can wrap the functions with dastavade and the output of that function is going to be a delayed object that delayed objects you can then operate on we can do slicing you can do method calls on it and those will collect those will chain together to make a delayed to operate in the right order to have the output also be a delete object and then when you are re when you're done with constructing your graph you can call that compute on it to get the output and if you want to get multiple outputs you can use dash.compute to to get the outputs of like multiple different delayed objects okay so base so this is the sequential code um the key here is really to figure out like which of these functions is important to wrap and dash delayed and which can be left unchanged so in this case the only function that you really need to worry about is the read so you can write a custom function that has the das delayed decorator that reads in the file and once that file is read in using this particular function it'll the output will be a delayed object and anything after that anything you do after that will just operate on the delayed object and that's all you need to do to get this thing to be a desk a das graph because um you can see like after this point um all of the all the next things that we're doing in this um for loop are just methods off that object um if you had some other function that was operating on even if you have some other function that's operating on the objects that should still work um and then you so you can construct the graph and call compute and then you'll do you can visualize it so this is the visualization of the task graph that we've constructed i'm not using the fancy new one so it's just a lot of scrolling um but you can see if you uh scroll enough that you see all the read me all the read files and then the group buys and get adders and all these tasks are how they chain together to eventually get the result so this is more of like a tree style so you'll notice that here i used dash dot compute rather than so when you have a number of of delayed objects to work on you'll use dash.compute the output will be an iterable so here i'm just plucking off the first item in it um and um yeah you can do that when you if you yeah so um so you can look at when you want to call compute and whether you want to call it on the intermediary values um basically anytime that the output is going to be relatively small that's an okay place to call compute or dot persist because then that data will be it for compute's case it'll be brought into memory for persist case it'll um or it'll be brought all the way into your notebook and for persist case it'll be stored in the futures objects that we will talk about more later on um so call when to come out of desk and all the way back into like real evaluated results is something to to sort of tweak as you're if you're developing desk workflows um and here there's just some more questions that you can consider um and then there's a bunch of links to the delayed information and we'll be talking about more of the lower level ways of constructing things with these low-level desk apis in a future notebook but for now let's take a 15-minute break and we'll come back we're going to talk a lot more about deployment and distributed clusters um so hopefully that all the dashboard stuff and all the client creation that we've been doing will make more sense then right let's get back into everybody so uh oh yeah yeah yeah yeah i think i can hear myself i think yeah we're nearly there this is like the home stretch now i appreciate these tutorials along um we've got about 45 minutes left we're gonna dig into uh distributed and and futures after this um so like everything we've done so far has been around using das using the api you know making use of it from like a python perspective and we've kind of we've dug a bit into the dashboard and things i guess but we've glossed over the cluster creation a bunch and now this is the time we're going to kind of come back around and say where are these processors coming from where you know how is this scheduler alive and i think one of the things that we've done in basically every single notebook is we've called this cluster object we've created this cluster object and then we've not really talked about it again i've kind of just moved on with what we're doing so let's like take a step down into the deployment side of things and look what that is so the way that das handles dispatching work to a cluster is by having a client object in existence in the in the global scope so you don't have you know when you call dash dot compute you don't have to tell it where the client is as long as you've created it as long as it exists um and in basically every example so far we've just created that without too much config right we've either just created the client object and instantiated it or maybe some of these options have been doing like n workers equals four or something but we've just created this client and we've not worried too much about it now das tries to help you in these situations if you don't give it too much information to go by so by just creating an empty client it'll immediately go right i'm going to go and connect to a cluster it will check your config and go i don't have any config i don't know if there's any cluster i'm just going to make one and so it'll create a cluster for us just leveraging our whole machine and it'll kind of pipe everything through the the keyword argument actually that we're passing to client the end work is one um the client doesn't know what to do with that so it passes it down to the cluster that it creates for you you can change these defaults uh if you want client to automatically create other cluster types if you like but let's kind of have a look at what that manual process would look like if you were going to create a cluster yourself so instead of just instantiating client we're going to import this object called local cluster and we're going to create one of these before we do anything so we have this class local cluster we're just going to instantiate it and create this cluster object and hopefully the the representation that appears is familiar right this is a lot of these representations we have of cluster objects get nested within each other so this is the cluster level view onto onto a cluster object so we've explicitly said i want to create a local dash cluster on my machine that's going to leverage my whole machine i haven't given that any config i can do um but this means it's going to inspect my environment it's going to see how many cpu cores i've got see how much ram i've got it's going to make some decision about how to carve that up das will create workers which are kind of um contain many kind of threads that can do things and it'll kind of make a decision i think if i run it on this laptop my laptop's got like 12 cpu cores and i think it creates three workers with four cores each um that's just kind of a thing that your dust does to try and balance the best performance it can it's just numbers that we've got from benchmarking and things by creating local clustered inspector environment it'll create a cluster it'll start the workers um we can see the scheduler that gets created here we can see the workers that get created all of this has been been done for us by this local cluster object now because we're seeing the representation in at the cluster level rather than the client level we get some extra fancy widgets here we've got a little scaling tab that we haven't seen before which doesn't do anything because we can't i can't scale my laptop and make it more powerful but as we move on we can see we if we are able to ask for more resource das can do that either manually or or intelligently for you but what i kind of want to show here is this concept of creating a cluster by importing a class and instantiating it is is a theme that comes up all the way through deploying your dash clusters now if we have a look at the the local cluster object we can see we can configure all sorts of things about our local machine we can say how many workers we want and how many threads instead of letting it do auto detecting we can tell it what ip address to listen on what ports it should be using all that kind of stuff around what the scheduler should be doing we can we can set that up here and all of these configuration options that we pass to the cluster manager will vary from cluster to cluster uh cluster manager to cluster manager because you know setting those things explicitly about my laptop doesn't necessarily translate onto i want to launch a cluster on an hpc system there are different options so although we're going to be instantiating classes all over the place um they were often the documentation says like you know insert your configuration keyword arguments here and then it kind of documents what those are often this is like what credentials do i need what um you know if i'm launching on an hpc what size should each job allocation be and all those kind of uh settings if you're launching on the cloud what what vm type do i want which base image do i want to use all sorts of configuration options so check the documentation on each kind of implementation but once we've got a cluster object they they implement the same interface we have this cluster interface which is designed to make your life a bit easier in switching from cluster to cluster it kind of makes life more straightforward when you are doing that i'm going to start on my laptop and then i'm going to scale to a server and i'm going to scale to a cluster and like as you're shifting your workflow you have this cluster object that you can work with and use um and you can interact with it in very similar ways so some of that kind of stuff is we can call get logs on our cluster object and this will give us there's no local logs for this one but if you click on the scheduler this is the kind of standard out from the scheduler process you click on the workers you can see the standard out from the work processes this is really useful for debugging um remember ages ago i was talking about the dashboard showing you big black bars when things go wrong when things go really really wrong these logs are the place to look for the worker that's actually died and really couldn't do anything you'll get like the full python tracebacks in these logs this is a great place to come and find that information and you can kind of drill through through all of those the cluster's also got information about you know where is the dashboard uh where do i find that it gives you the link if you just look at the dashboard link property we like reuse this in the various like widgety components but i was just chatting somebody during the break about it's pretty common if you are going to write a python script that's going to use das and you're going to launch a cluster you might import local cluster or whatever kind of cluster instantiate it and then print dashboard link and then do your work so if you're you know if it's a long-running script you can look at the logs you can find the link to the dashboard you can open it up and you can watch the progress as your script is running through right you can kind of get that information out of it and then once we've got this cluster object we need to tell desk to use it and so we can instantiate client objects and just pass it the cluster and just say like here is a dash cluster use it and so this is kind of equivalent what we've been doing so far i guess the lazy way of doing things is equivalent to doing this where you're kind of instantiating a cluster within the client but you can just pass the cluster object straight through to the client if you create that the client will connect it will use that cluster you can see the cluster type is this local cluster um and it's wrapped up this this representation that we've had before um but doing things this way creating a client and then passing it sorry creating the cluster and then passing that to the client is like a pattern that will become familiar if you're using different cluster types because oh i want to switch to using a kubernetes system now so i'm just going to import cube cluster and create a cube cluster object and then i'll pass that to my client and carry on as normal um now before i keep going with that let's actually dig a little bit down into like what was local cluster doing so local cluster is creating this scheduler and these worker processes for us and then it's just making sure everything's kind of talking nicely but we could go really manual and set things up completely like bare bones if we want to i'm just going to do that if i open up a terminal yeah that green is not pleasant but if i open up a terminal i can run desk shed july okay now i have a scheduler and this scheduler has decided that it's listening on this port it's using the default port if you start multiple schedulers you'll get you know 87.86 will be busy so it'll just choose a different random high port so this is the place to come and look for that address and then you know i could open up another terminal and i could say i would now like a task worker and you should talk to that scheduler here's his ip address and port and we should see if on the scheduler side it receives an incoming connection from the worker this is like the most bare-bones way of creating a distributed dash cluster you start the scheduler process you start the worker processes however you do that in whatever environment and then you know we have this address i could come in in my python session and let me just delete those if i do client equals client i can just give it this string address as well and client will be perfectly happy with that as long as it can find the cluster so i can connect a client directly to a scheduler that i just started on the command line um this is like a pretty common way to start clusters up if you're like playing around or or want to try stuff out or if you're developing that so this is kind of what's happening under the hood when we instantiate this local cluster object is doing a bit of inspection into the environment and then it's just trying to start these processes for you trying to uh it will start the scheduler it'll then figure out what that scheduler's address is it'll start the workers give it the address everything should be talking to each other so what are the requirements i guess for running a cluster like this in terms of like uh connectivity um i'll come on to this little bit more in depth but it's worth saying at the moment um i ran scheduler and worker here on my laptop or in this binder environment i guess and they can talk to each other but the scheduler is going to listen on these two ports the dashboard port and the communication port when we start the worker that is going to try and connect to the scheduler's communication port and it's also going to start its own communication port if we start lots of workers they'll all be listening for traffic and they'll be talking to the scheduler and you know when we saw those red blocks in the graph where the workers are talking to each other they will open kind of ad hoc connections to each other to that communication port to just say here's some data and they'll push it into another worker or pull data from other ones they can still work from each other and do all that kind of stuff so in order for all of that to happen we don't know what these random high ports are they're going to be assigned so the scheduler and all the workers need to be on the same network effectively there shouldn't be any firewalls or anything restricting traffic between them i think in the docks you will find the like high port ranges if you do need to poke holes in firewalls and things but then from a user perspective the client will only ever connect to the scheduler on whatever the communication port is for the scheduler so if you're trying to start a dash cluster inside some other systems hpc systems and cloud system everything will happen kind of internally between the scheduler and the workers and then the client just needs to be able to poke through to speak to the scheduler to kind of give work and get results back so you can often set up configuration rules you can use ssh port forwarding you can do whatever you like as long as your client can talk to that scheduler so it's like a very manual way of doing things but hopefully that's given you like a bit of an overview of what these processes actually are now these scripts are just you know these are just python like entry point console scripts or whatever these are just python libraries you could do instead of doing scheduler i could do like python minus m distributed dot cli.scheduler i think uh maybe not that right maybe it's task yeah okay so it is just a python process so in order for all these things to talk to each other you have to have python in all of these places you have to have the same version of dusk pretty much installed in all of these places you can get away sometimes with minor version differences but i just recommend you don't because it's a headache just make sure the same version of task is everywhere and then when it comes to actually executing your graph you're going to be doing stuff in the client importing libraries using them leveraging them to build up these graphs and then the graph is going to be handed to the scheduler that's going to be added onto the workers and they're going to do stuff and as part of that doing stuff they're going to import those same libraries and do the same things so ideally the python environment is going to be identical between where your client is being created and where the workers live so i'm doing this on my machine but if i was going to ssh to another box and start the das worker process i would want to make sure i've got consistent python environments between those two places again you can get away with like some minor differences but it'll cause you a headache most of the time so when yeah when you connect a client to a cluster it does do a version comparison it will print out a warning table saying i've found that you've got this version of numpy here i know this version of numpy here and they're not the same sometimes it will tell you whether that's like a warning or whether it's critical or but generally just try and make sure they're exactly the same and you'll have far fewer headaches when you're when you're working with this stuff um the other thing i try and say as well is try and make sure the scheduler has the same environment this is less important but occasionally the scheduler will deserialize things as it's passing through it's very very it's rare it really shouldn't happen really but we are kind of shifting back more in direction of using pickle um as part of passing things through and it doesn't mean things get deserialized often which just means if you're missing dependencies in the scheduler you can get weird errors so i just try and say to folks just have a condo environment script or you know something that sets up your python environment make sure it's the same everywhere it's much easier on like hpc or something where you might have a shared home directory and your python environment can just live there um but it's it's it's just much easier if you've got consistent environment around so instead of creating stuff manually on the command line because that's tedious we have all of these cluster managers that will create various clusters of various types for you so the most popular one that we see coming up in the das survey that's used a bunch which is kind of fun because it's uh it's like not the most sophisticated but it seems to be like one of the most useful is ssh cluster so you can import from the das distributed core package ssh cluster and then you instantiate it like you do a local cluster but the first argument you give it is a list of host names and those are hostnames you should be able to ssh to without having to do any manual password putting in so you should have keys set up or whatever but if that if your machine can ssh to those other machines when you instantiate ssh cluster it will open uh those ssh connections the first item in this list will get the scheduler command the rest will get the das worker commands it's a really neat way like i don't know if you've got colleagues with workstations that are not fully utilizing that you can ssh to i used to do this a bunch just just use ssh cluster and just grab a bunch of desktops pull them together into a nice little ad hoc dash cluster hey it happens all the time it's great um also with like servers if you've got big bsd servers or something this is just a nice way of like reaching into it and starting your processes automatically um there's a whole bunch of configuration options you can set around you know the ssh connection itself but um ideally if you can just if you can assess without a password then then this will work for you uh it will look for python in the same location the python is currently running the process that's running this all these things are configurable but um ssh cluster is like a very useful way of getting started like exploring outwards from your from your machine but both of those examples the the local cluster and ssh cluster are what we call like a fixed size cluster right i've my laptop i can't change the shape and size of my laptop and for ssh cluster i've given it a fixed list of host names those are the machines that will make up my cluster but we also have a whole bunch of cluster managers which can talk to some kind of other resource manager and ask for allocations of resource and if we can do that programmatically if we can speak to something and say can i have a machine that means that whilst we're running calculations whilst we're doing work we could speak to that again say can i have more machines or we could say oh i'm not using these i don't need these machines anymore and so das can scale up and down now we can do this i haven't got any huge examples in here because all of these require some kind of resource manager and it's a little tricky to show in binder um but we have cluster managers for kubernetes for hadoop for all sorts of cloud platforms we have stuff that leverages vms or leverages like container based platforms and we also have to ask jobq that will use resource managers like pbs and slo msg but whatever you kind of have it's just yeah just messages enough um the idea i guess with all of this is that we can just that's from your laptop or wherever you're running this it can just reach into other systems and start the processes that it needs um i often enjoy when you do that and then you use another tool like xgboost with its desk integration because when you have das workers and you use hdboost it will ask the worker to start an xg boost worker and like create a cluster so you're kind of like reaching into all these resources and then like stacking clusters on top of each other to this like nice highly abstract level of i don't know compute stuff it's fun um but yeah so we all of these cluster managers uh can reach into these various systems so for kubernetes we can ask for pods um on you know hadoop or a job shifting system we can ask for allocations uh on cloud platforms we can ask for vms or containers or whatever whatever your unit of currency is some of these will require you to provide um you know a docker image to run say as a containerized platform and it will ask you to provide some kind of config for the vms that are going to be launched or whatever that kind of thing is you have to to set it up a little bit and and the network constraints kind of apply but once you're in this state where you can instantiate cube cluster or or you know ec2 cluster or whichever one of these cluster managers you want to use um if it can create one allocation it can start the scheduler it can create more and start the workers and then you can do things like calling cluster.scale and just give it n number of workers that you want and this will just create more allocations and start more workers and they'll connect in so you can do this at like fixed points through your script if you know you want to run something on one worker for a little while and then you want to burst out for a little while and then you're going to come back this is a great way to do that manually but desk is aware of what's happening like we looked at that profile before that was like this is how long each of these function calls is taking within your your code and so the scheduler knows how long these things are taking as it starts running tasks through it's got the timings of those tasks and so it can do some quick maths and say if i was going to run this whole graph in five seconds how many workers do i need in order to get through all of my my things and so we have this other mode called adapt where you basically say to it try and do it in five seconds it's up to you to go and ask for more resources when you need it so das basically starts auto scaling as it's working through the graph so if the graph is kind of stretching out it'll start asking for more resource if the graph is reducing it will give up resource this is it's like super nice on resource managers that have very fast uh allocation like um provisioning so i've used this a bunch on kubernetes where it's like sub second to get a new container running you can have like many folks working on a cluster of 50 machines or something and they're doing these like really bursty auto scaling workloads where their dash graphs are like bursting out grabbing resource from within the kubernetes cluster and because you've got all these bursty workloads you kind of like averages out and you get folks being able to grab way more resource than they would normally be like um quoted um and they're kind of balancing out how that resource is being used and then if you're using kubernetes on the cloud or whatever when the kubernetes cluster hits its limits it could start auto scaling and asking for more vms and doing all sorts you used to see clusters of machines that would you know the kubernetes cluster would be at a few vms in the morning folks that start logging in they start running das work and the cluster would start scaling up and it would kind of hold steady as folks are submitting all this work lunch time it might start chunking back down again you can do a lot of fun stuff with like saving money on cloud allocations and doing stuff automatically but i guess one of the things that we've talked like not talked about hugely here is that every single one of these das clusters that we've played around with has been really ephemeral right we've just created this dash cluster and then it's gone again we've started it we've deleted it which makes sense when it's processes on the laptop but that can be more challenging when it's like what if these are vms in aws like they take many minutes and they cost money and like maybe i don't want that to be ephemeral so we do have other tooling for launching kind of fixed size clusters or clusters that are um going to be like slightly more permanent we're like trying to move from this place where we have some tools for fixed clusters and some tools for ephemeral clusters and try we're trying to push in a direction where you can kind of make these decisions a bit more uh ad hoc um but yeah things are not not quite there with that yet we also have uh das gateway which is like a centralized like job launcher so it's built like jupiter hub if you've used jupiter hub it can launch das clusters on your behalf it can proxy traffic through and it can do some authentication and things so it's quite a useful point if you want to like provide task as a service within an organization dasca is a cool project now i knew i had stuff on this anyway let me skip over that the last thing that i want to go over because we still need a better time for the future stuff is uh let's talk a bit about security so all of the connections that we've made so far between the scheduler and the workers has just been speaking like raw tcp to each other you could just listen to that traffic you could you know intercept that traffic you could do whatever right this is unauthenticated stuff so das also has tls support so you can encrypt any connections going between any of these components if you're running desk on an untrusted network you can generate certificates and the client the scheduler and the workers will do like mutual tls authentication between each other and then it will use those certificates to uh to encrypt traffic going between some of the cluster managers will do this for you if you do if you use like any of the cloud ones and you expose the cluster to the internet it will automatically turn tls on um there are like performance downsides to doing this um but you can also do it play around locally you can see this little example i'm using the python interfaces to create a scheduler create a worker and create a client and they're all using this set of temporary credentials that i'm generating on the fly but you could also you know use some other certificate manager or generate certificates yourself so if you're interested in like authentication of your das cluster have a look at this and the links out from here and then the last thing to kind of cover is we do have other protocols other than tcp for communicating between things you can use unix sockets if you're on a unix-based system to do like process communication without going through the network stack which can be great if you don't want to burst beyond one machine we also have web sockets if you have uh like funny uh limitations around how you can proxy traffic out some http hpc systems will only let you proxy web traffic or layer 7 traffic and so websocket mode will let you proxy that out but then if you're on like a high performance system with infiniband or mb link or some kind of fancy network architecture you can also use ucx which will upgrade your network connection between workers to use cool hardware like infiniband and and use rdma to do memory transfer between workers rather than going through uh the cpu and the network stack so if you're on like a interesting hpc system i would check out ucx and the documentation there that's i'm going to kind of talk to stop talking about deployment of distributed here and i think julia's going to come back in and talk a bit about futures and how they kind of live out in the cluster probably if you have like super specific deployment questions maybe just fine jacob yeah all right so the last notebook we're going to talk about is uh we're talking about futures here so futures is another similar way to delay it it's a way of uh operating on uh creating task graphs and running running distributed computations but instead most of the things we've been talking about so far with gas data frame and delayed and array they create task graphs and then they wait with futures um they start this you submit the task immediately and um das and it will return something immediately the future is object but the the computation will be asynchronous so it will just start happening when the resources are available and when it knows when the inputs arguments are are computed um and it will just run while you're setting other things up while you're doing other things that runs in the background and returns so you can keep track of these uh little futures objects they'll have a status of finished or pending or errored and then they have a results that you where the actual data is um so if you're familiar with concurrent futures uh it's it's basks equivalent of that um so it provides real-time execution but uh um but it can be paralyzed and non-blocking um so the important difference between a lot of this code is going to look a lot like the delayed code but the important difference is that futures starts immediately and that and delayed will just wait wait until you tell it to compute um so for futures you always need to be using distributed so it needs the client object to be instantiated so this is the same setup that we talked about in the delayed notebook of the type of workflow where futures would be a useful a useful thing to use um so the difference here is that instead of in instead of decorating our function we just when we go to well when we go to run the function we use client.submit so we use client.submit and i think you've probably we've sort of like hinted at this by using client.submit in other notebooks um to just make something happen on the cluster um so client.submit takes these arguments which is a function and then and then like whatever arguments are going that function so um here that we've got this function process file and then file name is the argument to that function and it returns a future object that you can keep track of um and look at the status of so here's our little um here's some little functions um that are not special in any way just regular python functions um we can run them serially like we always do um where we just can run one of them um and then we can also this the way that you would do this if you want a future is you do client not submit and then the function and then the argument um and this will return um this object that has the status of pending and it triggers the computation oh i guess actually graph would be nice here check uh i'll get the cluster map so we can see things are happening okay so when you do question estimate now to remember it i'll change that input i'll just change it um so it should immediately trigger so you might see that this little worker turned green for a second so that was the operation happening and then we can check after some time to see that now that our now our future is finished you can see the status change and you can check what the result is by doing future.result so if if instead of returning it right away we just put results in here then this whole cell would wait until the future came back so that's how you can get the real like value out i feel like this happens to me a lot where i don't know that something's going to output a future and then i get i've chained together a bunch of operations and um and i end up with these weird errors and use so if that if you're running into weird errors where um you know if you do something like this that is that won't work but you have to call that result on futures um so there's other ways to wait if you have um there's other ways to wait for futures uh so you can import like weight or you can get a progress bar that shows in the notebook that like at keeps track of how the um how the computation is going or you could explicitly wait and that'll block um so you can also gather instead of calling dot result on one future you can call client.gather to get the results from many futures all at once um so in das there's always or there's often like a dot method that you can call on the object or there's like a module level um method that your function that you can call on many things so that's this equivalent so you can also use instead of using dot compute or dask dot compute like we've been using in the other notebooks you can use client.compute on any das key on any dash key type thing and so this is a for instance a delayed example you can use client.compute on that object and the return will be a future rather than the real value so you can see that or if you're watching you can see that there are some little things started happening and you can get the result so the the one of the nice things about futures is the computation moves to the data rather than the data moving to the computation so if you're running this in your local python session you never need to see the intermediary desk never needs to see the intermediate values in the notebook it can all stay out on the workers um client.submit um pushes all the arguments to the cluster um and it returns a future yeah so it's it's similar to compute it looks a lot like compute but um but instead of um in this example we just passed an object client.compute on an object in this case client.submit it's a function and then the arguments so here's this uh doing an increase in increase and then the sum of the those um and you can see that it triggers right away and then but it returns the future and then the result is just sitting there waiting for when you're ready for it and the arguments to client.submit they can be regular python functions and objects or they can be futures they can be other futures like we have in here the argument here is a couple of a list of futures they can also be delayed objects all sorts of different things and das the client will wait until those answers are known those outputs are known before trying to evaluate so um what we looked at dot persist in an earlier notebook and what that is doing is it's creating the futures for every partition and then it so it holds that reference to that data and the workers point directly to the data so that's uh yeah so yeah we can um yeah so we can futures are a good way to control intermediary values and make sure that we're not holding on to extra data um and they'll be when you stop referencing a future its value will be forgotten so if you want to uh you can like delete future z and that will that will no longer hold on to that data so for passing data around to back and forth to the client you can use client.scatter to pass data out to all the workers but it's it's always better to construct functions that themselves read in data from wherever it's stored because that way it doesn't have to go through the clients at all so in das like whenever the workers can do things for themselves that's better than trying to do it in the client and then pushing it out to the workers so here's an example of us of another case where work futures can be really helpful api so this is a task that sporadically fails um some this might happen if you're dealing with input data that's like messy or like maybe it's image data that has like you know one in a thousand will have like a weird extra like a different shape and that messes up your whole operation um but um or if it's like a request that times out sporadically um then you can run so that's it that's more of this example it's a something that fails not based on the input just but just randomly which i've just made by using random so this will generate if you run this enough times like you'll hit an error occasionally so this is an example of running it 10 times and hopefully it'll hit the error i think it probably will um so we can look at the all those futures and we can look at their status and find that some had errors and then you can pluck off a particular future and retry it and so i plucked off the the number one future and it failed again but we can just keep doing that and now you'll see that subtle difference but like i've been retrying it and now it's now succeeded so that can be a really helpful feature of futures um so there's more concise way of of denoting that with ask when you go to use client.map or client.submit um you can pass in a retries argument to say i want that you know just if it fails just run it again however many times and then in this case also we'll need pure equals false to know that the input to the function doesn't totally determine the output so it shouldn't cache it so aggressively and if you run that you'll see that it runs a bunch of times and it gets the it gets the result um without failing um whereas if you ran it without wait if you ran it without these it will probably fail yeah so um that can be a helpful of helpful way of using futures um so futures they might be the the one of those cases where you'd want to use futures is just if you're used to this kind of like if this looks more familiar to you than all the stuff we've been showing you so far like you can go ahead and use futures um it's a common like mapreduce situation and if you're using concurrent futures already like it might look familiar and it might be like actually the simplest entry point into desk um the other the other reason to use it is that the intermediate intermediary results stay in the cluster rather than coming locally so new operations can can be chained together and you can set up whole flows that are working in the background but all your data is on your cluster and not coming locally until you need the like actual end result so that can be pretty powerful whenever possible it's good to move computation to the data rather than the data to the computation all right that's the futures one is short so i guess we can do a little wrap up um to sort of situate ourselves back in where we started so we talked about um the the different parts of das so the high level collections the best data frames that's great we didn't talk about bag but basically if you have lists or jsons you should look into bag and then we have the low level connect collections which is the dastoid and futures that are allow you to operate more on the functions and do things like just pure maps and for loopy type things you can use those to create really customized workflows and then we talked about the client which is what we've been instantiating in all our notebooks which talks to the scheduler this whole thing is the dash cluster that jacob was talking about being deployed different places and the scheduler talks to each of the workers which are your units of compute um and then there's all these libraries if you you know i'm going to plug the x-ray tutorial again which is happening um this afternoon and um i'll also plug the desk sql talk which is happening later i don't know if there's other dash key talks what oh and then there's a bird of a feather um which is thursday thursday at 6 30 p.m so if you're already using desk or you want to hear about other people who are using desk that'll be a cool time to just talk with other people and with that i think we can do questions if there are any questions left yeah um they have to be pickleable it's using pickle is that or is it using something else cloud pickle i think yeah matt god pickle yeah yeah it's because random has some state um and so if you run it a bunch of times um you will not get it won't like start succeeding because it'll cache i forget if it's that it will cache the state of random or if it'll just cache the inputs and expect the outputs to be the same it's that one okay it's that one so if you have a non-deterministic or if you have a sporadically failing task like this where the inputs are the same and it like might succeed or might fail then puree goes false we'll make sure that das doesn't aggressively cache that um and and and try to be too clever like it wants we want to let it run again so that it might succeed yeah for an ask a setting that will set everything to pure is that what you're saying i think it assumes everything is pure okay yeah and it has python in the same place it assumes the path is exactly the same if it's not you can set that as a configuration all right well if you have more if more questions occur to you you know find us there's also other das developers here um so we're we're plentiful you can find us what's up but we can put the vintage ones out here old school we'll try to maybe we'll try to find the new ones by the by the birds of a feather at the latest um so we'll definitely have them there so extra incentive to come to that this might be your last chance [Applause]
15,Network Analysis Made Simple,https://www.youtube.com/watch?v=LTnsaD-rXW4,welcome everyone to network analysis made simple and this tutorial is about modeling data as graphs and networks and like analyzing uh that data and trying to get insights from that data and also like you know building a mental model of how to look at data and like you know get a graph out of it and like if you ask me uh if you try hard enough anything is a graph so it's not a good idea but that's what it is and uh there's some introductions uh my name is middle said i'm one of the developers of network x and i work most of most of my time on networks thanks to a grant from chan's account initiative so like you know work on working on open source and getting paid for it yay and we also have eric ma eric ma is not present here but he's present remotely so if you are using slack there's a slack channel called tutorial network analysis and if you want to ask a question that you think does not you know make sense right now to ask like you know right now then you can post it in the uh post it in the channel and ericma would be checking their things so he can he can reply you uh asynchronous uh async in an interesting fashion and we also have dan schultz and he's the original creator of network x so like you know feel free to ask any questions you have and we also have jared melvin he's uh he's a release manager of network x so you know he's the one making all the releases and if you have any complaints that's the person to go to and uh please grab a post post-it note if you do not have it please raise your hand so jared can send you one all right and uh just before we start just like just to get a feel and vibe of the room um how many of you are expert in python you know what are generators you can just you can you can work your way around any random python code all right all right how many of you are comfortable with python like you know what's happening there if you read a bit of code all right so how many of you are comfortable with pandas like you know head tail things like that all right and how many of you have used jupiter notebooks before ah perfect something to learn there and uh how many of how many of you have used my binder before well we should have more hands there but all right so before we before we start anything i just want to go through a bit about the jupiter lab interface because this is what we will be using to go through our tutorial so like it's it's almost like an ide in a browser and the idea is that on your left side you would see like a panel which has you know which is just like a file explorer and most of our notebooks are inside this folder called notebooks and there are like different chapters and we won't go through all of them because if we go through all of them that's like eight hours of material um if you want to go let's do that but that's not so we'll go through a bunch of them and and the idea is that you know if you're if you're really excited what you saw today you can just you know go back and finish all of it and hopefully you would have a good understanding of how to use network x and how to model your data as uh networks uh with python so that's that's our goal here and uh all right so and again like if you have never used uh jupiter notebooks before so like like these uh uh like individual notebooks you have like a concept called cells in cells you can put in either code or you can put in like just text which is usually marked down so here you would see a lot of text and like you know you can just put in code here like a non-python code and i mean it can be anything it can be julia code too if you want but in this uh in this tutorial it's all python and this is and if you want to run a cell you do shift plus enter and that would run a cell and as soon as you run a cell you would have you know you'll have some number pop up there do not worry about those numbers if you're too scared by them but yeah so that's uh that's a jupiter lab ux and uh again just to react it this is a tutorial this is not a talk that means i do not want to talk for the next four hours and uh please feel free to interject [Music] oh yes yes i'll just bring back the url uh this is a i mean i was saying this is not a there's not a talk right so i won't be talking for four hours and you would get bored if i talk for four hours so feel free to interject feel free to like you know put up your hand up or you know if you have any questions if you have any comments if you have anything please feel free to uh please feel to stop me and like if if something does not make sense please let us know we have a lot of people here around to help you and yeah yeah yeah yeah and like yeah like if you feel like you don't want to stop me that's fine too you can ask dan like like some of some someone will come up to you and they'll they'll help you fix your problem and uh again there won't be any q a at the end so please ask all of your questions during the tutorial all right so before we start has like does everyone have this uh jupiter lab thing open in front in the screens if someone does not please let us know we're going to write a lot of code in this so you know get ready for that and it's fine if you're too like if it's fine if you had like a big lunch and you're too lazy in that case like if you don't really want to write code so what you could do instead is you can like all this all these jupyter notebooks are a jupiter book so you can just browse through the notebook right here too you don't need to run code but i would really suggest that you know you actually go through the notebooks with us and go through the exercises write some code and it's usually fun so all right so everyone ready all right so before doing any network science let's just like you know practice our muscles a bit with python all right so if if you go to uh chapter like you know zero zero it's called preface and if you open zero to uh prereqs dot ipynb it's a notebook you would get this thing that pops up everyone here with me yeah all right and also you can remove your post-its now so like uh we will use these post-its during the session in the center there are a bunch of exercises once you're done with the exercise you can put your post-it right there so like you know so we know that you know people are following along and people are not stuck so if you have you post it on your laptop you can remove it right now all right so first exercise let's just look at this bit of code right here like if you see that generator expression and for people who are not familiar with generators it's fine don't get too worried it's just it's just like if you're a very lazy programmer you don't want to write four lines of code if you're writing a for loop this is how you can write it and you can you can put in a for loop you can put in an if condition everything in one line and you can create a list out of it and and for the people who are experts in python we have some of them so what according to you are plausible data structures for s and my favorite things any guesses what could be my favorite things yes yes sorry exactly so so anything that you can iterate through that is something what like my favorite things could be because you you are writing the the inner loop inside is for s in my favorite things right so you need to iterate through it so like list is one of them tuple is one of them i mean it could be a dictionary too and it could be basically any class that has i to dunder m uh dunder method implemented so what about s what could s be yep any guesses so s could be a dictionary because like the square bracket is accessing one of the uh one of the element one of the keys of the dictionary so now let's write some code what is the data structure for names in this like how would you explain this exactly so so we have names which is a list of dictionaries and every dictionary in itself is basically like a it's a it's a way to store like your name and your sun name and first name or last name however it makes more sense and now it's time to write some code so if you go down there's a there's a function that is almost implemented and what you need to do is this function takes in a dictionary and takes in a like a query for the surn name or the last name and you need to return you need to return the surname which which matches exactly as in the query you need to return the dictionary of all the people in there and once you have implemented that you can test it there's a and once you're done please put your sticker right on your laptop like if you're if you're stuck on binder just refresh it is anyone stuck on setup yet oh yeah uh can you can you use another browser if you have another browser because that white page means that binder has loaded jupiter lab is not loading so maybe you can try a new browser but if you're still stuck please let us know all right so i see a lot of stickers right there and once you're done with your implementation you can uh like remove the comments from uh the next cell you can also like test your implementation if the next cell passes without printing anything that means it worked any questions till now readouts all right so now that we have practice practice our muscles a bit now let's let's just look at this bit of uh let's look at the goals of this tutorial and as i said that their main like there are two main goals is the technical takeaway is how to basically use network x network x is a graph theory library and i like network x because it has a very intuitive api and hopefully we'll see that soon and hopefully all of us agree with that and uh and basically how to use network x and tools in the in the wider pi data ecosystem to like model your data as graphs and the other main goal here is to use uh is basically to understand how to think about data as graphs like how how things link together so that's our that's our intellectual goal here all right so let's move on to what are graphs like what is this graphs that we keep keep on talking about like before before we talk about this like what do you think are crafts like can anyone come up with examples what is a graph or a network if you may facebook exactly and how would that be a network what like what are nodes or what are edges in that case exactly yeah so so for example on facebook every person is a node and if you are friends with someone you are you have an edge between them any other examples remember if you try hard enough anything is a network exactly molecules like molecules are in it i mean they are a graph and there's a big area of like you know actually like you know active research into that of how to use like you know graph theory and you know analyze molecules together so yeah they are a network too any more examples exactly yeah street networks so like you know uh how does google maps work i'm not saying they're using network x they shouldn't there but uh it's it's one of the ways like you know how do you find the shortest path like you know you want to go to the nearest bakery how do you do that like it's it's a network underneath any other examples how did peop like people who are not from austin how did you come here you took a flight right and flights can they be a network so like uh like it would be a transportation network in the sense that you know like nodes are airports and if there is a flight between them you have uh you have an edge between them then there are electricity grid networks of like if there are two transmission uh nodes and like if there's electricity going through them then you know you have this electricity grid network any other examples sorry yeah so in a lot of uh you know data analysis uh systems now like people care about reproducibility right so so you want to make sure that you know the steps that you follow are always the same and it and we cannot be sure that it's always a linear path so a lot of times you end up with this directed graph you know do this first and then do this this is a network too and you and you can analyze this information to better understand your analysis pipelines and when you're talking about facebook the other example here is twitter right but there is one inherent difference between facebook network and twitter network can some can someone think about that what's the difference between a facebook friendship and a twitter follower followee relationship exactly so in facebook we like if i'm friends with someone their friends pack with me that's that's a given and that's not the case with twitter like if if if i follow someone that does not necessarily mean that they follow me back right so that's that's uh that's something that is inherently different between these two uh kind of networks so after talking about this let's let's look at the formal definition of a network so like you know whereas mathematicians like to call them graphs and formally uh a graph or a network i'll use this term interchangeably a graph is usually it's it's if you talk to people who come from a math background they know they know this data structure from graph theory so they call it graph if you talk to like the cool kids in data science they know it from network science so they call it as networks but they are they're the same thing graphs or networks in this context they're the same thing so uh formally coming back to this there's a note set and there's an edge set the note set is basically a set of all nodes and an edge set is the relationship between them so like if there is an edge between two nodes they would they would come up in this headset i use the term set here what does that tell us sorry exactly so there is no first node in a network i mean you could model your data that there is first node but by in itself there's no such thing as you know first five nodes in the network because it's unordered like you cannot uh like for example in a pandas data frame there is this there's a certain sense of order in that you know this is the first five elements but you cannot have that in a in a normal graph that there's no such there's no such thing as you know this is the first node this is the second node or this is the this is the tenth node and the other thing is that it's unique you cannot have uh you cannot have the same node multiple times again there are cases where you could but if you're talking about a simple graph you cannot and this is why that set term is important and we should keep that in mind and we will use that later on and [Music] all right so examples of network now that we have a formal definition can someone come up with any other thing that i can think of what could be a network how would that be a network for instance yes yeah so so if you look into like the compiler of like uh the first step of a compiler would be to create an abstract syntax tree of your code so it basically like you know parses your python code then if you have a script it will pass it and it will create a sort of network of you know like how to do what to do so even like you know writing code to run that code you need graph theory and and this is one of the reasons there's something called graph lib inside the c like the core python implementation and because then because they need certain uh methods from a graph theory to like you know analyze and run the uh run the python code all right so uh we already discussed this types of graph it's mainly there are two types of graph okay i stress stressing on the term mainly it's uh it's it's when you have directions you it's a directed graph and would and that's for example twitter and if you do not have directions then it's something like facebook so okay quick question so what would be a flight network be is it a directed network or an undirected network exactly so i mean in a perfect i mean like technically it should be an undirected network in the sense that if there's a flight from point a to b there should be a flight from point b to a but that's not always the case so that's why you need to model this as an uh directed network and because the flight just goes one way it can go go back but that's another that's another edge in that network they are not the same edges so that's the directory network and we have the undirected network and i mean there are a bunch of other networks that you can come up come up with but at least in this tutorial we'll just focus on these two things and like this this is something that uh like you know like one of my co-authors of this tutorial he really likes like according to according to him uh according to someone uh uh going to someone uh he knows like edges define the most interesting part of the graph in the sense that nodes in themselves do not really teach you that much it's just a set of something right what's the fun part is how they are connected right so that's like the the study of you know graphs video on network science is basically analyzing those relationships so the heart of a network uh like lies inside the edges not exactly the nodes i mean the nodes can have some information but that's not necessarily what you know brings you to use like network science to analyze your data all right so enough talking about uh graphs and the philosophy behind that let's uh let's look at some code and for this notebook uh you need you can go into notebooks zero one introduction and we are on the second notebook it's called the network x api and in this notebook we'll just look at the data model of network x and again network x is only one of the packages in the in the broader ecosystem uh like about the python ecosystem there are a bunch of other packages uh and all of them have their trade-offs and uh this is like design choices we will use network x there are other like i graph there's there's graph tool there is network kit there's a bunch of other networks a bunch of other packages out there uh but i like network x so that's that's that's why we see this here so let's start so in network x everything is a dictionary asterisk right now and uh this may change in the future but at least for right now everything is a dictionary and uh uh like the data structure would be like if you want to access like you know node attributes or edge attributes then it's like a dictionary of a dictionary of a dictionary it's it's it's all the way dictionaries down here so and because it's modeled as a dictionary uh like if you know how to work with like you know python dictionaries a lot of this syntax would make sense so like if you want to access a node that you have added inside this network you would do like g dot nodes and square brackets and you just you know just try to get that node and in the case for edges you could use this syntax which is g dot edges node one comma node two and it will give you all the information about that particular edge and so let's play around with some uh data first so this uh this data set comes from from us from a study of like no seventh grade students and what they do is that they go to a bunch of students and ask them that who do you wanna do this activity a with like do you wanna play basketball like who would you like to choose you know from your classmates and people go around and like you know they point towards people and they they they ask this question for three different activities to every student and that's how we'll get something called weight later in the network but before we jump into that let's just you know let's just look at some things here so if you do like type of g so uh it will tell us that this is a directed graph and this makes sense in this case is because if if if a person says that i want to play basketball with this other student that does not necessarily mean the other other student wants to play basketball with you so life is tough and uh so that's why like like like this is a quick sanity check if you want to make sure how your graph object looks like is you do type of g and it will it will tell you what it is right now so it's a directed graph right now everyone with me till now has any questions any doubts all right and please feel free to like you know interject or like raise your hand please and uh like if it were a normal graph in the sense that it if it were just a undirected graph or a normal graph if we did a type of edge we would get something like no networking classes graph dot graph like the main thing that you need to worry about is this digraph and this graph like you can just ignore the first three bits there and so this nice looking thing is uh like if you want to access all the nodes in the network and in my opinion uh for example this is like if you want to find all the nodes we just do g dot nodes g is our graph object here and we'll get all the nodes and because the nodes are named one two three four they can be anything like in this in this case the data set does not actually give us the names of the students because you know i don't think that's a good idea and but anything can be a node in a network x graph and i mean anything again anything hashable can be a note in a network x graph and if you do not mean if you do not know what's hashable it's fine so for an example like integer integers can be can be nodes as we see here as we see see it here uh strings like like actual names in in this case could be nodes images an image can be a node two here uh like a like a sound wave which is encoded properly can be a edge can be a node too another network x graph can be a node so you know you can basically create you can basically create a network x graph where nodes are graphs so you know endless options you can go crazy and uh so so this is uh like if you want to access notes you do g dot nodes it gives you a nice like you know what's inside and there were 29 students so in this case it's just 1 to 29 integers and but sometimes what if you had like a you know 10 000 notes in here what would happen is g dot notes will print out all those 10 000 nodes you don't always want that so if you are a python person you you would think that i mean it says a node view but it looks like a container or like a list so your first guess would be oh i'll just slice it that should work well you're wrong it doesn't work but it will give you a nice little error message here that because of like you know technical implementation details you need to first cast this whole thing to list so now it creates an actual list till now but as soon as you did g dot nodes you you were just looking at thing it's a node view so you're just viewing at the internal data structure you're not really creating anything new but as soon as you do a list of g dot nodes you have this new thing you have this new container which has all of your nodes and because it's it's now it's just a list you can you can slide slices uh as you like so you know this is like the first five nodes again it's not first in the cell it's just it's just a sample of five nodes let's say that yep i mean if you want to if you want to slice it then yes if you do not want to slice it then no like because you you cannot slice the the underlying note view uh not view that you get yeah like that that's something that we look at later and that's that's called like a sub graph you know you can create like smaller graphs out of your graph but uh we will look at that in a little notebook so and if you if you want to find the total number of like you know how many uh nodes are there you could do like a length of g dot nodes and uh here it would give you the there are 29 nodes so like 29 students in this network you would also just do length of g that works too so it will give you the the total length or like a total number of nodes inside the network and there is another thing that you should know about it's called uh in network in network x it's not just the node you can also keep on putting data inside the node because uh and if you want to look at the data uh you can you can do something like this and these are called node attributes and like you know graph theories class network science and you can keep on putting data into that for example in uh in the case of facebook that data could be just like you know you have like a unique id is your node id then you can have like the name like where do they live how many you know how many facebook groups they're part of things like that you can keep on adding new data into that node which gives you more information in which you can use that uh while you're further further analyzing your data and just like uh just like uh you know g dot nodes it gives you like a node data view and you need to first cast this to a list and then you can slice it you know if you just want just a bit of uh you just want to have a look at you know what is actually coming out of this right so and you can also like you know go in and because it's a dictionary underneath g dot nodes also work with the with the square brackets and if you do g dot nodes and put in a node inside with square brackets it will just give you the it will just give you like all the attributes that are stored inside the in inside the inside that node data dictionary and in our case we just have this one attribute with every node it's the it's the gender of the student and uh and because all of these are like iterators in the sense that you know you can you can write a for loop through it so like if it were a list you could just do like you know for i in this list do i keep on doing something and because it's an iterator you can you can just write a normal python for n comma d in g dot nodes it's basically like n becomes your node and d becomes the data of that node and uh and it's it's pretty uh like analysts to like you know what a dictionary would work like if you do dictionary.items because a dictionary would have a key and a value and in in g dot nodes data true it's it works like a dictionary in the sense that there is a key which is a node and the value is all the data of that node all right so let's it's time to do some exercise so now we need to count how many males and how many females are represented in the graph and you need to write a you need to write a function called node metadata and once you are done please feel free to like know add a sticker put the sticker on your laptop and if you have any questions feel free to ask them any doubts is anyone stuck at this setup right now uh um if you're stuck that's perfectly fine too and i mean either you can ask us we can come and help you or you can also look at the answer and i mean again that's just one way of doing it there especially in this question there are multiple ways of doing this one and to actually look at the answer what you could do is uh like because we are importing this thing called node metadata and with the beauty of jupiter like ipython you can if you put in like two question marks in front of the function name it will it will print you the signature of the function and the actual source code of the function so and if you are done with it you can run the next cell to see if your code actually passes this test if the code passes the test hopefully it's right okay oh and also feel free like if you if you're done feel free to help your neighbor it's a it's a network x tutorial like no network yep [Music] um yep uh like if if you if you're having this error that says nams is not imported and uh like you can't import this nams thingy then [Music] you need to run python setup dot py uh develop like honestly in that case just just go to binder and start doing it on binder because that's just a local configuration thing and it would be better if we just use binder because debugging that can take some time all right let's uh let's let's jump forward to the next uh the next bit it's fine if you're not and it's perfectly fine and uh like like this is one of the solutions and here we are using something called a counter and like collections is it's inside the standard library of python and this is one of the most i feel like underutilized under loved package so it has a lot of fun things and you can just pass and counter and it will literally count like if if you count if you pass counter to a list it will count everything to for you and return that thing as a dictionary so see you don't have to write a lot of code and if we do that we have our mf counts and if we run this this is how it would look like and again this is just one way of doing it there are multiple ways of doing this this exercise all right let's move forward so just like nodes the other important bit is edges right so and just like nodes if you want to access edges it's just as simple as that we do got edges and there are your edges but it's a lot of them right we don't we don't really want to see all of them we'll just do like a just like list them cast them to a list let's just look at five edges and this is how they look like you know there's an edge between one and two there's an edge between one and three there's an edge between one and four and like a lot of bunch of edges too and uh if we do length of g dot edges that it says there are 376 edges in here so that means they are like in the in those 29 uh students like the notes in the network there are 376 relationships between them and if this is possible like because this is a special kind of network in the sense that it's possible that you know there are way too many edges here because a student a could say three different students for like three different activities right so it's there a lot of edges right here and usually the networks are not distance in the sense that you know if you have thousand nodes you will not have like you know you you won't have like hundred thousand edges in that so now let's look at uh just like node attributes there is something called edge attributes in the sense that you can keep on stacking more data inside the edge and which gives us a lot of information and like for example if we go to the to the to the airport to the airport network you could add information like uh to like in a flight like you know what's the price of that flight and if you uh what's the price of the flight what's like how much time does it take all of this information can go into the edge between like you know airport a and airport b so and in this case our information is count and how this count is calculated in the data set is if the student one choose the student two for one activity you get count one if the student one chose student three for like you know if if they chose it for uh two activities you got you get the count two and for example in this case student one chose six for like three activities so you know they can they can play three activities with them and if and just to access the edges this is how you could do that 15 comma 10 basically it's like if you want to find if there's an edge between 15 and 10 and what's like what's the data in there this is how you can access that and if you try to access for example if if you do like 15 comma 16 it will give you a key error and when this pops up this means that there is no edge between them in the sense that you know you are at node 15 and you are trying to you know get uh the information between 15 and 16 but because there is no uh info there is no edge between them you would get this error that pops up and just like node views that we saw and the edge views we saw again it's just a dictionary and if you want to iterate through it so you can do like for n one comma n two comma d which is basically like the node one of the h the node two of the edge and the data of of that edge uh and if you want to iterate through that this is how you usually write that for loop and once you know how to write this for loop let's uh let's do and do another exercise and in this uh in this exercise we're just like no we are writing we are writing a test in the sense that just to make sure that the data that we read in is is sane in the sense that the data is not corrupted uh we need to verify that the maximum times maximum times any student rated another student as their favorite is three times because that's the maximum if there's a four in there that means something is wrong with our data so that's the next exercise to to to write this h edge metadata function and just like before if you want to look at one of the answers you can pass in like you know two question marks and it will give you one of the answers of how to write this and if you are done please put a sticker back of your laptop and if you have any questions if you are stuck please let us know and also if i'm too if i'm too slow for you there's good news there's a lot of material in there so feel free to like you know go ahead like i won't take any offense please feel free to go through all of it and if i'm too fast please let me know because i don't want that and feel free to discuss the answers with your neighbors it's not an example and if you have written the function please run the next cell to make sure that it works foreign so with an out edge view you would get only the edges and as soon as you get the data view thing it's it gives you the edges and the data of the thing so for example if you do yeah like for example if you do g dot edges it will give you out edge thing and i mean and they're there like no two ways of doing this if you pass in data as an argument you get this out edge data view and you can also do this and it will also give you the data so like like because this is a directed graph there's a notion called like you know as dan mentioned like an outage like it's basically like there's a there's an edge going from one to two that does not mean that there's an edge from two to one all right let's let's look at the answer quickly so like one of the answers again there are a lot of ways of doing this so in this case we basically iterate through the edges and data uh data equals true in this in this case and we extract the count from every you know data dictionary of an edge so we get a list of all the counts if you if you print this out so basically you know the first edge has one the second edge has one so on and to make sure that you know the maximum of this is three we could just do the max of counts and we get three so there was no four here that's a good news and let's uh let's jump to the third bit of this notebook it's called manipulating the graph so so right now what we did was we just write in the graph and uh like there are some custom functions that are written inside this tutorial so if you go to the home direct home directory and like open this thing called dams folder you can actually look at like you know what is happening underneath so when we are loading this seventh grader uh seventh grader network it's reading a csv file which it uses pandas to read that csv file it makes some adjustments to that and then you create a graph using this csv file i mean you can you can look at this code if you want and this is this is how we built the graph that we are playing playing uh playing with right now but uh but what if once once play time is over now you want to manipulate the graph you you want to add you know add new nodes you want to remove something there's something that uh there's something that's something there's something wrong with it you want to fix that right so uh and this is where i you know like network x the the api is very intuitive so if you want to add node you do g add node and you can add node and this function is pretty flexible in the sense that uh you can add a node and this node bit could be an integer could be i mean could be any hashable thing and you can keep on adding uh node attributes and that is basically you can just name it anything you pass in the value you can keep on there's no limit to it you can keep on doing that and just like add node there's add edge but in add edge you need to pass in two nodes because in every edge it's two nodes and uh so you need node one and node two and just like uh add node in edge in add edge you can pass in edge attributes by naming them whatever you want them to uh to be named and uh let's let's just do an exercise quick exercise about this so in this exercise uh we found out that oops we missed two people in in our network so we need to add two nodes it's called node 30 and node 31 not so great names but that's how people are named and we need uh to add uh and they they love hanging out with each other so like no like if you ask them they're just like no this is the person i'm gonna do my uh my activity a with my activity b with an activity c with so uh you need to add these edges in the network and the nodes with the with the proper metadata and you need to write this function g uh like adding uh students and this is the answer right here if anyone wants yep the uh is the it's a dictionary that's getting added and remember this is a directed graph so if you add an edge from 1 to 2 that does not mean that you're adding edge from 2 to 1. so keep that in mind um yeah it's yeah it's expensive in the sense that you are creating a new copy structure it's a personal opinion don't do this if you have like a million notes no not yet yeah i mean i think this was hard because it's a function right and you want that contained function so i mean you don't need that g dot copy here but it's just if you want that data to be encapsulated inside that thing and once you are done with this feel free to run the next cell and it will like if everything passes you will get a beautiful all test passed and if you're stuck feel free to like call us out or if you don't want to do that you can just look at the answer by doing adding students and then like putting two question marks and jupiter will help you there we'll soon take a break please don't worry about that you all right let's look how let's have a look at the answer and we can do like energy at node uh 30 comma like you know basically put in all the uh all the metadata in here and we can do g dot add edge and keep on uh doing that with the count as the count as the data edge attribute and and then we can we can test this right you should always test even your data like a lot of people don't do that but we should test a lot of data too so uh so this is this is one way of doing that there's there are there are also other methods and if you have used network x before you would know them but uh let's not go into that and like this bit is just like some opinions in the sense that how to work with uh like you know like network x objects because it's it's a dictionary right so so you can write these quick list comprehensions they are very efficient like in the sense that if you're writing a for loop if you do like four you know n in something and then you write that three four line for loop if you can condense that to a list comprehension in my opinion it's more readable plus it's uh it's more efficient so if if if you're ever in that situation please try to write these list comprehensions and if you really want to go deep into the api you can you can look at the network network stocks and you can go to this thing called references and it's basically like all the functions and everything that's implemented in network x and when we have dark mode then if that makes any difference to you so so please feel free to go through the documentation and we're gonna have a break now for like you know maybe 10 minutes you know go do whatever you want and uh but if you are really if you really want to do this there are some further exercises that you can do in the break but you don't you're not required to do that and or you can do it like you know once you go back home uh but yeah so let's let's break for 10 minutes and wait at 2 52. 250. all right let's let's start back everyone is ready let's talk about visualize visualization for a bit and this notebook is again like notebooks zero one introduction the third notebook is called vis for visualization and this is like don't worry in this network we don't do a lot of exercises this is just looking at pretty pictures and you know appreciating the beauty of pretty pictures and in this notebook we would uh we would show like you know there's an inbuilt meth like there's an inbuilt visualization in network x but there's also another package called nx wiz and it's it's created by the other one of the co-authors eric ma and we look at that and it can give you a bunch of very nice pretty pictures without doing a lot of customization so let's jump in all right so let's load that uh seventh grader network that we had in our last notebook again and there's a function called nx dot draw that's it you do nx.draw you pass in a graph object and you will get this beauty so what's wrong with this or is this oh let's is there anything wrong with this or like is this beautiful yes that's subjective but but the thing is that we can't really make anything out of it in the sense that uh you know like at least when i like to play around with data the first thing i don't want to run any like you know hardcore analysis on it i just want to like you know plot it somehow and and try to deduce something out of it can i just get a you know a vibe of the data just by looking at a plot the thing with that is that just looking at this doesn't really tell us much we see that oh there's a there's like a very hair polish thing that that's the technical term hair bowl so there's something very like you know dense happening inside but we don't get much information out of it so uh i mean we can see that you know there are arrows there so like you know it means that it's directed so that's something and we can uh we can also tell network x to draw this with labels and you know maybe you get a bit more information i'm not you can see that and you have these nodes that are uh you know swinging around and there are a lot of customization so like everything underneath here it's a smart plot lab so what you could do is if you go to like nx draw network x nodes and for people who have never you know who haven't worked with juvenile notebooks a lot as soon as you write like not draw if you hit tab it will give you a bunch of options so you don't have to write everything see programmers are lazy we'll do everything for this so if you do like if you do draw network x notes and you pass in these two question marks it basically gives a gives us the signal signature and like the documentation so we can have a look at this so there's a there's something called like a node size so if we go back we can control the node size just by putting here node size if you do not size like let's make it a bit smaller let's do 50. by default it's something called 300. you don't need to worry about that but okay so now nodes are a bit smaller okay it doesn't help much so let's do there's an option called width this controls the width of the like of the of the edge so the thing is that because there's so many edges it's it's very dark it's very hard to you know find anything so if we just make it 0.1 then let's see what happens all right so it's something again it's it doesn't really telling you a lot of things but the thing that i wanted to show you here is that you know there are a lot of in like there are a lot of options there are a lot of knobs that you can turn around to get your pretty picture which is inside uh the like the basic draw package of network x but if you do not want to do that you could use something like nx wiz but it appears that all the graphs are being drawn in some kind of deterministic way yeah it's not actually like right now it's random it's not deterministic it's probably a coincidence but uh it's it's like like when you do when you just just do an extra draw it's basically giving you a random layout you can have like a proper deterministic layout if you do like something like draw circular in this case what happens is that it will lay out all the notes in a circular fashion and uh if you do like there are a bunch of layouts out there so there's draw circular there is like the command layout there's shell layout if you do this it will uh and there are a bunch of algorithms out there of you know how to like plot these things and it's it's a pretty hard problem like you know how to visualize a big graph it's it's not an easy thing it's it's a hard problem so just calculating the layout can be something that thing itself can be pretty expensive sometimes so it could take some time to calculate the layout and and yeah so in short it's a hard problem and a lot of people are working on that it's an active like you know research area of how to plot how to show a graph so let's default is spring layout oh yeah spring layout and spring gives you random right it's not arbitrary i mean yeah yeah yeah yeah i mean it's it's called the spring layout but you cannot like like you cannot guarantee that you'll get the exact same picture every time you would get the exact same layout but not the exact same location of the nodes sorry yes you can yeah yes you can yeah it would then then in that case it should give you the same one like like but honestly i would i would suggest not to use just the nx draw like if you want to you know build a publication quality picture from an ex draw package like you know try to like look at uh [Music] so basically there are there like three four functions it's called draw network x nodes draw network x edges draw networks as labels and labels so basically in this way you can control a lot of things like you know how the nodes are plotted how they look like how big they are what's their shape what's their color and uh and with edges you can control all the edges so if you want like like a there are a lot of examples in our documentation so if you go to like you know networkx.org and let's just go here if you go to gallery so let's play they're a bunch of you know just dark i don't like dark mode so if you look at this you know pretty pictures there are a lot of you know there's a lot of examples how to plot these things so you can look at the gallery examples and like you know we use you can use something like graphics and you can you can you can print out a bunch of pretty pictures but uh but yeah like but if you don't want to do that much effort what you can do is use nx wiz and nxt is a has a bunch of like inbuilt uh things that that you don't like once you get a hang of it you don't need to worry too much about it so let's look at something called the matrix plot so till now like for example if we look at this bit right here i mean this makes sense in the sense that you know we know it's a node there's an edges nodes are blue dots edges are like you know lines and that's that's what you would picture and this is a network but there's something like a matrix so there's like there's a very close relationship between linear algebra and graph theory because you can you can represent a graph as a matrix pretty easily and in the sense that like if you if you have like a matrix and uh like where the rows are all the nodes and the and the columns are also all the nodes and if there's an edge between those two nodes there's a there's a one there if not there's a zero there so in this case like if you look at this matrix right here if you look at what's inside here is there something that we can you know deduce from this like do you see any patent some sort of anything it's not symmetric yeah so the first thing is that you know if you look at the diagonal it's not symmetric and that is expected because this is a directed network so if there's an edge between one to two that does not mean that it's an edge from two to one so that is uh that is one thing that we could just look at this matrix plot and reduce any other deductions yep so so so that uh like if in this case we get that uh we get we get that deduction from the node attributes that are also plotted yeah so so there seems like you know there are no connections here you could say that any other like let's look at the diagonal what's happening at the diagonal and by diagonal i mean this isn't it's not from top to bottom it's important to talk what could we say about that it's empty yes so there are no self edges in the sense that it's it's good in this case because we know the context right which means that there's no student in the class who went like who's your favorite person they didn't say me so so yeah we don't have a lot of narcissist in those classes which is a good thing so so you can you can have self edges in a network x graph but in this case we don't have any self edge in the sense that there's no there's no there's no node which has a connection to itself and that's why the for example this thing is empty and you know again this is something that we can just look at a matrix plot and just just know that you know if the diagonal is empty there are no self edges honestly like i don't remember i need to look at that but yeah so uh like the two directions we have is again the diagonal is empty that no student is a narcissist and the matrix is asymmetric which means you know it's not like if it were a undirected graph it would be perfectly symmetric and again like in the spirit of pretty pictures then there's something called an art plot and what it does is it's again plotting the things that we have but because we are grouping the notes in a in a sensible fashion so like the first big the first big problem of any visualization thing here is how to like you know how to plot the nodes like once that is solved then you can worry about the edges so in this case if we like you know if we just you know create a like create just like a straight line and like you know group the nodes then we can get this like you know plot like can we do we get any information out of this plot i mean like yeah yeah so that's one deduction right so this is the like circle's plot is just like our quad is just to create a circle out of it and again pretty picture right so i like in at least in this case you can't really uh you know you can't tell you to do something that is very clear right now at least for this data set but this is another you know nice plot that we'll see later on of how we can deduce things if if it matches the data in this case we can't and that's perfectly fine because the the point of like you know a quick visualization is that can i figure out something just by like just doing like a quick dirty visualization that's what i need right now we cannot which is it is perfectly fine now this is one of my favorite plots it's called the height plot so what's happening let me zoom in here so let what's happening right here is there there are there are two types of relationships right here there is one is intra community and one is inter community so can we can we make any deductions right there that is one i mean again this is just there's just a guess that's fine because we don't have the actual numbers so like if we have something uh like the whole point is that you know if we start with a deduction we can actually look at the numbers so we know where to investigate at and like you can look at this bit here in the sense that it seems like there's a uniform distribution at one side but not on the other side it could mean anything but it doesn't really mean anything but that's again something that you know you know that you can start go look at and like think about it and yep which one yep yeah this one i mean like these are the ones that we added at the end so like the add node i mean it's just plotting them it's let me just keep it up no no it's nothing yeah so so like let's just concentrate here like so this is the intra community plot in the sense that like if we have two three types of two types of nodes in a network how are the how are the nodes in one like you know uh like you know one community i'm using that word but that's not the right word and one community are connected to each other and so this is like you know in this case there's like a female to female interaction is is what's represented right here and this is this thing is representing the the interaction between two different communities yeah yeah that's what it means like in this like it's just it's just a way to show it like we are not necessarily getting the best picture here because maybe this is not the best data set for it but it's one way if you want to look at like you know inter community connections and intra community of how things are connected inside a community and how things are connected outside a community then you can look at height plot yeah yeah it's just it's just broken into that toothpaste i mean like if you can if you if you take your network and like you know if you can bifurcate them into two types of nodes then you can look at this uh like this dynamic does that make sense for example it's very much possible that you know if you have uh let's assume you have you you have a network of uh like your local uh your local internet network in the sense that you have a bunch of servers right and uh you want to create silos out of that so so what you want is you want a lot of interconnection between these things but you do not want a lot of connections here because you want silos so this is one way of like you know quickly like you know picturing that oh do do i have silos or not yeah no i mean i think you could but not not this plot but all right so this is like you know the most the most important part is how to make notes that's that's the that's the million dollar question it's an active research area and no one knows the answer for this but it's a it's a hard thing if anyone figures it out given a million dollars so go work on that if anyone wants to do that but yeah so that was a quick detour to visualization and i think yes sorry is there is there any following technique that somehow that cluster is worth it automatically like if you have loosely connected yeah so in that case what you need to do is first you need to calculate those clusters and that i think we won't look at that in this tutorial but there's something called community detection you can basically like you can you can find those communities and then you need to manually like you know put in the positions for that nodes yeah there's no automatic way i don't not that i'm aware of you you need to first calculate the community to to get the layout and then you you can plot them out but yeah that's that's the way i think it is right now and and again like this is just if you want to send the network x land this is how you plot it but there are a lot of other tools out there there's something called gephi which is used a lot and uh like if you want you can use that then if you have like huge data sets and by use i mean like millions of nodes then there's something called data shader which has pretty nice uh techniques of like you know uh plotting these things and uh yeah like if you have any other questions about visualizations feel free to ask me about about it later so yeah and yeah so let's now now we have looked at like on the api of flickering water graphs we have looked at a bit of visualization so let's let's do some analysis now so so we're gonna look at three things it's called hubs paths and structures and if you if you actually go look at the documentation that's not just three things like there are a lot of things that you can do it so there are a lot of algorithms and you can you can go read the documentation if you fancy that but uh we're gonna we're gonna focus on just like these three things which are usually the most you know important thing in the sense that this is the the main thing that that at least i would do like if you give me a network data set these are the questions that i want to ask so the first notebook is hubs so if you want to access notebooks you go to notebooks you go to zero to algorithms and you put zero one hubs and in this notebook let's look at hubs and like you know fortunately or unfortunately this is a great thing for contract tracing [Music] so i don't know like if you want to find patient 0 if you want to find how like you know certain diseases spread in the wild so you can use uh you could you can try to find like you know which is which is the most important note like you know if this person has something it will it will spread quickly right so it's about finding important notes and in some cases you want to find important notes because it's good and in certain cases you you want to find nodes which are bottlenecks in some sort you know in the sense that if something happens to this node we are in big trouble in the sense that assume it's an airline network right if like for example if you look at a very important airport and like you know like the computers stop working there then it can disrupt the whole system right so it's it's important to like like this also happens in electricity grid networks you know finding that like know that weak bottleneck there so you should have enough redundancy in the in the network to make sure that you know you never hate the bottleneck and if the if something happens to the important node there's backup there so this is why like you know finding hubs is important uh in a network data set so and interestingly like we have a very nice data set for this it's called the socio patents data set and the way this was collected was uh it was at like a conference slash art exhibition and what they did was they asked all the attendees i think they gave them like some sort of like a beeper or something and if you are you know you walk around a conference you talk with people you network you create networks and the way this data set works is that if two people were around each other for 20 seconds it's just an assumption that if you're standing next to either for 20 seconds you may talk right so that's the assumption here and if the if if you are uh near each other for 20 seconds then there's an edge between you so this is how the data set was built and now let's see you know how it like how the interactions went ahead all right so this like when we when we when we load the the graph object and we do a type of g we get graph why is this a directed why is this not a directed graph any why is this a normal graph because i mean in the sense that you know if i'm talking to someone at a conference that they may not talk back to me but still it's it's we are near together so in the case if you want to contact trace it's important to know who's next to each other so let's just look at you know like quick things of what's inside the network so if you do length of g dot node and length of g dot edges or you can just do print g it will say that graph has 410 nodes and 2700 edges which means that there are 410 attendees in this conference and around 2700 like no relationships that performed and so let's look at the first you know measure of importance let's call that so let's ask i your job is uh you you work at in the data science team and at twitter and your job is to find the most important users like you know the most important tweeters i think that whatever they're called and what would be your first guess like how do you find that how would you say someone is an important person on twitter it's not a it's not a trick questions yeah followers you just look at number of followers right so in in in graph theory network science term that is something called neighbors like you know if a node is connected to a lot of other nodes that may mean that the node is important again it's not an absolute that may mean something like for example in a conference who would you think would be the most connected person in the sense that who would have the most interactions any guesses the speaker yes any other guesses yeah could be yeah see yes yes this is the answer that i love like if someone is going to the conference you have to talk to the person at check-in right so so it's usually the check-in person who is the most connected person or you know sometimes the keynote speaker you know they important people everyone wants to talk to them so that's that's again this is not this is not absolute science it's just it's it's a measure it's it's what you make out of it in the sense that if this measure works in your context then that's the right measure but there's no one right thing here so but so let's you can calculate the neighbors of a node by just using g dot enables and for example if you want to find the neighbors of node seven you do g dot neighbor seven it will return you something called dict key iterator like whenever you see something like this and you are scared don't be you're you're like on your solution to all problems in life is cast it to a list and this is what happens and once you do that so so you can see that node seven whoever that person is also interacted with node five node 6 node 21 and so on but there are 400 nodes in the network and we do see that this node didn't really interact with like you know anyone in hundreds 200 300 and keep that in mind we'll use we'll use that later and like like one sad bit about this key iterator is that you can't directly do length of g dot neighbors you first need to cast it to the list and you do length and you basically find the number of neighbors so the number of neighbors of node seven is seven that's not by design that's just that's just some that's just by chance so uh yep and now the exercise comes we need to find the most important person in this conference not in this conference the data set conference and that's the exercise so you need to find uh you need to create a like a data frame or a panda series i mean you can create whatever you just need to find the most important person and like the note id of that person and once you find that shout it out i mean there are a couple of hints there but you don't need to use pandas if you don't do not want to [Music] bye and if you are too tired after a long day you can just look at the answer so that there are two different types of implementations there's a third answer too and if you come up with an answer which is very different from either of these three techniques please you can like this isn't this is this is all available on github you can go you can actually submit that as a fourth answer yeah and i can't show all three in one word like after this exercise you will realize that this was a pointless exercise you didn't have to do it because there's an inbuilt function to do this do i'm if you're done with the exercise please feel free to shout out the answer which node has the most number of neighbors yeah so uh let's let's move forward and so we see that node 51 is connected to 50 neighbors node 272 is going to 47 and like and so on so we could say that you know note 51 may be an important person or you know maybe the person at check-in so uh and we don't know but we we could say something about our data you know in in our data set note 51 or like you know 272 these are important people and in case they had some sort of disease that spreads through air that's bad because it will spread fast if if that was the case with known 98 no worries node 98 only interrupted one of one other person so see maybe being an inter introvert sometimes helps all right so what we saw right now is called neighbors and if you want to come up with a fancy term to like you know to show people you can call this degree centrality and uh there are a lot of different centrality measures like degree centrality is one of them and i think that's like the the most common sense one in the sense that [Music] you look at how many nodes you're connected to that's it but the problem with this is that there's no way to compare it with two different graphs so like if in graph one the top node has 30 neighbors and in graph 2 the top node has 20 neighbors you can't really compare them you need to know how many nodes were there in the graph right so to do this people came up with degree centrality in the sense that you have a normalized version of like the number of nodes i'm connected to versus the total possible number of nodes i could have been connected to in the graph so what like suppose there are n nodes in a network how many nodes can one node connect to there are n nodes like how many nodes can one node to connect to n minus one i mean that's the right answer but it depends like how you define connection so like if you allow self loops then you can connect to yourself right but the general answer is n minus one because in a simple graph you do not want to allow self uh self edges and uh like when you calculate degree centrality in network x it it does n minus one the denominator is n minus 1 it's not n and like the the function i made you write in the na in the last thing you didn't even have to write that you would have just done nx dot degree centrality you would have done nx dot degree centrality and it will return you a dictionary with the key as the node and the value as the degree centrality of that node and like because it's it's a dictionary and if you want to do like some fancy things it's you can you can cast it as a panda series then you can play you know you can sort them you can you can you can do things with that what you do with panda series and uh so like it's you know you can sort it like this and as you can see note 51 now we get that note 51 has the highest decentrality which which is expected because we had note 51 with the highest number of neighbors and this degree centralized centrality is just normalized version that's it's just a don't get scared by the term degree centrality but yeah so this is one uh this is one way of calculating centrality there are a lot of other ways and all of those ways are implemented not all but a lot of them are implemented in network x and you need to like i would suggest go read the documentation and see which with centrality measures make sense for your case so please do that and uh let's uh and there is another thing that makes uh like you know makes a network uh a network in the sense that you can look at how things are distributed in the sense of for example how are dig how is degree distributed like let's talk about twitter right now again twitter like what would you expect the distribution of degree to be like any guesses so like let's say like you know user one has thousand followers user two has 200 followers user 3 has 4 million followers if you plot a histogram if you may how would this look like any guesses yep exponential dk because majority of the people do not have millions of followers i have 200. so see it's it's not a fair word and so you like like in like network science terms it's called like not natural science numbers it's called like the power law distribution and and again it's don't don't quote me on that but uh uh it's it looks like you know like the majority of the nodes are not that connected but more like a very tiny minority are connected to everyone so it would look like you know a a decaying curve but uh but that's if you plot a histogram but like eric ma like the co-author of this tutorial he hates histograms and uh i mean i get that it's it's very easy to lie with histograms like you know just do the right pinning and everything is everything is what you want to be so he loves this thing called ecdf like empiric empirical cumulative distributed distribution functions has anyone heard about that before all right that's fine i mean there's a there's it's there's a i mean it's it's all implemented for you don't need to worry about the details if you want to look at the code you open the nams directory you go to like you know functions and there's ecdf right there and this is how it would look like for first just for comparison let's also plot the histogram so let's do matplotlib pipeline and we can do prt.test so what i'm doing here is i'm basically creating a list of all the degrees in the in the network and i'm plotting a histogram of it and as you can see like depending on how i bin it i can i can get you know interesting results i mean like it's it's fine you you can get something in the sense that you know you have these three random outliers or something but it's still it's not that informative it could be more informative if you have something like this and what the idea behind this curve is that if you look at the x-axis there's a degree centrality and if you look at the y-axis it's like the percentile of nodes which have that degree centrality so if you go like so it basically like you know say that you know if you go up to like 0.09 like 99 of the nodes have less than that and you have these three outliers right that you can look at this ecdf and be sure that these are outliers and it's not a winning mistake so that's one way of looking at things why histograms are not the best thing in the world right you don't need to agree with this you just opinion and yeah so uh this is just the thing that you know like there are multiple possible uh denominator that we can have in the centrality thing and it's depending on how you how you define that if you want self loops it and the denominator is n if you do not want self loops it n minus one and let's plot a circle's plot of our of our people who showed up at this conference now maybe just maybe we'll find something here assuming it plots out yes so so one thing that i didn't tell you is that the notes are ordered in the way they came to the conference in the sense if someone showed up at eight a.m they would have legal zero if someone should have eight one they have one it's basically you know at what time you came to the conference yeah that's that's that's your node id and you see this kind of cool looking picture but there's a don't it's a donut if this were a randomly distributed uh network you would not see that you know the hole in the middle so what what can we say about that can we say anything and the and the other thing is that the nodes are distributed like you know in clockwise so zero is at the at twelve o'clock then one node two node three this is how the nodes are distributed and edges are basically like you know if there's an edge between zero and one it's like these are the edges and if there's an edge between 0 to i got 200 it would be like a straight line does that make sense more like it starts at the x axis oh yes yeah it's zero to zero to four hundred so the early birds yes and yes i mean that's that's one thing that we introduced that you know people who come together they stick together that that system maybe you know that that is what we like that that's why we have this donut thing right there is not a lot of communication between am people and 12 noon people so that's something and arc plot is just the same thing it's just it's just laid out flat and let's wait for this and yeah it would take some time sometimes depending on how quick your laptop is but anyway till we wait for this i mean you wouldn't see anything new because it's just our plot is a circus plot which layout flat and in this case maybe you get a better overview of that in the sense that you know things you know people who came together then you have these local clusters right very dark local clusters in the sense that the person who came at first there aren't a lot of you know long edges in the sense that there's there's not a long arc there are some long arcs but not a lot of them so you could say something along the lines that you know there are these localized communities all around the network so and this is why like you could you could have never figured this out if you just plotted a hairball right so this is why like you know effective visualization is nice we we didn't we didn't run any analysis till now we just did just let's just draw it out and like you know and assuming you know sorted by a certain order and uh yes and if you look at the visual insights it's just like you know there are local nodes which not interact with each other and so on so let's uh look at this plot so this plot is the the x-axis is the degree centrality and the y-axis is the neighbor's order and the difference between them what could be i'm confused sorry about this but let's skip that there's some something wrong with this one but anyway so uh they're not finding the hubs and in this we just saw degrees entirely there are a bunch of other ones like the famous ones are like a betweenness centrality which is really nice for finding bottlenecks and device and degree centrality you just look at what are my neighbors right and and and something like between your centrality you find bottlenecks and we'll look at that in the in the next notebook of how these bottlenecks can like you know make things good or bad so let's jump to the second notebook it's called zero to algorithms in zero to paths and like how many of you did like computer science and maths in like undergrad or something and taking the classes class on algorithms all right so you must remember bfs breadth first search and usually we would actually implement this algorithm but it takes a lot of time if you're not done this and that's fine so let's not do that but if if you want to do that you can you can do this later on and you can implement bfs from scratch but the idea here is that you know we'll talk about paths like that's another important thing where you're analyzing networks you know finding paths for example in a street network if you want to implement something like google maps you need to find the shortest paths usually you know how do i go from point a to point b and uh and these are very important questions so bfs is like one of the algorithms of doing this in the sense that you know like you know how how like how do i jump from one node to another node and you keep on jumping and you look you look at your neighborhood and as soon as you reach the target you have your path so like what i said if that does not make sense that's fine you can read about bfs later on it's a it's a well-studied topic so let's quickly look at shortest whoops i think i didn't import these things let's run the networks let's let's import things yeah let's uh like if you want you can implement your own bfs but i don't recommend that in the sense that everything is implemented already in network x so if you want to find the shortest path you do nx for shortest path you pass in a graph you pass in the start node and you pass in the end node and it will give you a path and and depending on what the graph is in the sense that this like bfs works perfectly fine if it's just a unweighted network and if it's just a it's just a normal graph it works perfectly fine but as soon as you add weights you add uh you know you add uh directions when as soon as you add weights you need to use other algorithms so there's dijkstra there's uh there's a uh a star so depending on that and the shortage path function in network x will take care of the best way of calculating it for you so if you want to calculate shortest path just use nx dot shortest path and you'll get this path so it says that the shortest path from node seven which is like like like the attendee number seven to attendee number four hundred is that seven interact interactive 51 59 51 interacted with 188 28 interacted with 230 250 335 and 400 so it's a long path right so they were pretty far away and for example if you did like you know g7 and g10 you would see it's it's a shorter path it's and you're not even jumping that much in in like you know in time and everything is you know like because they talk to each other and let's let's do a pretty plot oops let's yeah so if you look at this plot uh we are plotting a path and that's why we have these two parallel things by the diagonal because a path will always be like if we are taking a sub graph or of the the whole thing for example we do a sub graph by g dot sub graph and path and if we just draw this thing it's a path right so it's it's just a it's just a straight path and that's why if you if you plot a matrix plot of this it would just be a straight uh like diagonal plus minus one because it's a it's an undirected graph so it's plus minus one you have these two diagonals and that means that you know there's a path between like let's let's just say that node one and node uh node one and node two there's a part from two to three three to four four to five so if you have a straight path graph this is what the matrix plot would look like and and as soon as we take a sub graph and to take a sub graph uh i'm not sure if yeah you you need to basically pass in the nodes you want to take in for example if we have g dot nodes let's let's do this so we have a bunch of nodes right if we want to take us if you just want to look at how nodes 100 101 and 102 are interacting with with each other what we can do is do nx dot sub graph we pass in g and we pass in a list of nodes and we have this graph that we have created which is like a smaller graph of the bigger graph right and if we draw this out let's see how it looks like so i mean luckily in our case all of them are connected to each other so so we get this triangle looking graph and one thing that you would notice that you know once you have this sub graph even if you name it something like let's say name it q and if you try to add something to this you have your sub graph once let's add things to it it will give you oops q it will give you this error which is called frozen graph cannot be modified the the rational behind this error is that once you create a sub graph it's supposed to be a subset of the bigger graph right so if you want to modify this like you know subset you'd actually modify the bigger graph rather than like you know modifying which is like you know which is a subset so that's why it won't let you uh like not change things in a subgraph and yep so let's uh i mean you don't need to do this exercise so let's you know if you plot uh like these parts with neighbors you can get this pretty looking plot just wait for it so so basically it's like you know if you want to plot the path from node 7 to node 400 like these are the jumps that you need to make if you want to go to note 400 and i can like it's easier to do this in nx wiz than to do it with you know an extra draw i mean it's possible to do it with an extra drawer too but it just looks better to me just more pretty and let's quickly talk about bottleneck note bottle notes and in some cases you do not want bottleneck nodes and in some cases you want bottleneck nodes in the sense that [Music] you want to build uh you know uh internet network or like a local area network which has a bunch of servers and you only want one way out and one way in out of that so you can just like know stop that thing so you want your local area network to look which is to look like a certain bottleneck you know if i remove this node i can be safe that you know this part of the network and the other part of the network do not interact with each other and in some cases you do not want that in the case of like airports you don't want if one airport has you know like some software trouble it should not drown the whole system so to find that we there's something called betweenness centrality and let's look at that so if we calculate the betweenness centrality of this graph so now we have a new answer according to betweenness centrality 51 is not the most important node it's node 100 that's something i'm not again i'm not saying this is the right answer but it depends what you're looking for right and we before to get them to to get a bit more intuition about this let's uh uh oh no it's not sorted oh yeah it's not sorted thanks then good thanks for the catch anyway we can sort this oh yeah i'm not sure if this is the right one oh yeah this is true i think it will sort you in the yeah it's so if you if you saw it thanks to the cat it's again it's not note 51 but note 51 is up there so that's something according to this new betweenness mentality node 188 is the most important node and again that's let's look at why it could be so uh let's just compare these two sorting uh like you know two different kind of algorithms and the plot is like you know the betweenness centrality of a node versus a degree versus degree centrality of a node and if if if it were like no perfect correlation you would have seen a diagonal but we don't really see any correlation here and that's fine because what degree centrality calculates is finding the node which is calculate which is uh which is connected to the most number of nodes and what between a centrality does is it finds bottlenecks i know this may not be clear maybe a picture would help you here so we'll draw something like a it's called a barbell graph because it looks like a barber so what do you like which note in this graph would have the highest degree centrality let me just put down the labels too so like you know we have numbers which notes would have the highest degree centrality foreign sex yes because degree centrality is just you know looking at the total number of neighbors and you know which has the highest number of neighbors and in this case it would be note 4 and note 6 which are right here but if you just look at this network which node do you think is the most important note five yes i mean because five is kind of a bottleneck right yeah yeah yes i mean like all three are important but like if you look at the actual like we look at the score too but uh but the most important one would be five uh like it's the reason is that what between a centrality is doing it underneath is it calculates the shortest path between every pair of node so like you know if you want to go from this one cluster to the other cluster you always have to pass through four five six four five six four five six so all of these three notes would be important but uh because uh like there's this one path that you can go without you can't go with without five so five pops up with as the most uh you know important node and you're right like four five and six all of them are important like if we do uh for example if we do this and if we calculate between a centrality for this graph then uh like note 5 is like 0.55 and node four and six is point five three so it's not like they're not important it's just like node five uh comes as an important node which will never show up if you if you were just doing degree centrality so that's like the point is that you know there's no one right measure it depends what you want to calculate so you need to you know make sure you read the documentation and uh you know and like before using a centrality measure please read the documentation like make sure that you know this is what you want to find out and if that is what you want to find out then then you can make like you know statements like this is the most important note because you know crosstalk because we are finding bottlenecks and we do not want bottlenecks so like if something happened to note 5 or 4 or 6 that's not good but four and four and six is also like they have high degrees centrality so you get that information from them too but you will never get that information if you were just looking at degree centrality does that make sense yeah it's expensive it's it's pretty expensive yeah it's like if it's a very big graph it will be slow yep and that was a quick thing about paths and there are a lot of as you as you can see like there are a bunch of things in there we can't cover all of them but please feel free to go through the documentation and if something does not make sense please open an issue on github and we will try to like you know fix it and or pointed towards the right place and it's going to be four soon so let's take a break for like 15 minutes and we can power through in the last session then all right and please like if you want to know more like please go through the whole tutorial if you if you really like that and if you find any typos if you find anything wrong in this tutorial second is on github feel free to like you know create an issue there if you find something wrong with network x itself feel free to create a issue for that too is the rationale i mean yes i mean i think dan can probably answer that question better and yeah difficult things yeah i mean yeah i mean like like i was like there are a lot of like other libraries which are like much faster but the problem with that not the problem like the trade-off you make is that they would have a lot of algorithms implemented but they would not have the long tail of algorithms that are present in network x like in the sense that network at least in my opinion networks uh wants to be like a reference implementation it's like a reference book right when you look at how it's implemented you look at the code in network x and a lot of like you know the faster ones you know there's one like if you have billion nodes you want to use a gpu there's something called goograph it's by the nvidia forks and but the problem with that is they have like they had they don't have a lot of algorithms available they have like a bunch of nice algorithms that a lot of people use but it won't cover the long tail of all the algorithms that are out there in the world so they're like if you have a proper use case then you know then you could go use them and i mean and our plan is to like you know make things faster but without losing the like you know the readability and like you know the easy bits of python so so the idea is i mean it's way out in the future but the idea is that you know you should be able to dispatch from network x to like you know wrap like a cool graph to like a gpu to like like something written in c like right now it's all pure python and that's one of the good things because it's very easy to install you don't need to compile anything it's just download it and start running but hopefully in the future you should be able to have like more you know faster things let's say i have i want to do something but i don't know how to describe it i i know what i want to do right imagine what to find um the texture algorithm but i don't i didn't i didn't get to the hardware i said okay i'm trying to find the shortest path what is the best way to find the audio that's right for me you know what i mean yeah yeah i mean like i mean yeah and like networks is a pretty old project and it's like like fortunately or unfortunately for a lot of things if you google like a certain algorithm our documentation will be the first one that pops up and that's that's bad things in certain cases because then we are responsible for it but all the bad things in the world but when and like if the documentation does not make sense please please do raise an issue you know be like that i like that i get to close the issue it's fun to close on the show all right so the last one or 15 minutes wake up people all right so we're gonna talk about like just one like a quick theory bit we're gonna talk about structures and then we're going to see the beautiful world of game of thrones and let's start so another like you know very important bit in an in a graph itself is kind of finding structures in the sense that you know there is an edge right we know how edge edges are connected then uh like if you go like you know one step higher in the sense that a complex uh structure like the easiest complex structure in a network could be a triangle and triangles are important in the sense that it could uh like apparently like i don't know maybe it's just like someone told me i'm not sure if it's true or false but like if you for example if you open linkedin you would usually have this thing that people you may know you know things like that like how does that like how do you come up with that recommendation engine right like a very simple and naive way to do that is you know find open triangles in the sense that if i am connected connected to a person a and i'm also connected to a person b maybe person a and person b should be connected right like if you close the triangle you have this like it's a very naive basic recommendation algorithm but it works a lot so you know this is why you know finding those structures and you know completing those structures could be a could be a could be a fun thing and while you're analyzing your data so in this notebook will you uh we'll use a data set uh it's called the physicians network and what it does is it's it's you basically go to like a bunch of you know doctors and ask them like you know who is your friend or like who do you trust for an opinion like you know for a second advice or something who would you like to point towards and this is how you find those this is how you build the network and like you know how the trust is built so let's uh any guesses of how like you know what would be the type of the network i mean it's not quite clear because you know it could be a directed graph too in the sense that if a person a trusts b does that mean person b trust a it could be a directed network but in this case it's a it's an undirected graph in the sense that it's a it's a mutual uh it's a mutual uh relationship of respect it's not just like you know just a trust b and b does not trust a so let's uh let's i mean we don't need to code here but let's just think about like how would you how do you find triangles if i just tell you to come up with an algorithm to find triangles in a network and an and a triangle is just like a node a b and c these three are connected to each other and how do you start finding this if you want to come up with an algorithm like let's start at one node you want to say that oh i can i'm standing at some node do i want to see if there are triangles around me how would you do that sorry yeah yeah exactly so so like what you could do is that you know you're standing at one note right you could look at all of your neighbors right and if you see that if two of your neighbors has one connection then it's a triangle right so this is one way of you know finding triangles the other way is like you you look you you're standing at a node you look at your neighbor and then you look at the neighbors of that node and then you look at if i am in the neighborhood of the heart so you have this triangle so so like these are strategies of finding triangles and uh uh i'm thinking like do people want to do this exercise do you want to write code yes no no okay i have one yes do i get a second yes okay it's fine we don't need to write code it's it's i know it's tiring it's a long day and uh so uh like the the exercise was like you have to test like you know if they're if there are uh if they're triangle relationships or not and the code like the like the code that you you would have end up writing is basically this this is the code like you know you basically go you basically come up with all the possible pairs like if you i'm talking about this bit right here just to highlight this uh there's an inbuilt function called iter tools dot combinations and what it does is that it gives you uh like like for example you put in a you put in like a container object or an iterative and you put in a number and you want like how many uh like how many types you want to like you know take it out of them so i want to find all the neighbors and i want to like you know get two out of them one time and you you basically check that if any of my neighbors if any combination of my neighbors if they have a edge together then i am in a triangular relationship with my neighbor one and my neighbor two right so this is this is what the function is doing and if you find any triangular relationship you return true and if you do not uh then written false this this was the exercise and and you can you can look at you can look at it later and in this exercise what we were supposed to do is we basically plot these triangular relationships and as you can see that what i was doing with these nice explanations is that right now i'm standing at node 3 and if i look at my neighbors for node 3 my neighbors are 67 42 41 9 11. and all of those neighbors have a connection they have a connection somehow between them and we have these three triangular relationships so the these are my these are my triangles which are around me and uh and triadic closure is basically you know a fancy term of saying that you know there's an open triangle somewhere if i close that triangle closure then i get closure and like they can be friends like they should know each other properly and for example in this case uh uh you know you can implement if you want and like this this is the like this is how one of the implementations could look like you basically uh try to create like an open triangular set like to store all the possible uh triangle notes at the nodes that could be in the triangle relationship you go through all the neighbors and if my neighbors do not have a connection between them and they should know each other because that's the thing right like suppose you're at a conference right i meet person a i meet person b if uh first i mean if i'm talking to a person and if i'm talking to person b it maybe i should introduce them right maybe so that's like like if there was a if we live in the meta verse and like you know maybe that person no day when when it's walking around the [Music] world they would have a notification that you know maybe you should go talk to person b because you just talk to person this and then you complete the triangle i'm not sure i'm not saying it makes any sense it's just just an example so if you do the same thing and if you plot these things so we get this thing you know these are the open relationships possible so like you know apparently you know uh i mean you see some closed one but that basically means that you know there are other neighbors which you should be connected to so so if we complete all of these triangular relationships we have a more connected network we have a more you know everyone is connected to each other maybe that's a good thing maybe that's a bad thing that's not our concern but uh this is one way of implementing the recommendation engine a very basic recommendation engine which works which you can explain to people which is not unexplainable right and there's another term called cliques and just like you know like if you not think about network science or graph theory what is a clique in the english not in maths yeah so that's what like in a clique like you know if you have a clique of friends and these are the people that just hang out together they don't really interact with each other things like that so just like that in a network a clique is a set of nodes which are fully connected to each other and what do what do i mean by that so in a triangle is triangle a clique yes because there are three nodes and all of them are connected to each other and what would be a simplest clique in a network one set two yeah that's an edge an edge is a clique because there are two people or both of them are connected to each other in a triangle all three are connected to each other but like how should a four clique look like what do you think if i draw a four click how should it look like yes but just to make sure that everyone is on the same page sometimes uh like this is a forklift just like if it was just a square it's not a full click because in that case the diagonal nodes are not connected to each other so uh like by click and a click is a complete graph that's what it is so complete graph is a term where like every edges every node is connected to each other by an edge which basically means that you can go from one node to any other node just in one jump you should not be able to do you should not be able you shouldn't you don't need to do multiple jumps if you do one jump that should be it that's the idea behind a click that's the same thing if it were a five click or a five complete graph and uh you you would have some sort of this structure inside the network and and what i mean by this needs to be inside a network for example it could be a big graph like thousands of nodes but you have these you know little structures everywhere around and if you find these structures maybe that means it's something i'm not i'm not sure what but it means something right for example if you find open clicks in the sense that you find a you find a structure inside a graph which is almost a five click which means maybe like on this edge is not there which could mean that maybe these these two things should be connected it could be something it depends on the network again so k clicks is basically you know like three click three click four gig flight it just goes on and uh network x does have a way of you know finding all the cliques so uh so like if you want to find clicks and like this will give you a list it's unordered list of like you know all the cliques in the graph and in this in this exercise what you do is you know you just find the maximal clicks and by maximum clicks like you know if you take these set of five nodes there is no way of adding another node here i mean in the sense that if if this node was connected to all the five nodes here then you can add it here then it becomes a sixth click but if you add one more node here and it does not stay a click then this is a maximum click so that's the definition there and the fun property of clique is that a five click is a fourth league four click is a three click three click is a two key and so on for example if if you want to look at this if we nx draw complete graph how many triangles are in there that's a fun question i think so yeah i don't know the answer is like but yeah like so basically the idea that every four click would have like triangles and edges they would obviously have and in a five click you would have if you take any four nodes here that would be a fourth league right so uh the any k click decomposes to k minus one pretty easily that's the idea here and in this exercise the ideas you know you basically find all these clicks and if you have these cliques in a graph that could mean something i'm like i keep on saying these things in vague terms because it really depends on what your network is and what you're trying to find there so i i'm just trying to point you towards things and like like actually making sense is your job that's not my show so you know you can look for these structures and these structures could mean something and uh you can build i'm pretty sure like i'm not a chemist but i'm pretty sure that there's there are some things in like you know chemical networks where they look for cliques a lot so i don't know why they do it but they do it so if if if you are coming from an adjacent field and like maybe maybe you know network science can help you so let's jump to something called connected components that's my favorite bit and now you'll now you'll see something in this network like can we make can we make anything out of this network any ideas what can we deduce from this plot yep yep yeah so so it seems like there are like no four like three or four like distinct communities not coming like components let's call them components and it seems like you know that the top half is not really connected to the other bits there are like three little things right there they're not connected to each other but they are really connected you know in build and if you remember this is like you know this is like a trust network you know friendship slash trust network so it's possible that you know like there's one group of people who do not have a lot of like you know outgoing relationship to other group of people is totally fine and uh like you could you would like you would make that uh you can you can see that from here and again that's the beauty of good visualization is that we didn't calculate anything we just plotted it and uh like in this case even like a hair ball would make sense because that hairball would be like no distinct hair balls if it were like if it were like proper like you know separated entities it would be like distinct hair balls so even hair balls could help here and uh like network x gives you a function called nx connected components and it will give you all the connected components in g and uh to be for connected components they need to be not connected in the sense that if i am in the first component doesn't matter how much i run around like you know from one node to another i should not be able to reach anyone in the second component it's not like it's not just a local cluster like like that that concept of finding like local clusters is called community detection which is a totally different field altogether but in connected components there's like a clear divider there's no possible way unless we develop teleportation i mean by the way there is a there's another research area about teleportation in networks too but let's not talk about that but uh if you are if you are in your component there is no possible way of going to any other component so these are distinct components and like that's the definition so if you do nx dot connected components g you get this generator object and as we know solution to all of the problems of the world is list so if we list it you get this big thing which is basically like it gives you a it gives you a list of sets of nodes which is basically like the first set of node is like the first component the second set of node is the second component third set of the third component and so on so let's see how many components do we have so if we find the length of this list therefore so which matches our initial intuition that they were like these four components of that there's there's a top half and there are three components right there and they're not talking to each other this could this would even mean that because these physicians are located in different cities they don't talk just a totally valid explanation i don't know the explanation it's just so it's just what i'm what what i'm trying to figure out from this plot right so uh you can also like you know visualize this if you want uh as an arc plot and let's wait for this i mean it just you basically like you know find those connected components because we found those connected numbers we we name them zero we name them one we can name them two and three you can name this anything you want but this is one way of like you know visualizing those components so let's and that so this was about structures and hopefully in this uh notebook like you know we basically learned that uh like you know paths is one thing hubs is another but like a group of nodes is also something very important that you should look for and in this we looked at cliques and components the third very important bit is community we don't cover that in this tutorial no we don't but um if you're interested there are there are a bunch of implementations in network x you can go through them there's a lot of literature out there and it's a it's a heavily contested uh thing at least like academic research so that's always fun but if you want to know more feel free to talk to us later on so we have one hour to go and let's talk about game of thrones all right let's do that so if you want to look at this notebook it's in the fifth chapter the chapter is called case studies the first one is game of thrones the second one is airport networks so depending on if you're a airline nerd i am you would you would really enjoy that and again it has all the codes you know jfk dfw like if you if you understand that you're a nerd congratulations and if you want to do that you can do that later on but let's first look at game of thrones and like how many of you have seen game of thrones all right how many of you do not care about game of thrones that's a perfect i don't care about your phones too it's just for the tutorial you see you need to give what people want so people want game of thrones they get game of thrones all right so what what we are going to do right now let's let's look at this beautiful plot so for people who have either read the books or seen themselves this plot may make some sense so what it does is like it is plotting something something oh yeah we're gonna say communities here so that's good it's plotting communities and in the sense that like every color represents a community and the font size basically means which character in that community is important and in this in this very picture that importance is calculated by something called page rank it's also almost known as a billion dollar algorithm we'll talk about that later and like like this is our final aim like this picture was i think i made this in gephi but uh this is what this is this is what we are trying this is what we are going to try and understand what this network means what can we deduce from this and you know can we predict future maybe we can maybe we can't let's see so uh so let's first talk a bit about data so this data was basically like you know so like you know like really nice people out there they they collected this data by they just passed through and this is not from the from the tv show it's from the books so i mean there could be some differences but i'm not sure i'm not not too sure about that but uh so you basically go through a book you know like a pass through a book and if there are two characters who are in a 15 word radius to each other they are connected as an edge that's that's the data set i mean i mean we can debate about if it's a good way or a bad way about making a data set but as we'll see the the like the insights that we get just by like you know running this uh like using this data set is it's kind of nice so maybe this data set collection uh strategy works so like when we load this data it's just a uh it's it's a csv file so this uh like we loaded in a pandas data frame and we have this thing called source we have this thing called target which is basically like two nodes and the nodes itself are the characters so like no adam marvin and jamie lannister are connected the third is basically a weight is basically like how many interactions they have in the sense that if two characters in a book are you know are interacted or like you know they're near each other multiple times that's the count their weight increases that's just a counter and the book column just represents which book it is so there are five books in this data set so like you know you go through all the books and there's something called weight inverse and we which is basically uh like you know you take a uh you take a one divided by weight you just take a reciprocal of that and we'll look at that later why we need that but uh let's see so let's just do some pandas right now no more network x for example if you want to see how like you know rob stark the character is connected in book three so that's just pandas uh stuff right and if if that looks like foreign language it's perfectly fine you don't need to care about that right now so we we see that you know there is these are the relationships from uh rob stuck it's it would be either source or a target that's because this is an undirected network it's we need to check for both where it showed up it showed up in the first place of the second place and these are the other relationships that the character has and we're gonna create a bunch of graphs i mean you could create you can basically create one big graph and there's something called multi-graphs and network x which is basically like you know you can have multiple edges between two nodes if you want that then you can use multi graph but in this what we're going to do is we're going to create five graphs where every graph is basically a book so the character network of book one is you know graph zero because it's a list and like you know so on once we have the graph so for example the zero what graph in this list is the first book and if you want to look at the relationships like which are basically edges right so if you if you look at the relationship network of this first first book which is which is in our first graph we see that a bunch of characters they have their weight and they have a great inverse which we'll use later on now comes the million dollar question who is the most important character any guesses before we start for book one to win prizes we're going to use what we have already seen degree centrality right so what does degree centrality say about the most important character in book 1 and in book five too you know because like in this in this case study we're gonna see how things change like no book one something happens book two something happens things happen right so we would assume that like while things are happening the rankings go up and down because you know someone who's important in book one may not exist in book two right that happens too so let's look at the degree centrality of you know for example uh daenerys targaryen again people who do not know about this it's fine it's just a character just say that and it's supposed to be an important character and it says 0.112 we do not know what that means and i'm fine it's a number but we we need to compare things right so let's let's just go look at the top five important characters in the first book according to degree centrality again i'm not claiming this is the right way of finding things i'm just saying this is one of the ways of finding things so what do we get we get these five names and again people who have seen it people who have read the books i mean it kind of makes sense it's it's not outrageous right these five people pop up as the most important uh characters any complaints till now do you think this does not make sense that stock is important yeah why not and in book one you know this book one nothing has happened yet then robert baratheon that makes sense too tyrion lannister catelyn stark jon snow these are all you know important dish characters in book one so so degree centrality gives us a result which makes sense so like we could maybe use between a degree centrality as a you know character measuring uh metric but let's see about book five so things must have changed right a lot happened just like life so so now jon snow pops up to the top of the ranking this is book five and again uh we don't see it at stark anyway i don't know something happened and uh so things are changing right so like just by analyzing this you could kind of predict if you want to get creative with it you can kind of predict the arc often character of like you know how how they are moving around the storyline depending on how fancy you want to get you can do things like that and uh and like this is one of the theories and let's just take a quick detail that the theory is that every real world network in the world follows power law which means this exponential decay this is a theory which does not it has its like you know people there are a lot of people who oppose this theory too but i mean this is a real world network right but what is more real than game of thrones so and it kind of follows that thing right that we saw that you know most of the people are not that connected but you have a bunch of people who are connected with a lot of people so you have this exponentially decay thingy it's not it's it's not a it's not a statistical thing if you do any statistical test on this it will fail but that's the that's what it kind of looks like there's a there's a there's a dk here and if you want to get really fancy you do a log lock plot so if it were an actual uh like you know exponential decay this would have been a straight line but uh you basically take like a log of x axis and log of y axis and it's not so uh i mean but it still follows this downward trend right so it's somewhere close to uh like expectation in the sense that you know yeah it's a it's kind of following a power law it's again it's not an exact science and uh i mean this is supposed to be a exercise and in this thing we do something called a weighted degree and we just create our own measure in this and that this and the way we create this measure is we basically go to a node and look at all the neighbors and we we also take in the weights of all the neighbors you know like we have an edge with the neighbor right we summed up we sum all of it up and then we like know find the centrality measure now and if we look at this weighted degree centrality that we just came up with uh it still says that adult stock is important so in the first example when we calculated degrees centrality we absol we did we just looked at the structure in the sense that we did not we did not put any like no weights in there so it does not matter like you know how many times they interacted it was just like a weight of one is equal to a weight of 50 in that analysis if we just did an unweighted analysis but as soon as we put in weights i mean there is some change because jon snow went from rank five to rank three but the it's not that use of a change right so it says something that you know even when it's weighted adult stock is the most important character according to degree centrality now let's look at between a centrality if you remember it's like it's it's finding those bottom like nodes and even with betweenness centrality add stock is important so i mean now we can you know maybe you can say with a bit more confidence right yeah maybe it's it's it's something that you know that uh er stock is an important character in the in the first book and uh and even with degree centrality between centrality you can calculate weighted between the centrality and if you look we put in weight inverse here and because we do that because uh if you remember between us entirely centrality is calculating shortest paths and we if there are the weight is high that's a good thing not a bad thing but when we are calculating shortest paths high weight is a bad thing because it will it won't it won't go through that path then it will try to avoid those paths so what we do is we do this like a weight inverse because higher the number of uh higher the number of interactions we we say that that's more that's more important so we just do a weight inverse to like take this into account and even with we add now weights to between us still adults start right at the top so you know now we can be like you know extremely sure that you know yes we have like whatever weight you put in that weight would be used to calculate the shortest path and there's this other thing called page rank and you don't need to do this numpy thing anymore you have to change these things you just do like nx dot page rank and you put in these things right there and uh oops and again with page rank there's a weighted and there's an unweighted one but like like this is where reading documentation is very important because the way page rank works is higher the weight the better so we don't need this weight inverse now i i know this pretty confusing but like that's right read documentation and uh again like we have we have tried a lot of different ways and all of these ways uh test very different things like i have not explained what page rank is but maybe maybe if you have time we'll do that but all of these three algorithms and their weighted version are telling us the same thing so now maybe with some confidence we can say that you know in book one like you know these set of characters are pretty important so so who's the true thing who's the true queen maybe we'll answer that soon so and just to let you know look at the correlation matrix i mean it doesn't really make sense but the idea behind this is that do different correlation matrix [Music] do different centrality measures correlate and if there is a high correlation that means that we can kind of be like you know more confident about our results in the sense that if there's a high correlation then uh then like you know for example if every method is saying the same thing which which is using very different uh like you know ideas underneath then we can be like more confident yes you know the ranking that we get i mean we can come up with our own ranking like you know like a weighted version of all of these measures to calculate the ranking but uh we can look at at least the set of characters which like you know keep on popping up in the top five of the top ten to come up with the ranking so then the next fun part is how do things evolve i mean this can tell you a story without ever reading a book so if you look at this is i just plot for like you know three people it's the degree centrality of how eddard stark tyrion lannister and jon snow goes you know through book one to book five and you see that there's a sharp decline after book one for eddard stark that means something happened we don't know what but something happened and for like jon snow there's a sudden like you know revival from book four to book five of their importance something happened there too so like without ever reading any books not giving any spoilers we can still like you know try to figure out what's happening there and we just this is just a way of get this bit of like you know ugly looking quote what it does is it goes to every book and finds the first top five characters and takes takes a union of all the set of all those top five characters from all the books so you end up with these set of like you know let's call them core characters and we'll see that how the uh like how the how the how the measures change for these core characters so this is the evolution of betweenness centrality and mean it's it's it would be probably hard to read but we see this like a light blue thingy which does something weird right here so in book five stannis baratheon i don't know why is the most important character according to between his centrality what could that mean and you know let's let's not use game of thrones but like there's a character which is not supposed to be important i mean it's not supposed to be that important is the most important character in in one of the books what could that mean according to between his centrality there's one character which which just kind of i mean like that character is not not important but it you wouldn't expect that character to be the most important character but they just pop up yep exactly so so for people who have seen it like in book five something happens with like you know stannis baratheon and and because that character is uh you know talking to two different groups of characters which are very important so that character becomes a important bottleneck node and if you remove that character then you know you are breaking apart the network which is not good and and and when i say breaking apart the network it's not like it's not just like one node it's just you are reducing the connectedness of the network and for between centrality reducing connectedness of a network is a bad thing and and connectedness is an actual term and it's what it does is basically you if you want to calculate the average shortest path length in the sense that you take all the all the pair of nodes in a network and you calculate the shortest path length and you take an average of that so that's like the average shortest path length and if this thing increases that is bad for betweenness centrality which means that if uh if you remove a character and that increases the shortest path length average shortage path length then that is not good so that's that character is important in this case so let's see what's up yeah yep i don't know something happens you know what i mean i mean like he is important right because there are a lot of things that's happening in that's book four so a lot of things yeah that's that's not like synced up that starts from zero right so that's book four thing happened there one thing happened there and uh so so the like this is just my like you know conjecture what's happening there is that in one of the books like this is what stannis baratheon is that you know stansberry athena is talking to like no two groups because like there are two communities out there and this person is the link between those communities because this person shows up in in a conversation with this group and in a conversation with the other group that's why we have this like no surprising result and [Music] and as we know that you know as we saw this before that you know note 5 would have very high like the highest between a centrality all right so let's quickly talk about community detection and now we i'm seeing that yeah let's print this thing out so this is like one way of like doing this and you're building a matrix plot but you basically uh you you make sure the nodes are like you know collected together then the nodes of one are together the nodes of two are together like the first community the second community and this is why you end up with like you know individual uh blocks but if you if you'll see that there are still connections outside of the community this is a different difference between community and components in components this would be like you know actual blocks there is no connection outside their component but in communities you try to find clusters and that could be that there are thousands of ways of doing that and no one agrees on a like you know on like one best way like the one which is most widely used is called luwan community detection and that's what it's what is being used here but people have their problems with that too so it's um you need to do your own research of you know what would be the best in in your case so we do lu liuan and according to that we get these like you know partitions and uh like like if you look at one of the participants if you see like there's a lot of stocks in here so you know it could mean that you know all the stocks are together you know they may they make a community that which kind of makes sense so not too surprising here and uh like one one way of like i like to explain communities is that if you find clusters of high density in the sense that you know like there are a bunch of nodes which are connected a lot to each other that's one way of explaining what the community is it does not mean that they're not connected to other parts of the network it's just there's a high local density of interconnectedness so for example if you just plot one of the you know communities this is what it looks like so you can see that you know nodes are like there's a core set of nodes which are connected to a lot of like no other nodes in the network and it's the same thing if you part like if you plot another one and i can this is not exact science or anything this is just my way of explaining things is that if you calculate the density of one of the partitions and you find the density of the whole graph the the density of that partition is like 25 times more than the the whole graph so there's a there's a cluster of nodes which have high which is high density and like the way density is calculated is like the total number of possible edges there are versus basically divided by the edges there are divided by possible number of edges there are it's possible in a graph which would be a complete graph so if you uh if you have a complete graph somewhere in your network that community would have a density of one because you know it's fully connected so on like you know again not science but on average you you see that you know a local cluster of nodes which we found via luba community detection is 25 times more denser than the the whole graph itself so again we won't do exercises here and the exercise is like you know find the most important node in a partition so so if you saw initially we had these bunch of like you know we found these eight communities and according to that in every one of those communities we have like you know different people who are important so like and one of the communities it's italian lannister the other community is uh daenerys targaryen so like all of these like i mean if you if if you have seen uh this analysis would make sense these are important characters and they are important characters in their own community too so are we like are we convinced that network science can solve all problems see yes right and oh so so we are done with this one and we still have half an hour so let's do more fun stuff so another case study i mean like because i like i would just talk because it seems like people are tired so i would just talk for another half an hour would you like to see leaner algebra or do you want to see airport networks someone needs to say i need something right airports all right see you snooze you lose so so we'll talk about airports now so in this data set how i collected this one is i think there's a like you can go to the bureau of transportation statistics and they have like a nice you know you can take out a you can take a data set out there and it has all the flights like oh no all the scheduled flights from 1990 to 2015 in u.s and if you like you know like i i've removed a lot of things from this data script so like the only thing that i've kept is like you know like the origin the destination and uh like the ear and like the the airlines that flew that uh that that that connection so and the number of passengers over the year that flew that uh that flew that you know origin destination pair so that's that's our data that we have and what we'll do is like for example if we want to find like you know how many how many people went from uh like in new york to austin in the year 2006 the answer would be you know 105 000 and a bunch of like a unique carrier names which we can access but uh you can see that and it's not new york it's jfk to austin so if you put in like you know maybe like uh newmark it's ewr aws you have another 68 000 going from there if you put laguardia there are a bunch of other people not a lot of people are flying in 2006 but but we will see certain things that happen here like you know so what we'll do is we'll create a passenger graph and network network x has these inbuilt functions where you can import a graph from a pandas list or like no from numpy numpy arrays you can do the window those things and you basically tell it what is the source you tell it what is the destination you also tell it we use this thing called edge key it's because we are creating a multi-directed graph that's called a multi-digraph because as we discussed in the very first 10 minutes of the tutorial that airport networks would be directed because if a flight if there's a flight from a to b that does not mean there's a flight from a b to a it should but that's not that's no guarantee and that's why we add this thing called edge key so every edge key would be basically like no year 2005 this happened yet 206 this is the edge so like you know different pairs of source and destination and every edge key gives us the data for every other year and like for example if you want to access this graph so passenger graph is our network x graph we can do like you know as like i mean you can do jfk comma or string too but this is also one way of accessing the graph is like you put in the first node you put in the second node and in a multi-directed graph the third element becomes the h key and our h key is the year so it's jfk austin in 2006. this is the data we have and this is like this is just the uh this is just the uh the data we have and for example if you want to plot like you know how did people fly from jfk to san francisco over the years from 1990 to 2015 this is the total number of passengers and you you see like like certain things happening in 2001 you see like a sharp drop off and and there are certain patterns that you would see all across the data and um and like you know even within 2006 2007 there's there's not a big drop drop between new york and san francisco but in in the bigger data set you would see that they are like you know pretty big drops and i mean you can do this between any pairs of cities right so and you would always see that that you know there are certain drop dropoffs in 2001 and like 2007-ish and 2008-ish and if you do it for the uh like for the like for the whole data set between all pairs of uh all pairs of like no airports there's a drop in 2001 and then there's a drop in 2007 and this pretty much follows like like i'm not sure if the data for 2022 is also out there you would have like a big drop off in 2020 right so uh you would see you can see this uh things so uh what we do here is that you know like this was an exercise you know finding the most busiest routes and like and this is all pandas we're not doing this in netflix so like apparently the busiest route in 1990 was los angeles los angeles to honolulu and people stopped going to holland for some reason i don't know what happened and you can see that you know people don't want to go from at least from los angeles to honolulu i mean you could come up with a lot of reasons i don't know if it's the right reason but you can come up with a lot of them and in 2015 the most busiest route was los angeles to san francisco so you can get all these nice tidbits of information from this data set but now let's look at like the network itself so we create a like a year network in the sense that we just want to look at the 2015 network not the whole not the whole thing so we look at the so the 2015 network is it's a directed graph with around 1200 nodes which are like 1200 airports like unique airports and they have 25 000 edges between them so let's you know visualize them and i mean we need to clean this data because it's not that easy to get all the gps coordinates but uh i got some of them and yeah let's plot this out and remember this is like this is only u.s flights this is what you get like what do you does this make sense it looks like u.s but why do you have these dots right here it's all across the world what could that be any guesses sorry yeah so i think it's like you know you know u.s territories so because they i think they're counted as domestic flights so they are in this data and like if you like you know if you plot it uh like if you want to plot a network this is how it looks like so you can see that this is this is just alaska right there and this is like the domestic network i mean you can't again you can't make things out of this network but still it gives you you know some sort of uh you know some sort of information and the note color is basically like you know the degree centrality so if you really zoom in and start looking into this you you would get a lot of more information but let's yeah let's just take a detour about page rank that we saw in the last notebook so like has anyone seen this image before all right so this image is what you get when you google page rank this is on wikipedia and uh so the idea so the page rank is the algorithm which was behind google that like page rank comes from larry page yeah larry page his last name is page and web page so you know it all worked out that's how page rank came up and the idea is to basically rank web pages that that was the idea right you you want to rank web pages and when you search something on google so as you can see like this percentages that you see and like how big the balloon is is how important that node is and the idea behind this is the basic intuition behind this is that if an important node point towards you you are important it does not matter how many nodes are pointing towards you in the sense that in degree centrality when when you talk about decentrality it's just about the number of neighbors it does not talk about the quality of neighbors right so the idea behind page rank was that you know you come up with this algorithm that lets you you know also taking the quality bit and so you know i think in the earliest day of like you know seo like search engine optimization hacking what you would try to do is you know get an out link from your uh from an important page like assume python.org click on the python software foundation page links to your blog post that is much more important while you're indexing the whole web and like you know calculating page rank then a random blow random blog pointing towards your blog post right so the idea like in seo hacking was trying to get your links doesn't matter what kind of link up in like you know important places so like you know google will rank you up and like it will show you as like in the first result or something so that's that's why it's called the billion dollar algorithm you know because google made a lot of money out of this but again this is not what they use right i highly doubt this is what they still use but this was the initial uh this was the paper that ended up becoming uh google so uh and this is what we calculate in like you know the like the game of thrones network too like you know finding page rank of who's the most important but this makes um this makes much more sense in uh in this in this case study because for directed uh network and in in the case of an undirected network what pagerank does is it just creates two links so that that's something to think about but let's just see like you know how yeah i don't think we looked at this directed graph bit uh but let's just create a small directed graph and see how it works so if you create like g dot next diagraph it will create you create an empty object for you and if you create an edge you know one comma two so and this is like this we already know but for example if we access the edge from one comma two it gives us the weight but what but if we try to access from two comma one it gives us a key error because just because there's an edge from one comma two does not mean that there's an edge back from two to one so that's always something to keep in mind and uh so like this is the basic idea behind this if we calculate page rank for this we know that you know uh like if you want to find the important thing here i mean it's easy it's two like everyone is pointing towards two yeah you can see the arrows right so everything is pointing towards two so yeah two is important like assuming this like the web page of uh network and if you calculate the page rank that's true you know zero point five is the measure for two and you know everything else is much different but what happens when we like you know add an edge the edges from five to six like what do you think would happen to the to the page rank distribution does it change does it not change who is the most important now any guesses still too yeah i mean it should not change much for two itself i mean like yeah six is a bit more important now because there's something else pointing towards that but it doesn't change the fact that 2 is the most important bit but what happens if we add an edge from 2 to 8. so 2 is right here and we add an outgoing edge to 8. what's the most important one right it's hard to like just guess but it's just me that like in our case two is like now eight is the most important one but that's just that's the calculation thing right so it just happened that eight is the most important one but the idea behind this is that because two is important it kind of transfers it importance to node eight because it's pointing towards it so that's the that's the basic intuition behind pagerank and if you want to know more you can go read the algorithm implemented in network x so and and this is very important in like you know especially in directed networks so like you know if you want to find the hubs of uh in an airport network we need to do this and like one thing is that you know net page rank is not implemented for multi graphs that's why we we need to like create a ear by ear graph because um it's just it does not exist for a multi-graph how do you calculate the page base time for that so if we calculate the page rank score for like you know jf new york we get some number but before i run the next cell what which airport should be the most important airport in the u.s domestic network guesses sorry atlanta okay it's the most busiest one so maybe it's maybe it's the most important one too but it's wrong it's very hard to guess this one no it's domestic u.s network see austin no it's not so are we ready for the like more like it's not new york it's not los angeles it's not san francisco it's not chicago i mean they're one they're in the top up there but it's not them let's see it's a and c does anyone know what is a and c yes it's anchorage alaska any ideas why does that happen yep so so like the thing is that if you want to fly to anywhere in alaska you need to go to anchorage and all the important i mean i think this is just this is just my what i understand and a lot of all the important airports will fly to anchorage so all the important airports are pointing towards uh alaska and for example if you see in like this is like page rank if you see in between a centrality it's still anchorage because if you remove anchorage from the network you remove all the small little airports in alaska so you know surprise surprise it's anchorage and but if you look at degree centrality it's atlanta because when you're looking at degree centrality you're just looking at the number of neighbors it has in the sense the number of places it flies to so so you have like another usual people you have atlanta you have chicago you have houston so these are the things that you would expect but that's why it's important like you know degree centrality makes sense but depends what you're calculating right so that's why there's no i like to say there's no right measure it it just depends so no it's not it's it's it's it's just depends on the structure of the network that's that's what we that's what we care about because like in a perfect world you would not really care about passengers and revenue what you care about is a functional transportation network right in a perfect world but this is what happened and there's a nice blog post about this and it basically argues you know why anchor is not actually the most important one if if you want to read it it's a nice thing and but like what happens if we actually like you know try with weighted that's and because you know it's kind of a biggest network and we are calculating a lot of shortest path it may take a bit of time but let's wait for that yeah this is just 215. i mean things will change right so things keep on changing and we can let that run but anyways so the next bit is the other you know if you are running an airline i i assume i hope that you care that you know how reachable is your network in the sense that can i go from one place in like in a country to another place it doesn't matter how many flights but can i at least go there you what you do not want is like no connected components here you don't want a set of airports so you can do where you cannot reach at all that's not good right so let's just see if if it actually works so if you if you find components we're going to do something called weakly connected components it's a directed graph thing that's we don't need to worry about that but this is what we are trying to find is that our all the like are all the airports in this network connected to each other and it turns out no there are three components one is the big one which is basically all the airports then there's another component which is just two airports which means that people just fly from this airport to that airport that's it and there's a third component which is just one airport that means there's a flight in the data set which flies from this airport and it comes back to this airport that's it but that doesn't sound nice right so let's see what are these airports so i mean like i i don't know these codes right but like if you if you google it and if you read about this it just seems that uh like the ss ssp and spb are like c plane airports so even those things are like scheduled flights so they are in in the data set but you know it's just like you know two c plane airport so it just flies from one to other the other other and the third one is aik and it just seems that you know it's it's something is it's an outlier in the data center there's something wrong with it because even the flight that is in there it's it doesn't it it doesn't really make any sense so i mean i feel like we can just remove it so like this is the flight it's just like you know a aik to aik some right air service zero passengers it just seems an anomaly so like what well we don't like them we remove them life is life is fun now so if we do is the graph weakly connected now true so in the sense that now we have a connected graph which means that you can go from one place to another which is a good thing but let's talk about strongly connected it says that it's false well that is not good so the difference between strongly and weakly connected is very important and we'll see this here so if you look at this bit of graph can we go from one node to n to any other node in this network if i if i start from three can i go to every other node yeah so in this case because you can you can just keep on going like this you can visit every node in the network even if you're just following the directed edges right so this is called strongly connected graph in directed graph in the sense that you even if you do not ignore the direction then even if you if you if you just follow the directed paths you can reach every node in this network which is good and let's add a new thing here so we have now something which is pointing from three to four now this is not a strongly connected network because if you end up at four there's no way there's no way of going back which is not good so like imagine in your airport network there's a there's a airport where you can fly two but you can't go you can't go anywhere from there so that's not good right so so what you need is well what you need is a strongly connected component you don't need a weakly connected component here so uh after after removing those three airports we have a weekly connected component but our but it's not strongly connected yet so let's see what is the problem like what are these airports that are not in there so now we have this thing called bce right apparently there's an airport where you can fly to but then you're just stuck there for some reason there's no there's there's no scheduled flight out of this airport and just like that again this is like a small airport i don't think it's it just seems like an anomaly we don't we can just remove it it's a normal we don't care about it so what we will do is we'll just work with the you know biggest component that's what we care about like that's what you want to find so so we have this strongly connected component we remove that thing that we do not like and so now this cleaned up data set is the one that we'll play around with so now we we started with 1258 airports and now we have this 1190 airport we basically removed all the airports which are with bce we don't want to you know we don't want to worry about that right now and now this is the this is a this is like a trillion dollar problem it's um it's about finding the average shortage path length if you're designing a transportation network you want it to have you want this number to be very low in the sense that in a perfect world you should be able to go from one place to another place very quickly if not quickly but at least not too many changes right so this 3.17 number means that on average it to reach from one airport to any other airport in this network you need to change three flights which yeah you need to change three flights i mean which could be good which could be bad but in a perfect world what you would want to do is uh you know bring this number down ideally one but you know that's not effect i mean that's probably not financially viable right but uh like if like if you can come up with a strategy of how to add flights assuming you do not care about money that does not happen but assuming you do not care about revenue money if you were to add flights like where would you add those flights that this average shortest path length goes down and i mean that's pretty much the last exercise right so um and we don't need to write this but you know what are like if people can come up with ideas you know how can we reduce the average shortest path length here any ideas where would you add an edge to reduce the shortest path length average shortest path length um yeah that that's that's that's one way right and uh [Music] i mean like that's like no and again this is not there's no exact algorithm for this but uh like adding like you know the the rules you know when we do this exercise usually it's like you know the role is you can add no more than two percent of the current is like you can't add more than 500 flights and you need to bring that number as low as possible the best i've seen is 2.85 but let's see but any other ideas how we can bring this down and we are almost done with the tutorial so yep we can do that yeah like because we could do that but you know i don't know maybe it will work but yeah that's one way of looking at that thing but anyway so you know like again the the point of this exercise is like you know to think in network you know how to how to add edges how to move with them like how and how adding an edge or like you know removing a node can change a network is is an important thing and i hope like you got like a good at least an introduction of where to look at things so yeah so if you have any questions feel free to ask them if not i'll be around so thank you [Applause] you
16,Ploomber: Developing Maintainable & Reproducible Data,https://www.youtube.com/watch?v=sOgS0QcIEWI,okay so we are going to start now thanks a lot for joining today my name is eduardo and edo here we are the maintainers of plunder so the way this will go is will be explaining this uh can you hear me okay great um so we'll be running this network um most of the commands are for the thumbnail so i would recommend you to open a terminal and then just run the commands that's uh that's going to help you to remember um the details otherwise just like if we keep running her running it's kind of easy to forget so just to exercise your muscle memory please type the commands in a terminal um a few of them are in python but i'll let you know um if you have to switch and it's gonna be mostly a combination of uh running a few commands we're actually explaining a few things then running a few commands and there will be a bunch of exercises for you to do a few things and start learning uh the framework and then we'll help you to get through these exercises okay so let me first get some some background on why we built this library and uh why is it useful kind of the use cases so i spent like about six years working as data scientist and either spent okay i think the same time like 600 years working in that engineering and one of the biggest issues that we saw is that it's really hard to take the take whatever code you put in jupyter notebook into production and production means many things can be i want to schedule a report and then i want a report to like send that to someone or it can be a mobile phone in production or anything in between really and there isn't like any reason why we shouldn't try to improve this workflow like what we've seen in with most researchers and and companies and data teams is that they take these notebooks they're essentially prototyping a notebook then they clean up the code which makes sense right we should always clean up the code but then they start um refactoring this code into python classes and functions in some cases makes sense but in some other cases it feels like we are just trying to fit our code into the target platform like this or airflow and there isn't any value when we are forcing or we are modifying things just to fit whatever platform we are using so that's that's the main motivation for plumber how can we use the tool um that we like the most we like to get your notebooks a lot and use them in a much more maintainable way so we can deploy these notebooks or make them more maintainable that's that's really the most important thing um we'll start with legacy codes so i'll show a notebook that i already have to show how you can get get going from an existing jupyter notebook but this really applies to like even if you're starting a new project you can start this from scratch but in many cases you may already have some notebook that you want to clean up like create a more maintainable project so let me open that one so i'll go to um and please make sure that you are in the root folder of the repo so you should see um the material now i i made a few changes last night so please make sure that you sync with the upstream so with my ribbon case if you if you clone my repo just do git pull if you fork the repo then you need to run a different command i sent the command last night so just go to the slack um i need and you'll see you should see the same files that i have on on the left if you don't um i'll just run the brief explanation and then we can help you all right so let me open the existing notebook so i'll go to material and then notebook okay so on your left you see what's a typical jupiter notebook you'll see that i'm importing a bunch of libraries like the typical libraries like this is a machine learning notebook but this can be anything um just it's just for an example i'm importing a few libraries um important thing i'm naming the sections in my notebook so that's like an important um detail for the next step we we can put any names that we want but i have a load section i'm just importing my libraries the details for this network are not that important it's just to set the the example and then you'll see what we do with with this notebook i'm just cleaning some data and then i'm cleaning that data so i'm getting that data and then applying a few filters with pandas then plotting that data and then a new filter and then a new histogram then i am uh training well actually splitting my data in training and testing so you see that i'm taking my data frame and then that's my x and then this is my y and uh using train test split and then that gives me my training and testing so that's a another section then i am training a linear regression so i just import the model and then fit the model and then generate predictions and have an evaluation chart and same thing for a second model random forest like the same idea import the model feed them all and create a chart so a pretty simple thing let me run run so you can see that it works and this is just like some typical example of something that you may have already for any use case that you have so the only requirement for the next step that we are going to do is to have these markdown headings you can see this is an h2 heading and you can see that this is running so i run the whole thing all right so what's what's the issue with this notebook this is actually a short notebook but in reality when we are doing a real project this may have so this one has like 20 20 sales yeah exactly 20 cells um a real one or a more complex one would have maybe like over a hundred if not more and then things get really complicated when we have a lot of code in a single file for to get started it's hard to collaborate with someone else if everything is on a single file it's hard to test and if we make one mistake um we may start having some hidden state and then the cell's output doesn't make any sense and it's just like really painful so what we do is the idea behind bloomberg is it's a lot easier to manage small things than on a big notebook so that's that's what we're gonna do we're gonna break down this large notebook into something that's easier to maintain and collaborate and we're gonna do that automatically that's that's the important thing because if we go the manual route it's gonna take us like 10 minutes maybe for this one maybe less than let's say five but if it's a longhorn it's going to be like 30 or more maybe we'll make mistakes and we don't want to do that so let's let's go the automated route so that's the very first step in this um in the workshop we are gonna go up to this so i'm in this like this cell right uh is the is the large uh the front large enough can you can you read it okay great um so let me go now i'm gonna i'm gonna be typing the commands on my terminal um just so i i recommend you to do it as well so the first thing is i'm gonna move to my material folder and you'll see that i have my notebook the notebook that i just show uh and then i need to run this command so this surgeon library is also from us it's something that we need to help people adopt bloomberg so the way this works is it's gonna turn your people notebook into many files and it's going to figure out the dependencies and all that stuff and we did that because when we were talking to people that wanted to use blunder they told us that they didn't have time to refactor their notebooks uh they didn't want to risk themselves breaking the code so we need this so they don't have those issues and they can um automate the process all right so let's see how we do this we do surgeon and then we use the refactor command and then we pass the path to the notebook so you see that i'm doing surgeon prefactor notebook i pi and b and then there's a few configuration parameters you can you can actually skip this but um these are useful because you may need to make a few changes to the notebook after you refactor it and passing these flags is going to help with that process so the first one is i'm going to use the parking format for some of the variables because remember since we're going to break down this into multiple files now we need to store the output of the first section and then load it in the next section and you'll see this uh in a diagram and the next one is the file format so if if i don't pass this argument the output is going to be a collection of ip files which is fine but we also have a second option which is the one that we recommend the most that is using pi files and thanks to the integration that we have with qp text you can open pi files as notebooks now what's the benefit that the main benefit is that you can use git github to manage these files because if you push the jupyter notebooks to github then it's a bit difficult to manage and reveal the code that's just like a simple trick to make this um easier but if you feel comfortable with your pterobox you can skip this part and then the output would be ipad mi files but let me pass this flag and then i just type enter and you'll see a bunch of output here just like next steps but since we are in this workshop i'll tell you the next steps okay now let's let's see what we have in our folder now so this is new this file is new this tasks folder and also the requirements so a few things happen here surgeon extracted the dependencies and integrated this file files but you should already have the dependencies if you don't please um do a pip install our requirements actually i'm gonna i'm gonna do that it shouldn't install anything because i already have everything but if you don't please please run it and then let's uh let's see what this tasks folder has we see a bunch of files and if you pay attention to the names you'll see that these are the exact headings that we have in the long notebook right so we have uh clean actually load is the first one load clean uh train test split random forest and linux version and that's that's it's just copy pasting the code and then adapting in a bunch of cells at the end at the beginning so we can communicate this but it's it's exact same code now since we broke down this into multiple pieces now we need to essentially define how we are going to run this thing because all of a sudden it has a bunch of files so if you click here or go to material pipeline general you'll see that we have this new pipeline general file and this is the most important file in in any plumber project because this is where you specify the steps in your pipeline now we are talking about pipelines before we were talking about the notebook now we're talking about the pipeline and a pipeline is just a bunch of steps it's just a fancy way of saying i have a few scripts that i want to run we are going to get to the details of this in the next section but the important thing to see here is that this pipeline channel has two required things for each task the first one is the code that we are going to run and the second one is the output of each task now um the command that we just run generated this file automatically but if we were to start a project from scratch we would create a clean uh pipeline jammer like an empty file and then start typing things but since we started with the legacy notebook we have this already okay now let me run the second command so you see the dependencies in this task so i'll run this command timber plot i'll just copy copy so it's a plumber plot double dash backend d3 so we are going to see a graphical representation of our pipeline and you'll see that it's the same thing that we have earlier so pipeline html now if you open this thing you'll see that this like message just click on trust html right here and you'll see the plot so let's uh see if i can make this larger maybe move this uh here here okay so you see a bunch of things here it's it's exact same um the exact same structure that we have already so i'm loading some data then i'm cleaning that data then i have a train test split and then this is important this is uh pumbaa is figuring out the dependencies and it's trying to parallelize as much as it can and it figures out that if i'm training two models and both depend on the training data they don't really depend on each other right so i can run them in parallel that's why we have this uh structure here and this means that when we run it locally we can run both in parallel and moving for these two it's not a big deal but if we had a hundredth then that would make a big difference in terms of runtime and if and we're also going to show you how you can export this to a larger infrastructure like airflow sloan skin flow and this type of structure helps a lot if you are doing running a lot of things at the same time okay now let's see what's next uh i'm gonna explain in more detail the pipeline channel that we that we generated so let me just run this and this is gonna print the contents um well there isn't actually much to say here apart from what i mentioned earlier that we have the source so the script that we're going to run and you see that i have source load pi clean pi drain test split and then the other two and then below uh the source i have product which is the output and i have two things for you well this is this is actually important i have two things for each task i have these uh data so the the data that i'm loading or cleaning in this case it's actually four files um but i have another thing that's common to each task this nb thing and you see you'll see this is um an i jupiter notebook and you might be wondering why do i have a jupiter notebook as an output the explanation is that each one so we are going to orchestrate the execution of this whole thing each one is going to generate a copy of each file so essentially when we run our pipeline plumber is going to convert this python file into an ip file and then run it and then every chart or every table that we have in the code we'll see a copy now this is important we are running many things or we are maybe running some experiments and we are modifying a few things we can keep a historical record of each execution and then we can compare those executions and this is also an easy way to generate reports because we can change the extension of this we can put like pdf or html and then we get reports for each task for free so that's uh that's why we have this this notebook as an output all right any any questions until here or any issues all right i'll go to the next section then now um if you want to manage your pipeline then you have to use the command line interface right and this section in this section i'll explain the most important commands to manage your pipeline um let me find the right section okay here now if you have everything installed you can run plumber help and you will see that we have a bunch of commands i'll explain some of them the most important ones but after the workshop if you want to learn more um well of course you can go to the documentation but you can also do the double dash help to see uh other other things okay now let's let's run the first pipeline you will be running your first lumber pipeline and all you have to do is execute plumber number build and then type enter and you'll see this loading pipeline output let me make this wider and then the pipeline will start executing meaning each notebook each short notebook will start executing which is what you see here you see it's going um it's executing the fourth notebook right now then it's going to execute the last one it the parallel execution is not enabled yet that's why it's uh running things one by one okay i have an issue with the size of my terminal but it executed correctly and actually if i go to output you'll see this new folder output and you'll see all the files essentially all the files that i show you in the pipeline journal these parche files and the jupyter notebooks you'll see them in the output folder actually let's open one of them uh let's open primary forest you remember this is the output notebook this is not the source this is the output notebook so you see that it's executed in order and it has the outputs so now you can review your results and maybe you change change some parameter in the notebook here rerun and then you can compare those notebooks all right so now that's for the execution of the first first notebook let me go to the next step now you can also use python to execute the pipeline and that's useful if you have extra logic maybe you want to i know maybe check a server or you want to do a few things before you execute your pipeline so everything that i'm running on the terminal you can also run it from python in case you need to add any extra logic and that's what i'm gonna do now you can [Music] run this from the um i'm importing the plumber package and then passing the path to there you can see here on tumblr.spec blackspec and then initializing it with the pipeline general file and then converting that into that object so that object is the internal representation and then this returns me a tag object and then i run build force equals true and i'll explain why i need to pass force equals rule in a second but if i run this the same thing is going to happen the same thing that i run on my terminal you will see that it's running the clean task you can see the progress here now the train test split now the random first and then the last one is going to be the linear regression all right we can ignore these warnings okay now let me show you something here um in when we are doing data analysis in in many cases we modify a tiny part of the code and we want to update the results so let's see what happens if i just run the same command that i ran before so you see i'm turning it around from rebuild and it doesn't run anything you see that it in this column called uh run it says false false false false um that's because i haven't modified anything in my pipeline um so there isn't anything to learn because if i want the code it's going to produce the same output or roughly the same output right because we may have some maybe statistical testing or some processes and the output may change but ideally we should have the same at least for our for our own definition of the same it should be the same output now let me modify one of these files and this is where i'm going to show you the that i can open pi files as notebooks so i go to tasks and let me let me modify train test split you'll see here that it's a dot by file right if i double click it opens as a notebook so it behaves like a notebook but it's a pie file now let me add a print statement just so i have a different output hello from sci-fi okay now i can run this interactively i can come here and and this is a really important part of the data analysis process right finding things interactively see how they change results and let's imagine just like experimenting with my code but at some point i want to work on my pipeline because first i want to make sure that i run if i run this notebook from top to bottom hit once because when we are experimenting if we move cells around we may break it but most importantly i want to make sure that all the dependencies of this network are also executed and let's go back to the diagram i modified oops well it's a bit difficult to read but this is the one that i modified and you'll see that it has two dependencies below so now if i go to the terminal and i run plumber build again now bloomberg will see that we changed one of the tasks and this is actually the first one that it runs and then it goes and executes the dependencies but it skips the ones that go before that notebook so right now it's executing three out of five tasks and it's saving saving some time you see here in the summary that it's keep load and clean now here we only have five notebooks and we are probably just saving like 10 seconds because this is dealing with a really small data set but if we have like maybe let's say 10 gigabytes of data and each notebook takes like 50 minutes or 30 minutes then we are saving a lot of time and that's really important for this iterative process that any data analysis project involves and now let me show you a new command let me go to the terminal if we do plumber status we'll see a summary of our pipeline and you'll see that here right we have um the first column is the name of the task the second one is um the last time we run it so you see four minutes ago four minutes ago a minute ago these are the latest ones 58 seconds 50 seconds then if it's outdated meaning has the code changed or any dependency has changed the output files the docs strings the notebooks don't have any documents right now so this is empty and the location so this is just a summary this is important if you have a large pipeline and you want to locate files or you want to see the status of the pipeline okay now uh let me see how much time we have for this section so i don't go over time i think we have an hour let's see yeah okay uh all right let me just see what's left so i can make sure we finish this first section on time okay cool now let me let me tell you uh one important thing about the integration with uh jupiter lab so we have an integration with jupiter lab that allows you to run things without fiddling with the output path so i explained before that since we have multiple notebooks right now we know how to pass the output of the first notebook to the second one the output of the second one to the third one and so on but if you open if you install bloomberg in the process that runs jupiter lab and then run if and then you open one of your tasks and when i when i say tasks i mean anything that appears in the source uh section of your pipeline general let's let's actually open one um let's open i actually have one open so i'll just click on this one again you'll see this uh new cell now we don't go with these ourselves this is automatically injected by plumber when you open the file and what it contains is the input path you see that the upstream meaning the dependency is the clean task and i have the paths here so then i can use them in my code and i don't have to hard code any paths too so that's that's good and if i change anything in my pipeline general then i'll see a different set of values here and i also have the output path so i i should respect this these values and use this when i um store my output so you'll see that in my code when i load my input data i'm referencing let's make this larger i'm referencing um the upstream variable so i'm loading my input from this variable and when i store my output i am using the product variable so we we follow the we follow what we said in the pipeline journal the product and we use product here and then the other one is upstream now this allows you to go back and forth between your notebooks because i can just open the random forest one and i'll see the the same thing here this is what we call the injected cell and i can just run things i don't have to add any code to make sure that i'm loading the input or anything and you'll see that i'm moving around between files and it kind of feels like a single notebook because everything's connected but it's not a single notebook it's actually five and they are actually pi files so you know we kind of tweak a few things to make this work but it makes it really um interactive and easy to use you have to deal with the details now i i modified a bunch of things just carelessly open and executed files but thanks to bloomer i can just come back to the terminal and execute plumber build and it will figure out what changed and execute accordingly and actually change the load by file so that's the first one and in consequence it will have to run every single step so the bloomberg build is kind of a safe place if you feel like things are not working just go to plum rebuild execute it and it will figure things out um all right let me see um oh one important thing in some cases depending on which jupiter lab version you are running you may not be able to open pi files as notebook at least not with this double click trick so that happens to you you can do right click on it open with notebook if you don't see this option then plumber is probably missing from your jupiter lab now one thing that um might be happening and this is a bit like well not complex but it's like a detail in jupiter like many users sometimes um are not able to do this and and it's uh ninety percent of the time is the same reason when you install plumber make sure that you install it in the same process that you where you execute jupyter meaning you do pip install plumber and then you call jupiter lab because sometimes if you install plumber in the kernel in the process that runs the kernel you won't see this thing and this is especially this is a problem mostly when you are running a hosted version of jupiter because in those cases in many of those cases you cannot run you cannot install new packages in the jupyter process you can only install packages in the kernel so that's um that's a bit of difficulty but we found that many users were struggling with this and we added one command to manually inject this special cell and actually let me recap and explain again because this is an important detail what i explain about the injected cell so when we open a jupiter notebook or a pi file that we want to use as part of our plumber pipeline we have to tell plumber which notebooks we want to execute first what we what we want to execute before we execute the one that we are editing so let me open um let's say linear regression and then you'll see this special upstream variable this is how we tell plumber that it should run one task before or actually it can be many here it's just one with this upstream equal strain test split we are telling plumber that it should execute train test split.pi before it executes this one because we want to use the output of train test split as an input and this is how bloomberg figures out the dependencies and i can i can probably add more things like another and one more and then it would be three dependencies uh these notebooks don't exist so i'll just delete them right and once we specify this uh cell then plumber knows what what to put here in the special injected cell so let me go back to my main notebook and um just to recap so upstream equals the name of the notebooks that we want to execute before and this is what we call and then what plumber automatically injects is this remember what we call the injected cell it figures out the content of the cell based on what we told we wanted to execute before now this happens automatically if we are using jupiter lab but many people like to use pycharm or vs goes or any other editor if you don't use jupyter lab you can execute this command and this will inject this cell based on them on the dependencies that you specify all right uh any any questions until here or comments should i go back and explain anything anyone having issues with the setup nope uh yeah so do you yeah so let let me see if i understand correctly so you're saying i modify i add something here yeah so this is what you edit and let's say you need a new value or new variable new one and let's say new let's see csv now if you uh let me actually save this uh this is train test split so let me open that one you see new one so this is um the the benefit of this is that you don't have to keep track of this it automatically updates um but that's that's that's a good question and if you are not using jupyter lab then you have to run run it manually and run this one now i think this is actually a good exercise let's see that we kind of we made a mistake we said that we were going to generate an output this new one output but we forgot to save it so let's run plumber build and that's why i mean that plumber build is the safe place because it verifies a bunch of things and we'll see the error here okay so we have an error and it says error building task train test split the run the task runs successfully but the out the following products are missing so it's telling you it's telling you you told me you were going to generate new csv but when i when i run my your notebook it's not it's not there um and and this allows you to kind of keep things in check and then you have two options maybe you forgot to store that file or maybe you don't need that output file so in our case i don't need that so i'll just delete it come back and execute plumber build and now it won't complain and you see that it's starting from uh train test split it's not running what happens before that's a bit hard to read because of the font size but you can see that load clean um these last three are not executed because we didn't modify the code okay let's go back to the notebook all right now let's uh [Music] learn about pipeline parametrization this is uh i think this is the last yeah okay this is important and really um really useful for uh your projects so in many cases we want to run the same pipeline with different values um maybe we are analyzing um let's say geographical data and we want to run the same analysis for different areas or maybe we are analyzing financial data and we want to run the same analysis for different uh companies right it's a pretty typical use case plumber allows you to parametrize your pipeline so you can run the same thing with different values and there are a bunch of ways to do this let's go with the simplest thing and the use case that i have here is um having a sample parameter given that plumber allows you to run your pipeline with a single command it's um and i do this with all my projects i always add a sample parameter in the very first notebook the one that loads the data so i can switch that parameter and run the whole thing with a sample of the data and this is what we call smoke testing essentially we want to make sure that things run we are not worrying too much about the output but the very first sanity check that we should apply is is my pipeline running and if you are just checking the things wrong then it makes sense to just run with a sample of data right it depends on your use case but in many cases it's okay to run with like maybe one percent five percent so let's see how we do that [Music] we only have to make a small change and we have to add a new section to our task we add params and below that param section you have to add the indentation here so don't forget that and then sample equals true let me let me do that and then you'll do the same thing so let me go to my pipeline journal load and then params all right so i added this a new parameter and then let me close this and open load.pi and just as i showed in the previous example when we were answering the question you'll see that i now have sample equals true in my injected cell i didn't have to add anything plumber grabs the value puts it here and now let me run the pipeline again and since i modified the first notebook it has to run everything again and now i'll um once it actually finished executing load so i'll open it you see that the output notebook also has sample equals through but i'm not using the parameter so i'll go to my source which is my in material tasks load i won't add any logic to sample my data i'll just add a print statement so when i re-execute this this is going to print the value of my parameter so i'm running plumber build again and let's wait for the load notebook to execute so okay it's done let me go again to material output loads okay it's a sample is true now let's say we want to change the value so i'll do it from the command line that's convenient every parameter that i expose i can switch it from the command line and i'll execute only the first task because this is executing everything and this allows me to show you one new command so i'll do plumber task load and then i'll pass this double dash and then env this is the prefix for each parameter that i expose and then double dash the name of my parameter sample and then i'll say false since i'm running plumber task and not plumb revealed let me see looks like it's not recognizing my oh i know why before i expose my uh my parameter as an argument right now it's it's just hardcoded so i just put sample equals through that's why you cannot switch it so the way to switch it is i need to create a new file so if you only want if you only want to change your parameters from the pipeline jumble you can just do it this way but the most flexible way is you create a new file and do emb.jaml so you see i have a new env.jaml and then i say sample true [Music] and then i go back to my pipeline general and i change this for this placeholder so you see i have a placeholder here and that's the only change that i need to make now if i come back and say uh task load double dash help it will show me which parameters i'm exposing and you see this one so now i can come here and say enb double dash sample false now let's uh wait for it and let's open the file output great sample is false um okay so now this is your time to learn how to do this parametrization trick so if maybe you did it already but if you didn't please make the same change so add these params sample through to your pipeline and then modify load.pi to print the value um make sure that works then create the env.jmo so the same thing that i did i'm actually going to show you how they look like and then make sure that you are able to change it from the the terminal so i'll show you my env jamble and my pipeline channel and i added the solutions for the exercises in a new folder but please try to work on them and then if it doesn't work let me know uh but you have the solutions here so let's uh give um 10 minutes to work on this and maybe run a few commands that i run earlier um and then feel free to raise your hand if you need help from me or if you have any questions right now you can also ask and if anyone got here a bit late and missed the the first uh maybe 20 20 25 minutes we didn't change anything besides the if you go to workshop.md make sure you have the latest commit in the repo the only thing that you have to run to get to the same point is the very first command surgeon refactor and then you can work on the parameter thing okay we are going to start the second block so this is going to be another full hour and we are going to see some advanced features and we are going to start modifying our pipeline to add new things and i'll show you how to create many tasks without much effort so you can run the same notebook with let's say 20 parameters at the same time so all right let's uh let's start so up until now we use the existing notebook and that gave us the pipeline jamal but we didn't create any new tasks we simply added a new parameter and i think that was it the left the rest was left as as is as the output from the initial command that we run but at some point we may want to incorporate new tasks in our pipeline um our project may grow in complexity or we may break down existing notebooks into multiple pieces so we have to edit the pipeline.jumbo manually and that's what we are going to do right now so here i have um the example of how essentially a template of how we add new tasks to our pipeline.jaml it's the there are only two requirements well two sections just give me one second okay um so two required sections for a new task you can see that i have source and here i need to put the path to the new task so far we've been working with pi files i also mentioned that this works with ip files i'm not going to show this for the workshop but we also support functions so you may have functions as tasks you may have a mix of functions and scripts or notebooks and you can even use sql scripts so in like a real world pipeline um maybe a machine learning pipeline maybe you have 20 tasks five of them are sql tasks to get data from the warehouse maybe some of them are functions and some of them are scripts so you can combine them depending on your taste or what's uh required and you can read all of these in the documentation but do you specify the source here in this case it's a path to the file and then the second section is the product so these are the outputs that we are going to generate as part of the the execution of the script in this case or a notebook scripts and notebooks work in the same way functions and sql scripts work a little bit differently but not so much now let's talk about scripts and notebooks so i said um you specify the outputs that you want to create here and then you take those paths that you specify here and you make sure that you store those files there the only the only output that you specify but you don't create is this nb this one this is automatically created by bloomberg when you execute the pipeline so um i'll mention this again because might be a bit confusing but we have two files that have the same source code right so in the materials tasks folder we have a load dot pi file which is the one that we modify that's our source code but when we run plumber build we get a copy of it in material output load ipm so that's the path to this notebook so remember that we have this output notebook and the one that if we want to modify the source code we go to tasks if we want to see our results we go to output and that's pretty much it that those are the things that you need in order to create a new task so that's gonna i'm gonna add a new task to my pipeline and then we'll add a bunch of code and then you'll do the same thing okay so let me go to my pipeline journal okay so i'm in my pipeline jamal and i'll create a new task so first thing is i need a single dash here because this is a jaml file and this dash represents a list and i'll say source i haven't created the file yet i'm just specifying where that's going to be so i do tasks and then let's call this fit and just to show you that this also works with notebooks i'll do ipine b this time and then product and then remember the indentation so i'll add two spaces here and then i'll store my output notebook in the output folder output and then i'll call this fit and i'll show you a new thing here so i'm going to change the extension so i'll do html so the same output just with different format okay now i go to my terminal and make sure that you are in the right i'll give you time to do this but if you are following me make sure you are in the right folder the one that has the pipeline jumble now we'll do bloomberg scaffold so what this command does is that it's going to run through your pipeline jumble and check if there are any files missing and if they are it's going to create them so that's just a convenient way to create new files because it can be a pi file ipan b function sql script and plumber is all about saving you time so we don't have time for creating files because we got more important things to do so we run boomer scaffold all right created we need one new task sources so if i go to tasks and i see this new fit ipnb okay so now this is like you creating a new task and the first thing that comes to your mind is what do i need to use as an input and you remember that you have this trained test split task and if you are going to fit a new model then you should use that one so i come here and when you run plumber scaffold this also creates like some commands sorry some some comments with pointers and i'll change this upstream equals none to a list let's say string test split so i'm saying i want to use that one as an input so i click on save or you can click here as well now i come to file reload notebook from disk and you see that i have injected cell right that that's if if you are using jupiter if you are using vs code or pycharm uh you have to run the other one the plumber and nbc inject but we don't have to run this this one here so let me go back to my fit ipad b and i can start running things interactively so i can run my thing it doesn't have anything so let me put this here i'll close a few things here and let me find my linear regression um notebook because i'll be copy pasting a few things from there so i don't have to remember what i was doing um okay so the first thing is i'm gonna load my input so i'll just copy this cell and you can do the same thing just open the linear regression file and then copy the code we are just going to make a few changes here but for now let's just copy and you need a few imports so let's get this here and you can see that i have my data loaded here i can print so this is why i like notebook so much because i don't have to think too much i just print variables and tables and then i clean it up later and then let's get this this model here and then let's fit that model let's make some predictions and then let's create a plot okay that's that's it we added a new task to our pipeline now let's see the diagram again let me open a new terminal bloomberg plot so we see the new diagram with a new task i'm not in the right folder okay so i open uh pipeline html then click on trust html and you see the new one right and it's it's parallel to the other ones uh meaning i can now uh run this stream parallel this three they only depend on train test split let's go back to the workshop notebook see what's next okay so now um it's time for you to add this fit.i pi and b or v.i it's the same um so remember the steps right uh go to your pipeline jumble edit edit it add a new task let me show you my pipeline journal so you see how it looks add a new section then open the terminal go to materials run tumblr scaffold open that file and then copy paste the contents of linear regression to that one so i'll give you let me see how much how many things we have with this section i think this is going to be short so i can give you more time for the exercises yeah okay um let's say 10 minutes for finishing this part and if anything isn't working raise your hand okay looks like uh we can get going all right so i um briefly mentioned this at the beginning but i forgot that i have a diagram so i'll restate what i mentioned bloomer keeps track of your source code so whenever you modify one part of your pipeline it will only execute that one and any dependencies that come after this is extremely useful when you are doing rapid iterations but also when you are running things in the cloud and that's actually the last part of the workshop running this thing in the cloud because cloud um computing gets expensive especially if you are using gpus viewer can save some money and sometime that's that's great so uh here i have a diagram it's it's actually similar to the pipeline that we've been working on i am this example by plane has two branches it's loading two different data sets like load data and load another another data set then i'm cleaning that data actually both data sets and then i'm training my model so let's say that i modify clean data i just maybe just add a few lines or change the parameters if i rerun plumber build i won't run load data or load another or clean another i'll just run the file that i modify and then the one that comes after um this is really useful especially when your pipeline grows like if you have 15 20 tasks and you can run three or four on a given change that's gonna save you a lot of time so just to exercise this this part of what i just explained we'll do a really quick exercise so maybe let's say five minutes um probably it's probably gonna take less than that but let's say five uh please go modify any of the tasks anything that you have in your pipeline just add maybe a print statement at the end or the beginning then go back to your terminal and then execute plumber build so i'll i'll show you so you see how how this works let me open cannot see my tab bar okay i'll go to tasks fit and then i'll add a print statement here saying hello again from scipy and i save this let me see if i can fix this thing well looks better so i'm in fit.ipm and if i go to my terminal and you execute plumber build only fit.ipnb will be executed so make sure you oh you see oh actually i i probably modified something else um that's why load is running so let's finish this run i'll go back to fit.ipnb modify something again and rerun plumber build so this is a good thing right we don't have to keep track of the dependencies or anything we just come back and run from revealed okay so it's almost done all right so executed everything probably modified something that and i forgot um so i'll just add something else store and re-execute plumber build [Music] okay now it's going straight to fit and you see the summary here it only executed feed so make sure you see the same thing you go modify fit run again or you can modify other tasks as well and you should see that only a few rows from this summary table have this run uh true the other ones should be as false so let's give five minutes and you can practice like maybe modify a few [Music] and then we'll come back to see testing now let's talk about one um feature that's uh pretty nice to make sure that the pipelines have a high quality data which is uh always an issue right real world data has tons of problems and bloomberg allows you to add certain rules to make sure that uh whenever you run something that doesn't meet the expectations that you have about the data it complains as you can see bloomer complains about pretty much everything but that's actually a good thing so we don't make mistakes so let's talk about data quality tests um the idea here is that at every step in your pipeline you can define some rules about what are the expectations about that script or function or notebook for example let's say that you are getting data from a warehouse or maybe some broad data that you got in a csv file or any other format and you know that maybe your model doesn't work with uh empty values like nas or maybe you have um let's say a column that should be a numeric value and should be positive that's also something that you may expect from your data if you don't have these rules in place uh you'll end up checking manually this over and over again and i can give you a concrete example something that happened to me at work i was working with financial data and i was getting essentially portfolios from the clients um at the company that i was working and at some point they had i think some migration in the database and i was getting data that i should not use for my project and i completely forgot and it was actually pretty easy to detect those records and i had a model in production and when i re-run my pipeline my model looked pretty amazing and i was really skeptical [Music] so um look at me i took a look at the model training script that's why i realized that it was amazing i felt really good at the beginning but i was really skeptical so i went back to my data and it looked really different like the distribution of the y variable look really really different and i i didn't believe so because how can the distribution change in like six months that doesn't make any sense so anyway i had to spend like two three hours debugging talking to people who knew the data and then i realized that i had records that shouldn't be there so i had to read lucky look at me i didn't deploy that model into production went back to my sql queries fix the thing rerun it and then my model went back to being okay not so awesome but still okay and then i learned that lesson and that's how i ended up implementing this thing in bloomberg because i needed a way to put those checks in place and force me to run those checks all the time so i didn't forget so that's the idea so let's do the same thing again let's say that we want to make sure that we are loading data and we have some rules about what to expect right so it's pretty simple uh in plumber we can add a new section called unfinish and unfinish takes one argument it's a string and the first part of the string is the name of the file where we are gonna put our tests it's gonna be data under score quality dot pi but we omit the dot pi for this parameter so it's only data underscore quality then we have a dot and then we have the name of a function the reason is that we may have more than one maybe one function per task so we may end up with a data quality file that has maybe 10 functions that's why we have to specify both so let's say uh we are working on the load dot pi and this is our raw data set right so i think once we start working with the data maybe like a few days after we get a new dataset we have some pretty decent idea about what we should expect about that data so for this example this is actually a pretty well-known data set the housing i think it's a boston housing data set and we have the variable this is actually a regression that we are training for this sample pipeline the details are not important but this is the y variable this is the value that we are predicting and one simple rule that we can apply is that we should not accept nas this i mean this is just a rule that i came up with but you can define your own maybe you can accept or you can tolerate up to five percent or you can have any other rules so this is like open and it depends on your data set and then we define a function and this function takes a few parameters they have to you have to put the same names so right now we have a product variable and when you have a product variable in this function bloomberg knows that you mean the output the output from this task so essentially if i have one finish here and my source code is tasks loaded by i'm saying if i have product here i mean what goes here right this is actually incomplete because it should have data because this one has two outputs so this should be like this okay now it looks better and the function is pretty simple i'm just loading the output uh it should be df i'll just fix it so it's consistent okay so i have product and then i have a df inside and that's why when i load my data in my function i am saying in product then df so it pas it gives me this path df and then i'm reading the file and then i'm checking that this column made house val doesn't have an ace this is just pandas um pandas functions so i'll do that let's see uh let's go to the pipeline jumble and then i'll go to my load and say on finish data quality no enace unfortunately bloomberg scaffold doesn't work to create this we are working on it so we'll have to go this low route and create a file so i'll create a python file and i'll just copy paste what i have on my here you can do the same i'll give you some time to finish this so but if you're following me you can just copy paste this um snippet i need to rename this so let's rename and this is actually going to pass because um there are no nas in this data set so let's uh just force these to fail and i'll just do like race i'll just force these to fail okay uh let's uh make sure that we got the right names data quality known as data quality 9s okay good so let's say that we get a new data set we have our data quality tests so we don't have to worry about anything we know we have our tests we just come here and execute from rebuild again let's see what happens interesting i didn't modify load so i should force the task because it didn't run so let me force the whole thing all right so it's executing load okay and you can see this is um the traceback so we format the traceback so it's clear to see what's going on and you can see a summary here it's saying my loath task has some issue and if i scroll up i see that this section corresponds to the load task and if i read the error it says oh no some mistake so in a real uh use case this would be like you have an ace or you have values outside certain range and this is only telling me that something failed but it's not telling me why it happened right that's that's for us to to find out maybe maybe we should be filtering the data or maybe something went wrong when we were loading who knows but at least we know then we find out in our case um we can simply go here and remove this so it doesn't break and then let's uh rerun the pipeline again and then if if there are no nas in that column then this should um execute without any issues all right we passed uh pass them the load task so great we have our first data quality test let's go back to them okay so now exercise um please repeat the same thing that i did so just to recap i added a new section to my loath dot pi called on finish and then i created a new file called data underscore quantity pi with this content now if you run it again remember to pass the force flag so you run the whole thing plumerville.force it should pass because your data doesn't have an ace then go back to the data quality just as i did and just raise some issue run it again oh uh oh yeah sorry i think i i didn't explain that so um i i was mentioning that um where's my file okay here we have the incremental bills right so if we modify some file then it re-executes um if we let's say you run plumber build next time you're run plum reveal it it won't run anything so what we are missing and that's something we have to implement is detect if you added a new quality test or modify the quality test because right now plumber doesn't know that we have a new quality test so it's like i you haven't changed anything so i i won't do anything yeah exactly uh well you cannot define where to start but you can define where to finish for example if you have like a hundred tasks and you have 10 nodes so let's say you have something like this but instead of one you have 10. maybe you only care about one and you can tell figure out what to run but i want the results from this one and you can do that with partial build build i i think it's partially let's do double dash help so i remember yeah partially and then it will run until it finishes that one yep so i've um yeah please um create your data quality task and um i'll give you uh three more minutes because we have to cover the debugging section and then we'll have another break so one thing that i forgot to mention in case anyone stuck with this exercise is that data quality should be in the material folder not inside the tasks folder is anyone having issues or can we can we go to the next section all right okay you know oh okay sorry can you repeat the question oh why why should we in the material folder it's because uh the way the pipeline general imports functions so it looks up in the folder where the pipeline journal is that that's that's why if you if you put here tasks should work or i think you have to create the init file i'm not sure i think it depends on your python version okay um so i'll be quick with the next one and then we have a break the next one is uh debugging so when when developing plumber we spend a lot of time thinking what can go wrong because most of the time things go wrong so i think it's it's important to have a good experience when things are not working and data usually has a lot of issues and models don't work and things break um so we have a simple feature that allows you to debug notebooks we are still working on improving things but we have this simple way of debugging so let's go to the material tasks load and then i am gonna copy paste this um snippet so this is a function that divides two values so takes x takes y and then it computes x over y and here if you take a look at this snippet it's um it's obvious what's what's wrong right so i'm computing one over zero but in many cases when we are working with real code we have so many functions or we are loading data sets and things break and we don't know exactly what's going on so it's important to know how to use the debugger so python comes with a debugger and boomer integrates with that debugger so you can quickly start the debugging session at this specific task so let me copy this and i'll just paste this at the end okay now i'll go to my terminal and i'll show you before i show the debugging thing i'll show you the interactive console because it's also pretty useful for other things if you do plumber interact tumbler will load your pipeline into memory so in a python object that you can explore and you can use for debugging for example so it finished loading and now i have this dag and this is the python internal representation of your pipeline and it behaves kind of like a dictionary so if you do list you're trying to force this into a list it will print the values the tasks that you have in your pipeline and there's a one-to-one uh relationship between the commands that i execute in the terminal like the bash terminal and the ones that i can execute in python i can do dag status and i'll get the same table actually when you execute the command line we are running these um methods in your your dac um you can actually run it from here with dac build you can plot one of the important things that you can do is to get tasks in isolation so let's get loath i need to pass the name and this returns me an instance of a task so i'll do task equal stack load and now let's say i want to um debug this thing maybe it's breaking and it's actually breaking because i added this divide one over zero thing solid task debug and now we are in a debugging session now um you can check out the documentation later but essentially a debugger accepts a few special commands so you can go line by line and move between well different lines of your code so we know that things are going to break here but let's assume we don't know exactly why so i can enter next and this will take me to the next line so check out that i'm in the line number 20 and if i type next and then type enter it takes me to 21. now it remembers the last command that i put so i can keep just putting enter enter enter and that's what i'm gonna do the import statements are taking a few seconds but let's say i'm investigating why this is breaking and so far so good nothing's breaking and now i come here and i know that this is gonna break um so if i do next it will just run and break so i'm gonna use a new command and do step and step means go inside the next level in this case level is the function definition so i'll do enter and now you see it took me from line 73 to line 70 because that's the function that's the line that it has to execute next because i'm calling the divide function so it took me back a few lines and now i can use next and now i'm in line 71 and the good thing is that here i can print the values of my variables so i can do x 1 and i can do y zero so here here is like easy to understand what's going on but if you have like 10 variables or you are calling functions inside functions then using this next next step and there are fewer commands that i'm not going to show today you can debug things pretty easily or at least it's uh it's easier than if you do print statements i sometimes do print statements but sometimes it's a bit more difficult so i use the debugger and here you may say okay you understand what's going on one over zero that that's breaking so now i can go back to my source code and fix it if you want to exit the debugger you can do quit and you go back to the initial state and you can restart a new debugging station if you want and or you can start a organization for any other task so we are a bit over time so we are going to start the break right now uh 10 minutes uh but if you want to stay to run the debugger thing that that's fine if you have questions i'll stay here so we'll we'll start um again in 10 minutes so 10 um 10 20. all right we'll continue so this is going to be a bit shorter like 45 minute session may take even less than that so so far we've seen that given that we can execute our pipeline with plumber build command it's easy to just modify things and run now many times we want to go even further and automate like have a full automation to make sure that the pipeline runs now in some cases it's a bit difficult because if if our pipeline like in many cases we are dealing with big data sets and maybe your pipeline takes a few hours to run and or maybe even more than that so it's not feasible to keep running it uh every time we want to test so let's uh i'll just briefly explain how software engineers do this and then we'll see if we can adapt this this idea to this this pipeline um thing so there's something called well first there's something called testing which means we are going to write another program that verifies that the original program works and for example we may have a function called add and it takes two arguments and then it adds the two numbers so if we want to test that we may have some input test test cases and check that calling add with one and one returns two two one three returns five and maybe we come up with a few more and then we can call a command to run those tests and we may have like 20 50 60 depending on how complex our code base is and since we want to make sure that our code runs every time we modify it or maybe you or anyone in your team you want to run that every time right and usually these these are called unit tests they only shake a small part in your code base these are like easy to run they can run pretty fast the main challenge here is that in the pipeline world these things don't apply that much well we can of course have unit tests but the important thing is to test with real data so we are kind of on a comfortable situation that real testing means testing with production data because if the data changes it may break our pipeline so we want to know but also we wanna test continuously but our pipeline maybe takes a lot of time to run so you know it's a kind of a difficult situation to be in and that's why i think we haven't really figured out what testing for data science means but let's take a stab at this and see if we can come up with some simple simple stuff now if you are using github there's a there's an easy way for you to run your pipeline and actually run any command that you want whenever you push or someone opens a pull request and a pull request is just someone else wants to contribute to your code and they present the changes and then you decide if you take them or not and as part of that process you want to test their code to make sure that it doesn't break your project and github has this feature called github actions um it has many many things but what we want to do is to run some command whenever someone either pushes to the repo or opens a pull request so you can see um you can read the documentation to get all the details i'll just quickly mention what's going on here this is another pipeline uh sorry this is another jaml file and the way to tell github to run this whenever you push to your repo is to put it inside the github folder then workflows then put any name you want in this case it's testing yaml and i'm saying i want to run this every time i push or open a pull request then this is just some configuration saying i want to use linux you can also use mac or windows and then i have a few tips these are pre-configured steps that i can use so i don't have to install python or like do many things so this is just like boilerplate these few lines and then i just install the dependencies and i say pip install our requirements and then i i move to my material folder and then i run plumber build with the sample parameter and look that i am not checking any output it's not like i'm verifying well except for my data quality tests right so that's a good thing my data qualities are going to run here but apart from that i'm just i mean it's good if i can add more data quality test more the better but right now i only have one and my recommendation here is that you have this sample parameter because it can detect many things like i can tell that when i when i was working um because i'm doing plumber full-time now but before that i was working as a data scientist but i would say that in 90 of the cases these pipelines break because you missed one dependency or the pandas api changed like something really simple and the consequence of that simple thing is to break your pipeline um you don't have to check your output you don't have to do any fancy stuff just run your pipeline as often as you can and and you'll find a lot of things now we can get fancier than that and start doing things like more data quality tests more unit testing but believe me this simple thing helps you a lot and also what i realized is that running with a sample of the data helps in most cases because maybe you're if you're training a model and you're training with the five percent of your data your performance is not going to be as good as with 100 of your data but if you are using a column that doesn't exist that's still gonna break so most of things are still gonna break and running your pipeline is not going to take as much as much time so i recommend doing this this is called smoke testing we are not testing the output except for the data quality part that's called data quality or integration testing it has many names and and if you're not using github if you're using gitlab or something different then you'll still have the same features except the configuration file will look different now if you forked my repo you should be able to create this file so this is an exercise um please create this dot github workflows testing jaml file and then put the contents of this snippet there then create a new commit and then push to your repo now maybe if you don't have the fork if you only clone my repo you won't be able to push but that's fine it's not a big deal i can show you how that looks like if you do it so let me go back to my folder actually we cannot see it here but let me go to my terminal [Music] and let's go back to the root folder and you'll see that i have this github folder and then workflows so i have the same thing that i just explained same content and then let's just modify one simple thing actually let's commit the changes that i made probably just the data quality and the new parameter and then i'll just commit the changes and say changes from workshop and then i'll push and now let me find my repo okay let me zoom in and if i go to the actions tab right here you'll see that something oh i see someone open a pull request that's great uh this is you see this like jlo thing spinning this is my change so what's happening is i push to my repo and it's going to run my testing.jmo and we'll see the uh smoke test thing here and if we click here oops now it took me to the wrong place let's uh go back okay cool so now you see it's running my pipeline that's good now in a real use case it might need more setup because you may be uh loading data from an external place or that's usually the the first thing that you have to do get your data into this system so you can run your pipeline or you can run with with synthetic data that's also another possibility the problem is that this uh aesthetic data might not represent accurately how your production data looks like and when i say production it's your like real data doesn't have to be anything that you are like running in production so that's how it works uh you'll see that i have this uh green thing going on here it means everything's working now on purpose let's make this fail and see see what happens the let me let me make the data quality test fail i think i didn't commit all the things because this should break and it didn't so i'll probably miss some of the changes let's actually take so another useful thing is that we can uh check uh the actual source code because we can go to actions and then see uh main and then we can look at the code oh i see why so let's debug this thing it's a good exercise you see that i um this is supposed to fail and it's not failing so let's actually see what we are running and now i remember that i'm running from a specific folder i'm actually running the solution for exercise number five not from material so let me fix that thanks all right so let's go to our um exercise material exercise 5 and then i'll make this fail so the right folder so let's simulate data quality error so this is what i'm going to commit you'll see the um no not this one wrong file all right good so i added one new line i'm essentially just raising an exception to simulate that something went wrong with our data all right let's push this now it's going to take probably like a minute or so to reach that uh point did i push oh yeah okay so same story again i i created a new commit and now i pushed and i just need to wait this is also testing other things because i it has to install all the requirements so something that happens very often is that we forget to include some dependency and then we may have a like a two month old notebook that it's really important but we never documented the requirements and then someone tries to run it and it's completely broken so if we keep running this thing every time we push to the repo then we'll know if if it's not running then we should fix it and then it's just like a sanity check if if these things are running then it's a good thing um if we don't run this often then we risk ourselves having issues like missing dependencies or uh so you mean changing it locally yeah it's the same thing you can use jenkins so anything that allows you to run something when you push it's pretty much the same thing yeah it just changes the configuration file okay so we see here that run pipeline failed and if we look at the traceback you see breaking things on purpose so it's not good that it's failing but it's good that we were able to see this um example um so that's that's it that's it for this uh first part now um it's your turn to do this thing so please let me let me find the right file here so please uh create now remember that this is only going to work if you forked my repo if you push and you see an error it's probably because you clone it that's fine no big deal so if you fork it please create github slash workflows slash testing and then put this content there push and then open your fork and you should see something going on in your actions so if you go to your repo your fork you go to actions and you'll see something here otherwise it will look completely empty now um let's uh let's see how much how how we are doing with time so we have to finish okay we have enough time um i'll give let's say 10 minutes to do this thing now if anyone has any questions about any previous sections or issues or anything in general feel free to ask now so we'll continue with the so please work on this testing thing and then we'll go to the next exercise or you can you can actually start working on it i saw two people open pull requests um so you can also work on that so let's uh give 10 minutes all right uh we are going to continue now um it's the same same topic collaborating now now what we did is we added this automation that allows us to test our pipeline right really simple uh setup but pretty useful now collaboration is a big part especially in real projects most times most of the time it's more than one person working on the same thing so for starters now we have our pipeline in multiple files that means we can collaborate um easily because maybe someone is working on cleaning data someone else is uh training models or maybe working with a different dataset so the good thing is that we can break down the logic into multiple pieces and then different members of the team can modify different parts um and that's great because if we have all the code in a single jupyter notebook it might be a bit difficult to incorporate changes and it's a lot more complicated so that's the good thing about breaking down the logic into multiple things uh now at some point we need to incorporate other people's work in our repo right um or in our branch depending so the difference is when we say pull request it means you have a repo and then you have a fork which is essentially a copy and then you want to open a pull request so you merge that code back in the main repo but we may also be working in the same repo and then you create branches so you have the same repo but different people might be working on different branches and you still open a pull request so it's the same idea except in the first case in the pool in the fork case you have multiple ripples copies of them and the second case you have a single repo but the idea is the same thing now one of the main challenges when working with notebooks and using github for for collaboration is that the inb format contains both code and output in the same file and github doesn't support that impulse request i think gitlab does now or like pretty recently but github which is what most people use um doesn't support this so if i use an ip file and i modify a cell and i add this is what i'm doing here in this image i am opening an ip file creating a new cell and just adding one comment and then let's say you want to merge that in the repo this is what the maintainer or the owner of the ripple will see like you cannot understand what's going on because it contains a bunch of details and it's hard to see if if i should merge this or not fortunately since we are using dot py files it becomes a lot easier because we don't have all these extra format now the disadvantage is that we don't have the output but that's uh that's uh that's one trade-off that we that we have right um hopefully uh github will implement decent uh div soon um but for now we can uh just uh stick with them um by files so let's see and i don't want to bash on github streams by the way i'm sure they are pretty busy all right let's um all right let's uh see what's going on here i see um someone um three people open pull requests uh so this is this is interesting because you can see here that i have a green uh tick here that means this person um either open a pull request and his code pass so that's good he didn't break anything we don't see this here possibly because they uh fork the repo before i added the github thing so that's why it's not running so now if i come here i can see that he made changes to a few files and since we have dot by files i can see the code clearly right even though edo can edit this from jupyter lab i can still see the code and i can put things like comment previewing his code um say y4 maybe 4.4 and then we can start the conversation about this code and these changes and then i can once i i actually one thing i just recently learned is that you can pinpoint to um like a certain number of lines so i can see i can just click on this and then drag my my pointer and you see i'm like selecting two lines because i want to point something there actually let me just with more lines okay so these four um this is not great feedback but it works uh and then i can just tell edo that i want him to change a few things and i can say request changes let's get that uh okay submit review all right and then he will get a notification and then he can address the changes and this is great because we're still using jupiter now here i see another pull request oh and and this is actually pretty useful because this is something that i um i'm a mistake on my end so let me explain how what you're saying here on your left you are seeing the workshop.md that i have and i made one mistake because i had double dash env single dash sample if you run this it's going to break so my mistake but thanks rob for fixing my code thanks a lot so he realized that we have that issue and he fixed it by adding the double dash so now i can just come here and say approve and commerce and that's it now we have his code which fixes uh the mistake and that's great now if people people can uh pull this new version and they'll see the fixed thing now uh let's see it's gonna here oh this is also pretty useful uh thanks brian for this pull request uh let's see um okay yeah this is this is nice so uh github actions has certain commands um that allow you to do certain things so i use the cd command to move to a certain directory but brian is recommending us to use the working dash directory thing which i think it's it's good because it's uh it's cleaner and this supports any uh operating system right so if i were testing with windows linux and mac then the cd won't work on on windows so this is good so i can come here and actually one thing that i skip in the previous pull request is that i can approve running the the test so let's see what happens let's uh run the tests it's going to take probably like a minute or so and this allows us to quickly check if these um changes that he's proposing um are gonna break anything so hopefully not because it's just changing the configuration let's uh it's quite um in the meantime if you are able to [Music] open a pull request that's great so the way it works is i i cannot unfortunately i cannot show it to you because i cannot fork my own repo but i think i do i think i do if i have different accounts yeah well i have a few other accounts but i don't want to create repos there so i can click here and uh let's say you go through the different um like the configuration and you click on fork uh an easy way for you to open a pull request is just to use uh github from the browser so you can for example come here and [Music] click on in this thing to edit online and you can add a comment hello or something like that and uh then you can open a pull request i can't remember how it looks like i think yeah i think you have to click here on pull request and then you should look the same um and then you should click here and compare across forks because this is just comparing within the same repo and then you find your name here [Music] create pull request and then i should be able to see it hmm okay something something's not working so let's see let's see what's going on oh i think it's because of the test yeah it's because my pipeline is is broken so that's that's also something important to mention um if the pipeline is broken then every pull request that someone uh creates is gonna break as well if um unless they fix the precise issue that we are dealing with in this case um the pull request is only addressing the configuration but i still have that that mistake um but anyway i i know that this is this is going to work so i can just merge uh and you can also merge this actually this is gonna create a few extra files so i won't mesh this right now uh so that's that's it that's how um pull requests work and just like to quickly recap what i um what we did is we configured github actions to run a script whenever we push to the repo and then once we had that automation we created a pull request now if you can create a pull request that's great um as part of the exercise um if not you you get you get the idea you can modify the code now thanks to the automation that we have that automation will run as part of the pull request and then if things are working well then we can review the code and mesh it and add comments so that's a great way of collaborating and given that we break down the logic into multiple files then it becomes easier to maintain and also since we are using pi files instead of ip files uh we can see the differences in the scripts like did i add a new line did i deleted something that's pretty useful um alright so let's see i think we have seven more minutes yes okay so i'll give you um seven more minutes to uh go through these two exercises like the automation for github and then open a pull request so if you have any issues so one maybe you'll have the issue that when doing a git push you'll see an error a permission error that's the case uh that's probably because you cloned my repo instead of working to fork you can click here on my repo you fork and then you'll be able to modify that and push um so yeah hopefully you'll be able to open the pull request and and also to create the automation so um six more minutes and then we'll have um 10 minute break and then we'll have the last block which is um running things in the cloud so how do we run this pipeline in a larger infrastructure that's going to be exciting all right so we are going to enter the last block thanks a lot for your time i hope you're learning and this is this been fun so the last part is about scaling up your experiments so how can you reuse your pipeline the first of it first part is locally how can you parallelize experiments essentially taking each notebook as a template and then running it with different parameters so there are many many use cases for this uh but i'll just show a pretty common one on one that i encounter at work and the reason why this feature exists is that let's say you are working on a machine learning pipeline and you want to use the same training set but for different models and then you want to see which one performs best maybe you are tuning hyper parameters and then you have some evaluation plots and and you want to compare those plots to see how different models perform now thanks to plumber given that it generates an output notebook we get all these notebooks uh for free and then we can get those notebooks compare the charts compartment compare the metrics and then find out which one is best now in many cases you want to use the same code and only change a tiny part of it just like one parameter and that's the use case that we have right here we are going to use the same fit dot pi but we are going to add a new parameter and then we are going to pass a bunch of parameters and we are going to create copies of that task okay so essentially we are going to do something like this we have some training data and then we generate a bunch of tasks in parallel so how does that look like in plumber we create um a new section called uh let me enlarge this grid and then we'll we can pass a bunch of uh parameters it can be more than uh one in this case i'm just passing uh model but it can be many and then it just computes the different combinations like if i have my parameter called a with values one two three and then b with values let's say one two three one two three again then it computes the product of those like one one one two one three and then it's like nine combinations that's why it's called a grid in this case we only have one parameter meaning um we are going to create one two three four tasks that uh with the same and we are going to execute the same file so let me let let me go back to my uh pipeline.js and let's make the changes here so the first thing is i'm going to tell plumber to execute things in parallel and that's it now um i'm gonna change this fit task i need to add a name because they'll get different tasks we'll get different names so i'll name fit dash there's a way to customize this but just for simplicity this will generate fit dash 0 fit 1 until it finishes with all the parameters now let's actually pass the parameters so i started with grid and then i passed the name of my parameter which is model and then the value so i'm going to copy paste this so i have to type the whole thing okay so this is what's going on i have my uh this is actually a notebook so i gonna create i'm gonna use this as a template and then the first copy is gonna get this value then the second one this third one this and the last one this i am not using the parameter yet in the source code so that's that's the next step but right now i'll show you what happens if we do boomer plot have a different configuration so that's why you have to pass this this parameter but you should have um i think you don't need it okay all right now we see that this pipeline looks different and now these are the previous tasks that we had linear regression random forest and fit now fit disappeared and converted into fit zero uh so from fit zero to fit three right so all of a sudden we have with the same script well actually same notebook we have four tasks and we can run them in parallel but before i run them i have to use that input parameter otherwise it will just create the same output so i go to my tasks open fit and you'll see that it it has to select one combination so it's just arbitrarily selecting the first parameter that i passed and then i can run things interactively so let me run this so you see that it has the this um this parameter oh i deleted my output so let me come back and execute plumb rebuild so it generates everything up until then actually i don't want to run the whole thing so i'm going to create a partial build and i'm telling clover execute everything until you reach train test split because i need that training set so i can run things interactively to develop my new fit task and this is running things in parallel it actually doesn't have anything in parallel because they all depend on each other so we'll just wait okay now uh let me go back to my feet and then this should run now okay great i have my input now i need to change this code because um here i'm hardcoding my linear regression and i don't want that i want to take that input parameter and based on that parameter i want to instantiate a different model so i'll just show you the snippet you can copy paste this and i'll explain this i'll explain what's going on here let me paste this here um so let me print the value of model it's a string and it has the name of the module that i have to import right so model uh cycle sqlearn dot linear model dot linear regression this is exactly how you import the model so i can can do from learn port missing one in here so you can see it's the same thing right now i'm gonna dynamically import these using the function now let me see i'm going to take that um string i'm going to split it based on the dot and i'm going to leave everything until the last dot on module name and everything after the last dot on attribute so let me run this and let me print this so we see what's going on here so you see i have sqlearn dot linear model and this um this is like a convention in python meaning i'm not gonna use that so i'll just ignore that it's it's a dot actually i can print it here it's a dot an attribute now i'm gonna use import lib which is um part of the standard library in python so you don't have to install anything extra and now i'm gonna dynamically import that module with this import lib.import module and this is gonna get me the scikit-learn module so i'll run it and you see this is essentially the same thing as doing import scikit-learn.linear model except we are taking the string as an input and now i get the attribute and that's it now i have the class just the same thing as i did here now but now we are using the input parameter so i can delete this thing and let me actually change the name of this variable because it's not a linear regression anymore for this case it is but for the other cases it's going to be a different thing so i'll just call it um reg okay uh one thing is this is returning me the class right not not an instance so actually let's name this rig class and then drag and now it works okay now let's run the whole thing and um see if this uh works i'm gonna remove this last argument so hope hope people can see the bottom actually hit me here it is okay now i'm going to execute plumber build again and now we'll run all the fit actually linear regression random forest which are the ones that we already have and the new ones uh it's gonna run as many things as it can apparently you're seeing that it's a lot faster because since the models don't depend on each other they can run at the same time okay it finished now let's actually take a look at the output and see if we have different models we can name this which is probably useful because fit 0 doesn't mean much we can add the actual name of the model but for simplicity i kept it that way so here we see that this is the output notebook right and this is the one that trained a linear regression and we can see here linear regression let's open fit one this got the support vector regression and we can see here that it's printing the other class the next one has the gradient boosting regressor and same story here and finally the random forest right uh now if i um if this um like a real world thing i could take those notebooks and compare them and maybe so one important thing is how different models behave differently right um and this chart is like really basic thing but i could compare those like put actually let's do that let's take this one to the right and um let's keep this one in the left i mean they look similar but maybe this dot is interesting to you because why the random forest has this dot here and the gradient boosting doesn't have that is it noise is it the model who knows but it's important that we can quickly generate these reports that's that's why the output reports are important and and i generated this as html you can share this with your colleague and ask ask them hey look with this chart and then look this one this looks different so this is pretty useful we also have another package that extracts the outputs and compares things so essentially to automate this manually comparing notebooks process uh but maybe maybe some other time but that's the idea you get a bunch of output notebooks and then um you can compare them you can run them in parallel a pretty pretty useful thing to do now it's uh your turn to to run this exercise so let me get to the workshop md so now uh just to recap what um what do you have to do is to change the pipeline jumble the first line that uh please add a line at the top that says executor parallel and then change your fit task which is the the one that you added in the previous exercise if you didn't do not have this you can edit uh random forest or linear regression which are the ones that come already with the existing notebook and they have pretty much the same code then you don't forget to add the name that's important otherwise bloomberg will complain so add a name whatever you want i recommend adding a dash at the end so you have zero one two three with the dash uh splitting the name and the number and then the most important thing is the grid then below that you have the model this can be any name grid has to be great but below you can select a name and then a list of values now this given given how we modify the the task this has to be valid scikit-learn models so any typo here will break your pipeline and then you go and modify that task and change the logic that creates the the model and let me actually modify this um because i i skip one important part so please um if you copy paste this the snippet then take a look at the screen because i'm gonna make a small modification because this is gonna be the class and then and then this is going to be the instance so it's important that you that you have this this is this is what you have to add to your fit fit task so you dynamically import the model and then you get the object and then you can call fit on this on this one okay so i'll give i think this one involves many steps so maybe let's see how we do in 10 minutes again if you have any questions raise your hand i'll leave this thing so you can see what what needs to change so i'll start a bit early because um we don't have much time left so this is going to be actually pretty easy um i'll just uh explain a few things about the other open source package that we have and then it will make a demonstration of executing uh things in the cloud okay so pretty quickly so far we've been running things locally which is uh great in many cases i think if we are not dealing with very large data sets we can just get a big big machine and run things with bloomer build and and it's all good but in some cases we may want to run things in parallel or get like really large machines or another reason is we may want to save some money because if we have a big machine open running all the time can get pretty expensive especially if we are using gpus so we can export our polymer pipeline and run it in the cloud without any code changes it just requires a bit of extra configuration but we support exporting uh to kubernetes using argo workflows so we automate the process of generating the app so for those familiar with argo we export the spec which is the spec for you we create the docker container we grab your requirements and create the image push the image and those things in in simple words what we are doing for those who are not familiar with kubernetes or argo we take your code and generate an image a token image which is just like an abstraction allows us to freeze your code send it to a different machine and execute it now we can execute it in different machines meaning we can parallelize a hundred or a thousand of experiments using the grid filter like that's pretty useful when you execute things in the cloud parameterize uh send your code to a cluster and then execute a hundred things at the same time but we also support other back-ends um q-flow we also support that that also runs on kubernetes um actually it also runs on argon workflows but it has a few different uh things uh now slurm we also support slurms so um i think that's probably going to be attractive for many people in the audience because we got that request from uh some researchers so they were using plumber i think they are doing computational chemistry and astrophysics so they had the notebooks and they were having lots of issues to scale that um in their slurm cluster and they were doing things manually uh copy pasting code from the notebook to a script and then figuring out how to run those and then figuring out how to run those in parallel so we wrote a grabber i think it took us like two or three days and now they are running things in their in their uh slurm cluster so that was like a great great story so if you have any requests like features or anything uh feel free to ask us we are always happy to chat with the community and see how we can help we also support airflow that's probably more applicable for people working in industry that have this workflow orchestrator same thing we export your plumber pipeline to airflow you don't have to code anything we give you the airflow back and then you can run it and finally if you are using aws aws has this aws batch service which is pretty neat it allows you to spin up machines i shot them shut them down after you run certain script if you use barebones aws badge it requires you to learn quite a few things so if you use our integration we do all the things submit your jobs as long as you have the aws account and and a few things configured it will run your notebooks in the cloud um so you this is open source this is free you can choose whatever backend you want and and run your pipelines we also have a plumber cloud service which automates all these things all this configuration and we have a free plan and also we always work with researchers that want to scale up their their code so if you are running into issues with your infrastructure you can talk to us we can give you free gpus and you can help us develop a plumber by using it that's the best way to find issues and the best way to get feedback just like use it break it and then let us know uh all right now um since using this the open source thing takes more time more setup we'll show you how to run things in the cloud using polymer cloud but internally we use our open source package we just have a bunch of other things all right so i'll let edo run the last um part of this workflow workshop and he will show you how to use plumber cloud so what we'll see now is how bloomberg cloud works uh as eduardo mentioned there is a point where we want to scale we walk on a local laptop and we want to scale our experiments run multiple pilot experiments for that we develop bloomberg cloud um there are instructions in the guide of how getting an api key i already have it set it in so i don't need to do cloud bloomberg cloud set key and oops jumped for a second all right so what we'll do is first of all i'll list the previous executions that i have so let me clear the screen and what i'll do is plumber cloud yeah one sec all right let's move this one a bit here all right so it's a bit unreadable that way okay bloomberg cloud list and what they do it lists all of my previous executions so what i can see now let's give it a few seconds we can see there are a few historical runs two hours ago 14 hours ago and eight days ago um what i will see uh what i'll try to do now is i'll list this specific um execution and what i'll do is plumber cloud logs and i can give it the execution once again what happened here yeah then let's copy the id over here on the top i think then yeah it broke because of the zoom so and then what we do it actually brings the detailed execution of what we executed in the cloud um we can see here all of the tasks were executed five out of five we can also see what's the actual status of my pipeline so let's do bloomberg plot i already have it here in materials let's open the pipeline let's trust html and we can see it's basically the same status that we had with edvardo before we actually executed it locally what we can do now is bloomberg cloud field and then it wraps everything into a zip file uploads it into plumber cloud um it ignores the output because those are files that relevant to local runs and then it's starting to build but they do behind the scene it goes to to our cloud it executes it through a ci and then it saves all of the artifacts what we can do also um and you can do it on yourself like once i i yeah sorry for that like the hdmi is for the key when i move the laptop and so what we can also do here is we can actually go down a bit to the cloud section we can also take all of the historical artifacts so whatever we have in the output folder uh plumercloud saves it locally and then we can download it into our laptop so let's do it now let's create a temp folder so make deer temp let's move to temp and then do plumber cloud download all what it will see what we'll see now it downloads all of the artifacts of the local files so we can see all of the reports the outputs the products and that way if we want to compare for example um one gr grid experiment to the other or different models that's the way to do it um if i'll go here let's go to outputs for example then i can see let's let's remove this a bit further that's the maximum all right so i can see like all of the train test splits over here the pico files the actual data if i want to save it [Music] and yeah that's that's pretty much how it works now you can do it on your own there is a instructions over here on how to get a key basically you need to go over the registration flow uh you put the email there is a cloud free tier where you actually get up to 50 hours per execution per month um what's that ah per day sorry for that um and setting the key is pretty easy so far we've been working with the cli same for the cloud you can do set key set the key that you get from the console and then you can start building besides that there is a few helpful links in the bottom so there is the documentation we put a lot of effort into making it uh as detailed as possible api calls and so if you see anything that's missing just let us know um of course you know like we put a lot of effort for it it's an open source so if you can please show your support uh starting the repo i joined the slack community we have an active community of 330 20 people and it keeps growing by the day um [Music] anything else
17,hvPlot & HoloViz,https://www.youtube.com/watch?v=bGbt_-WDz3k,increase the size a little bit here uh we're going to download the project into this directory i'm going to make a new temp [Music] and i'll run the download command oh okay it's not going to fetch the project and we'll have it locally here i can now cd into it and this is our project from here the next step will be to run the notebook if you're familiar with kind of if you've used a notebook before you can choose whether to do any kind of project run jupyter notebook which is the classic interface but you can also do jupyter to lab instead which is what i'll be using to demo so this is now going to install the project so it's gonna go fetch the oh i missed the step i did not export this absolutely okay um so yeah so so far we've kind of installed hopefully you've installed mini conda or condo um you've fetched the anaconda project if you're on an m1 mac you've set this environment variable which unfortunately you need for now and now we're going to do anaconda project run uh jupiter lab or jupiter notebook depending on which notebook interface you prefer all right that's now fetching the packages might take a little minute um so if you want to follow along and you're not set up yet you can go to installation if you're just trickling in so we're now solving the environment it's installing it thank you if you have if you've got everything set up make sure you want to make sure it's working so there's going to be a notebook in the tutorial folder uh so examples tutorial if you go to setup we'd be good if you run through that notebook which basically just checks that everything's installed correctly okay uh so after it's it's absolutely it's also it's also there at holidays.org okay so yeah another kind of project uh run jupiter notebook or droop your lab should be everything you need and that should launch a jupiter server [Music] and basically this is what you should be seeing now and again once you've done that go visit the examples tutorial open the setup notebook and run through it and it looks like it's pretty good see no reds i don't see every all the greens yet but all right so you're gonna have a little bit of time to run through things uh you're gonna have to have a little bit of time to run through things while i kind of give an old broad overview of what we'll be covering and what these tools are so don't panic if you're not ready good to what's the atrocity conduct where it kind of puts that principle out what it would take to activate that environment okay you don't have to do that it would work but you don't need it you can do the whole path all right i think we're almost there now all right ready to go uh av are we okay are we okay okay all right let's settle i guess okay i think we're good yeah all right so i'll one last time if you're not set up yet go to holovis.org installation follow the tutorial installation instructions uh you're going to have a little bit of time while i go through a quick overview um yeah all right welcome to the hollywood tutorial in sci-fi 2022 is uh the first time i have stood up without a mask to give a speech to a bunch of people wearing masks so very hard to judge how i'm being received so be elaborate with like no that means stop what that means what that sort of thing so i'm jim bednar i am the director of custom services at anaconda but that's the last i will mention anaconda because that's my employer this is the holovis tutorial holovis is an independent open source project i work on it as to four or five of the people in this room um so uh philip rudiger is the driving force between most of holovid behind most of all of his most of the lines of code that are in there are his and most of the architectural decisions are his and i kind of coordinate things so he's the real person who makes things happen and he'll be giving the tutorial until his voice gives out and then if that lasts the whole time fine if not i'll take over and uh so for those of you following at home uh the way to follow along with this tutorial is to go to holobiz.org h-o-l-o-v-i-z.org and follow the tutorial instructions under the installation tab so everyone here should have already done that if you haven't done that put a little red post-it note and one of the people here will help you the people to help you are sophia over here ian over here maxine over there me jim and philip if he's not talking all right so i'll hand it over to phillips thanks so much isn't it exciting to be back at sci-fi it's been a long few years i'm very happy to be back and welcome to the hallways tutorial um so if you are not familiar with all of this you might ask what is it um so about seven years ago uh we joined anaconda and yes that's also the last time i'll mention anaconda um and started building various tools for work doing interactive exploration of data in notebooks in particular although all the tools here work just as well in a notebook or if you're more familiar with kind of working in ides and so we built tools for quickly uh exploring data visualizing data and then publishing the data and sharing the results with other people and so all the tools that we build together kind of work together to allow you to have this workflow starting with some data you explore it in a notebook or in your ide you then iterate and iterate and test some hypotheses um you then generate some figures and now you're at a point where you want to share that and so that's where some of our dashboarding slash app development libraries come in and really what we'll be focusing on is a this exploratory step and then moving on to the sharing step and so all of these are a set of tools that help you do this had to take you through the entire process so hollows means the whole and so it's meant to give you this entire process from exploratory analysis to sharing your analyses all right so you might ask well does holobiz not already does python already have enough packages to do this and yeah in fact oh yeah in fact it's so so many libraries that it doesn't even fit on the screen um so yeah the python visualization landscape is huge right you've got various libraries particularly macbook lube came very early it's a great library to kind of generate static figures and then there's uh various javascript based tools to kind of do interactive plotting and it's just a huge variety of tools and we were like well let's give people the power to kind of yes do static figures but also let's give them the power to uh do interactive plotting and so on uh really dig deep into your data do things you can't do with static tools and then also work with very large data and so we kind of built an ecosystem around this that will let you explore data easily [Music] why are there so many tools in the python ecosystem well again it comes down to different requirements right you've got requirements where you want to simply build a quick plot you've got very very customized figures that you want to stick in your publication but then you also have this process of kind of interacting in jupiter and there's very various jupiter-based tools and then you kind of move on to tools for kind of building applications on a server that you want to deploy um and all of these conflict right there's always like a tool is best for a particular job and that's really not what we wanted to do we wanted to kind of take you from this from give you the power to do all the things that you might want to do so yeah that's how this helps it's we have tools that work very well in web browsers so that they can easily be shared and so you can have remote data you can have remote compute execution and most of our tools always see focus on python that's why here at scipy but we also don't want you to have to write kind of javascript or css although if you really want to customize things you can do so um and you can also use these tools whether you have very small data sets that you can easily display in a single browser or we have tools that allow you to scale out and we'll kind of give you an overview of what all these different tools are so it's a little bit abstract right now um and what we really want to focus on that this interactive interactivity works the same whether you're in a jupiter a notebook environment or you want to deploy it or you have a workflow that's not jupiter-based although everything here obviously is going to be based on notebooks and one of our real mantras is that we want high-level interfaces that give you the power to do things quickly give you the power to kind of explore your data very quickly but that there are not dead ends at any at any point you can kind of drop low into a lower level api a lower level interface to really customize your plots further could build um customize your applications further and so lastly we kind of try to focus on apis that you already know right we there's enough libraries out there so we don't want to come up with too many new mpis that uh you're not familiar with and so that's really the focus of this tutorial uh we're going to be focusing that's a lot of libraries uh we're going to be focusing some on api as you hopefully are already somewhat familiar with um yeah so finally i'll give you a quick overview of the different libraries that are that are encompassed by the hall of his name um so the first one is panel um we're going to get to that towards the end of this tutorial panel is a library that allows you to quickly prototype applications in your notebook and then deploy it um and so we focused on kind of giving you the power to use all the tools that you already know if you're a polo abuser you can use mountpoll lib to generate your figures if you already know if you use plotly or bokeh you can use those right and similarly there's a whole page of various components that you can use with panel so really the focus there was a take you from notebook to deployed app very quickly and b give you the power to kind of leverage the tools that you already know similarly hb plot is a higher level interface on top of some other libraries we built in particular holidays and geo views and in fact data shader that lets you quickly generate plots in a single line so if you're already familiar with pandas.plot hp plot is basically that except it gives you not just uh static multiple lip plots but bokeh plots that are interactive that let you kind of explore things with widgets that you let you drill down into your data into very large data sets using the data shader library um so data shaders library to quickly rasterize really really large data sets it's a million points a billion points trillion points you might have started after scaling out start scaling out to a cluster um but that's what that gives you and then there's a few libraries that are kind of uh in the background well lumen in particular is a library that's we're still developing it's a low code solution on top of panel that lets you build dashboards with the yaml file we won't cover that today but what you might see a little bit today is param param is basically the lower level api that everything else is built on it gives you basically it's if you're familiar with python data classes it's very similar to that but actually precedes them by about 12 years something like that um and that's what kind of allows you to link widgets to your plots and kind of give you the interactivity and finally there is color set which is just a set of perceptually accurate color maps whether continuous or categorical that you can use to kind of accurately plot your data so that's a lot of tools but we're really going to be focusing only on panel and hp plot here and dig a little bit here there into some of these lower lower level libraries that kind of make up this entire ecosystem all right so yeah the for us the ecosystem becomes a little bit smaller these are the tools that kind of we leverage um as part of this ecosystem it's you can see here basically a lot of these you can just drop your plots into a panel but then also all of these and geo views and hp plot kind of allow you to generate plots with potlib allow you to generate plots um with plotly and bokeh so yeah in this particular tutorial we're going to be working with earthquake data a fairly large data set it's it might not be relevant to all your use cases but it's it's a great data set to kind of work with for several reasons it's pretty large so we can kind of show you some of the um support for large data we have it has many different data types so it has date times it has place names it has magnitudes and so we can kind of really see how how we're going to get a feel for the data of these different data types using these tools and finally yeah there's two core apis we're going to be kind of looking at in particular there's the hb plot dot plot api um or in fact uh hb plot is what we're going to be calling mostly so if you've ever used pandas or maybe x-ray you know that they basically give you an ability to quickly generate a plot in a single line from your data frame from your data set xray data set and so hvplot again just lets you do that in an interactive way so that you get bokeh plots plotley plots mapoli plots you get widgets and we'll see that and then secondly we'll be looking at the dot interactive api so interactive one of the core superpowers really of pandas and xray is that it lets you build these pipelines um data processing pipelines right you chain a bunch of methods together you do i don't know you re-sample your time series and then you do an aggregate um or well i don't know if that makes sense but you get that you get the idea you can chain various operations and dot interactive gives you the superpower that at any point in that data pipeline you can insert a widget instead of a regular argument which really if you think about it that really gives you the ability to kind of instead of typing 10 times like you change some argument 10 times and re-execute the cell you can now generate a little app to kind of scroll through different resampling intervals or whatever else you want and so dot interactive gives you the superpower of using apis that you already know but making them interact and the final section in this tutorial will take you through using panel and the panel in this process of this tutorial will be building various visualizations tables and so on um and now we come to the we come to a point where kind of yeah this analysis is interesting i want to share this with my colleague i want to send it to my boss because he doesn't like looking at a code um and so that will let you kind of share build you a little application that looks uh pretty nice uh to share with in your organization all right um so that's the kind of broad level overview what you'll be learning today um so let's jump straight in i think mostly everyone looks to be set up so we can jump in from here let me just move over to the tutorial all right so as i said before you have the choice of uh using either classic notebook or jupiter lab um i'll be using jupiter lab just because it lets me navigate more easily between these different notebooks and so on um but again if you if you actually like drupal lab you can do anaconda project run jupyter lab instead of jupiter notebook so we're going to jump straight into the first section of this tutorial so again yeah we're going to jump into the if you go to example tutorial number two plotting uh that's where we're starting okay so this will be an introduction to hp plot in particular um as i mentioned already many libraries actually have a plotting api like dot plot on pandas and so in particular obviously pandas does x-ray um if you're not familiar with xray xray is basically a library for working with raster data sets it's basically a pandas version uh the it's a pandas for graded data so images stacks of images and so on so it's a great library i don't know if it applies to your particular domain but if you do i would check it out and hp plot also lets you work with other data sets so das network x and a few others but we'll be focusing primarily on uh data frames so as you usually do you start with some data in this particular case we are starting with this earthquakes data set um if your machine is a little bit ram limited i would recommend kind of taking a subset um maybe a million rows instead of let me remind myself i think it's 21 million is that right oh i think it's two million okay well 2.1 million okay that's not too bad hopefully that fits into your memory but if you find your kernel crashing for whatever reason i would recommend just taking this upset working with yeah absolutely um so the easiest way to do that is to just basically do you use the i like method of pandas and i guess that nope that's too many one oh that is very surprising okay well the easiest way then then is using sample precisely uh so yeah oh yes that's obviously yes oh my okay i'll just write it out because apparently i can't do exponents okay so this will still give you oh right i was just confused by the shape here uh so yeah if you if you want to do that you can subsample with the sample method and give it n and then one million okay so we're in fact actually going to work with a smaller data set because most polling libraries are actually not too happy if you just particularly interactive following libraries are not too happy if you're dumping two million data points into your browser your browser starts complaining a little bit and things slow down or crash entirely so we're going to start with a very small fraction just one percent of this data set which is still 21 000 data points and if you've ever used um the dot plot interface you would have seen something like this right you can just uh call use the dot plot accessor on this data frame you call scatter you tell it i want to plot long longitude against latitude and you get a one map that's pretty nice um if you've uh in your environment you should also be able to kind of switch between different microlit backends so for example um this is a nice neat little trick so you can actually get um interactive plots with mappolib just by changing uh my polyp inline to matpotlib widget and so now we have kind of a somewhat interactive plot here we can drag around you can see it's not super smooth right when i drag around it's it's not perfectly smooth but i can resize i can do a few things so that's pretty nice uh but really my project is right what's actually happening here is it's rendering the plot every time i drag it and then sends the png to the browser um which is different from what we get if we use hp plug so if we just by importing hp.pandas we're basically adding this hp plot accessor to our data frame and now instead of calling dot plot we call dot h v plot scatter and we get an interactive vocab plot so that's pretty neat there's a lot of affordances that gives us right we can now use the zoom tools i can box zoom around explore this data set like this i can hover over individual data points to give me the exact coordinate or if i were to say basically i can give it a hover column i can basically say i want to also see the magnitude i can that did not work i think it's called actually called mag yeah i can now hover over it and i get the magnitude and so already we get a lot of affordance just by doing this one import and changing dot plot to a hp plot pretty neat it's a good start so yeah let's try as a simple very simple starting exercise try that just change i just gave away the solution um just you can give additional arguments uh to this plotting function for example to change the alpha the opacity of your of each point um because actually if you look at this we're kind of massively over plotting here right there's a lot of there's a lot of points in one place and i can't really distinguish them and so changing the opacity actually kind of helps so just a second to do that um another thing we can do is kind of use different plot types um this is actually again overplotting can be solved using different methods one way to solve over plotting is manually adjusting the alpha another is to use a plot type that aggregates the data so another one we could instead of using dot scatter cam again giving away answers here instead of changing the alpha which kind of now we actually see where there's a ton of earthquakes compared to let's compare these two right uh we can now see that there's here in the pacific west there's a ton of earthquakes um using different plot types for example a hex bin plot we can aggregate our data in different ways um to see the same detail but using an aggregate right and so again you see here the highest concentration is here um but one thing we lose here of course is we're now aggregating the data we no longer kind of can hover and get information about each plot point each point in the data at this point right we've kind of i've hinted to you change the alpha i've hinted to you change uh the plot type to hexpin right but of course to be effective you kind of need to be able to figure out these arguments yourself and hp plot has various uh approaches to making this easier uh one important one is just tab completion if you're not familiar in notebooks if you press tab you kind of you access your some object and then type something you can do tab and the tab completes something for you and if we tab complete on the hp plot accessor we kind of get a list of different plot types that are supported of course it depends on like what is your data we might not be able to generate a um i don't know oh lc plot which is a way to visualize stock data doesn't make sense for this type of data of course um but this lets you get help for different plot types the other thing is for the arguments of a particular plot type you can use hp plot.help so if i do hp.help scatter right i get a list of kind of the core arguments um and in fact we've mostly tried an hp plot to make sure that the dot plot data interface mirrors the dot http plot interface right so that all the arguments that you usually pass except for a few kind of big size that's very specific to matplotlib that might not always work um but in general we've tried to kind of keep api compatibility there so here we can see right the core arguments here x and y for scatter make sense uh c is basically a color if you want to color by a particular variable right we might want to do a small df dot hb plot dot scatter x longitude y equals latitude c equals mag and now we're coloring by the magnitude and we get we get a color bar and all the other nicest messages um there's a full list of parameters that are supported and kind of tries to explain a little bit what they are and what they do we obviously won't go through all of those because that's a lot um but they roughly categorize into kind of uh the core parameters that specify what are you plotting right how do i map various columns or fields in my data set uh two visual attributes that you're putting on screen and then various options that control kind of various things about the plot color limits whether i want a color bar or not various things about the axes how large why wide and tall is my plot and so on and then the other ones are data shader options we're going to get into that in a minute again data shader is about aggregating really large data sets and we'll see that um if you've got um geoviews installed which is kind of has a few more heavy dependencies because the geostack and python is kind of a pain to install um but if you do have it installed um you can use these geographic uh arguments to kind of put a coastline in the background uh project your data in various ways and do a bunch of other things put some tile kind of a tile source in the background and then there is here just a list of style options which control various aspects of your plot line colors and so on okay so hopefully that helps hp you also have the ability to kind of okay i want to only see some subset of the options um but yeah play around with that see um it's just a quick way to get get information okay next um let's again what we the first thing we did before we got into the plotting right was take a subset we took one percent of the data here and still we had a bunch of them we still had a bunch of points on screen too many to we were over plotting even with that one and so that's where uh lit data shader comes in data shader we're now uh yeah here we're plotting the small data set again we can enable data shader simply by calling by telling it to use rasterization or rasterize equals true uh and what that does is basically take our data set aggregate it which means just compute a an image and we send that image to the browser which means that no matter how large our data set is on the server what we actually send to the browser what we display is much smaller and so if we do that you can see here obviously small data said it still works but if we do it with a large data set we now have all the 2.1 million points and on my local laptop kind of i'm bounded by the memory of my my laptop right i can i can load probably about a billion data points onto this laptop and the nice thing here is we still get the interactivity right we can still if i zoom in i can still scroll down to the individual data point um but at the high level i get an overview of all the data and so we don't lose kind of the fidelity of individual points um what we do you because it is still in aggregate we don't get kind of magnitude information about every point but we get count information within each pixel and so that's pretty pretty neat um if i had a billion data points this would still respond pretty interactively um one thing to note here is that i've kind of changed the color normalization uh color normalization is a pretty deep concept right if i uh by default you're used to using linear color maps right um not that nice it doesn't really like okay there's maybe like two three pixels uh with a ton of data points in it and the rest is just flat it's just using the same color and so um one approach to change that is to use log normalization and now we see a lot more structure right we can now see clearly differences in magnitudes right there's three four here there's 40 50 400 in this little pixel here but if you have really weirdly distributed data you can use something called histogram equalization which trying to kind of tries to make sure that uh for each color um points are equally distributed between the different colors and so uh it results in a weird color map and maybe you shouldn't use it to interpret things uh kind of quantitative things but it gives you a very very clear idea of the structure of your data so it's usually a good way to kind of do exploratory analysis you've got this huge data set you're trying to get an idea the feel of the overall structure and then after that you can kind of dig in and okay maybe log is enough maybe even linear is enough but it's a good way to get started um okay so in this tutorial we're focusing primarily on pandas we're going to do a little bit of x-ray um if you have even larger data we recommend using das das basically lets you scale out your data frames um to a cluster or multiple uh processes on your single machine hp pod works just as well as patented with dash as it does with pandas so you instead of doing import hiv plot pandas you just do hp import hp das um and it dot http works just the same we also have support for a library called streams for streaming data we have support for a library called intake for which is very good for data catalogs so you specify i don't know your organization has a entire catalog of data and you want to catalog it intake is a great solution there we also have support for geopandas if you work with kind of geographic vector data geometry data and we have support for network x for basically plotting graphs you can check all of that out at hbplot.holoviz.org all right let's do something let's do a quick exercise here we've got our larger data set um let's filter our data down a little bit so take only earthquakes with a magnitude larger than five remember that the magnitude two column is called mag filter it down and then just change the color map let's start there and go from there give me a minute to do that that's good okay i'll give you another few seconds and so hopefully most of you have some familiarity with data frames so this probably didn't present much of a challenge here um so we only want it um earthquakes with magnitude greater than five so we use the kind of syntax to filter that we then do h dot hb plot scatter again uh one thing we have to be careful of and that's kind of a to-do item for us is if you forget if you're working with this large data set and you forget to give the rasterized argument you are going to crash your brother so i should have warned you earlier and hopefully none of you did that um so longer that is a very good point in fact i think someone brought that out the last time we didn't fix it um yeah that's a very good point it should have it it's a it's a bug thanks for catching them i think it's yeah it's um yep that's true well that should also be the default so i'm a little bit confused here yep there's um good catch all right uh so let's keep moving um hb plot has quite a range of plot types uh we're not limited to kind of so far we've seen scatter plots fine we have the ability to do statistical plotting right aggregating your data um we're gonna here um first clean up our data a little bit we make a copy of our data and then we're going to only choose um earthquakes where the magnitude is greater than zero because you can't have a negative magnitude earthquake but of course real world data is not clean and we didn't pre-clean it because we didn't well yeah i wanted to give you an idea okay so again um you can use the monopoly interface to do this and as you can see here we can just the same we can use dot e3 plots to get a histogram plot um maybe yep let's give you a minute to do the exercise which is just to generate a kernel density estimate plot of the same data and again if you don't know how to do that you can just do the tab completion trick and run that and find out which plot type you might you're meant to use make sure to tell it which column you want to plot we want a kernel and density estimate plot of the magnitude this is actually somewhat slow because computing kernel density estimates over 200 2.1 million points is not entirely trivial um okay so statistical plots uh there's a number of types um statistical plots is anything we where we're aggregating there right we're doing some aggregation whether that's simply counting the points um like a histogram or doing a kernel density estimate or um yeah you can do that in 1d or you can do that into the actually the hexman plot we showed earlier is another example okay so we'll move on uh let's actually deal with some categorical variables um so far we've kind of plotted x versus y y where both are numerical values um but we have good uh hp plot handles um yeah categorical variables perfectly well uh we here we're going to use some pandas magic to kind of create three different categorical classes right we're going to create a shallow we're going to split our data into shallow earthquakes intermediate earthquakes and deep earthquakes easy enough we simply use the p cut method on the depths column to find our our bins and then give it some labels specifically shallow intermediate and deep and now in our clean data set uh let's have a quick look at it it's always good to inspect your data we now have this depth depth class column in our data and so to do categorical plotting by some variable right or we can simply call we can pass this by argument and because histogram plots aren't um you have to layer them or stack them we're going to actually change the alpha so that we can see all three categories through here um this is one way to do it um obviously this is not that readable so what you might try is yes we're going to still use the buy arguments but instead of plotting it on top of each other like we're doing here we actually want them side by side and so a simple way to do that is to use the subplots argument so i'll give you a few seconds to do that oh yeah oh yeah it's one thing that's because again we kind of get the affordances of an interactive plot and we can show and hide the different categories and so one thing actually that would be nice is if we zoomed in onto the categories that were currently highlighted um we can we can also control the um alpha of the muted categories so if i i could in fact hide them entirely if i set muted alpha equals zero and hide this uh now this plot is hidden entirely so that's maybe a slightly hidden feature and then we can zoom in and see the distribution of this class versus the others so hopefully you've had a minute to do what i was just showing you uh you can do subplots equals true and then instead of overlaying them we get them side by side maybe that's a little bit wide right so that's also a little bit narrow so we'll just do 300 and now we get three different plots side by side uh one thing that hp plot does or in fact the underlying library hall views does to kind of be helpful is it links axes together so if i zoom on on this magnitude axis down here it's actually zooming on all these three axes that's sometimes helpful sometimes not helpful in particular it's also linked kind of the magnitude or the count axis here right um we can disable that by saying link axis equals false well that's actually not shared axes sorry let's share it axis equals false um and now they're independent axes we can zoom on them independently and we actually see kind of the axis here the axis labels are separate so we can see kind of it doesn't show you the absolute comparison between these two category three categories uh but we can actually see the distribution right it's not squashed into a little thing okay we're going to keep moving here so grouping is another so this is another superpower that's basically you're just not going to get out of dot plot uh which is grouping so grouping previously we use the by argument to kind of group by some variable uh but maybe we don't want all of these on the screen at the same time right we've got a huge data sets we just want to see a subset right now um and so grouping basically instead of using bio we use the group by argument we're now giving you a widget that lets you change each category and so here we're creating again another statistical plot which gives you kind of a contour around the densities of these points and so here we're plotting depth against magnitude and we're kind of looking at the distribution of these three um and we can use the widget to kind of compare so this is helpful particularly helpful if you're going sliding through a huge data set of different times right i could have yeah it's very helpful to kind of explore larger data sets and you can group by not just about one variable but multiple variables right here i won't demonstrate that right away we're going to now create another categorical variable again by using the pd cut feature the panda's cut feature to kind of classify earthquakes into different magnitude classes so these are the official definitions of different magnitude classes uh so we're going to classify um earthquakes with magnitudes greater than eight uh into great earthquakes uh and then so and so on here's the table that tells us what this uh categorization means and now we can basically do an aggregate over these uh and generate a heat map over these variables um again so we can work yeah hp plot is very versatile you can work with basically data of any type um and it has a wide range of plotting methods so here now we can kind of get an idea of what kind of earthquakes are most most common and where how deep are they usually and it seems like light and shallow earthquakes are almost common so heat maps again are useful for comparing categorical variables and we'll get into kind of other raster like plots a little bit later okay um i've told you um that yeah so it pandesal plot gives you the ability to generate multiple lip plots um hp plot originally was written basically to support only booking plots which is we worked very closely with the bukit team it just made sense to focus on that and gave you the ability to do um kind of uh these exploratory and interactive plots but um in fact more recently uh we've now added the ability for hp plot to also output mapper lip plots itself also outputs plotly plots itself so you don't need to necessarily switch back to uh the regular uh pandas dot plot you can still use it to generate maple lip plots and if you just like potly a little bit better than bokeh because you like these i don't know just like the look would be a little bit better you can now also leverage plot it's not quite as well supported um so you will probably find a few more rough edges um but we're also working on addressing that and so here yeah basically you can use the same plotting library to plot the same high level api to plot uh using all three different quality libraries which we think is pretty neat okay so yeah that's an overview of the various features in hb plot one other thing is of course you might want to save this you can one nice thing of course of notebooks you can share the notebook all the interactive features still work some features won't work right for example uh data shader in particular every time you zoom in and out it's re-rasterizing basically the python kernel is remasterizing your data and so if you export the notebook to a standalone html file it's not going to do that but most of the other features such as zooming on pots and all the nice features of bokeh you will still get in a static export what you can also do is you can export hp plot two different two uh different file types you can export particularly to png and svg and you can do that for uh both uh lib and poke pods so my project of course supports outputting to png and svg directly okay has a little bit of a workaround which is it actually runs it silently opens a browser in the background takes a snapshoot screenshot which maybe is kind of crazy but it works um so even if you want to do that let's quickly do that um here we're actually exporting the apollo plots that's maybe not that exciting but you can see here the hp plot save method quickly lets you export any pod that you generated and so that also means um you can kind of get access to an uh figure and then you might be a master at maple lib right and so uh if you render your hb plot object that you generated with hp plot uh two map public figure you can drop down to the map for live api and do whatever you want right you can use the neat little feature of using xkcd for styling with your output and now we can generate a little plot in the xkcd style and that goes back to the point we're making earlier we always want to give you the power to kind of drop back to apis you know um hvpod's api is powerful but it doesn't extend to kind of having every single tweaking every little thing about your ticks or whatever you want and so if you want to do that you can always use hp plot dot render to either render to a map live figure uh or if you're using bokeh to block a figure and then you can use that api to kind of do fine final tweaks that you might want um yeah that covers the introduction to plotting any questions at this point yep ah that's a very good question um so in the past um okay let's take a step back so rasterize data shader actually contains two components right it's a pipeline of things you're doing rasterization just means aggregate your data into a fixed size pixel buffer and then that means basically what you're getting out is an array of values right and so the in the simple case is just count values at each pixel location um what data shade does is take that and actually render it into an rgb image and so you can do all the what you would usually have your podding program do you can have data shader do basically color map map oh okay so where was i um so yes um so the shade part of data shade basically means color map or um yeah color mapping yep yeah that's a good point um this also however means um okay where are we going pretty long with all this stuff so yeah i could switch here to uh from rasterize to data shade uh i also want to switch back to oh i don't actually know how to switch back using sorry i just need to switch back to bokeh okay um so you can see here the plot actually looks identical that's because we actually support all the color mapping options in bokeh now that we used to only support in data shader but there's a caveat right when you hover you get no information um in fact i guess if i try and enable uh hover over equals true i can try i'm not sure oh yes what you get is this very helpful rgba value which tells you exactly nothing unless your machine i guess um so we really in most cases we really recommend using uh rasterizer because it affords you it gives you a color bar you can actually yeah the actual data is still there it's not just transformed into this abstract rgba value no problem that's yeah i should have covered that earlier um i think i've forgotten our yeah so i've talked at you a lot there was a few exercises there but they were maybe not that engaging so we're now going to do a little bit of a longer form exercise um so if you in your tutorial folder you using classic notebook you'll have to navigate back to your file browser one second um there is an exercise folder and we're going to be doing the plotting exercise i think i don't know what the timings on that are i forget i think i'll probably give you a few minutes to kind of run through this and then we'll quickly run through it together if you have questions i will be available uh so maybe i'll give you oh already after plugging i guess that was a pretty long one oh uh so for anyone watching at home and maybe you guys if you kind of go away from this and you still have questions and you don't know where you get an answer you can go to discourse.holobiz.org it's where we kind of suggest a forum to ask questions about any of these libraries and we're always happy to help so how's everyone doing anyone done how's everyone doing i know the last part i realized i maybe should have gone through tutorial three first um so if you're not getting that one um it actually involves laying two spots on top of each other um and that was meant to be covered in section three uh but it's a good lead-in right so yeah we'll we'll be covering that in a minute so i'll quickly go through this right how's everyone doing did everyone anyone finish stuck good great it's a good number thumbs up uh so i'll quickly go through this um how are we feeling should we just power through the next one and then take the break i think so okay um yep um does it actually do anything where is that oh i think that's probably a leftover solution that should not have been in there um oh what it does okay i'll show you what it actually does you can tap on individual points and so in certain cases that's helpful in particular if you're interested in getting the location at that point and we'll show i think there should be something lighter where we actually use that no it information not be yep okay so pretty far right um this exercise is really a about working with um graded data and so if you're not familiar with xray again it's just it's very similar to pandas but for graded data so here we have this 2d array um pretty large actually 8 000 by 4000 pixels so that's pretty huge see most of it is nan's um it gives you this nice little wrapper um to kind of show you the coordinates and the actual underlying array um and this particular data represents the population densities across the world and so that's interesting right so you have information about populations you have information of a different type about where earthquakes are and now you want to combine that information in some form right um and so uh hb plot or the underlying library hall views makes that kind of co-locating data or kind of plotting two things on the same plot pretty easy using this multiply operator so multiply in all of these means put these things on top of each other and so that's how we get this co-located plot uh which gives us kind of this population density map uh with the uh most severe earthquakes on top of it and kind of hear if you're interested in kind of harm mitigation or whatever you might say oh well maybe it's not that maybe don't put people right next to a fault line or um whatever um so yeah that's what we're going to be looking at next which is tutorial 3 composing thoughts and i think this one is actually relatively short so hopefully we can go through it pretty quick and give you a little break okay let's jump on in here again we're just doing our imports um if you again if you're ram constrained i would recommend at this point to go back to this previous tutorial and just restarted here kernel kernel restart it'll clear out the memory that that particular notebook was using no reason to keep that memory will be particularly as we go through these tutorials uh you'll just be accumulating memory if you forget to do that and so uh that's what i thought yes yep sorry about that all right so let's jump straight into that one i'm going to close this again ideally clear previous notebooks restart the kernel and then start at tutorial number three again yeah we're going to load our data we're going to import hbp.pandas to initialize the hp plot method and the first thing we're going to do is resample our data so here we'll get a weekly resample of our data and then plot that so here we can now display that and now we can see here kind of we get a nice time series plot again the nice importance is bokeh we get right here if we're zoomed out far it kind of just shows you the years if you zoom in it has this nice daytime daytime axis which kind of gives you more resolution as you zoom in further and so that's pretty nice um so yeah we have this one plot um and now we've always just kind of implicitly used the display machinery of the jupiter notebook and we've never really inspected what is this object that we're getting returned from these hp plot methods right um we've always just put that at the last at the end of a line and then we immediately get the output and it kind of seems like it's just a metapod live like macbook right where it just displays inline uh implicitly but actually what we're doing here is invoking the display machinery which means that this object that we're getting back from hp plot is an actual object that we can kind of we could in theory actually save to disk we can do other things with we can keep a handle on it um and then later combined with other plots and that gives you a lot of nice affordances and so actually uh when we do that with this weekly count plot uh we can see what is this thing we can use print instead of using the just invoking the wrapper machinery and now it tells us well this is a curve and this is a curve of counts over time right that's pretty neat um underneath this we're not going to dive too deeply into this this thing is actually a hollow views element is what we call it and elements are just different visual elements that you can compose together to generate your plot and elements have dimensions and that's what these are you can think of usually we write the this notation here gives you the key dimensions or you can think of them as the independent variables first and then the dependent variable second or what we call value dimensions or b dimms okay so again um now we're going to do a similar re-sampling but for magnitude again we can print that and now we again see here we now have a curve of magnitude over time um and then we can compose this well that's not really very sensible right we've done composing uh we saw that composing in the exercise we just did um well that's not very sensible right these we're now putting two different variables on the same axis instead we can use the plus operator in fact to do subplotting right um in metrolib you'd kind of create your subplots and then insert your two axes into that subplot in all views we have the concept of a layout and layouts can easily be generated using the plus operator and then we can decide how many columns do i want to lay this out in i can do one column so they're on top of each other i can say i think three columns is a default so if i leave this out it's just going to put them side by side i could yeah if i made them slightly smaller you actually have them side by side and so composing plots in this way um gives us yeah the ability to kind of you can generate a plot you can then decide well do i want to overlay that do i do i want to combine it with other plots and so the affordance of um having these objects returned by g plot is uh that we can we have this compositionality right each component is separate and we can later decide i want to put this on top of this i want to put an annotation on top of here i want to put them side by side and so on so that's kind of related so this is kind of a diversion here and we actually saw this before um we can hear we put two dimensions side by side we plotted magnitude and the weekly um the yeah magnitudes side by side with the depths or actually just counts of the earthquakes sorry um [Music] another way we can treat dimension is by mapping them to some other feature of our plot and so here as we saw before we can just use c equals magnitude and this works just the same as it does in my potlib so we can color map we can do things like map it to the size actually scale times 10 or something um but yeah we can additional dimensions uh we can map onto colors sizes and um whatever else we want in fact there's entire system for kind of mapping columns in your data onto visual properties on screen and if you want to dive a little bit deeper into that there is entire user guide and the hall of views section about that i will cover that here but again if you want to dive deeper into some of these systems you might want to actually look into all the views itself okay again um now that kind of we're plotting scatter the scatter of longitude versus latitude um and we've added this extra dimension you might have already kind of seen we have this distinction between scatter and points um here this is a little bit weird right actually it's like i was talking about independent and dependent dimensions before um because polyviews and hdplot are declarative libraries you're actually declaring something about the semantics of the different dimensions of your data right and so when you're declaring something as a key dimension or independent variable um the latitude doesn't depend on a lot of longitude right it doesn't really make sense and that's why we kind of have a distinction between scatter point scatter and points uh so if i do that here um we actually get a point object where longitude and latitude are the independent dimensions magnitude is the dependent dimension and conceptually that makes a little bit more sense and that's actually important in certain cases where uh holidays kind of infers things about your data for you and we'll see that a little bit later um yeah um brief interlude so we kind of looked at color mapping here um color mapping is a really deeply complex field um in particular color maps there's lots of arguments for color maps um in fact i think one of my favorite sci-fi talks is entirely about color maps years ago um and i think that's when the tide started turning against uh jet in particular just how harmful it is um and so color set is a library that we develop or jim i think jim is actually the only person who's been working on that um it ships with a large number of perceptually uniform colors and perceptually uniform basically means um that there's no weird kind of if you um made this black and white you would not have weird changes in brightness right so here you can see here it goes very clearly from very dark to very light and that's one of the main issues with jet right if you plot it jets um the brightness of jet as a curve you see weird jagged edges all over the place and that's really not a good feature for interpretability and so um if you're interested well you should be ideally you should be interested in plotting your data accurately um ideally use perceptually uniform color maps metapod lab now provides nice ones like varidis or magma and i think there's a third one um but color set provides a whole range of other ones and that might be to your liking and it also has various nice ones for kind of large numbers category categories and so on um and so you can use that um you can use that in place for when you're doing the c-map arguments in your hp plot call you can just provide it with this color map here and yep that's the end of that you might want to yeah to kind of get an idea of the different ones use the swatches argument and then it will present you with a range of options for different perception unit color maps okay let's put that together this warning how to get rid of that yeah so yeah it really ships quite a few it even ships ones that are very suitable for kind of uh accessibility uh for color blindness and so on so you can see protonate tone i probably can't pronounce these um but yeah it has a wide range of them and um yeah if you care about accessibility which you should it also has great ones for that and so let's see what that looks like when applied to our data yeah easy enough uh you can even refer to them as strings you don't needly necessarily have to import from colors that blah blah has only have color set installed uh it'll try and find it from callers so the supported ones are everything that's in my port live everything that's in bokeh and then if you can't find it in those it'll try and look it look it up in like i said okay um if you're working with geographic data you often want context right so these plots here great white background doesn't like okay i can sort of guess well just because we plot the whole data we can kind of guess where everything is we can kind of here see the pacific ridge i guess um and here the i guess this is the pacific ridge or the ring of fire is around here but enough in many cases you won't be able to guess the context of this and so you want some some context for your plots and using the composability you can import various tile sources from hall views from hobbies elements tiles so you can import for example this esri tile set which is kind of if you're familiar with google maps it's just tiles the further you zoom in the more resolution you get and so if i just compose that we now have a nice plot with um the actual background which gives us context and so i don't have to guess that um this is here the pacific coast for example i can see it directly again you don't actually have to import if you kind of if you get used to this a little bit and then kind of know which towel sources are there you don't actually need to import the tile source from all of these you can just request it i don't know we should have a oh that actually worked um oh yeah so it has nice error messages if you kind of want to guess one um it gives you a list of available color maps available uh tile sources that you can use to give context to your plot there's quite a few of them so yeah if you need more so here if you actually inspected the data you'll have noticed that we're cheating a little bit here oh no actually in this particular case we're not cheating um one second longitude oh i'm surprised oh i guess we have geo views installed yeah yes oh yes okay yes that's precisely right um so if you are dealing if you're kind of work with geographic data again we've mentioned before that we have this geoviews package um which lets you deal with different projections and so in particular um here you'll see that we have these duplicate columns in our data one of easting and northing which is this weird number which actually is i think meters from the null points whole island somewhere here in the off the african coast um and so that's what if you want to plot things on top of the tile source you need to convert it to mercator we've done that manually here if you do that often we recommend you install geo views which will actually you can tell it's this is in this particular coordinate system and then if you overlay it on tile source it'll know i need to project this first to this other coordinate system uh to over overlay it and so that's what geoviews gives you it has some other functionalities um but yeah that's the core of it it basically wraps a library called cardo pi which is kind of the recommended projection system for python these days and also when you're working out with map lib that's the standard these days so yeah i'll give you a second to play around with some different tile sources remember if you type one it'll give you a nice list of them so you don't necessarily have to import them and if it doesn't you can complain to us sorry i recommend just copy paste change it and there's some fun ones i think yeah go yeah so hopefully you've done that oh that's not correct so here's here you can see what happens yeah so here we've plotted longitude of latitude on top of our tile source and they're in completely different coordinate systems and now we have these all our points are now centered around null island and north island isn't actually real it's just what we call this point in the uh just off africa um so yeah if we zoom out here a little bit a lot um we can see is okay so here is well i think we're not going to learn much new here uh [Music] because we already did the exercise that we were meant to do a little bit later um but yeah we're going to load this data set of um the population density again um yeah we're going to clean it up a little bit um briefly about x-ray again i told you multiple times now it's like pandas which means that um if you're used to working with like images in numpy uh you're working in completely abstract coordinates right you're slicing column row 10 to 50 or whatever and it's totally abstract but usually your image actually has coordinates it has there's a real world unit to the different coordinates right um and so the nice thing about x-ray is that it has methods to for example slice i want to size the y-axis for latitudes between um 10 and minus 10 and i want to slice along the x-axis from 90 degrees to 110 degrees it actually lets you work in those coordinates and that's really really nice so again rasterize our data set if you see this warning we're i think trying to get rid of that and so yeah um yeah we can plot it we've done that before uh we can see again here this is something i won't go into too much detail about uh but whenever we have kind of some really interactive element what's actually happening here is that um as i zoom right we're sending back the bounds of this plot back to python python then re-aggregates our image and sends it back to the browser so there's this loop here and to handle that in hollow views we have these objects called dynamic maps which basically are basically wrappers around a function which is given the bounds of r of our plot the function then returns a new image that is then rendered so we can always inspect the contents of this thing just by accessing dot last and so here we can now see um the current basis current state yeah so basically here this is now a static thing right this dynamic thing dynamic map thing re-evaluates whenever the zoom extent changes when the access dot last i get the last state this is a static thing it doesn't re-render so as i if i zoom in further here basically i get more pixels i don't get more information doesn't easily zoom out it's just white space okay um yep so finally let's put all of the things we've learned about plotting together here uh we're now yeah going to plot um yeah we're going to plot our rasterized population we're going to plot our high magnitude points the most severe plots overlay them um give it a nice background and now we get actually yeah this is closer to a plot that we might want to share with someone obviously there's more many more customizations we might want to make but this kind of gets us to a good place oh well that's a good question it's not even working right um really we should have i think this is from unfortunately we keep reusing this tutorial um and so it seems like it's just not okay i should have applied it here um [Music] yeah i won't go into too much detail here um that was just left over from an earlier thing um the basic idea just not to divert too far but relabel is a method for hollow views so it's one level deeper you can relabel objects and in theory it uses those labels to label our plots it looks like in fact we also use information about the dimensions of the data in particular here and this clean data set i think had this band dimension and that takes precedence over the label when you're titling a plot and that's why we ended up with that weird title so sorry about that it's another thing to fix i think maxine can you keep a list of all the fixes thanks that's true um okay so yeah that's that's much closer to where we want to be uh so here you can see again when we're composing we're kind of generating these these complex compositional objects um and hopefully one thing you've taken away from this right um hp plot kind of focuses on this this very very high level thing we start somewhere uh we generate a plot very quickly we add a few options and we get kind of from point a to b really really quickly we can go pretty far we can get nice plots we can get interactivity we can get widgets we can plot these really large data sets but maybe i can't yeah tweak every little bit about my plot and so very low level if you really need that you can drop down lower and that's why we kind of the schematic kind of highlights our approach right most of our tools are very high level um you can always drop down to the lower level and so we have this layered approach where yeah you get all from point a to b really really quickly uh but then to get to your final destination you kind of drop down to this lower level and um do the final tweaks in some lower level api are in that little local area you don't have to learn all the steps you only have to learn the difference between where you go with the eye level is where you need to go so you want to look up how do i change labels i'm having you really mess with that one detailed well that's also the other nice thing is right that's not something you remember even if you used napalib for 10 years you're not you're not going to remember every argument to x take end take label formatter right that's never going to happen you're still going to be looking that up and so you get from 8.8 to be quickly and then you're going to be doing what you've done anyway um which is nice uh so that's a little bit about the philosophy behind the tools so that covers the plotting section of this tutorial and next we're going to dive a little bit into how do i make my plots more interactive how do i add some widgets to my interactive pipelines and so on and that's where we'll go next and then finally and the last section we're going to cover how do i go from these plots and a few tables into an application that i can share with someone else cool so we'll take a holland's break 10 minutes all right let's get started um before i continue i'm just going to show you quickly what we're working up to um so we're trying to build a by the end of this we're going to try and build a dashboard right or a little data application um and so that's what this is going to look like it's not the cleanest thing i'll show you some other nicer looking dashboards in a minute as well um but yeah the basic idea is that yeah we're going to kind of add some widgets to our plots we're going to combine things together add some deeper interactivity where we're clicking on a plot and some other plot updates and that's what we're trying to lead up to right and so here uh we have a nice little panel application uh we have our header area we have our main area we've got our little map that's already that's the first component that we've already built right that's what we've been building up to so far um and now we're going to be adding some more components here uh to improve to kind of add some interactivity and then add some deeper richer interactivity so here for example have the ability to click on a particular point find all the all the earthquakes near that region plot those plot their magnitudes over time uh plot a histogram of the magnitudes of earthquakes in that region for example be able to kind of link the color map on the left to some widget or kind of the distance around which we were searching for earthquakes so we clicked on a particular point and now we're going to kind of plot all earthquakes near it within a 1.5 degree distance or a 0.8 degree distance or even tighter um so that's the rough thing we're going to be working up to for kind of just a general idea of more of the kind of kinds of applications you can build with panel you can go to examples.paivas.org which has kind of a whole set of different examples uh with different use cases you can check that out um so some other ones might be this dashboard visualizing uh the penguin data set which i don't know familiar with but it's just information about various penguins um and here we have things like link selection so we can select on this plot and all the other pods update that kind of interactivity or this example here which is basically optimizes a stock portfolio a little boring but there we go we can click on the plots and see computes various things about what is an optimal portfolio and so on we have different components we have widgets on the page we have tables on the page and we have different tabs for different data that's the kind of thing we kind of hoping to build by the end of this right and yeah you'll want me masters at it but you'll have the rough rough idea of how to go about building these things all right so the next thing we're going to jump into now let me find this is this notebook here again don't forget to restart the kernels in your old notebooks just to free up some memory all right so we're going to jump into this number four interlinked plots and so if you saw some of that already how do i link a selection in one plot to the selection in another plot well there's inbuilt functionality to do that so let's start by again we're gonna run our imports this is the first time i've i've done it a few times but it's the first time we're going to import all views directly so we're going to use functionality that's currently present in hall views we might expand expose that functionality directly in hp plot eventually but here for now we're going to invoke that directly so again load our data select our most severe earthquakes and now we're going to show you how to do link brushing and i'll explain what link brushing is in just a second okay so we've let's start by we've created two plots right we've created a plot of uh the histogram plot of the magnitudes again um and we've made that responsive so it just resizes to the side of the screen we've created a histogram of the depths and now um we're going to compose those together into a layout using the plus operator one difference here is we're going to make an instance of this link selections object from holoviews and so we'll show you what that does in just a second uh so yeah we got these two plots we already knew kind of it links um axes together if you if they're the same axes right here we have separate axes um but what this length selections thing does is it actually links it inspects these two objects it finds out they're coming from the same data source so we're plotting two different columns of the same data source which means that if i select on one plot it can go back to the data source and actually select those same points those same rows in the table from the in the other plot and then updates the visualization in response and so here we can really dig into this this is really a really super power to dig into really complex data sets with 30 columns and you're trying to figure out kind of how what is the what are the relationships in this data how does magnitude affect the depth um are deeper earthquakes so i can reset here so i explained a little bit about kind of what these selections do so let me select here i'm going to select deeper depths you can see here and it's a little bit hard to see because we're selecting just a subset and so histograms are not the best it's showing the absolute absolute frequency of these so i'm going to zoom in a little bit here um other things we can do here is we can select how do i combine different selections right by default every time i select something new it replaces the previous selection what i can do also is do unions of selections and so if i do select here and then hold shift i can also add a second selection to that so here i'm selecting low and high for example i can do things like exclusions or unions right or intersections sorry so for example if i let's again start here start here and here it's not going to take the union of those if i take the difference instead so i start here and here well that's not the right behavior so sorry demo effect i think i did something wrong here and setting it up but yeah that's super general idea um i'm not quite sure why doesn't work in this particular case so and if you want to reset it you just click the reset button and it resets it for you in this particular case it's also showing the lasso select it probably shouldn't because it doesn't really make sense to do a loss of selection on histogram it's more for selecting scatter points or whatever deb's greater than 600 let's select here right oh actually that's i'm taking the difference in this case so let's try that again little variables yeah so if i were to interrogate this myself using code right i'd be here for all day like do this experiment well four magnitude is greater than 600 plot this and then reevaluate and you're there forever this is just a very quick way of going about that and uh the more complex your data set the more complex there are plots the more difficult this gets right and so link brushing is it kind of automatically figures out how the different columns are linked goes back to the original data set finds does this column exist in both both these tables and then it replays whatever transformations you've done on top of them so here what we're doing right is um taking this geoplot so we're again just doing a points plot of eastings versus northings putting that on a map and then doing the depth histogram and so now this probably actually makes more sense to you the previous one was kind of hard to how do does depths relate to i just have to relate to magnitude you might not really grok that um but here this is maybe more interesting uh you could for example say let's say in the here the specific region particularly over japan what is the distribution of depths versus um what is it in a region like this and again well yeah i don't know if there's the problem is there's probably nothing um so this works really probably better if i uh did this with the larger data set and we're hopefully going to see that in a minute or otherwise i'll live code it up for you so this is a pretty quick tutorial it just shows you um how to link different things together and i'll briefly actually just out of interest i'll briefly code up the full data set that's a df df.hb plot the points easting northing uh rasterize equals true so we're now working with the full data set right so that's makes it a little bit more interesting and we're again going to do i'm just going to copy the histogram from here magnitude histogram swap that out for the full data set responsive equals true do one plus the other and then add the link selection on top this one okay that's going to take a little bit longer that's not that pretty i also want to add the tile source for it tiles entry i guess make it a little bit higher okay oh and also c norm equals eq hist okay so now we have something that's interpretable i can zoom in a little bit and now we can still so now let's here on this histogram plot just select earthquakes and magnitudes greater in this particular region or this region and so yeah no it still works right this is the really neat thing um because holidays does all this uh tracking of different information these objects know what data set they were derived from it can track the data set entirely through this pipeline of operations that's being done so this particular case the operations are computer histogram or rasterize this data when i do a selection it takes the original data set and overlays that with a different version of the data set with this new selection applied to it and so we can actually inspect what that looks like so underneath this all lies a so i'll look at that a so-called selection expression was which basically gives us an expression of what the exact selection we've applied is right so in this particular case here i've done a selection from a magnitude of greater than 2.67 um combined with a selection of uh magnitudes less than 4.79 and if i know if i also add a selection on here to it right now we now have a combined selection of both of these things uh the expression becomes larger which also means that we can access the current selection in python right we have a selection expression we can apply that to some data set so i can say filter the most severe earthquakes well in this case there's nothing in there because we've kind of selected out the most severe ones already and so we're going to apply that to this data set and now we get back we can basically filter the data set by the selections we've applied so now you can do other things with it for example i could do something neat like especially show a table if the current selection is done in our application is and in fact that's kind of the direction we're going in from here we're now exactly making the transition from uh plotting just put something on the screen for me it looks pretty it kind of gives me some idea to analysis to kind of digging deeper getting getting to grips with our data set uh drilling down and then kind of understanding the data set in different ways and so yeah link selections is one way to do this uh it provides powerful and it magically does this for you you can do a lot of these things yourself in theory right but um the link selections component really uh makes that easy yeah you have a question yep oh yeah drawback of this of course is of live coding is that you guys don't have a copy of this absolutely that's smarter all right everyone has to look away for a second internal slack it's half a second so we're okay um is this the right one i don't want to do that oh okay yeah there it is um that was a smart way to do it um okay so that was a quick little section but hopefully it kind of revealed some of the power that these things give you right we're not just plotting we're not just throwing our data at this plotting program and having to plot it we're actually declaring things about our data that then this program this library can use in various other ways okay again a constant reminder oh actually we have an exercise yeah the basic idea is that filter basically applies to any data set you give it so you can do your own selections um use the filter method and it'll subfilter it just the same so if you have this abstract expression which basically captures our selections then we can apply it to any data okay finally make sure to restart your kernel keep reminding you okay all right yeah now we're making a pretty drastic switch we'll still be i think plotting a little bit um but now we're getting into the realm of interactive pipelines interactive exploration of your data and so that's kind of a separate thing and so we're also this is the first time you're going to be introduced to panel um and so that's where we're starting here okay so first thing first uh we import panel we load the extension previously when we used hd plot it was loading an extension for you automatically hp plot import hp plot.pandas or import hvp.xray m basically initializes the extension for you loads all the components javascript components for you if you're working with panel directly you need to do that explicitly and similarly if you're dropping down to all views you also need to do that explicitly okay so uh panel again is a library for building applications dashboards in python um it's optimized for use in notebooks but works equally well in um in a uh kind of ide so there's workflows there which i won't cover in detail maybe at the end if we have some time i'll quickly demo what that looks like because it's an important workflow in fact one thing i might quickly demo is the more powerful workflow for building applications in in the notebook okay so um you might be familiar with ipad widgets um this is in effect pretty similar right we instantiate a widget um we get the widget object and if we put it here it'll render itself and so when i drag this and we it sends the current value back to python uh we can access it on the value attribute and if we set it from python similarly it syncs and updates the widget state on the front end yeah so yeah try try playing around with the widget c as the value see the value respond i guess it doesn't need that um yeah so as i drag we always have this interactive loop and now let's actually connect that widget to something right widget on zone doesn't do anything it just reflects the value to python and now we can connect it to something and now we're again going to load all these different libraries we're going to load hb plot and and we're going to do some transforms on our data right we might do something simple actually we i think we probably did it before we're going to select the data only data points that are greater than 5 in magnitude simple enough um yeah that's pretty straightforward right um but what if we could instead of manually having to kind of retype this say i want magnitude is greater than five magnitude greater than 6. um i adjust it manually to get the right number of points or i have some other criterion for what i want to do what i want this data to look like instead of that we use dottingdirect that interactive is kind of another magic thing which turns your regular data frame into an interactive object that now um allows you to instead of kind of having hardcoded values like 6.2 greater than 6.2 i can actually substitute the slider here so i can say actually i want to filter my data with the magnitude slider as the max maximum magnitude value and now if i drag the slider it automatically re-evaluates our data pipeline and the output gets updated and so yeah in this way we can kind of start building interactive pipelines um of arbitrary complexity insert widgets at any point right i might say well well actually that's probably the next example um so in this particular case the output here is obviously just a data frame and the data frame has a nice wrapper in the notebook and so when i update it we see the output update here the table update uh but in fact um any object being returned by this interactive pipeline will get a wrapper it'll get displayed if i drag it for example i look at the shape of this data instead of seeing the table i now see the shape update similarly if i want a mypolyplot as output here i want to see a map live plot of the magnitudes the history that's a histogram of the magnitudes um that is now the output every time i update the slider the plot updates and of course this works just the same if we substitute the microbial plot for a uh edgy plot so that's pretty neat right we start with kind of code that looks familiar to you you're if you know at least if you know the pandas api you've seen slicing before if you know the dot plot api you've seen plotting before but now instead of kind of being fixed into fixed by what you've done what you've typed manually you can just substitute various arguments for widgets and you get an interactive application so let's try something different um here we have a date range slider so far we've filtered by magnitude we might want to use dead filter by a particular date range a date range slider yeah yeah that is a very good point it in fact as long as i implemented it correctly it should cache everything up to the point where the widget is injected right so let's say you've got a 10-step pipeline and somewhere in the middle your widget is spread does something like i don't know filter something everything up to that step 5 should be cached it then filters and then obviously it has to rerun the rest of the pipeline because it's dependent on the value um yeah so here we have the state range slider um for um panel widgets um which makes it distinct from any other panel component um the value is always the current value of the widget and so in this case this date range slider returns a two-pole of timestamps and so if i again if i drag it the timestamps here update that is not quite what we wanted but i guess the on initialization we get the pandas type and then afterwards it's just returning regular date times and internally those are interchangeable um yeah we can now for example create an string that formats these nicely so that we get start is at this time and the end is at this time and we're going to use that f string a little bit later on um but now we're going to combine our two widgets to build a little bit more a complex pipeline right in this case all we're doing is filtering still but we're filtering by the value start by the value end and this now gets into param i told you earlier i wasn't going to dive too deeply into pram but this is one concept that you really need to understand to get an idea of how things work here um so previously what we did right was just i was saying dfi magnitude is greater than the magnitude slider what this really in in effect is saying um is the value parameter of the magnitude slider is greater or is greater than this yes so let's let's see what that looks like um in effect so all parameterized objects such as panel widgets have the concept of so we have this max slider and we have the current value but we also have the concept of a parameter which is basically effectively saying um this is a reference to the value at some future point in time right and so when we're using this parameter value we're effectively saying to this interactive object when this parameter changes query the current value at that point in time and so when we're saying magnitude is greater than the magnitude slider we're saying watch the the value parameter on this magnitude slider and when it changes look at the actual value at that point in time and then re-evaluate the pipeline with that current value yes yes that's just because widgets and panel basically have one canonical value and that's what distinguishes them from other components which means that it's perfectly reasonable to say well whenever you reference a widget what you actually mean is the value of that widget or the parameter that represents the value of that widget um and this is important in certain cases right um widgets actually have a whole bunch of parameters so for example if i again tab complete right there's a whole bunch of parameters we could depend on here most of these obviously don't matter to you but [Music] in this particular case right we're filtering by this and we're trying to do two separate filters greater than and less than and so we need to reference these two parameters separately and so we're saying greater than the value start parameter and less than the value n parameter if i did but doing gram value wouldn't work because you can't compare the index to a tuple which is the actual value okay so yeah here you can see it still works we can all both of these widgets um work to control this i can scroll through time i can still filter by magnitude um and yeah we can build complex pipelines in this way jim you made us do the exercise before we were meant to um yes um so the exercise here is basically doing what we just already did um so let's do the second exercise which is try filtering a subset of the columns and this is maybe a little bit complex you might have to basically decide i'm going to give you a hint basically it's the idea here um we want a widget that allows us to select between the columns and so a good widget for that might be the multi-select widget and select widgets always have a set of options and so we might want to say let's create a widget that has allows us to select between all the different columns we might give it a fixed height like 400 let's say oh that's a bit high 200 150. basically define this and then try the basically use the interactive dfi object to filter the columns that you want to see displayed and this does presuppose some knowledge of pandas apis but hopefully that's okay is get a copy of the mouse put that in your little analysis pipeline and get your job done delete it when you're done you don't have to be leading up to a big app and you deploy whatever you're my research the idea is to just get a line or two of code and then you get to your data that's the goal and you need to get confused while we're doing this try to keep that goal in mind i don't have power to explore whatever i'm doing typing is painful if you're constantly changing parameters you're there all day okay in this particular case it's really quite simple the answer was simply index your dfi with the selection widget again you could have done if you want to be fully explicit you can filter it by the program value and this will let you select multiple columns well it should confused by that okay must've done something wrong here oh yeah that's actually an issue i think maybe it was quite as simple as i thought particularly because oh we'll see that in a minute actually that's a good lead-in to my next point um often in certain cases um the widget value doesn't directly map on to the argument that you want to pass to your data pipeline so for example um in certain cases your widget returns a tuple but the arguments expect a list right so that's a problem um and how do you get around that well you can use functions instead and we're going to see what that looks like now um so let me just think about this particular oh okay yeah um so here we now have a select widget that selects between different types of earthquakes or basically different causes for types of earthquakes just regular natural earthquakes someone blew up a big quarry there was an explosion somewhere i guess a glacier collapsed those are all different causes but now you want to map that onto something else i don't think this is a particularly good example because in this particular case we're actually filtering directly by the type but let's ignore that for a second so here the input function basically is um we're starting with some data that we we could also be starting with some data that comes from the rest api or something and we have various widgets as input in this particular case you can imagine let's say yeah we wanted to kind of query from the rest api all regular earthquakes all the quarry blasts and so on and so we start with defining this function which fetches our data from somewhere in this particular case it's just fetching it from this data frame that we already have in memory um and then filters it but you can imagine is it requests i don't know using requests in here and fetching it you can use that as a starting point as well and so here what i'm doing is i've defined this input function which grabs the data from somewhere and is then given this argument which comes from this widget so here i've got these various event types that i'm selecting from uh i'm binding that that widget to this function and so that whenever this widget changes uh this bound function gets called it reevaluates it and we're we're kind of filtering the data in this way and so um basically instead of having a data frame as a starting point you can have a function as a starting point for your uh exploratory pipeline and then you can call still call interactive on that and have that as a launching off point right and so here yeah we've kind of done something to fetch our data with various arguments that have come from these widgets we've turned that into an interactive pipeline and then we can again use our magnitude filter to filter that data that was just queried you can imagine yeah i could now whatever is happening here i could now be basically filtering uh fetching that in this function uh that's particularly interesting another use case for this right is you've got various data sets on disk or a bunch of csv files on disk and you want to explore all of them you just basically get a list of all the file names you put those as options into your select widget to find a function that loads basically just pandas read csv or read parquet or whatever and now you can explore an entire folder of csv files very easily right and still have your interactive pipeline chained after that and yeah as we already saw before um interactive and holoviews works just fine you can have your interactive plots chained into your interactive pipeline and the nice the other nice thing about this right is that um interact interactive pipelines can branch off right you can have here i think before we defined our filtered filtered pipeline yeah up here we kind of defined this pipeline up to this point which is just a bunch of filters and then we said let's generate multiple outputs from this which in this case we're just creating two histograms uh so the nice thing about that is right um up to a certain point um the calculation is the same both of these plots are fed by the same pipeline so it's not filtering the data twice to feed these two plots it's doing it once um and then generating these two outputs from it and obviously you can have any number of outputs generated from the same thing okay now let's we're building up to something slightly more interesting we're again starting with just a subset of our data and then making that interactive one thing you're seeing here is that interactive previously we just used dot interactive and then called it without any arguments um we can actually pass arguments to this which are arguments to the layout that we're generating from the output of this so in this case we're telling it we want the sizing mode don't worry too much about what the sizing mode means but in this case it just means make the plot responsive in width so that it fills the screen um i'm then again defining a date rain slider a magnitude range slider again doing our filtering on that data set and then generating this geoplot um which is fed by this again ooh how pretty it's flickering that's not not too nice um we're gonna see that's a problem so in certain cases right what's happening this interactive pipeline is basically reapplying our all our transforms and so on getting our output and then at the end it's re-rendering the output entirely that's not always what we want all of us actually if you saw earlier some of the widgets we in the hp plot sections we didn't get a flicker when we kind of dragged switch between different plots and so that leads us into so-called terminating methods for interactive so by default when we render a interactive pipeline it's giving us the widgets it's giving us the output whatever the output might be whether it's a plot it's a table whatever um and it's putting that into a layout and underneath it all there's just a panel layout a column of these widgets and the output and they're put together and that's just what it renders as so what we actually have is this geo object right and when i plot this what i get again is another interactive object right underneath it all it's kind of changing these things and doing all the magic but what what if we actually want a hb plot object as the output what if we actually want a panel object that we can put we can further manipulate or what if we want to apply linked selections to this thing um linked selections all these linked elections know nothing about the interactive objects they don't know how to deal with them and so we need to terminate our pipeline to give a link link selections function an object that it knows how to deal with and there's one use case there's many reasons you might want to terminate um which i might go through in a minute uh so here we're terminating this mag print and i should probably we should probably work on giving these things better wrappers so that we can actually get some information of what this pipeline is right right now it's just this generic interactive object i now terminate that into a holoview's object and now we get a more familiar wrapper of this so this is a dynamic map now um which again renders as a plot um i can yeah so we're determining these two things then laying them out and now now i can apply link selections to this because this is now a hell of these object and so we now have our pipeline um which is fed by these widgets and so i can also terminate and say just give me the widgets that are feeding this pipeline right and so i now have this interactive um output that um we applied link selections to i have these widgets and so now we have this crazy interactive pipeline right it takes a little while to get your head around we've got a data set that is filtered by these widgets it is then fed into this views plot this hall of use plot has this link selections applied so that we can further sub filter the data hopefully that makes some sense it's a long chain of events and but the brilliance of this right is that we've done all of this in using apis that hopefully you've already learned you've already know about whether those pandas apis or these hp plot apis or the link selection api um so there's four um four termination methods in total let me quickly show you all of these um so hollow views i did not like that oh okay no you can't if all of these doesn't like the output of your of your pipeline isn't the whole of these objects you can't terminate as apologies um so in this case filtered sub range i have two options i can just give me the panel and the output of this thing which in this case is a table just give me the widgets or give me the layout of these and that is a panel object and so we're going to dig a little bit more into panel objects pretty soon so here you can see filtered subrange dot layout terminates into a row of a column containing another row of the widgets and this interactive dynamic function which basically updates the output whenever things change uh that's probably a lot to get your head around you probably have to play around a little bit um but what you should remember is that um if we for example terminated this geo object um into a holoviews thing and now i did geo dot widgets we're now having hollow views take care of updating the plot and we no longer get this weird flicker so that's kind of a solid distinction um previously basically panel was in charge by default panel is in charge of updating things and with panels in charge it basically just says i'm going to re-render this thing in its entirety all of us can be more smart about this thing and say well this is a plot the previous thing was also a plot i'm just going to update the data in this plugin hopefully that there's all kinds of subtle distinctions here that you may not get on the first go um but yeah there's a number of reasons to to kind of terminate your applications you want to get a really polished all right um i think we're gonna do this exercise now i skipped the linking plots exercise we have time at the end we might jim if you could give me some indication of timing that would be good but let's should build jump into this exercise number three again it's in the tutorials exercises folder and feel free to ask questions any points and yeah jump into the exercise and again we're going to give it about 10 to 15 minutes but yeah you're building your dashboard notebook you kind of i don't know we're going to load quickly our data let's say we're loading this data in we are now you want to display that as a table for example widgets tabulator so if you're rejoining online we are now i'm just giving a quick demo of a nice workflow of working building dashboards in a notebook that is in the wrong place okay i'm just going to use some just just gonna make up some data real quick just to demonstrate this uh so yeah imagine i've done some exciting tables i've generated some exciting plots um i panel now has this concept of making things servable which basically means added to my dashboard and so in jupiter lab if you're using jupiter lab we have this preview button this preview button basically lets you see what does what does notebook look like if i deployed it as an application and so here for example we've now kind of just done this i can click this render on save button i can say template equals fast for example i can use a nice custom template i can change the title now and so in this way i can i think it's state template.title my beautiful dashboard so in this way we can quickly iterate create our little application and then deploy it um and we'll briefly talk about some deployment options and but there's some documentation so that was a total aside but just if you're using jupyter lab this is a really neat way to kind of iterate and build in dashboard okay so let's jump into this dashboard section here again tons of imports here in fact we are now in panel basically certain components are optional and um so you don't want to load all the different components always um so if you're working in a notebook you can tell it load this component ahead of time and here we're trying it to load the tabulator components so in this reference gallery if there's a custom component it'll tell you what you need to do to load it here at the top okay so we've now done that we are now going to see what pains are so pains as i briefly told you before panes render some objects it might just be as simple as a string here here we're rendering the string as markdown pretty simple we kind of have this logo path rendering an image right we just give it to the spain object and it renders it and then all of these have a variety of parameters to customize it right i can say height 150 whatever so again if you go to the reference gallery and go to this component it'll tell you kind of the general kind of sizing related layout related things are listed in this customization guide the various parameters to control that particular pane are listed in this section and then yeah it explains a little bit about the component of how to use these different parameters okay so now we've seen yes a pane can render some string it can render an image but often you don't need to tell it right it has this helper function called pn.panel which basically takes your object and finds the best what it thinks is the best representation of that thing so you give it a string uh it'll infer well markdown is a good way to render a string if you give it a logo to pass to the logo it'll only automatically figure out well that's actually a good way we we don't want to see a byte's representation of the image we want to see the image and so it if we print these objects it'll again it'll just be a png object but you didn't need to really declare that because it's pretty obvious that the file ending in png is a png phone um so yeah what i recommend is um you can either print or use type to kind of confirm pm panel what did it actually give me that's how you confirm right simple enough similarly um internally if you give a panel layout one of these things so if you give a panel layout a string it automatically panel doesn't but default doesn't know how to render the string it'll ask p in the panel to infer what this thing is and how it should render it and then we get back basically this new composite object which is a row where the first element is this markdown object and the second object is this pnd object and then it will render itself okay so now we're again going to load our data we're going to create some plots again so again our rasterized population overlaid with the sample points and now we're going to create a little mini dashboard pretty yeah simple enough we've just created a column of our little header and of our little plot here all simple enough um and yeah it's pretty straightforward so far of course a lot of the layout options there's a lot of options and you might always have to look those up but the basic concepts i think are hopefully pretty straightforward these panel objects also have very standard affordances so we already saw before that servable means added to our dashboard one other thing is you can always any individual object you can say uh particularly if you're running this basically on the command line like in just running ipython in your in your terminal uh you won't get these rich outputs there you can use dot show and that show basically means spin up a little server um and then open the browser window with this thing displayed in it so that's another quick way to kind of quickly preview what this thing looks like um alternatively um we saw this preview button before um preview button internally really all it does is run panel serve and then serves this notebook so if we here in this interface i could quickly i think i'm not sure this actually actually activates my environment so i might have to do conda activate dev 3.9 you usually activate the environment you're working in and now i can do panel serve um i think we're on dashboard 6. i don't even know if he marked it as servable i guess somewhere in this notebook i probably did i can now give it the notebook and i can say show it's now oh okay it's telling me this port is already in use so i'll pick a new one seven it does the same thing it spins up the server runs our notebook and picks up any servable components um to display okay so this is what we're leading up to in this moment um and so this is basically panel serve uh whatever notebook or script you've generated is the main way to deploy one of these dashboards so you can go to heroku and deploy it or you can there's any number of services we're still working on kind of collating that list of services and it's always expanding um okay so yeah let's put things together right we've kind of we're again going to just rehash what we did in the previous notebook we're going to create our interactive pipeline uh we're gonna lay that out together with our um little header and now we've built a little dashboard which gives us this interactivity again here i should have probably terminated this and done geodesic.widgets separately so i don't get the flicker right um it's not entirely we don't want we have control over whether it should update the ranges in this case i didn't set that so i'm kind of distorting our plot a little bit but um if i cared enough i fixed that now but i don't really care okay so just to rehash right um we've kind of covered uh what are panes in uh panel panes are window panes onto whatever object you're displaying we can automatically kind of automatically infer for you what the ideal representation might be you may have to correct it in certain cases a data frame could be displayed as using the standard wrapper or it could be displayed as a tabulator widget and in that case you have to declare it explicitly explicitly we've discovered a little bit about how to lay things out and again to get an overview of all the different layout components look at the reference gallery so for example i could use tabs i could say i want a flexbox right and so now uh it's reflowing things depending on the size of the screen i don't actually know why it's for flowing in this case but yeah so there's a number of layout components um we've seen how do i serve my dashboard how to preview my dashboard or my little application um so show is a quick way to do so serverable is the way to declare something if you want to panel serve it and then we've kind of started building up to an application that's actually somewhat interesting it does something that someone might have someone else might actually want to look at and so in the final version here uh we've built a little dashboard using everything we've learned so far right um one particular thing to note here is um panel has the concept of templates by default right there's no template well there is a template but it's just a bare page white page kind of boring in the background um if you declare in the panel extension i want to use a particular template and there's a few of them whether that's material ui based or fast ui based or bootstrap based you can do so and then that adds basically different different regions to your little application i can then customize certain things about that template give it a title set the width of the sidebar for instance and then i can use the layouts to layout the different components mark different components is servable um so here the servable thing basically by default servable just adds it to the main area if you use this area argument you can say well actually i want this component in the sidebar i want this component in the header and so forth and then i have my full dashboard pretty nice so let's run that again i'm going to serve this again and look at that dashboard i marked the wrong thing observable and that's what you get sorry about that yes i've done oh yeah i've added a show here which shouldn't be there i don't know if i can write the entire thing but yeah um so hopefully you can do so uh to activate the right environment you might have to okay so i can quickly show you how to you would do condo activate and default and that should activate it so you cd backed into the main directory then cd back to the tutorial and then you can do panel serve zero six dashboards dot fp one b and that should launch the little dashboard for you the reason you get accuracy so yeah now we've built a little application um it has nice little the template has nice little affordances such as sidebar that you can expand and collapse and we've got a table we've got a plot they're all linked together in fact i'll quickly show you something about linking these plots together in a minute um yeah so so far previously one thing i hadn't shown you uh previously is we did link selections with plots entirely um in actual fact we have control over with this link selections object we can actually generate generates a pipeline that eventually feeds into our tabulator widget so here we have the tabulator widget we start with our filter data we pipe that into the link selections filter function um and then we tell it we're depending on the selection expression don't worry about all this it's i know it's a lot your brain's probably right at this point it's just to show you kind of you're not limited to plots you can chain pipelines in arbitrary ways you can have nice little tables and you can compose the entire dashboard in this way okay so at this point uh we're going to let you build your own dashboards and i would perhaps i think everyone's brains probably a little bit pride maybe partner up uh but it's up to entirely up to you so we are i think now in the building a dashboard section yes i would very much recommend like you've got us here if you want to any concept you don't quite get um we're happy to help you with um also there isn't more there's one more section with more advanced dashboards i'm not entirely sure like your brain has been filling up and filling up and filling up and it's it's a lot um i don't know how much more i can fill in there at this point um so yeah i recommend asking questions and i might kind of dive a little bit into some other features in the last chapter all right i imagine everyone's brain is fried but i'm going to try and run through one more section and then i'll do some demos of something we've been working on which is pi script and how that interacts with panel so hopefully you can build finish your dashboards i always love to see what you build so you can also share those at this course at holidays.org for now we're gonna move on to section eight we're gonna skip section seven uh just because it's quite deep in the weeds of hollow views and we're instead gonna get a little bit more in the weeds on panel and see how some of that works okay so let's get started uh tutorial number eight is on custom dashboards um so far all the dashboards kind of all the a lot of the interactivity we've built into our dashboards or our little data apps has been using the interactive api and so that's powerful particularly as you're working with kind of data but you're not always starting out with the data sets you're not always manipulating pandas data frames or x-ray data sets the world of data apps or just generic web applications is much wider and so we're going to introduce you to some ways to add interactivity to your applications using some other approaches okay um so that's what this notebook is about so we're going to start again reload your load panel pn extension maybe i'll even load a component like tabulator for later use and we're going to load our data and we're going to start in the same place we're going to create a widget you know how widgets work now they have a value they have parameters and here that actually will matter a lot more and be quite important so again we have our value we're going to create a second widget so to allow us to filter by a place name for example and now we're going to define if instead of defining a pipeline a kind of a pandas pipeline we're going to define a filter df function so here we're just defining a function which does the filtering um combines our filters like the lower filter the upper filter adds basically filters on the place name based on the string um and then returns the head of the data reframe so how do we make this interactive well we kind of saw something similar earlier we're going to use a functionality called pn.bind so panel.bind basically means bind these parameters or these widgets to this function such that when those widgets or parameters change the function gets reevaluated the output updates right and so here we're going to do place again place our widgets in a column we're going to bind the magnitude mag filter to the magnitude range arguments of this function find the place filter widget to the place argument of this function um and then wrap that in panel pnr panel so that we can give it a width so a lot of us you asked me just now about kind of how do you size tables correctly that's actually a pretty major issue right now we're working on a complete overhaul of the layout and so that that will eventually just resolve itself right now we've kind of just manually set the size um and so now if we evaluate we should probably evaluate the other cells if we evaluate this we can now for example search for a place japan and now it basically it recognizes the place filter has changed it calls the filtered df function with this new place name this now gets it uses the string contains method on this column to filter the table with that value and then updates so pretty similar we could have probably achieved this with the interactive function but underneath the hoods this is basically what interactive is doing um but you could do anything else you want in here it doesn't need to be a data frame we're working with it could be i don't know you could run a database connection you could uh do any other manipulation you want what this yeah we can see here what this looks like is we've got the row containing this column containing two widgets and then we've got this param function object if you kind of remember from earlier that is exactly what the interactive pipeline actually returned so here we've just done this step manually um but basically it works the same way and so now a quick exercise i try to declare a spinner widget so pm widget spinner and then declare a function that depends on it um or well maybe actually create two spinner widgets and then add those together and then return the value and um lay that out which one we did two one hand you might want to provide ranges on these uh so so yeah should end up with something like this change it up a little bit um so yeah pretty simple you declare your two widgets you declare a function which takes the values of those widgets as inputs and then you create a little layout here i basically added some text in the middle to do the functional expression uh the expression that we wanted again you may have to declare the width manually here which is a little bit annoying but again i'll be resolved soon so now i can do 10 plus 10. that didn't actually work you can also directly return a panel in here okay so let's see is that the solution we got oh we haven't updated the solution so you're getting okay so this is an older api we now recommend you use pnd bind instead of this decorator but everyone knows about decorators um so this is um one thing to note about bind is you can both find the positional arguments or you can find keyword arguments so i could say uh u1 and value 2 and then even though i put them in a different order right uh value two value one uh that'll still work so yeah um if you're familiar with funk tools partial in the standard library it's basically equivalent except it has that additional ability to kind of watch the changes in the parameters and tell panel when to re-evaluate it okay next we're going to look at callbacks um it's another api to do interactivity um certainly bind is very very useful um but usually it's a reactive way of expressing dependencies sometimes you want to just change one thing you want to have a side effect that gets triggered when you click a button or you want to have a side effect that gets triggered when a value is above a certain other value right and so that's where callbacks are useful callbacks and now we're getting into program again here we're going to declare a slider we are now going to select a particular row and create this row pane from this selected value so we can briefly do that manually we're going to look at the row pane row right just selecting one particular row um so that's nice enough um and then we declare a callback that updates this object whenever uh this parameter on this row slider changes and so the api for that is row slider dot param dot watch so we're watching this this particular widget and when the widget changes we call this callback this callback could obviously do anything um here you can provide a list of parameters that you're watching right and here we're watching the value you can also watch what i don't know you might have custom parameterized objects with any number of parameters that you might want to watch and that's when this comes in useful as well and so here we've now basically what happens right we change the widget uh the watcher gets called the watcher updates the object that this pane is displaying and so when i drag it the pane updates and the display updates fairly straightforward um so these two apis are in your journey towards building more complex data applications uh these will come in very useful if you're just focused on kind of quickly exploring some data frame stick with interactive totally totally fine but callbacks are very useful when you have very complex dependencies uh this pnbind function is very useful again it's kind of an intermediate level api where it's pretty simple right you just declare a function it returns something that thing updates when the widgets change while here you have with a callback you have much more control over kind of what changes you even so here i think one thing i glossed over is that instead of getting the value in this case what we're actually getting back is this event object so let's print that we get back this event object which gives us some more context about this thing right what what was the object that changed it was the insulator um what was the old value before this happened it was this was a new value it was that right and so you have some more context you can have only when the old value was this the new value is that you make a change you have you have more information you can do more precise updates and so on um any questions about that part how are we doing on time the design of that data frame is about my data and i'm exploring my data frame now i'm adding interactivity to it but really it's still all about the database now part of that what he just told me was i'm trying to build an app i want to put some stuff so we have appropriate apis that are designed for different use how you get to those objects and the best way to get to those objects depends a learn from there drop down eventually and in fact each of these builds on top of each other right the interactive api is built on top of bind bind is built on top of these callbacks [Music] neatly stacks um and you're as a user use what what makes sense in your particular use case um so i don't know if we want to go all the way through this final book we're definitely yeah i mean usually jim spends the first hour just talking so um yeah i don't think i'm gonna do that so i might show you some things so obviously we've kind of covered exploratory exploring the notebook you iterate so on uh you build an application we kind of showed you how to prototype an application how to run an application uh what we didn't show you is how to deploy it um there is multiple approaches here um i won't go through the process of going log into aws set up your ac2 instance or whatever you can do that um there is um if your organization manages a jupiter hub there's some plugins to help you kind of share it with an organization and um i will be i think writing a blog post about some of the deployment options very soon um so yeah we'll go i won't go through that either what i'm going to go through is a new approach um so if you kind of listened to pycon our ceo peter wang announced a new project called pyscript and pi script is basically the idea of writing um python entirely in html so you don't you don't need a server it's running in your browser and it's built on a project called pio died and pio died um basically on uh technology called wasm and um that allows you to kind of write entire applications entirely in html so if you were around the 90s and wrote your first geocities website um you can now do that but now you have the power of python and it's awesome and i'll dig into kind of some demos that we can build with panel we're still the next releaser panel actually is going to be focused around the idea of quickly building panel applications that run entirely in your browser you will even provide utility to kind of take your application spit out an html file which uses pi script and pio died um to run so let's kind of see what that affords us right and so here we're going to start this little application one big caveat or drawback of pi script right now is it's pretty slow but pi script and pi data themselves are not slow what is slow is that it's you're downloading python into your browser and unless you've already done it kind of loaded this page before and your browser's cached things you're downloading all of numpy every time you're accessing a website you're downloading all of pandas you're downloading all of uh bokeh panel and so on your entire stack gets downloaded every time it's kind of crazy we're working on improving these things but what you get out is pretty cool so before we try out this application let's just look look at the instrument and surprisingly it's actually readable this isn't we're not in javascript land where you have 50 different frameworks and you need to kind of i don't know learn all of them so here what we have is a pi script tag we have our usual imports we have panel we have pandas we have param um and now we define an application and i didn't actually show you this way of writing applications it's built on param but the basic theory here is this is python in your html file pretty neat uh we've defined various components here we have uh some callbacks so we're depending when the our parameter changes do this and so we have various methods here that basically do the same things our callbacks the callbacks you just learned about did um and then we basically point it to a data file load that data file instantiate our application and just say servable but in this case serverable doesn't insert it into a template it actually targets a particular dom element so here up here kind of is the layout of this page right so we've got a header bar we've got a little we've got this div element and we get an id called widgets and then we've got this other div element called plot which is where we're going to insert the plot and so from python we can now say directly inject this python object or the representation of this python object into this region of the page so that's pretty cool right um we can even manipulate the dom directly from um so um yeah all we've done here is said take this gl plot insert it into this particular dom element take the these controls or these widgets panel widgets and insert it here and then finally we've created this pi reply component which basically gives us access to the running python session in our output and so just the same way as you have a notebook we now have a notebook cell effectively in our dashboard so that's quite neat uh so here this particular example is just this 3d plot right displaying new york taxi data i can change the radius you can see how responsive this is right because we no longer have a server that's communicating with our browser uh which is potentially sitting halfway around the world right and we are this is all running on my machine it's not even sending any data right so this is really neat for applications where the data is sensitive and you don't want to have a server and deal with security issues you can build entire applications you can share send as an html file in your organization as an email they open it it fetches the data and then or even you embed the data and then they can use it so here we've exposed some controls we've exposed the radius and we can see what how the taxi trips change by the time of day we've got a little play button which um basically we can do at a periodic callback which just changes the hour of the day so we can quickly go through this see how responsive this is right it's pretty neat and so here's the real estate superpower now you're now an advanced user you've built this dashboard you've we've only exposed these few controls but it's a python session so we've got our app object we've got basically a parameter that's the current view that we're displaying and so when i update this i can say df and df i think tip amount is greater than df tip amount quantile 0.95 so let's display only the taxi trips that have where uh the tips were and then above the 95th quantile right we can do all kinds of custom queries in this um and we're no longer locked into kind of this a either we're now in this hybrid space where we're building an application a nice looking application no longer are we locked into the notebook format or whatever um but we still have the full power of python to do custom analyses um and this is really only to start so i basically got i got the task of build some cool demos for pi script and that's what i did uh i didn't spend very much time on it and there's cool things i came up with so i'm i'm very excited about this this opens up the ability to kind of work with data in really new ways uh build applications in really new ways and yeah it's only early days there's still tons to improve um maybe we'll look at one more demo oh right um i think this could actually work as a one second i don't think there's any reason um right so let's try it so let's try a different one panel k-means is another nice one um yeah should work just fine yes so i know it's it's not even an http server involved right i emailed this downloaded into my own downloads folder just double clicked it and i've got a nice little application again built entirely i think this one's actually built using apis that you would have learned about by now uh so here this is actually running uh scikit learning clustering on this penguins dataset and so here is a vega plot of these three species of penguins we have a table down here of the different species um of all the different penguins there are various measurements about them just their bill length the beak length the length of their flippers their body mass and so on and now we're running psychic current learning clustering again entirely in our browser um and finding kind of we're trying to find clusters cluster these species into or just sprout data into three different labels which pretty much coincides with the um actual species so you can see here there's some misclassification i've encoded the label the clustering has done um with the shape and then the color represents the actual species you can see here mostly squares are meant to be red there's some circles which are wrong um and so on but again entirely running in a browser we have a lot of the function now we have all the nice functionality that you just learned about so we have linked selections for example here so i'm kind of filtering moving around our table updates and response i can change what we're plotting i can even rerun the clustering algorithm to generate different different numbers of clusters obviously we know it's three species but we can change that and again we can just inspect the source code and yes there's a bunch of javascript imports up here but really this is let's count 159 to 84 70 lines of python for a really nice application you can just send it as an email um so yeah the main reason deployment is hard i we need to do more work on document documenting deployment but this skips that step entirely it's really pretty exciting and it's only early days um i think i'll call it a day at this point um you guys can ask questions but really many thanks for attending it's been great being back at sci-fi have a great time thank you [Applause]
18,Exploring Open Quantum Systems with QuTiP,https://www.youtube.com/watch?v=cUm8lD4btIQ,i had a little badge just made with the q-tip logo i'm going to give each if you want if you want one so maybe i should start by yeah i mean just say hello we're quite a small group so i'm hoping we can kind of keep this informal um feel free to stop me kind of whenever and ask questions um and i'm hoping that yeah i know when interesting things come up we can just have a conversation about them um in terms of sort of how the tutorial is going to to work um uh we have a kind of i guess sort of kind of exercise which is going to be the kind of focus of what we're doing which is try and kind of really simulate kind of ready almost at kind of a reasonably low physics level a uh a neutral atom quantum computer um which is actually not it's not a huge amount of code um if you're using q-tip and but it is there is a lot to understand and i think it's really important to kind of understand what we're doing as we go along because otherwise it'll be really hard to use it later and presuming the point of the tutorial that you want to build you can do things um yeah so that's kind of how how things going to work uh my name is is simon i'm currently lucky enough to be employed to work on q-tip full-time by a research institute in japan called ricken i'm not currently based in japan because of covert so japan just closed its borders and i would hide in the middle of all of this um so at a fun time like i rented out my uh brings out my flats and the name was told no you can't come to japan and sold my car anyway it was a huge mess but it was also i think yeah it was also kind of lucky and so now i'm working for them remotely um so i'm based in the netherlands uh where me and my wife live and she also does math and physics um that's originally from from south africa in fact was kind of in south africa well i mean except for some some short trips kind of my whole life up until june last year um anyways that is that is me and before i get started also i'd like to maybe learn a little about all of you and particularly the stuff that is i guess important to today so like how much people know like what things should we just skip over and be like okay we all know this um and what things yeah where kind of where should we kind of spend our time um also i'm intending first to kind of take a short break sort of every three hour or it gets a longer slightly longer break in the middle and hopefully there'll be coffee and snacks and things but please let me know if i forget um oh and maybe we'll just start kind of uh on the right you can just say like one or two sentences about your background and yeah all right yeah see you oh okay my name is do you have something specific that you want kind of wanted to get out of this tutorial like just curious i'm very curious to know quantum technology technologies and how it will empower you know machine learning uh decisioning okay and to know the trends i'm a grad student at uab in computational solid state business and i just thought this would be a great well awesome um i've actually had quite a bit of luck using q-tip for for undergrads and maybe we can chat a bit more later but i think i mean i think there's some some really nice things about these kind of simulated quantum systems from a from an educational perspective obviously the problem was like a real quantum system and you kind of see this if you're sort of working with students is it it's either all like equations equations equations which is one part or then you take them into the lab and suddenly they're spending like two hours trying to properly align like the laser and the polarizer and understanding what is going on it's gotten lost in this sort of pile of lab equipment that they're trying to learn and manage so uh yeah so similar simulated systems have this sort of potential to like look inside and kind of debug either the code or your understanding um awesome and so i guess oh i guess you know a lot of physics okay i mean there's kind of an infinite amount to know at this point at least effectively infinite compared to like one person's lifetime in britain [Music] um i'm really just here out of curiosity i don't know much about the quantum world i mean i have like a super super basic understanding but not you know as it like really protects quantum computing but yeah i just thought it would be super interesting so i find that people by like by super basic people mean everything from like i don't know like i'm richard feynman okay but physics is only like yeah okay and kind of have you done like a first year physics course at university or i have you know my question would be like what is the cuban type thing that's my level of understanding okay so if i'm slowing down i'm just here out of curiosity but don't worry too much about that um yeah well i'm going to do my best to keep everyone everyone with us and on the same page i would rather actually spend more time kind of kind of getting everyone to understand the basics well because that will at least give you a solid foundation for doing more yes so i'm brian i'm from the naval nuclear lab in upstate new york and i've got an experimental physics background so this is uh very interesting to me just kind of seeing like how it all fits together and kind of the computer science information type space um yeah i mean computer bit is used well actually q2 10's not to be used well there are some experiments that are very comfortable with like tools like youtube and happily use it themselves but i think what i found more often is that the experimentalists will come to other people who are sort of theorists who use q-tip and be like hey i'm working with this thing in the lab what can you tell me um yeah um so great um okay maybe since people are okay yeah so i'm going to go to do some kind of high level stuff first um and oh wait there's actually one more thing i wanted to add some specifics about kind of what people know so i guess hands up if you feel that you're comfortable with say just kind of like i guess basic linear algebra so like matrix matrix multiplication or happy okay um so uh i mean like eigenvectors i can pick eigenvalues okay a little bit shaky um uh that's good um yeah i mean we actually need very little um of uh linear ultra beyond um i mean beyond matrix multiplication there's some things i guess quantum mechanics uses sort of linear algebra in some sort these strange ways that are not typically taught in their kind of uh i guess basic physics course so you have these kind of outer products so um which is just kind of matrix multiplication but it's kind of machine multiplication which sort of you just never do in like a first year when you're all at least i didn't um yeah so shout outs if there's any kind of linear algebra stuff which is is odd uh complex numbers how are people feeling okay good everyone seems good on comments so that's that's useful um because quantum mechanics is filled with complex numbers um cool uh awesome um i want to say one more thing before we get started which is actually just to i guess obviously like this tutorial doesn't exist in a vacuum um so um there is um kind of obviously q-tip uh q-tip itself um is primarily funded by by rickon in japan who i work for um yes rickon actually were working on japan's uh nuclear weapons program at the end of world war ii um but then and obviously when the world war ii ended that that was horrifically shut down um but the japanese kind of just sort of renamed it and got rid of the nuclear weapons program and then a few decades later renamed it back to ricken um and it continued on its way but it became a lot more diverse it was initially very physics focused but now it covers uh i guess yeah a kind of sort of fundamental physics kind of material science biology quite a lot of neuroscience um so quite a very wide range of things um then um sort of on the the open source side um so okay yeah so qtip was uh started about 10 years ago by two phd students who were at uh who were working at ricken um and well they kind of started sharing their phds and then they sort of it really got going during their postdocs at ricken um and so yeah those are paul nation and robert johansson uh all nations are actually now at uh with the various couscous teams at ibm and in fact if you look at kiskit you will see lots of similarities to branches of q-tip code because paul wrote puzzled him and obviously he was like i don't want to really start this from scratch i'm just going to write a slightly better version um and now q-tip is run actually by a very kind of sort of widely dispersed group of people mostly with uh within europe um but there's a kind of core admin team who weeks meets once a month um not everyone is always equally active um so nathan uh shamana is uh heavily involved in the unitary fund which is this uh i guess it's a sort of it's a small i guess sort of investing and sort of donation fund that funds specifically open source quantum projects and they have these uh uh grants that they issue regularly and already like a five-minute grant application so just like a google form that you just type in some stuff um so if you want to build something like on top of of q-tip and it would be useful to have like a little bit of funding for it uh you can just look up unitary fund fill in their form um and they all kind of because they do quite so they do quite small grants so like one to four thousand dollars typically um but it also they also kind of give them quite quickly so you can kind of probably get a grant like a month anyway if you want to have money to do stuff with youtube like i know bold like a tutorial or like a little course i don't know what people want to do but um anyway it is there um [Music] and then um there's one other person other than me who works relatively full time and he will actually be here at the main conference on wednesday uh his name is eric gagier and he's in sherbrooke in canada um and he works for a big hpc um group there okay um cool okay so maybe getting into kind of what qt does a little bit so if you've done a kind of basic physical course you're probably mostly focused on i guess what are called quantum systems but um which are um people usually mean kind of very isolated systems and the initial system that people looked at was really the atom of course there are different kinds of different kinds of atoms different elements different isotopes within elements but a lot of quantum mechanics just came out of examining these um these atoms and that's going to be well i mean atom's also going to be important to this tutorial because we're going to look specifically at neutral atoms and we look at kind of the use of the physics bit later um so um the um this world of atoms turned out to be to be quite strange um so i mean it's it's a very i mean atoms are very small so just the um i mean the radius of of the atom so that's kind of roughly sort of if you would imagine sort of where the electrons are of course now we know from quantum mechanics that the electrons are not sort of anywhere specifically exactly they're sort of kind of like the state is a more distributed state um then that's about like one so 0.1 nanometers so 10 times 10 meters um and then the the nucleus inside that is more in the range of kind of one temperature meter so it's like 10 to the minus 15 meters so the so the the distance well the size of the nucleus compared to it's like only like one over ten thousands of the distance from the nucleus to kind of where the electrons are very roughly speaking um i think maybe something that i want to say just about kind of i guess the development of quantum mechanics is that um i mean i think it's because became over the last 100 years just sort of this kind of um kind of adage that is sort of trotted out that quantum mechanics is kind of super weird not understandable um but i don't think that people developing quantum mechanics kind of thought that initially i mean if you look at quantum mechanics a lot of the um the tools used like kind of like hamiltonian mechanics and the ground mechanics these were all formalisms from from classical mechanics um that's i mean that people like schrodinger and kind of direct were kind of very familiar with and if you kind of read their papers like i mean they they are clearly masters of these classical mechanics because obviously that's what physics was um kind of prior to this and um and a lot of the stuff that we consider strange and sort of weird in quantum mechanics i mean even stuff like heisenberg's uncertainty principle um and the fact that um kind of our um sort of measurements that we do or like our operators on this like space of wave functions that actually all crush crops up in kind of classical mechanics if you're um if you're working with continuous systems um so um so i think well one of the things there are definitely some there are definitely some very kind of strange things looking in quantum mechanics um but there's also a lot that we can kind of understand i guess using our kind of classical intuition um um [Music] okay so qtap models kind of open quantum systems and i'm hoping that we'll kind of properly maybe get to some like real open quantum system stuff at the end um but what is the difference between an open quantum system and a normal quantum system so a normal quantum system which i guess to distinguish you can call a closed quantum system is kind of completely isolated so we know um exactly its state we know that it evolves according to the to an equation called the schrodinger equation and um and that just evolves completely sort of deterministically using this kind of linear equation in in time and at some point later time we have this kind of exact thing and we kind of uh well maybe we'll look at schrodinger equation later but it's even not that hard to solve we know that kind of we look for the kind of eigen states of the kind of evolution operator and then we decompose our initial condition into these eigen states and then we know how those eigenstates evolve and because it's linear we know how sort of the composition of them evolves and so then we kind of solved everything while having to do a bunch of like kind of painful mathematics um but there is one thing which sort of happens sort of completely outside this formalism which was probably always sort of how to connect this completely isolated system to the to the lab because obviously to get results out there needs to be some interaction between the lab and um and the system being studied and um and in atomic physics it's kind of simple the way you interact with i mean if okay or symbols maybe wrong word but at least it's kind of conceptually simple the way that you interact with an atom is with light um because it's mostly sort of well at least yeah an intermittent mostly interact with some particles that are coming in and out and given the sort of scale of kind of the atom the particles are really traveling a very long way so i mean it's kind of traveling like a photon is emitted by an electron which is a kind of from this little kind of atom which is say 0.1 nanometers across so it's like and then it travels say like one meter to some detector probably but on the order of one meter so it really travels like 10 to the 10 times more than the size of the system that was being kind of studied it's really made it a very long way away um and um yeah and then kind of what what we found was that there was a strange rule for determining kind of when photons were admitted than what we detected um and i think we can sort of summarize what happened there was like well we have a rule which works but we don't really understand why in any detail um and um and so this kind of question of how these tiny quantum systems couple to kind of larger environments and what already happens there remains i mean to this day i think still a little bit of a mystery um but in the intervening period we did actually uh i mean we i mean humanity physicists um did make some progress on this like what happens if you take a small system and kind of connect it to a larger system so people started kind of asking i guess good physics questions so kind of what if we don't try and connect it to a big system what if we initially just connect our tiny system to another tiny system and then say separate them and kind of ask about how those those interactions work so this study of kind of quantum systems that are connected to other quantum systems is called um it's called open quantum systems so maybe just and typically [Music] all right that's visible to everyone okay so we have some tiny system and then i can just draw around it um we can have a bigger environment and the the kind of starting point of open content systems is that you make a joint system which is both the kind of system and the environment but um and now conceptually this is all completely fine you can do this within kind of ordinary and quantum mechanics you have some very large kind of closed quantum system now um and um but this system is too huge and unknown to solve so what you have to do is make some sort of um approximation for the environment and how it interacts and so didn't use this comment to label where the interaction is and then what you do is you take the approximations and you try and use those approximations to simplify kind of the dynamics and work out how things are going to evolve um so maybe what are some of the simplifications that people use um so um [Music] one thing you can can do is you can assume that um the um that there's that the the system has kind of like no um or it means i should look at my list um um well the the most the i guess this is one of the simplest assumptions is that the kind of bath is kind of completely dissipative and what this means is that kind of say like a photon can sort of leave the system and enter the environment but nothing can come back so um so dissipative is just a sort of one-way flow of um kind of well you can think of it really energy but just a white one the system kind of affects the environments can sort of lose stuff to the environment um but nothing ever comes back um another assumption that you can make is and this dissipative model is kind of one of the easiest to deal with um um so another assumption that you can make is that the the bath has no memory of previous interactions with the system so um that means that kind of in if um say a photon is emitted by the atom um or if a photon is absorbed by the atom the environment always just kind of stays the same it's kind of from time to time it's it's always kind of it maybe has some sort of some since uh well yeah from time to time it's just kind of always the same and nothing's kind of happening there um and then and then things got kind of more complicated so um you can then do things um uh when the where you assume that the system does have memory but that's um the free bath is is gaussian so it's kind of a thermal system but it does have internal states and say photons can kind of enter the internal thermal system and kind of sort of fill up kind of energy levels and things yes so when you say that that is that e sorry thank you people seen that phrase a lot of the literature yes yeah so sorry that is that i'm committing this the same kind of problem that you see in the literature which is that people tend to use environment and path somewhat entertain interchangeably um i guess the word boss comes directly from thermal bath and kind of most of the kind of situations that you kind of imagine there is a um kind of a thermal character to the to the boss yep so those simplifications of the system are those written in order of like increasing complexity so like a distributive system is simpler than a micro venous gaussian or they're just different ways um so they there is a sort of uh i would say that the kind of markovian case um contains the dissipative case the the gaussian case i would say is more complicated than markovian but they're sort of different assumptions not um not i wouldn't say completely that one contains the other yeah um so um so kind of open quantum systems started out kind of again as sort of really if people were like really want to understand some like fundamental problems better um and then and along the side i guess the sort of theoretical development kind of going along with it was a lot of experimental work that was quietly happening in the background and um and people started actually kind of really making these kind of dissipative systems so people would do stuff like they would make um kind of a cavity with two mirrors and they would sort of shine kind of a bit of light into the cavity cavity in the light would bounce backwards and forwards many times and it would enter some sort of kind of stable kind of uh sort of thermal state and then you would put like an atom in the center of cavity and you'd watch how those things interact and so the atom could kind of lose a photon but then a photon the photon wouldn't just disappear off into the middle of nowhere it would sort of bounce backwards and forwards with the other photons between this mirror and you get kind of all sorts of kind of exciting states um people i guess started making sort of things like josephine junctions which are these kind of superconducting uh junctions where um um because sort of you can get sort of superpositions of different states of current flow and these are from a quantum mechanical perspective almost basically macroscopic like objects i mean this current flow that is making your kind of quantum system and what isolates it is the fact that the current is not really coupled to anything it's just got a flowing friction to sleep by itself and then obviously with the engineering of kind of these things like like these josephine junctions for all sorts of amazing applications like there's a really simple circuit you can make which can sort of measure individual quanta of magnetic flux and then once you can measure quantum magnetic flux you can convert other things you want to measure into sort of changes in magnetic flux and then you can measure all sorts of things very accurately and these are called squids and people very excited but then of course in reality nothing is completely sort of frictionless there's also little interactions people want to understand all of the details and so kind of open quantum systems came in again um great um cool um and so q-tip is very much designed to i guess support um kind of people who are doing research in open quantum systems um to do research in open quantum systems you kind of have to also do research and close quantum systems i mean you have to have all of quantum mechanics around um and and so q-tip supports closed quantum systems not for for working with available so you can imagine that as you make kind of different approximations here your the equations which govern your dynamics the difference you need different kinds of solvers um you need actually ready the solvers in some ways or the easy part it's actually constructing the system to solve the right way is often the hard part um and um and i think maybe something else which distinguishes q-tip a bit because there's a lot of there are a growing number of very specialized tools um is that q-tip um has um just actually really nice ways of describing really basic things like quantum states and hamiltonians and combining them and visualizing them and there's all kinds of common things you want to do like find eigenvalues um find eigenvectors um so kind of all of these all of these these things sort of exist with integer and i think that's kind of part of what makes it a nice i guess like a nice teaching tool but also makes it very convenient at the research level it's quite nice that and we'll actually maybe do this a bit but later you can actually take like equations from papers and sort of type them into cute if you know kind of roughly how to translate them and it's probably no different to some extent to having to type those equations in later i mean uh it's a slightly different sort of language and formalism but that's kind of the same level of translation and that's that's quite nice um and also kind of avoids lots of mistakes like the more translating you have to do like i make enough mistakes already without things being complicated um i think the other thing which is great about q-tip is it's very general purpose so it obviously comes with some downsides so if you have a very specialist task i would look for a specialist tool before kind of writing stuff but um if you're kind of generally exploring it's kind of really nice to have a general tool tool as well um wait okay so so much for kind of introductions maybe any any questions or more things we want to talk about right the second are we good to go good is there a website for this um i'm going to put these kind of all online as soon as i've cleaned them up a little bit but but yes but you don't feel the need to copy anything down i will make them available um yes so when you talk about open quantum systems it's strictly a system interacting with its environment it's not two systems interacting together question um it's not really a nonsensical question so you could for example you would use it's really about i guess the size of one of the systems and whether the entire system is whether whether this complete system or well the complete picture of both the system plus environment like if you can just solve that by itself then that's the best thing to do you just use ordinary quantum mechanics you solve this this system and and then you can extract whatever information you want from the the result and so that's like first prize if you can get it um so what makes this system open is i guess so people often do study kind of small systems coupled to each other maybe let's make this super concrete so let's say you have like two say atoms here coupled in some way maybe by the spin of the electrons or something um and i know here you have kind of a kind of a heat box already like a little source of photons um and here you have another department sort of potentially drawing the side of it lower to indicate that this is like slightly cooler then um this is like kind of like a heat engine quantum heat engine thing that people study and um you look at kind of how the heat flows through to the system so i'm not sure if this quite quite covers the case you're looking at but if the um and the models of these environments are are quite quite simple i mean if they're again some some sort of probably something like optical cavity or something um the yeah so i'm not i'm not sure if that counts um but the reasons for using open quantum system techniques is that the environments are very big um or you can't know everything about the environment it's also kind of another common case so you know some statistical properties about the environment but there's no way you can kind of know if kind of internal behavior exactly um um so if it's really big or or if it's just kind of really noisy so um yep that's good yes so with the three different models we got up there so dissipative markov and gaussian is that are they do they differ in the sense that they have different noise models or is it also in like the way that you actually solve the behavior of them um it's actually the way that you you solve the behavior of them and i guess these all cover actually a few different kindness a few different specific kind of approximations live within these broader descriptions so yeah it's more actually about how if you put on your like quantum information hat it's about how information moves moves between the system and the environment or you put in your physics hat it's how energy moves between the system and the environment um yeah so for example particularly this uh gaussian ones i'll say this i don't know like i mean so probably like at least ten major sort of techniques which are fall within that which is share some what what this gaussian approximation really gives you is that when you plug the fact that the sort of environment by itself it has these those kind of gaussian distribution of of states um to start with when you plug that into like a giant path integral your correlation functions turn out to um to only depend on differences in time um and that makes a whole bunch of things things similar simpler so um yeah and maybe i'll say there's also there's a lot of stuff that's kind of i haven't mentioned here and stuff that i also don't kind of quite know and there's a lot of new stuff happening on the on from the quantum information side um so i guess you can look at like tensor networks and stuff as either as kind of new techniques from quantum information or as techniques that are kind of new techniques within open quantum systems wait yes good cool um okay so um i want us to kind of just do a quick run through of like um what what really is like this device that we're going to be simulating what does it what does it do um and what does it physically look like and one of the things i always struggle with when i'm reading papers is you read a paper about some things like spin qubits or like neutral atom devices or photonics and then there's like city references in paragraph one which are supposed to kind of somewhere in those city references a description of what's actually going on so you can visualize um the device that you're working with um but um you know so i thought it would be nice just to go through and really get like a physical understanding which will help us later when we for thinking about all these strange things uh let me just make a big image okay so um yeah so these images were made by a french team just outside paris um they were all in kind of academia doing um these kind of neutral atom devices and which we'll get to more in a second but then they founded a startup called pascal.io um and i stumbled across them and i got some students to do a sort of similar thing to what we're doing in this tutorial now um anyway but let's have um let's kind of have a look at um kind of what's on the screen so these are these are pictures of atoms in space and you can see on the kind of axis here you can see this is kind of quite a hundred kind of micrometers um and each of these little points is uh uh you can see from the fact that the points are a little bit blurry and also from the fact that they're and quite big that these are not still kind of very blurry pictures of the atoms um and kind of remember that the siphon atom is so 10 to the minus 10 meters and so here this is like 10 to the minus 6. so they're they're very far apart um much further apart than you would get in the solid or even the gas so you can kind of really tell from the pictures that these sort of atoms are very far apart from each other in a vacuum somewhere um but then the question is sort of what is what is holding them up and how did they get get here um and i don't know i think yeah and also i think it's really kind of just important to say it's kind of incredible that we're sort of have arrived in a world where really we can make a picture of the eiffel tower by arranging kind of atoms in space kind of however we want um and as you'll see in these devices where um already able to control kind of each well really an electron within each atom kind of individually and kind of and move it through kind of quantum states kind of with a lot of precision which is which is kind of incredible i think if you've gone back to the founders of sort of quantum mechanics and be like hey we can put like 100 atoms like any way you want and kind of probe them individually um and kind of make them change see how they interact they'd be like okay let me like have the experimental device i'm just going to churn out pictures um but i think it's kind of just exciting that this is one of probably like eight different like physical systems which are really getting um kind of getting like a really solid experimental handle on and people are already like all of the people who are doing uh like building quantum computers like whenever you speak to one of them they're all like okay like we're done with like sort of 100 cubit devices what we're planning for now is like it's thousands and that's kind of the next next goal or um or like no these are actual photos yeah so these are actual photos so i'll show you the how the photos were taken in a second so i mean they're um yeah i mean the actual spot obviously so there's a lens that takes the lights being emitted and kind of uh splits it out for a ccd camera let's oh i mean maybe just say we can also tell from these pictures that kind of things must be they must be neutral atoms because even if they had like a single charge the sort of the like electronic propulsion pulling them apart would be huge um well initially it would be very huge and then they would disperse rapidly okay um okay let's maybe actually have a look at the actual short device um which all right so most of these pictures i've borrowed from the the pascal team so okay here on the left we can see the vacuum system and that tiny little dots is where the atoms lift um and so there's a little um yeah so the atoms are um uh um they're uh okay uh alkali so the important thing is that they have just a single outer electron which makes everything like easier to control um and so you can see here this so um because my first thing you can see is like the purple the red and the green lights are all it's all laser light so i guess we kind of needed quantum mechanics so we can invent the tools which will allow us to investigate quantum mechanics more um this uh we're getting looking for these on the left so the red light is the light that is um being used to trap the atoms in clicks um and there's a little plate here on the on the right the uh the slm which um so the spatial light modulator and very star trek made like spatial light modulator but it does exactly what it says on the tin yeah there's a kind of bumpy surface so the laser light bounces off this kind of very carefully made bumpy surface and then interferes with itself and uh where you get um uh contract of interference you get these little uh little traps which hold the atoms in place um and you can make sort of all sorts of like complicated configurations that's how you kind of get sort of um yeah by um by changing the bumpiness of the surface so you've got a buggy surface layer which you can control um and then you have this next thing which is the um the purple light okay so the purple light is used is a um so that's like a just a single laser and it's used to move the atoms around between the tracks um and we'll actually watch pretty shortly watch a very short video of like how they do that so so okay so you use the red light to set up all of these little traps in space then you take um your a tiny bit of a bit of kind of gas filled with patterns and you just let it disperse into the traps and then so they kind of obviously then the population of atoms in the traps won't be exactly what you want but then you use the tweezers to move to atoms around um and then uh the green light you use to um to photograph the atoms um so um you you shine light on them and look for their their um and this dichroic mirror this means a mirror which reflects lights of different colors in different directions so that's really nice it's just when the measurement comes out it gets bounced towards a camera which is really a ccd camera um kind of like it would get in um yeah in our cell phone um and then the i mean if you use the vacuum systems the vacuum system also kind of very cold inside um so that's uh actually the laser i mean it's it's very cold partially because you kind of have to be very cold if you're going to have a vacuum um that also prevents the atoms from kind of bouncing out of the traps and the traps themselves actually by trapping the atoms uh um confine their kind of their momentum um as in the momentum has to be kind of close to zero because they're not leaving the trap and that also cools them down um or maybe i'll just say one so this um this is uh this aod the acoustic like optical wait um oh deflector yes another like star trek kind of name cusco optical deflector that really is you basically put frequencies into it and it kind of changes kind of wiggles the mirror according to the the kind of the signal generators you put in so yeah so this is kind of like a tiny like 80s disco there's like laser lights and like sound playing and like yes well not yet um also i wanted to just maybe look uh um i just wanted to look at this uh picture of the experimental apparatus it's very similar uh except that makes sort of uh makes a few things clearer so you can actually see there um the um the kind uh structured light modulator um you can actually see really that is actually one of the pattern on the picture is actually one of the patterns they use for creating lattices um and you can see it's got a sort of uh it's just colored according to the the phase so obviously the face is for light is determined by how far it travels so travel slightly shorter path or slightly longer path um anyway you can kind of control this 2d surface yeah and then what you get out is the um from the in the traps is a fourier transform of this 2d phase so you kind of um then that's kind of how you actually design the trap you want what you have to do um okay let's pick watch the short video of um [Music] let's make it bigger yes [Music] we're going to kind of hop through some of the um experimental pictures again but this is all going to be just in 2d and the way you do 3d is you have multiple layers of 2d and you fill them one layer at a time um there's also i guess one sort of technical detail here as you you might sort of initially think like okay we just fill the traps um with as many like as many as we just put in like slightly more gas than we need so exciting more items that we need and then we fill up the traps as much as much as we can but actually what happens because the traps are so tiny if two electrons are in the same trap they kind of collide with each other and then just scatter out of the trap at high speed so you actually end up under filling the traps and then we'll see what happens next [Music] okay maybe this point here is which creates this sort of kind of one micrometer wastes which the atoms are trapped near [Music] yeah it's kind of looking at the temperature here and you can see those things [Music] the real signals that that come out so if you kind of get light you get this sort of um bumpy signal but if you don't get this kind of lower weakening signal so this is kind of real data [Music] [Music] and let you just pause here so these are some pictures of the the shape put onto the modulator and then the array that you get out and maybe if you're really good at 2d fourier transforms you can even somehow see that these are related [Music] okay so here the the dots are all of the traps that have been created and these little boxes are where they want the atoms to end up so this is them actually moving like one atom around at a time [Music] [Music] [Applause] [Music] foreign [Music] [Music] [Music] [Music] um i think they just made i don't think they actually make well maybe they did actually make the words but yeah i wanted to say a little bit more about just by trapping one atom um because that was worth one half of the nobel prize in 2018 so um it was clearly not it's literally not a trivial thing to do but actually it's kind of a whole topic by itself just like doing the kind of mathematics of how the traps work um wait um okay maybe before we um before we kind of i guess move on from the any questions about just the physical setup at the moment um anything you want to look at again it was milliseconds was the the counter so i think like tens of milliseconds and during that time it was pushing like pushing around physically yes it's amazing it's so cool yeah so so bear in mind that the um kind of speed is kind of velocity um so i mean they're not i mean they're not moving big distances which is what helps i mean so they're moving like each of those traps is only like a couple of um micrometers apart so you have to move like ten to minus six meters um yeah but that was um um slow so that was slowed down a thousand times so if you if you can maybe use that to get a visceral feel of how long this takes um [Music] because you know how like it was moving stuff around to adjacent tracks yeah right so given any initial given any initial configuration of the trapped atoms and the desired pattern have they optimized for like the minimal number of steps to like rearrange them oh so it's a little bit tricky because there is a kind of it is a closed loop control thing as you said um but you don't know exactly which traps are going to be filled initially so because you just have a sort of gas of atoms which go into the traps and then uh you actually have to take a photograph using the ccd camera that we saw and the the green light and then the computer has to process the image and then um find out where the atoms are and then design the moves and then control the um the little sort of disco light display which moves stuff around the aod um yeah so all of that so finding algorithms to kind of do that quicker is is really important um and i think as as we will we'll see you've got i mean it's quite common in quantum systems so as you slowly interact with the environment which happens kind of to some extent whether you like it or not you're typically you lose some kind of of the kind of um special behavior that you're trying to kind of trying to keep um because it just kind of gets lost and kind of merges in with environments uh by a noisy process um so there is a kind of clock that is ticking on all i mean this happens in all of the sort of quantum devices at the moment um and neutral atoms are actually fairly good so the kind of amount of time that they can last if they're just sitting there um is on the order of milliseconds um which is quite long compared to um or like a thousand times longer than like the superconducting qubits just because they have a much more kind of macroscopic thing that's actually like physically on a piece of silicon and such um so yeah but making but all of those midi sets like all of those sort of uh milliseconds count and if you've got a thousand and you use like like 50 kind of as part of like your setup uh yeah it's it's not like a huge problem but it's something that you would like to be better um cool um so i want to maybe talk about like what does one need to make a quantum computer um and in uh 2000 um physicist named divincenzo came up uh who actually was working at ibm came up with a list of kind of criteria that are called the divincenzo criteria and usually they're listed as five there's two which are for creating like quantum networks um but i thought we just have a look at like what we've how what we've just seen matches up to those criteria um so is there any kind of eraser yes um so so you need first of all you actually need like a scalable physical system uh with keywords and we'll talk in a bit more like what qubits are really um i mean they're just like two level well two state quantum systems or like quantum systems with two basis states but we'll look at them a bit more um but in our case the our qubits are just going to be the individual neutral atoms actually any electron on the individual neutral atom but let me put just like a big tick here for this particular physical device so it's really i mean from a point of view of kind of the under of the like physics underlying system it makes kind of great qubits in some sense because the atoms are very far apart they're neutral like they're not really interacting with anything um they're held in place by uh by the traps but the traps are not really kind of affecting um kind of the state of the system too much the decoherence times i mean the amount of times that it retains its states nicely are actually kind of ludicrously long um and as you can see like i mean ibm now are starting to be like okay we'll have i think like like 100 cubits like super like super conducting qubit systems maybe like next year or something um uh but the the neutral atom people like 100 like we can really like 200 we can do those easily um so that it's really great on that side um and i guess you can now sort of see a bit of like computer science creating in here so you've got to have like a way to like represent your kind of the state of your system um you have to be able to initialize the system um so um so we've seen i guess the kind of moving around of the of the atoms the the one uh one downside of the um super con i mean of the initial atoms uh with initializing things is that um it's quite difficult to initialize individual atoms kind of it's sort of with the superconducting qubits you can really just feed in different signals into different wires and you can sort of all you can initialize different atom like different qubits differently kind of uh by feeding them different signals here you kind of just sort of try and move everything into one drive everything into one particular in a kind of energy state and you just sort of bathe the whole sort of one micrometer sort of region um sort of yeah so the whole like little region with the with the atoms in um and you kind of drive them all into one starting state so you kind of have one global starting state which is uh not the end of the end of the world by any means but uh it means that if you want to get a more complicated starting state then you have to do some other transformations real system after initializing it um so i'd call this one like a semi-tick for mutual items like maybe like could be better um and people are sort of looking at how to do this um i think one of the things that you can kind of if you had like really like a optical tweezer per atom like you can do more but it yeah um okay so the next thing that one needs is this long decoherence times so decoherence is the kind of technical term for just sort of losing your quantumness to the environment um and it was probably i mean say the coherence sort of program was probably one of the first really big successes of like open quantum systems was just kind of i guess showing that it happens quite generically and then we'll put that like a double tick there so like it's good um okay so the next one on the list is to have a you know universal set of of gates there's a set of operations um you can um there many of them are analogous to existing kind of classical logic gates which which you know and we'll kind of go into kind of how they all work in more detail for them just going to give us a very high level picture um the gate switch up so a universal gate set has to have essentially be able to do any operation on a single single qubit so like a single bit classically um and then it also needs to be able to do one operation on a pair of of qubits and usually the gate that people use is the for this is the c naught gate so which is a control control dot we'll get into but more is just a two qubit operation um so the single qubit gates for neutral items are awesome um you can kind of use the lasers to control um these things what the the and we'll look in a little bit more detail about why it's sort of so awesome later but it's all kind of the gates work very well maybe like among the best out of all of the possible options uh the downside which is not in here is that the two qubit gates uh such a bite and need to be made better but this is not an uncommon situation for any of the technologies at the moment but i'd say it's maybe more of a problem for neutral atoms anyway kind of make really implementing these gates kind of in q-tip using like real sort of like evolutionary system is where we're going to spend maybe i'll put a spa here because here's where we're going to spend most of our our time i guess for the rest of the tutorial um and here so like single cubic gates tick uh two qubit gates a really active area of research to make them better um maybe we'll say there's quite a lot that one can do with bad gates um so if you come from any kind of like if you've done any kind of optimization world or machine learning um actually having some noise in your system can be really useful when you're doing optimization because it prevents getting stuck in local minima or um yeah um or your system becoming overly sort of optimized to some like particular case the fact that you have a bit of noise injected is not an insurmountable problem but it is a problem if you want to really do like circuit-based quantum computing the kind of holy grail that people are sort of looking for um they will say personally oh sorry yes um so because you mentioned multiple gates working on a single cubit whereas like in traditional computer science there's only one unit yes not good yes so we're going to get into some in more depth but maybe we can start now so okay so so classically there's only two states um uh quantum mechanically i'm going to write down zero and one again but just in like a slightly fancy way and then you just put some little like strange brackets around them um and we'll get more into like what these mean but essentially you can read this as this is the state zero and this is the state one but in the quantum case these are not the only states so these are if you don't know the elder these are the basis states and then the full set of of states is kind of alpha one where these alphabet are two complex numbers and we'll see later that the that the the squares of their lengths need to sum to one and there's also uh another bit of a strange criteria which is that um you can multiply this whole thing by e to the i theta so like you can do into i theta and that these are equivalent states so these are like an equivalence class of these things um but we'll go into this in more depth but it i mean it's kind of a really good it's a really good point like in the classical case there's kind of only well there's two gates this one can do nothing um and there's two which is kind of invert them those are the only kind of operations you can make um [Music] in the quantum case there's um yeah in the common case there's kind of more you can do because for example you can can sort of swap zero and one which is like not um but that has all kinds of other different operations so if you just write down the truth table so then the corresponding quantum gates will do exactly the same thing to the to the basis states so um but then if you were to look at what was happening what would happen if to this state so apply not and because we using like we'll see these are all linear operations it becomes that's so like not psi is not zero plus not one right and then again we can know from the table what those should be so that's one plus zero so zero goes to one one goes to zero and now we're back at the same stage we started out with so so there are states which not leaves like quantum mechanical states which not leaves um unchanged and these are actually the kind of eigen states of the not not gate wait uh okay good question uh i wanted to say yeah one thing so i mean there's maybe i guess it paints like a picture of sort of a lot of work to do um but uh because you actually don't need things to be perfect to do these kind of optimization problems um they're already actually very useful and in fact uh the the people who made like exactly these new neutral atoms we uh devices we've looked at now are have been selected to be installed in high performance computing centers throughout in europe in fact the the eu has bought two of them to start with and randomly one of them is going to the ulika super computing facility which is semi next door to me so it's like in in germany that's um i don't i'm in a very narrow part of the netherlands um yeah so it looks like people actually start using them soon and then the last one uh is you have to be able to uh perform measurement on your qubit search you have to kind of read out results afterwards and we're not going to talk about that kind of a huge amount mostly because we're assuming i mean we can get the measurements out of our simulation kind of really like really easily um but we we actually will be doing a lot of our work kind of on the actual like looking at the actual states inside the simulator like something you couldn't do like in a physical experiment at all but the the downside for neutral atoms with measurements is that they um you have to do all of your measurements at the end so the measurement is what's called a destructive measurement and what it does is it causes the atoms to kind of emit light but also ejects them from the the traps and you measure all of the atoms at once so you shine like a laser all of the atoms and then they um kind of bounce out and you get a picture of which ones were in like the like whichever state you're measuring and that's it's kind of great it's quite a simple procedure um but and it kind of early to low but it's it's a bit painful having to only so there's also a theorem which says you can always move all of the measurements of like a quantum circuit to the end so it's from a theoretical point of view it's not a train smash at all but it is very painful from like a practical like standpoint and this also makes some things harder the fact that you can't do like a measurement and then change what gates you're going to perform on later is a restriction yeah um cool um so that's kind of where we are with sort of the five five criteria and we're going to focus our attention um so maybe just quickly i'll kind of end off this little section by just looking at the so looking at uh wanting a quantum network like why did vincenzo even add in this like quantum network thing and the the answer is that probably as you scale system scale systems up initially you're going to need to be able to move information from like one quantum computer to like another device next door somehow um and so these two are quite fun so the one is the ability to interconvert stationary and flying qubits so stationary cubits are in our examples atoms flying qubits are almost always photons because photons kind of like to move um and don't like not move so i think there there's some like reasonable hope i mean we're already interacting like doing all the interactions with light so probably it's not too hard to imagine some sort of protocol where you have a photon interact with uh one of your initial atoms pick up some sort of quantum information from these atoms and then kind of fly off somewhere else so that's um that seems good um yeah and then some ways to kind of transmit the uh the photons between devices and gets get them back in so um i think neutralizing is probably fairly solid on that ground i'm not aware of anyone actually doing anything in that direction at the moment um yeah people are building quantum networks not sure if you know like china has like a few thousand kilometers like quantum key distribution thing and i think u.s and europe are frantically catching up um so there's a lot happening on the uh the quantum networking side partly well i guess they're two big uses one is um just security of information just trying to kind of secure it and the other one is probably more related to real science like if you can really distribute light around and maintain phase coherence suddenly you can do like interferometry like kind of analog interferometry sort of between very distant things um you can imagine like a piece of light coming from a star impacting sort of two atoms and two devices really bringing those together interfering them um anyway but so exciting um but not what we're going to look at today cool okay i'd like to have a little break now we've been going for slightly longer than an hour um so what i would like to do just before people go on a break if you can just open up jupiter and whichever jupiter you used hopefully to run the little test notebook that i post in the instructions and yeah just can't get that open because we will sort of uh look at that a little bit next and yeah and then can have a short break and i know how long a break do people want like five minutes ten minutes okay let's kind of get back here around five in five minutes or so and we can we can see where we are and if you're a couple minutes late that's fine okay so i wanted to start the next section by just giving like um and you're actually welcome to sort of type in your like some of these things into your notebook just so you kind of have them kind of look at um but don't necessarily try and type all of them um so yeah just some imports at the top so we just i mean q-tip and uh we mostly use numpy for the value of pi um but time is also useful for like cosines and sines and things and then i use matplotlib for plotting um and then what we're going to try and do in the rest of this tutorial is fill in um this little neutral atom circuit class and so what maybe um maybe i will kind of quickly scroll down to how you use the class and we can go back up so um so you pass in a list of atoms and just kind of their positions um so this is just like an atom one at the x y coordinate like one zero and the other one is one zero one it's just like an opposite positive square and we pass them into initial atom circuits and then we can run the ideas that can run little circuits by so there are three gates which are single qubit gates um and they're they're called rx r stands for rotations there's a rotation about x-axis rotation about z-axis rotation about axis and um and this zero says on the first qubit it's on qubit zero and then this pi over four is what angle to rotate through and we'll kind of get to all that i can understand it a little bit better um and then we can run a um yeah we can then we can run a circuit which includes this uh cz gate which is a controlled phase flip which also will get in a bit um and i will use that to implement a c not gate so that's the kind of this control control knot which we'll also get in a bit and then and then we can plot all of our our results nicely um and kind of look at what's happened anyway but we'll have a little kind of class that we can use to run these these circuits um and uh okay so you can see here there's this um there's these three single qubit gates the three rotations around three different axes we'll get to that in a bit um there's uh the skate h which is called the hadamard gauge gate um which we will also get into a bit its action is just to take like i'll state the stage zero gives trunks so if we write down its kind of actions here the the hadamard gates usually represents a confusing efh which is also useful like the hilton space and the hamiltonian it maps um the state zero to the states zero plus one and it maps the state one to the states zero minus one okay so it is and then it also happens to be its own own inverse so if you if you just plug it in or all of there you'll cover if you put in zero plus one you can see the two zeros will add and the two ones will cancel and you get zero and then when you finish but um put uh zero minus one and then the the um the ones will add in the zeros cancel so awesome um so that's the hardware gets just implemented because it's very like practically useful and it's just kind of like he used used a lot um and we can implement it in terms of our rx ry and our z gates um and then there are two two two qubit gates uh one is this control control z which will kind of get to what it just says is if the control qubit is one then uh uh flip the phase of the of the target qubit so that'll essentially introduce a minus sign um into um the like it'll change here we change beta to like minus beta um and then yeah and then run will be the thing we actually used to run our circuits um which so i mean yeah and right now even if you just write down my class neutral atom circuit and yeah you cannot leave everything else for later just have something to remind you what we're doing oh um okay so we actually kind of covered a little bit the most introductory what is the qubits um kind of earlier during our question so just a reminder kind of classical bits states zero and one uh uh quantum bits have basis states zero and one the sound is exactly the same of course except that basis state implies that there's a kind of whole sort of linear combination um and well maybe actually let me just quickly uh maybe actually let's just see like in your notebooks like just how to create these basis states in in qubit so in q-tip so if you type in like b zero equals q-tip dot basis two comma zero what that says is uh please make me a basis state it says there are two basis states in total and i would like the zeroth basis states um and then you can print it out or just let's like jupyter like automatically printed for you um and you'll see it prints out a also shout out need to make this a bit bigger was it fine um um cool so when you print it out you see a nice representation um so um the maybe you can start with this quantum object data bits so this uh is just a little so we know that we're taking linear combinations so that not be too surprising that we see our state is represented by a column vector and under the hood so this is kind of a the basis vector that it's q-tip is using to represent our our basis states zero um and it's the states kind of one zero um this uh shape thing at the top here uh the shape equals two comma one that's just the numpy array shape of the underlying data i just that just says it's kind of convicted with um okay the type is uh is type kit which means it's uh it just means it's a quantum state maybe maybe we can just quickly do where the strange like word kit comes from so um when you do effectively the dot products of say if you want to yeah if you do if you have one so you write the states like this so as we've been doing kind of uh all along as a sort of straight bracket the reason that we write it like this is that we write the the inner product so the um yeah the inner product of the two so that's just um a numpy is just kind of uh the like one zero kind of dot products kind of one zero um the so you know as you'll see in a second so this is the contactor and then this will be the row vector for the same thing um but as you'll see in a moment we tend to work with these row vectors and column vectors separately quite a lot so uh uh directed at the stream synthesis or he just did this in two um and he said like okay well this is like my state's one zero and then i will use this other part has the row vector and together these are a bracket and then so this is the second part of the bracket and this is the first part so the bra is the first part and the second part of the kit and there's a missing c in the middle um it's like a bit of a silly pattern but it's also kind of very useful to have like simple words for describing these things um so yeah so kit means it's a states and in our case a column vector in our representation and you'll see there's a few in addition to brown kit you can also have a few other kinds of states in qubits in q-tip sorry dims is short for dimensions and the dimensions are it's at the moment it's not interesting it just says it's dimensions two comma one which looks a lot like the shape but written out in kind of a strange way but what happens in q-tip is that so if you have if you have just one like one qubit so one neutral atom then new exact dimensions are are like the ones we see now like that um but if you have two two qubits then you would end up you have end up with four basis states so you would need uh to have a four here but in q-tip if you had already had two qubits it would write this as like one cubit two cubits it would look like it would look like that so it would be two comma two and then to get the full like size you would have to multiply the two twos together and then if you had three q this would be two comma two comma two so like the two thirds it would be like two like in in times um and that's just very helpful physically because it helps you understand the structure of the underlying physical system so in some sense like this is identical to for one um that when you're working with the system you might for example say like i want to know about the state of qubit one or qubit two and then if you had four there would be no kind of organization to the system so yeah so it's just it's kind of so it encodes similar information to the shape um but kind of split up to tell you about the structure of the system um yeah can you explain again sorry the arguments so why is there like a what's the two oh so the two is the number of bases the total number of basis elements so in our case we just have zero and one um but actually in a little bit we're going to have three um so yeah um yeah and the zero is just the statement yes it's just that yeah so the if you have if you have two base elements then your possibles like states are the zero states the first one and then you have three it's a zero one two and yeah um cool okay so now you kind of understand um kind of how cupid youtube stores these things um and um so you can also just print like if you need to know the type by itself you can just do like uh like b0 dot type for the state or something or the shape or the dimms um yeah and then if you look at the so b1 here was just the second basis element exactly the same except now it's zero one so you can yeah i mean this is just the if you were given like a math problem and your basis elements like e one or like e1 through en then the simplest sort of like representation you could come up with them is just to make a pile of vectors which had one in each of the different positions in the vector and that's exactly what what we do inside q-tip um okay um oops cool now maybe let's um maybe just hop back to our um and unusual atoms a bit um because okay great we know how to write um basis states in computer but what actions what actually are like we've just constructed these like these vectors what what like what is it what do they actually represent um so as i mentioned the the atoms in our kind of neutral atom device so inside that little vacuum like chamber are are usually kind of alkali metals um and most typically cesium or rubidium and we'll just focus on cesium for um for concreteness really um and i think it's actually the ones used in the array that we saw um okay so atomic number uh 55 so they're in total 55 electrons and 55 protons um and then in terms of 133 new like uh um well what do you call protons and neutrons and something like controlling the blank um uh anyway that yeah 32 protons and neutrons in turtle inside inside um and one of the things which is kind of great about cesium is essentially occurs only in one isotope which is like a small one but it means that all of your atoms are like slightly more identical than they would be inside your quantum computer that you're trying to build so that's great um yeah and then if you look at the arrangements of the of the electrons um you can see that these are the kind of electronic orbitals starting from kind of one which i guess is sort of the smallest and going all the way kind of up um and you can see in the altimas orbital like the first one there's just one electron in this in like the sixth sort of set of orbitals so there's only one in 6s um [Music] and yeah and kind of an ionization energy of like just like 3.89 in electron volts roughly if you are dealing with like any sort of kind of uh physicists who just work spend all of their time working in um with these kind of like atomic systems you'll see they'll give the energy in like centimeters to the minus one um and the the reason for that is just kind of uh if you kind of work in sort of in these kind of natural units where you define kind of h bar to be one then various strange units get identified with each other um and so centipede like in inverse length is in fact unit of energy um but yeah just do not be be too too well don't be like me i was like what's going on like do i even have the right number but yes um okay so um maybe where did i just come back to these plots in a second i should maybe put them further down um okay so um here we have just the table of the ionization energy of the um of the of the outer um electron so this is just so we're starting here with just kind of 6s that's the lowest sort of energy level okay you can see energy level zero um because that's it's the lowest energy state that that electron can be it can't be in any of the lower states because they're all filled up by other electrons um and then if you move it up like one level it goes to the six p level um where it has slightly higher energy so now it has like 11 000 like centimeters to the minus one um and there's actually two uh different states available in the in the p states this one was like spend one half and one would spend three halves we will not have to kind of worry about this too much um but i think what i want you to just see and we'll see this in the plots in a little bit the energy levels kind of increase but they they start increasing more and more slowly so what happens is that the energy levels get closer and closer together as you get kind of further and further away and eventually there's some level where it just leaves the atom so this is this kind of ionization kind of limit and so kind of the last energy level is about kind of 31 000 so it's quite difficult to see that so maybe let's just look at or maybe also need something maybe [Music] i'm just going did i make this does this one get bigger okay so here are all of the energy levels that we kind of looked at before um but i've just actually broken them out by by their spin so that's just so that they overlap a bit less and a bit clearer but you can imagine them to all be stuck sucked up together so here at the lowest energy level is uh the states kind of as uh 6s so that's just in the outer electron in its lower state and then um what happens when we kind of shine a control laser on it is that we can move we can add this kind of energy or subtract energy from it and move it between these different states um so if we were like real experimentalist designing like a device like this we would want to pick states that that worked well for us um so uh yeah so just kind of looking at this we can say pick the states say 6s and 7s um for a few reasons one is that they're kind of nicely uh sort of separated from each other because you don't if the energy difference um between two states is too similar then you might move um the electron kind of between both yeah yes you you can only essentially select the energy difference that you're supplying by choosing the wavelengths of your photons so if two energy uh differences are too similar in the drawing then you won't be able to control those transitions separately um so they're widely separated and the energy difference is not kind of too similar to any of the the other energy differences that that they are even just sort of if you just sort of i bought it you can see like okay there's quite a big gap there there can't really be that size gap anywhere else because like there's not enough space like left before you ionize the electron um cool so when we're creating our basis states here so we can just say okay uh state maybe state zero 6s state one is is 7s um actually i mean this is fine for now doesn't make too much difference um when you do this um [Music] the often actually they choose zero to be the the seven s state and one sort of to be the six states and they might not choose exactly exactly these states but they might often choose uh zero to be the higher one um it's just kind of a matter of convenience i'm mostly telling you so you don't get confused if you encounter encounter this um cool so maybe i want to just go back to the this other plot um so i think one thing i wanted to just point out is where these low energy states are in the kind of overall picture so here the dotted blue line is the the potential well that the electron is experiencing so that's the attraction from the nucleus minus the repulsion from all of the other electrons and so it's effectively the attraction by sort of a net one charge and uh the shape of this potential level is not not the actual shape i cheated a bit of formula just to make it like more apparent on the plots uh it actually comes kind of quite a bit sort of steeper um but these sort of 6s and 7s states are points they kind of right in the bottom of the potential um and that's important for two reasons one is it means they're not like likely to accidentally get ironized and fly away but it means that this uh position is more confined closer to the center of the atom and that's further away from surrounding atoms so um if you then just look at this this plot here we can see so remember that as we get higher up the potential well the states get closer and closer together so i've drawn so three states close to each other and you can see there the potential well is wider so there the states are more sort of distributed and you can see that it kind of like it gets sort of exponentially wider as you get closer to the electron being kind of completely ionized um and this effect of the kind of electrons kind of moving so away from the atom as we kind of drive them up is how we're going to get two qubits to interact like two neighboring atoms to interact um okay does that all make sense so far yeah okay um so i mean let's see if there's more to look at here okay so uh okay let's talk about uh okay how the two qubit interaction is going to work a bit because that's why we were in fact need three states rather than than two so um okay so okay now let's look at where do we um okay this is this is just kind of how the the distance varies with atomic number number n so the n is like the sixes and the sevens so you can see as the um as you go up like six seven compiler goes up as n squared and because you kind of can make this number almost as loud as you like it's kind of in square it's going to be quite big once you get quite high up into the kind of number yeah the numbers of the uh of like which yeah the count of which energy kind of level you're in um and then hello okay welcome back i think it will we'll be fine you'll see once we get back to the code you haven't missed too much on the code side um okay so what we're what are we looking at here in this plot um so here and so what we're going to do is we're going to select one of these very high energy states as our third state that we're going to use for to get atoms to interact and then we call call those high energy state r so we can have either zero one and r as our three basis states or a b and r or whatever you want to call them um and now uh these states rr means that um the both an atom and its neighboring atom are both in a very high energy state um so um and then this is the the energy of interaction between the electrons in that in that state so you can imagine that you have two neutral atoms here and then now kind of there they're both in the states where the electron cloud and multi-scale bit further away so this is like um as you kind of um depending on this on this distance here it's confusing what's going to call r there is some um kind of energy associated with these electrons being close to each other um and yeah so now this is an actual like real diagram with kind of actual this is actually the shape of the curve um so here when the atoms are far apart you can see that the kind of energy of interaction is kind of relatively kind of like constant and kind of low and there's no real interaction so on the right hand side of this picture but then you can see as you bring the kind of atoms close together um you can the sort of energy of interaction sort of goes up very quickly um and is almost a kind of wall and this this wall in the energy is called the river blockade and what it means is that if you imagine that we had a sort of a small like laser so that could push um the states um that one of the electrons into uh this r state then if if we tried to push a neighboring like electric electron like enabling atoms electron into that same r state uh it would be prevented because this kind of this energy of interaction so yeah so this kind of ripple blockade effect where you get a very strong interaction between two neighboring kind of atoms or at least the electrons of two neighboring atoms but only when they're both in this very sort of like high end so far from the nuclear state is what we will use to kind of generate the gate so we will yeah we will kind of either move or not move some uh some states kind of into this sort of our state um and um and then there will be a situation where the two navy atoms are prevented from both going into our state that they can easily go into our state if any one of these differences yeah um so this is what you were saying before that where you basically put the you put one atom or two atoms into the r states now you have these two electrons blockading up against each other yes you put you put two well you prevent it from putting two which is what gives you the interaction right right so now no one's saying you're trying to do that um cz game right yeah does that action implement the cz gate or now once you have these two electrodes interact and then you apply another operation that then all right maybe i will just show the drawing of the protocol which should now be understandable so where is this is uh wait this is sorry this is from an actual paper just because i'm too far in the bottom of paper this way um so this is the actual protocol for maybe let me just so this drawing on the right here shows that the actual protocol is the three states so let's just concentrate on the top left block for the moment so in the top left block we have um a so you can see two two atoms side by side so the control is the left atom and the target is the right so the control yeah so i maybe don't want to go through this completely now but so what you do is you try and move the control from the state that it's in and in this top uh top left block that's in the state one uh so you can see there is the yellow dots on the on the stage state one you try and move it up to um to this r state but that fails because you put in a laser pulse which would only drive the state 0 into the r state so so nothing happens there and then what you do is you drive the target q-tip into the r state and back out um which with what gives you um kind of the face flip that you want and then if now we move to the the right we're now the control to the same situation but the control keyboard is in the state zero now the first laser pulse drives the control keyword into the r state then the second pulse which would drive the target which in the previous drawing would have driven the target into the r state fails because now the energy required is like gap is much bigger and then afterwards you drive the control item back down into the zero state where it was um and and then a similar thing happens for for the others um and yeah and this kind of interaction means you get sort of three cases where you can drive um one of the atoms into the r state and and uh and one case where kind of none of them kind of get driven into our states and but yeah and the fact that kind of that in one's in one case this case where they're both in the state one the fact that nothing happens in that case is what makes the case different and that gives us the interaction we need and maybe we'll come back to that in a little bit more detail later um okay let's actually [Music] so maybe let's now go um maybe let's keep our two states for the moments and so we can visualize things um but yeah well maybe actually feel free to let's actually go back and actually make our states we can call them like a b and r or b0 b1 br whatever you want to call them but now put in a three instead of the two um and yeah and just kind of quickly make kind of like state three zero state three one and say three two and those will be the three states that we work with for the rest of the tutorial so oh and i shot when you're happy to go on good okay great um okay so now let's keep our um maybe also make another like b0 and b1 which is just the original basis unless you will maybe already have them because we're going to look at um how we build up these other the kind of linear combination plus beta1 um and maybe i'll kind of just reiterate again so um the the alpha squared and the b just well the length of l squared are saying each would have to sum to one uh but actually q-tip will take care of that for you when you tell it to do and we'll see in a second how so don't don't worry about it for now uh the the other thing that the the fact that kind of the states which are have this uh can which just have two states which are related by um being multiplied by e to the i theta so some complex number with length one um are the same state uh q-tip doesn't take care of for you um and there's a very good reason for that which is that um if you have uh if you have uh if you have just one one atom so one cubit um your your state is written like this but when you have a if you add a second qubit and we'll see how to write that in a bit so it'll essentially be combined there will then only be a single global phase for the whole system so whether or not you want to keep track of this whether these states already physically the same depends on whether you're considering an atom to be really just the whole system by itself or part of some larger system that you have to happen to be manipulating separately um cool um so i think now this actually just i mean just prints like a few things which sort of show kind of um what happens when you multiply um these different states so you can multiply one by like complex number like one j or you can just add b zero and b one subtract them and i guess you see sort of all like kind of the kind of obvious results so now you have a state which is uh kind of one j so it just has like i well yeah uh kind of i and zero it's now slightly more difficult to read um but and of course that state is actually the same physical state as b0 was because it's just multiplied by some some phase constant um yeah um and then yeah then we can kind of add things together so okay so and the reason that um the reason that these alphas this kind of self-esteem that peter squared have to sum to one is that if you were to kind of measure whether this state was in state 0 or state 1 so kind of detect whether in our case whether like a photon had been emitted or not um then the likelihood of kind of seeing whatever is supposed to happen in stage 0 measurement is uh alpha squared um well the length of the squared and the likelihood of seeing whatever is going to happen in stage one is beta squared and since all of the states i mean those are the only two states one of those two cases must happen so that's up to one all right so that's kind of why we need that um okay so obviously it is quite important to sort of normalize the states that this is true um and so let's just kind of have a look again at the next bit so you can type if you have a state say psi which is kind of b0 plus b1 you can use dot norm and then it will print out the the length of the state for you um so it'll just kind of take the dot product and square root so this is um kind of 1 squared plus 1 squared which is 2 and then square rooted which is like 1.414 and then um if you were going to normalize the state you would divide it by the by the norms you divided by sine of norm or multiply it by like 0.70 um but q tip also just has this handy function.unit so if you call it.unit it will just return the state divided by its norm and and now you're kind of your vector looks kind of less and less nice um but you can still kind of you sort of get used to just seeing these numbers cropping up whenever you're doing the right thing um and um yeah and then kind of when we were talking earlier i was speaking about um so these are um the kids and i think the uh the bra and together they form the product so there's a way to keep it from getting from the kept to a bra from the project hit and that's just um so if we just had real emergencies it would just be the transpose so um but because this these are complex matrices it's actually the transpose um followed by taking the complex conjugate of each individual entrance so you also see it called the adjoint you can see why that will be the case so if our state coefficients are alpha and beta and we want to get the norm then if you were to just put alpha plus pizza there we would get alpha squared plus beta squared but of course l squared might still be some complex numbers from some reasons not the length of the buffer so we have to just take the complex conjugates of each of those and then we've got alphas times alpha complex conjugate plus e to the b so that's that's good and q has a method for doing this transpose plus complex conjugates uh which is just called dag so which is just if you read the academic papers you'll see it written as like like psycho like plus um cool and maybe just if you do like v0.dag um you'll see that this type comes out as as bra and otherwise it's kind of and it's a row vector and otherwise it all looks kind of very familiar okay which okay um so maybe here like okay we've done in the products we've done sort of um so we know what we know what a bracket is but maybe now it's but in quantum physics you also use kit bras a lot um so that is written like this right and but what what is this and it's exactly what you would expect from kind of normal sort of um and then so that's i mean you just multiply this out using normal matrix multiplication so it's one times this row so it's one zero and then zero times this row so that's zero zero so that's one times one zero and zero and you can do that in computer quickly and this will kind of do it for you um and yeah so you get this uh whoops yeah you get this kind of operator here um what does it actually do um is i guess um a question so how to read these expressions so um so like a kit just means this is a state and then you you read the bra as um kind of um give me the components in the direction uh in the direction psi of whatever it's acting on so if you were to write that's like our state zero or maybe maybe it's actually verified i'll slip away so this says what is the component of the state 0 in the direction of my state's psi so this is like what is the components in running specs and congratulations sorry so and now you can um so now you can read the norm using that so it says um give me the direction of the state psi in the direction of psi which is just its length so again give me the components of psi in its own direction which is which is mean and then what is the normal position where we can find the read this so this is just an operation that we apply normally as we would function so it gives me the component in the direction psi and then multiply it by the state state psi or you could generally have another thing here so um so it says take the component in the direction psi and then that just gives you a number and multiply um well yeah the complex number and then and then multiply the states five by that number um so it's just a kind of it's just a linear operation which takes like a projection in one direction and then maps it to uh and to another direction so i'm sorry if i missed that so when you have that so the inner product is like a dot product so i i understand what does that give you just mechanical is that does that give you a two by two matrix or is that yes yes it gives you a linear map from vectors to vectors and the other way it does it if you have this because it's do a projection onto the state of psi which gives you some number and then multiply the state you're sort of projecting two by that number okay this is just looking at um so so one thing that is a kind of an important got here because of this fact that there's some this global phase that could be in front if you if you just take some vector b zero and then multiply it by uh i mean like one j or just like i don't know like e to the i pi or something it'll give you a new state b0 which is by construction equivalent but if you compare them equally in q-tip it'll tell you that it's like it's false isn't there not equal which is is true if you're not considering them as being the entire system if you're considering to be a part of some system but how do you check if two states already the same physical state um so the way that you do that is uh by calculating the the overlap which is in fact just uh the inner product between two different states not science side that's like fine sign and if that and then overlap you can show is only one if the two states are are identical um and so if you take the overlap of like um of b0 and b0 rotate by some phase that's um that overlap will be will be one um oh what's a little bit confusing here and it's also worth pointing out when you do these kind of calculations in q-tip you will get out um the yeah you'll get outside like a complex number buried inside like a matrix so so you have to kind of fish it out which is um a little bit annoying but anyway so this is how you kind of calculate it okay probably especially if you've done this the first time like you now you sort of know i guess algebraically like okay these are linear combinations the alpha scripts of b squared are some two one uh i must remember that the things with the same global phase are equivalent but you probably have no idea like what the state space actually looks like um but luckily there is a really nice way to visualize these things which we can kind of try now um called called the block sphere and um um so the the idea he well um so the idea here and you can kind of see that you can capture the fact that the lengths have to some the squares of things have to sum to one by replacing alpha and beta's magnitudes by a sine and a cos of some like single angle because then you have sine squared plus cos squared equals one um so that takes care of the um so if you write phi it's like cos of something plus sine of something then you know that alpha squared plus b squared is going to be equal to one so now you've removed that whole you've taken care of that constraint and then you know that only the relative sort of phase is important so you just write a single phase in in front of so you just arrange for the the say the constant phi to be a real number just kind of like rotate it by the right face and then you only have the difference in phase for beta and so that's kind of e to the i phi and now you've got sort of two two angles right you've got theta and phi and starting to remind you of spherical coordinates um it is not exactly the same as spherical coordinates because of this um this need to divide by two which is an important technical detail but it's not actually going to affect us now it just means that the uh the structure of the space means that the kind of angle kind of um is that you rotate through kind of moves like twice as quickly as you would would expect but i think the important thing now is to plot this for yourself in q-tip so you can make if you type q-tip dot block it gives you this like uh yeah so this this representation of the space of like the space of all possible cubit states on the sphere is called the block sphere um and it's like an extremely useful tool for telling like what is going on with two qubit states um so here like after you've created the bot sphere you can just add whatever state you like so here i just edit b0 and b1 and just add them add them with this add states method just give it a list um i set the colors explicitly uh this is a bit of a wart in like how cute youtube works the blossom implementation is very old and actually one of our google summer of code students from last year is busy slowly fixing it up for uh q-tip five which is an expert version of like the next major version of q-tip so hopefully it'll get less clunky um but for now you just set a list of colors one for each state that you want and i like to do this mostly so i don't get confused about which state is which i know that r is like red is the first date and blue's the second um and at the moment you call at the end you call block.show and then hopefully you also get kind of arrows pointing in the right directions uh maybe get your get your pictures up and give me a thumbs up and then we can just talk through um kind of the important things here so um i think uh there's a whole bunch that we can learn from this picture so firstly the zero basis states is kind of normally at the north pole and the one basis status at the at the bottom so the south pole um and importantly note that they're not orthogonal in the ordinal like geographical essentially they're orthogonal in the in the space of um uh in the space well in the cubit space so in this two-dimensional hill that's basically constructed so like zero kind of that overlap between zero and one is nothing um but they don't appear orthogonal on the block on the blocks here so that's just something to kind of keep in mind because you can't understand orthogonality from from this picture um the um and you'll yeah we'll kind of look at small small states in a bit um but also you can see so this x and y axis so remember i said that we're going to increment rotations around like the r x and then r y so those rotations are about are about this x-axis so the rx is a rotation of x-axis r y rotation about the y-axis and then the uh the z-axis is the north-south one so rotations around the z axis of just like like spinning the basketball um and yeah i know kind of you can also see why um the rotations are kind of all of the two bit um but i call it the cube of operations that you can do is a rotation of some sort because those are the only kind of kind of movements that you do around around the space given an initial state in the final stage it can always be expressed as a rotation of some sort um we're going to implement kind of all three kinds of rotations just because it's a little bit less kind of head scratching but you can't get away with two and in practical implementations you often often or you well it kind of actually depends on the physics of the problem if it's easier to do two and gives you like better results you do only two and if it's easier to do three you just do all three so uh oh so maybe i should say one of the one of the very unfortunate things about quantum mechanics is that there's no good visualization for three states or or anything like anything above um that that i know of and you we will actually see some of this here often what i well like often what i end up doing is i end up kind of reducing essentially extracting like only part of the state and just getting like so just looking at the relationship between two states and then plotting them on the block sphere as though they were a two-state system more generally if you've got even more states you just start looking at kind of different projections of your states trying to measure the amount of entanglement between things it all becomes kind of very sort of challenging and case specific [Music] okay one of the cool things now is you can just kind of like plot a whole bunch of states at once um which all have are just related by global phase so here i've just taken uh b zero and just multiplied it by kind of uh e to the i times theta or from theta and zero to two pi in like just some steps um and you can see they all end up just as one line on the blocks here and uh to convince yourself that um that like it's not like some problem with the implementation of the of the block sphere you can do the same thing but now you put in a phase between uh between b0 and b1 so you have b0 times some phase plus b1 and now you actually see kind of 16 kind of arrows so here b0 plus b1 ends up on the um uh just along the x-axis and so this yeah so this is that that state where that on the controller the x-axis is the state zero plus one and then these just kind of rotate the phase around that cool everyone happy block sphere is going nicely cool okay let's see if there's any okay good cool okay i think what we should do to now um is actually to start by trying to implement um kind of this the state function um and kind of what we what we want is just so um maybe i'll actually just uh find a little place to play maybe let's do it here no okay let me just just execute the second and what we want actually is to do is be able to call something like so um we want the to have a function where we can just and we'll kind of see well see why we might want a function to do this especially if you're dealing with lots of qubits but we want to just return something which is this like the first atom in the state zero and the second atom and also in its own state zero so two atoms come side by side electron say one electron in states six s the other electron in state six s 6s so we have to kind of learn how how to represent this um and so um so let's just make uh so i'm just going to make us won't actually have to do this so i'm going to make uh what we call this let's call them atom uh so this is the state b0 but for the first atom so and i'm just going to do it with with two basis states to start with um yeah okay well maybe yeah let's do it like that and then i'm going to make the second atom and so that's how we represent the the two atoms um separately um now how do we get the combined state for the whole system so i'll just call this b00 which is kind of the both of them in p0 and there is the operation called q-tip.tensor and that takes a list and you just give the list of the systems you want to combine all right let me just okay so so that is how you like type it in q-tip but what actually has happened here and what is the what is the like what is this tensor product thing um maybe this um okay maybe something or this should also be a bit bigger [Music] okay so i think that's what let's just look at what's q-tip produced so um we can see here the dimensions are two comma two so that means we have two systems each of which have uh dimension two that have been joined together um and um and there are four basis states and the kind of reason yeah and you can and you can see there are four elements so that means there's four com amplitudes so there's four basis states so the basis states correspond to um exactly the four basis states that you would i mean if you just had a classical system you would have zero zero zero one one zero one one and these in fact again the same way when we created the um the space for when there were two basis states where we just created a state for each basis state exactly the same thing happens here those states often name things like uh and now there's uh the the general state is sort of like alpha zero oh actually zero zero plus zero one uh plus alpha one zero plus alpha one one okay so that's the most kind of general state now so there's now four amplitudes uh again the four amperes you still have to sum to one you can still use q q-tips like dot units if you need to sort them out and there's now again kind of one global phase and now you can kind of see why q-tip didn't handle the global phase for us because um because there might be like an important phase difference kind of within some of these these states that are kind of important um because now before they would have seen two like global phases and now there's only only one so um great um okay so yeah um how much sense is this making i know for me just like thinking through why um you get out this kind of sort of structure it's like a i feel like the more i dig the more sort of like i ask questions about why things are this way but um people kind of happy with at least the construct for now so so well yeah cool okay um so now what we would like to do with uh with our state function is to be able to write something like the state zero zero and to kind of get out uh like an actual like uh like an actual like q-tip state so and maybe let's just run through the different kind of combinations let's look at zero one so zero one is the second state so choose the third so yeah it's all just like happy binary kind of like indexing so okay good um maybe i say it's also uh notice here that here we could write the state zero 0 but imagine we had a state well actually let's just make it let's just make [Music] let's just make so well these are now ones so just make this one and just and just make these again and then now we can make one one and then okay now it's just well okay let's just print p11 again okay okay so that's we know but now let's add them together and so this states which is like uh just zero zero plus one one um is called as kind of the bell state um and um it has this kind of interesting property that you can't write out you can't find so if you have the state zero zero plus one one is there isn't it's not equal to say if i it's not equal to face some state psi tensor product some state five or any psi or five so there isn't a way to write it as as uh as two there's no longer any way to write it as the state as two separate systems now they're just kind of you can only think about the whole state and if you ask questions about kind of one of the particular parts you get out any probabilistic answers all right in you also see um these tensor products written like just next to each other um which can be very confusing if you're kind of not used like used to you see a big complicated expression and it's like oh this just looks like multiplication um and then sometimes you'll see things written written like this um so yeah so this is just like the tenth place symbol so yeah let's just combine yeah it's just intense product of those those two yeah if it's going to be confusing i recommend like if you're writing company expressions just be a bit more explicit for yourself um otherwise it can get hard to figure out what is going on okay so i think it well um so you can see now from all of this typing while it would be nice to have a function which would just say just make these states for us not very difficult function um so we can just make a list of states which start out starts at and i know for like each character in s and then if and if it's zero then we do like q oh let me just do maybe let's just call this um like states or something and c is zero we do like we just call this q equals um oh sorry this is too tinted um and it's also actually useful sometimes to do like um and then you can do like and then you can do like states states append q dot units just to normalize everything afterwards and then return q-tip tensor states and now maybe this will even work who knows okay so then we can choose things like zero and one whoops oh oh i'm seeing some eyes okay um okay so am i just going to give people a few minutes oh cool one of the points where things start to get a bit more confusing so uh okay so uh okay so zero plus means the tens product of the state zero with the state plus okay so um so what does that mean so now we just write everything out so that's zero plus one and then i'm actually going to write over here just the factor of like one over square root of two to me um so just to normalize the state now the so there's actually like a sense of products here between these these two so the tensor product is linear in both of its arguments so we can just multiply like this this out and so this becomes zero tends to zero plus zero tensor one and also times whatever the square root uh which if we just write that slightly shorter it's uh it's zero zero plus zero one so and now you can see why q-tip kind of outputs what what it does because you can see that what this output is so this is the vector uh just one zero zero zero and this is the vector zero one zero zero so and just through the factor of square root of two so [Music] yeah so that's what zero plus is the tensor product which in state zero in the state plus and you can imagine that things get sort of yeah so now that's kind of you've just had like all even combinations so okay um so one question is how do you visualize what is going on on here so let's just call this states i don't know zero p or something like zero plus and just save it okay so um uh sorry okay so how do we know what's going on so there's an operation called the the partial trace and i don't want to go into too much detail um about it but what you do is you just sum over sort of kind of all of one part of a system and you're left with just another part so it's a way of like separating this whole system back into two pieces so we can do um equals so and now we're going to say we want to keep um actually not shipping it well do we need brackets anyway so we want to say we're just going to keep the first qubit um so [Music] now what we get out is a thing called the density matrix um which this density matrix happens so we'll get maybe a little short description of what density matrix keys are in a little bit but they're another slightly different way of representing uh renting representing states and now we can do let me do okay so what is great is that we can see that um our um that the box here can just render these uh density matrices as though there were memory states and i'll talk a little bit about what dense matrix are in a second but let's just uh also maybe look at the state of so now we'll keep all right so again take the combined system so b0 plus and but now we'll keep the second whoops now we'll keep the second um state rather than the first one so let's keep the second atom from the first one and then now we'll plot both okay so now you can so now you can see that so um the first atom is in the states zero so yeah and then the second atom is in the state plus um if you maybe this if we quickly run this again but do let's do one of like the awkward states so um yes now you can see we have gone um um oh i didn't ah i know what happened so we need to make this unit vector so i didn't normalize them afterwards and that's what's happened okay it's better else working maybe i'll have to i did it a slightly different different way um let's just stick it here um okay maybe the important point to take home here is because if you can't separate the two states then this kind of whole procedure of using the trace to visualize things on the um on the box here becomes more tricky um i know it stays just the same okay so okay so we now have kind of a state function and some way to to visualize it um awesome and how are people because quite a long time since we took to the break um how are people doing and to be one like a five minute break now or do people just want to continue cool okay cool let's do that and then maybe when we come back i'm just a little bit worried about time um maybe i will kind of i guess kind of work yeah we can start working through how to implement the kind of rotation gates and things and see how how far we get um [Music] maybe also um yeah let's take a break and then we'll kind of lean back in afterwards and find new way it's just hard so foreign so i thought i was getting sick of something um so so when it comes to like many body systems what are what's kind of the upper limit that people have been able to simulate in q-tip um it comes down a lot to kind of hardware but sort of like like so it's like 10 cubits like 15 cubits is like fine even right or like messing around with on your laptop um 20 like you kind of need like probably like a little like hip-hop circles or something which is like very deep like yeah yes it's kind of unusual yeah and then like 50 uh yeah so there is actually a flowcase solver built into q-tip which you won't look at today um but um you can do kind of both dynamics so yeah so and another q-tip has this range of different solvers for different cases so we're going to take an ac sole which is struggling information so that this in any cell which is the master equation so the monte carlo is solved and i guess those are the three that like chain of the workhorses um subspace procrastination um there's one little kicks which try to take into account the symmetries of the hamiltonian and kind of avoid like doing they reduce the space by taking it in yeah uh which works well if you have such symmetries and doesn't make sense um so uh then this i guess the one that like spent the most time on recently which is the uh hierarchical equations of motion solver which is a uh falls into like category three of the kind of calcium approximation right um and yeah but the yeah and the the kind of field is i guess used for like kind of fairly cutting-edge work we have these complex interactions with environments which once you find it right um so like one of the things that we that you can sort of synthetic with the heel that's the thing called biological decoupling so you have a cuvette which is like interacting like losing information to the environment and then but then that is the kind that you know the kind of interaction is important and you know that if you like do something like like just flip it from like zero to one there's the interaction starts going the other way and so then what you can do is you can let it go for like another few nanoseconds one way flip it have it come back for the same number of milliseconds and as long as you keep kind of flipping it backwards and forwards you can prevent it from coupling to the environment such as called dynamical decoupling um i mean that's the simplest version if you probably if you wanted to do something more complex for like a real piece of piece of hardware oh maybe i should show people if you haven't been there before there's this page of like q-tip example notebooks which range from like how to use python and numpy to like um some yeah to um like how to do this actually i mean uh cute also sort of built into it um like a pulse level kind of circuit simulation so you could you could take what we're doing today and then turn that into like a i mean it is yeah into like uh implementation inside q-tip which would allow you to take like q-tips sort of circuit description and execute it on like your model of the device so there's a yeah anyway if you if you are looking for kind of examples of what kind of people have done like this is the the page to start so qtip.org tutorial so if you go to the qtip you can just look under documentation tutorials and you'll find yourself here um uh okay so what i wanted to do was okay i think um i wanted to cheat a bit and actually oops yeah we can edit this i'm just gonna try okay let's just start again yeah i find the okay i'm not sure this is just a visualization function which um i wrote for myself like a few days ago it's actually something i want to contribute back back to q-tip on the visualization side and it does well maybe you can kind of copy it into your notebooks for now um since it's not part of the mainstream what you're doing but you can see now if um if we um plot um like the states um v zero p let me just do this um you can see the first stage is in state zero and the second state is in state class here i've just rotated the block sphere slightly inside the function just because it was more convenient like angle to look at things from when there were lots of states in this particular case um but so what that does is so it just creates a normal block sphere with some nice coloring um and it just um so it just um if you pass in a list of states then it just goes to the list and it adds them each to the the block sphere and it gives them all so qubit zero gets the first color qubit one gets the second color um and so that's the computational base states block sphere kind of plotting that's all just standard stuff we've done um this like three basically a three to the n system two two level system so that's the thing which takes this um kind of some like three to the n or two to the ends or some three to the n system and basically extracts just one qubit part of it to plot on the bosphere is so we can take the partial trace like we did before so that we've done um and that gives us row the last bit is slightly dark magic which i had to kind of do by hand and so that is the thing which converts back from the density matrix to an ordinary kit um and it didn't exist so i just had to do it by hand and as you can see here this actually like literally like i'm literally taking like i'm literally solving for um for what is the state's psi such that this is equal to my density matrix rho so this is the remember so the density matrix of a pure state is um this outer product of the two states so um yeah and yeah um and so it just finds out what's what side is by literally plugging in like alpha plus i beta and kind of alpha minus ib and multiply everything out and solving for it based on the four entries and the kind of in the in the density matrix um anyway but we don't have to worry about that too much for now if you're happy to to use it then you can also make kind of little little plots like these um cool are are people happy with their kind of implementation of the the states function i think i can just cut that and plunk it into mine which awesome okay so let's let's start with then like another simple thing that we can do right now i always remember this interview with linus two volts where they asked him like what his process for like coding was um and he that he was like well i never like use pen on paper and never write on the whiteboard like i only type stuff into my computer and he said but my process is that what i do is i select the thing that i understand best and do that next and then i hope that once i've coded that i understand everything a little bit better and then i understand something else what enough to start coding so so we know how to make our three bit our three like basis states um so let's call them like just because we're going to need them a lot so it's gonna i'm just calling them a and b rather than one and zero just so that i can make them have shorter names and then i'm gonna call this other state r which is going to be the kind of that very high level state that's we're going to use for two interactions okay cool and then we can also maybe simplify this here by just doing like subscriptions and and that's i don't think it's really specific okay cool and then let's move that one up so that it's just hide this way and then let's do here we'll do circuits and close and and i'm just going to fill in some atoms and then here let's do this check if everything still works okay great um okay so let's uh let's start by um actually just thinking about more about the problem so we're going to implement these gates by kind of firing laser pulses um at individual atoms so we're going to have um kind of some laser pulse and uh it will uh have some characteristics which would find a bit um and it will start at some sum time say t0 and we'll go to some time t1 and so the pulse will kind of be pointed at the system for kind of some amount of time dt so we know that our circuit is going to essentially consist of a like a list of laser pulses that are going to be fired so we can do like self dot pulses equals just kind of an empty list and then uh we're going to just for convenience one it's not wanting to like keep track of exactly what time every pulse needs to start so we just send the pulses in one after after each other so we'll just keep track of basically the last time that the time when the last pulse ended so that we'll just call t the time of when the last polls ended and initially it'll be zero okay so um so wait okay um okay we also maybe we should look at our box here again okay so um what we need is we need to somehow fire a pulse which is going to if we want to do rotation around the x-axis which is going to um kind of rotate things in around the x-axis so [Music] um in order to do this we are going to um to need to define a pulse and here we're going to need a little bit of um this one okay we're going to need a little bit of this kind of the schrodinger equation to understand how we're going to do this so um [Music] i'm going to think about the threading equation a little bit like in terms of of of q-tip so uh yeah it's the sun and everything's track which ones are big and which one's not yet um okay so the threading equation was is kind of written down in terms of the the wave function but the exact same equation is going to work for our kind of discretized states so it says the time derivative of our state psi is equal to minus 1 over h bar times this operator h which is called the hamiltonian which basically just describes how your system evolves in time uh operating on the states state size so um yeah in uh in q-tip we usually work with uh h well you you uh it's kind of up to you whether you see q-tip as always setting h-bar to one or whether you see q-tip as expecting your hamiltonian to already have been divided by h-bar it's kind of up to you when you're working with the system which way you you look at it but in either case internally q-tip will just have minus i times the h that you give it as the kind of evolution operator um and yeah um maybe we should just go ahead and see like how how to um to solve this in q-tip so i may want to mention just one thing in q-tip you can also supply a time-dependent hamiltonian and we'll see how to do that in a second um cool so okay so first of all we have to define kind of a hamiltonian and the the matrix which performs a uh [Music] a rotation um around the the x-axis so it's in fact a matrix that will um rotate the state 0 into state state 1 is the the operator that's um that will well it is what's called the generator of the um of the transformation of the rotate of the all the rotations around x-axis so what that means is that um so this operation called sx um and you can actually just print it out in q-tip which you can recommend so it's q-tip dots sigma x um and you print it out it's just this makes tricks kind of zero and one so if you think of it in terms of a gate its action is to take the zero state to the one states and the one state to the zero state so just swaps um just swaps the two basis states around um and you can imagine that what the hamiltonian does is that it applies if this is your h so if you set h equal to this then what it does is instead of applying this whole transformation at once it applies it like parts of it in little steps over time so um yeah so the time derivative will be kind of a part of that flip um in fact the i mean if you just plug in if you were to plug in exactly kind of x then you would get the kind of time derivative of psi is kind of minus i times the swap states um but then if you if you instead of plugging in in sx you plug in some small like smaller multiple of sx you can get it to kind of move slower so so that's what what we're doing here in this little cell so we create the sigma x matrix and we then multiply it by kind of the speed of rotation we want so we want in one and so one time step will give us um kind of sort of all of all of g so what i'm saying is going to rotate um over over sort of a time of 10 and in whatever kind of units we're working in we're going to rotate pi and so then i said h equal to g times times sx so this gives us a so g is called the coupling constant and just says how quickly this operation the swap operation is applied and um yeah so by making the coupling higher it's applied more like more suddenly making it lower is applied despite less less suddenly um so the schrodinger equation gives evolution over time but we also need an initial state so i'm just going to pick one so i'm just going to pick the basis states um to zero so just what like the zero states um i'm going to let time run from zero to 10 seconds just in 20 steps just so there's something to look at and then you just call q-tip dot sc solve where c stands for schrodinger equation sulfur you give it the this evolution operator the hamiltonian h and your initial state and then you tell it what what times so the t this does two things it tells you the start and end time of your evolution but it also tells you what points in time you want to know the results at um so it's the point in time that you want to know results at and then the entire evolution starts at the first item in the t list and ends at the last one and usually the first item is is time zero um yeah so the underneath just to kind of be clear the solver doesn't takes it doesn't hop from one time to another in t list the underlying solver where i carefully take small enough steps to keep accuracy and such but it will just kind of only output results at those times so you can imagine for very large systems you might not kind of want to have the state too frequently or if you're evolving for very long amounts of time you just don't want to try and store everything um cool and then we just plot everything on the on the block sphere again and so now so what i've done here is that i just made the the earlier states more transparent and the latest states uh less transparent so you can kind of see what's happening and so here you can see we started state 0 and then we rotate kind of slowly like around and after 10 seconds we're back where we started so we've now done a kind of complete kind of um a complete rotation around the block spheres and all the way from like the start to the end um so this is kind of exactly the rotation that's uh that we want um as in but we will adjust the amount of amount of time that or either the well just either the coupling or the sort of coupling cons yeah we'll in fact adjust the coupling constant g but uh when you're designing these real devices you might want to keep the coupling low so that you're not putting a lot of like extra power into your system heating it up and possibly messing with sort of surrounding atoms um and rather have it on for longer but on the other hand you also don't want to take too long because then your system sort of starts slowly becoming noisy and kind of getting kind of uh entangled with the environment um yeah so [Music] so we want to kind of do a gate which sort of is very similar to this except for one thing i said we want to change the time and well we want to we want to just adjust the coupling constant based on how far we want to turn but we also have a problem which is our system is a three qubit system rather than a a two qubit system um so maybe i will just in the kind of interest of time to sort of cheat a bit and walk kind of through things so um then we can sort of explain a little bit okay so ah maybe did i i just wanted to see if i did an example of oh uh yeah oh i do want to actually sorry i mentioned two other things about sesol because we're about to use them uh one is that you can apply an argument called eops which gives you that allows you to calculate expectation values of states essentially kind of allows you to sort of produce sort of artificial measurements of the different states rather than storing the full state themselves our system is small enough that it's fine and it's actually nice to work with the full stage if you can because then visualizing stuff is easier and then lastly there's an options object which is a instance of qtep dot options which allows you to specify um parameters to the to the underlying um like ordinary different material equation integrators and that's quite important because um especially that there's a in steps parameter which says how many like micro steps can be taken per um kind of big solver step before the integrator complains so if it's not managing to maintain accuracy and is needing too many steps it will throw an exception and then you want to increase end steps usually and for small systems that means you just wait like five seconds rather than like one second or something um but the number of internal steps can get quite large so i think the default is kind of maybe a bit low it's like one thousand um but like ten thousand fifty thousand two hundred thousand like internal steps are very common numbers to work with um and i guess what you need really depends on the entries in your hamiltonian matrix like whether there's people whether there's big ones and if yeah if there's combinations of big ones and small ones then usually it takes longer because it's trying to sort of maintain accuracy even though there's kind of big changes okay it also depends how what the time dependence of your hamiltonian is like okay so maybe just want to um just kind of maybe go through these little bits i guess that solver must be like pretty specialized for these specific differential equations um so it's actually not that specialized um the uh i mean we have uh we have now and in so in version okay in q34 so what we're using now um we really just use mostly like like is like basic solo you can um yeah and there are kind of various options for using like fancy solos so sometimes like we use things from intel's like mkl library um multiple performance um what is usually very different is how you actually can sit up the right hand side of the equation for the like for you to solve so q-tip sets up the right-hand side of the equation for you which in the case of the schrodinger equation is really simple right it's just like minus i times what you had before so that's that's easy but in other shoulders it gets so oh one complication the australian equation also if you you can also evolve density matrices rather than initial states and it all kind of works exactly the same uh except that under the hood uh q-tip knows to use to detect that it's the initial state of the density matrix and knows to use the density matrix formulation of the schrodinger equation rather than the like the state formulation um yeah um five we have uh we've written our own kind of flexible data layer so if peter 4 underneath the hood everything is sparse matrices which is problematic if you have like medium sized instrument like dense states with kind of entries everywhere it's also you know but in q5 we have a plugable layer so it uses numpy arrays for uh for small things and then sparse matrices when you start constructing things together and you can also uh this sort of i guess alpha level sort of uh stuff i'm running uh using basically gpu backed memory for the um this kind of states and the solvers and we have now our own implementations of um there's these uh generalized sort of runge-kutta class solvers that were um developed by this guy vern he's got this amazing page just just classified like all of like the best like solders and shows like this parameters so we have our own implementations of seven and then nine actually different number of coefficients specifically so we can use our data layer in the back end so that you can like tell like computer fight you can tell if you solve to if you pass it in the state as far as it uses like it does all of the calculations using sparse matrices but if you pass indents before patient can dismantle these um and if you pass in like a gpu backed array uh it will use the gpu like like uh currently occupy uh stuff to uh to write everything uh i'm not actually quite sure how successful like no one's really used we're actually looking for someone in a way to use this like gpu back-end for like some real things see how far it can go uh we have a google summer of code student commitment to doing some benchmarking and maybe if he does well enough you'll get to kind of benchmark some of that stuff um yeah so that is the yeah that is the kind of under the big thing kind of along the lines of you know i was gonna do you have anything that's like um sort of like cluster solver so you like give it an mpi communicator and you have like a big you know yeah so the closest thing we have is we have a tensorflow data layer back end um so you can use tensorflows kind of uh kind of i guess distributed computing like support but i would really i mean that's one of our sort of projects that we'd like to do and actually we have capacity from outside but we kind of need someone who has a real problem and like a real piece of hardware to run on and and time to work with us i guess because we would like to just add a back end which i know uses something like uh like legion or one of the like really big sort of modern kind of um yeah like linear algebra on supercomputers packages as a back end and see how far we can push it um but yeah but we only kind of i mean i guess so my main task at the moment is getting like the alpha version of q25 out the door and so hopefully that will be done before uh before end of september let's say um and then we can actually i guess start asking people to be like hey can you kind of give this a try or yeah kind of what you think sorry pepsi the silver silver sleeve uh i'm not sure um uh okay so um so maybe i'd like to so this pulse that we're going to generate here um so it's going to kind of walk through so here i is which qubit we're going to apply to and we're going to we're going to apply it to the iq with by instead of here we just had sx we'll have say if we wanted to do it on qubit 1 we'll have just the identity tensor with x so this is kind of no change and then if we wanted them on cubic zero we'll do the other way around and if we have like four of them we'll just have lots of copies of i and we'll have the operator we want stuck somewhere in the middle okay so that's what the is for and so theta is how far kind of we want to go and so we're going to um how far you end up turning is essentially the area and if you plot time and the sort of amplitude kind of alpha uh the coupling strength is the area under this this curve so we're going to just use a square pulse um but in real hardware you would use a um like some sort of shaped pulse um weirdly the most common pulse people use is the blackman pulse which is the same as my wife's surname and she also did a whole bunch of time series stuff but she's not that that blackman but um but anyway he the blackman pulses were optimized to cause as little noise on like other frequencies as possible so when you shape the pulse obviously the fourier transform longer is a single frequency so okay so we're going to work out basically we know that we want the integral to be theta so we know that we want alpha times t to be theta so um alpha must be theta over t so okay so that's just this here so this v is the uh coupling strength um all right i i i put like uh o for omega equals v so kind of v was the kind of velocity and then omega is the actual um um the actual coupling strength to put in because sometimes for some systems there can be some sort of like other factor involved um but here they're just identical so i just um kind of stuck them in and then here we want to okay i'm going to make this uh let's just call this like place op or something um okay now uh we have sx here but let's first let's just first deal with the the things that are easy and it will wonder about this so so this is going to uh so let's just write like this so what we're going to do is just do um so we're going to do so um we're going to do i don't know ops we're going to just make a whole bunch of copies of of i so this will be like oh now q q i gives the identity matrix um on three basis elements and then we're just going to times it by just the number of atoms so just self.n and then we'll just fill in oh here did i first here i think and then we'll just tense them and then okay that's done and then okay how do we actually add the pulse so this okay so here we're just going to have a pulse which is an operation and then dt okay so um okay so okay the current the pulse is going to start at the current time t and then at the end we're going to do self.t but we're going to just add the end of the pulse to the end um so now we can just do kind of end equals self.t plus dt and then at the end we'll just set this okay so okay so that deals with the timing but now what are we going to add to our so uh do we just call this pulses uh and we're going to append okay so we want to append our operation but now we that operation if we just append it like this will just be applied the whole time um so what we want to do and here is um so uh in q-tip you can just uh you can pass in uh time-dependent hamiltonian and the way they work is you pass in just a you make a make a tuple which just has your operation h and then a coefficient function like f of t and then the applied thing is just f of t times h so what we want since we have a step pulse is we just want an f of t that is a step function of the time that you want so we can just do here like f equals [Music] okay so we can just pass it like just make lambda t um and uh this args problems if you need to pass extra function like extra arguments into your step function then you can find this aux parameter so then yep so if if time is between uh start and end then apply the pulse otherwise uh multiplied by one otherwise multiplied by zero and now we can just pass an if okay so okay so now we've kind of added our pulses and updated the time that's great and now um okay what is this uh what is the self that underscore a b underscore sx so um what we need to do is we need to now be able to write this matrix as x but we want to only write it we want to now write it for our states a b which are part of some bigger um bigger system so how do we do that sorry so we did by noticing that sx is the swap operation and we can write that in so so essex swaps two states so what that means if i write this down in terms of zero and one means that a goes to b and b goes to a we know a and b are now arbitrary kind of states so what we want is we want um [Music] to write this so we want to take the components of state a and now make it the component of state b so this says take the component of state a and multiply by state b okay um and then we want to we're also going to take the component of say e to state a and then apply so um so this is how we write the this swap um for kind of into any two states so that you could just publish a larger system so this is kind of what we need to do our swap now as it's kind of like system um so yeah and i'll just i guess in order to uh now maybe just to save a bit of time just do the other ones as well so um i think i just wrote it here just so we have them around for ease of use okay so okay now we have uh implemented our first get and the other ones are going to our y and our sale again very similar and now we just need to implement run so um let you just so run is actually not too bad [Music] actually i'll just take so uh yeah i changed around a bit for convenience so here rather than passing in the t list i just use the fact that we know that the time has to go from zero to t which is our last pulse so we'll use that and then just put a number of steps equal to some rate per time which defaults to 20. and then i added one just so that we get like even spacings from zero to the end um if there are pulses then it constructs um okay we can remove this for now [Music] it constructs the uh the pulses from this q object evo well you don't actually have to use q object either but what q object evo does is it takes a list of these um like h times f of t and that represents the hamiltonian which is the sum of so if you have q of evo times like a sum of these or a list of these like tuples um then that's h equals kind of the sum through i of n of h i times f i of t so it's just this whole sum i'm going to give you a time dependent system and and then yeah and then we have our h and then we just call kind of sc solve in the normal way i just specified the n steps equals 200 000 just to like avoid kind of hassles and yeah we pass in the t list and options and we return the results now hopefully actually maybe i um just actually maybe bring the example that we were trying to run closer up so maybe we can just uncomment one so just do an rx skate just rotation x and now let's remove these maybe we can just try run itself oh uh oh we don't actually need to play in here cliff and maybe we can just chomp these just to save some just to get some save some scrolling around apparently we don't actually actually don't want any of this we don't want this last circuit for now more errors oh pop is not defined oh uh what should we do uh oh because we didn't do uh so we didn't pass in we called it pulse not up okay once more okay so here what i've done is i've so we started in uh in state one one and so the red represents the first qubit and the blue represents the second qubit and you can see that's um these are the kind of and we've done a rotation of first one so you can see the the first qubits and so the second qubit has just stayed where it was because we didn't rotate it which is good and then the first uh qubit has moved from one to y maybe you'll actually maybe let's just look at the evolution uh maybe it's just uh for now let's just have adjusted that you can see the second sorry the first qubit has just moved from one and moved now to the y axis because being rotated around around x by kind of one quarter of a turn around the box here um so that that is our x gate um which um are people kind of happy with the kind of x-gate at the moment i know kind of have sped up quite a bit um but the kind of the y gate is going to be sort of exactly the same but not exactly the same but very similar so [Music] it's going to but now we're just going to just rotate about y so yep otherwise it's the same and yes now let's do maybe let's rotate the second one about the y-axis since the way that's the axis right cool now i can see that now we've rotated its the first qubit of x-axis and the second qubit about y-axis so looking good um and now um so of course that is also the same but the the control physics underlying all of this are a little bit different but let's just maybe keep things simple for now um which um and now if we can let's just do one to start off with let's do both and then let's try them together wait um or maybe let's also change the sign of pi just to make them go in opposite directions a little bit tangled great um so [Music] um yeah so now you can see that's kind of each of them so in opposite directions and it goes like a rotation about um [Music] about x to y and then about z to x and then about y back up to zero so they kind of um yeah so we can see i mean now yeah now we have sort of all of the um the um yeah we have all of the basic gates where all of the single qubit gates and i think now what i would like to do is to go through you know last little bit just to go through how to do the um uh control z gates and then maybe declare that so maybe we are okay so um so maybe now let's go through this procedure in a bit more detail um so you can see here kind of here our quantum state and now we're going to use uh these swap rotations so the sx that we've been doing but now instead of them going between the zero and one so the amd states is going to go from um the um from there we're going to okay so we're going to do rotations between either or between the zero states and the r states so we look at the first one so that and we're only going to do ones which do one complete swap so there's just a um a kind of one pi uh so yeah so we're going to do on the control key which is the first one we're going to try and rotate state 0 up to state r and if we're in state 1 it's going to fail so so if we just look at the top two drawings which are when the control pivot is in zero the control pivot is one if we start with the control keyword in state one it's going to swap kind of amplitudes of zero with amplitude of r but if the amplitude of zero is already zero that's not going to change anything um and then uh at the end we're going to try and swap it back back down just don't do that and then on the right when the control is one it's going to succeed so we're going to kind of swap up and just say r um and [Music] so so yeah so the first pulse is the pulse one and either it will kind of have some effect on or not so if the controller is in zero state zero they'll have some effects if the controller is in state one it's not going to have any effect so here you can see pulse sort of one and three um um both have haven't have any effects if the control keyboard is one so sorry it's a control pivot so it doesn't say zero so if the control pivot is in state zero we're going to successfully push it into the state r um which is then going to uh block the um interaction um going to block any chance of the target being moving into state r because of this interaction because they're nearby um and the interaction strength will be kind of will depend on the distance and so yeah and then what we'll do is we'll try and um so if you look at the target qubit we're also going to now try and swap state zero or move state zero into state r um but we're now only going to succeed in in one case so we're only going to succeed when the target doesn't stay zero and the control hasn't been pushed into state r so the control has been pushing state r is going to block the target kind of keep it from getting there so it's just going to be more of these swap gates but now we're going to fix the angles that we're going through to either pi or two pi and we're going to apply it to states of the control qubit or states of the target so we're going to need three pulses so attempt to drive the control up into state r attempt to drive the um the target kind of up into state r and back down which will succeed or not and we're going to then attempt to bring the control back from state r if interstate 0 if it moved so there are two parts to this so let's maybe cover the the um um let me do this okay so so here there's um how we don't actually need that this vc is just the um again the kind of amplitude for the each of the pulses on the control qubit so it's just kind of one half of pi and one half of two pi and then um we're going to do again place up and we don't need the pins anymore okay so we're going to make three pulses and those are just like pulse we've lost that screen there's going to be pulse the first pulse on the control keyboard which is here which is going to be just a pi pulse trying to drive the zero states kind of up into r and then there's going to be a pulse on the target qubit which attempts to drive it all the way up into kind of the state r and back down which will kind of do nothing if it can't get to state r um and um and then lastly we're going to bring the control keep it back down okay so we need to define these um this arx um okay so that's going to be like the swap gates but between um a and r rather than kind of the state's nb let's do what you think um um okay so now we're going to check that that runs so we'll just put in a slightly uh actually maybe we can do this with just this one so okay uh we're going to apply a seaside gates and in order to see the face flip what we'd like to do is to um so remember the phase so the face flip is just going to put a kind of minus sign in front of um in front of beta so you need a state that that will only that won't be visible in a single state if if you only have state zero or state one adding a minus sign to beta would at most introduce like a global phase so you wouldn't be able to see it on the box here but if you have zero plus plus one then putting a minus sign in here sort of switches it to the minus state so we're going to use the plus state and kind of hope to to see it getting from getting flipped um and okay so let's see if we broke anything um so okay now we'll probably need the second blocks here so okay um so here you can see kind of initial states uh the blue cube it was an x and um the other qubit was was in y but now maybe see we can see this here but when it's a zero it still got flipped um and the reason for that is we haven't actually if we go back to here we haven't actually put in this change in the energy into our hamiltonian any anywhere so um what's happening at the moment is that there is no kind of gap here that prevents the interaction from happening so then we now have to describe in our hamiltonian what that gap is and so we should we should do that maybe it's actually easiest to i'm sure one of these has okay maybe i will just let's find it in the paper what it has to look like so uh up here in the description of the system so okay so here is the hamiltonian so what we've done here you can see is the big sum of the eye and we've added in those his or all of our driving terms that we've added into our ph event so we've done all of those but this is the interaction term um so um the interaction term is just kind of a sum between all pairs and then you only count each pair once so that's why there's the kind of i and j and then atom doesn't interact with itself so that's the i kind of like greater than j and then this number in front this c6 is just the coefficient that you calculate from the physics of the system and then the rij to the power six is the curve that i showed kind of like uh near to the start which is like the one over r to the sixth coefficient and so we well um i haven't included the distance yet but just sort of assumed that the atoms like distance one from each other um and then this n9nj is just the um [Music] the um projector onto the state r so basically how much of like what is the coefficient of the state of the state r so uh and so that's so n i is just the uh the projection onto the state r for the atom so it's like r times r dot dag and you don't just see it up there so we can add that to our hamiltonian um [Music] so just you know we're going to make this this h which i'm just calling h rydberg because it's for the red blockade um but it's a little bit frustrating and something should really add to cubs it's just something which allows you to construct basically the zero matrix easily um so here i'm just constructing the zero matrix with the dimensions that i want so if you pass a cube it's just dimensions it gives you the zero matrix and that's just so we can have something to add all of these different contributions to um and then self difference please so now i'm just going to um i come up so what we're just doing here is just making all of the interaction operators we need so this is kind of a list of rri and we actually don't need that anymore um and we'll need to define just this rr so that's um let's just make one representation i've got another zip cool okay so so this here is just creating the zero matrix this is just creating the list of operators we're going to use and then we just do exactly the sum that they do in the paper and we just add um the oh i made the coupling one but it's just oh sorry i made the coupling sorry the coupling must be quite high so that the energy gap is big enough so and then maybe [Music] maybe this will even work let me have a look [Music] oh i didn't actually i thought i wasn't using this anymore but actually i just made um oh ah we have created the we have created this h root but we haven't actually added it into our into here yet [Music] maybe this is better yay okay so that didn't flip it and maybe no one did yay okay and so now we have kind of everything we need um we are like two minutes away i'm happy to stay for a little bit longer if people want to look at things but i'm also happy to kind of call it here it has been like four hours which is i mean i think a little bit brutal on everyone would actually much rather have like you know like a whole day or something so they could be a bit more great anyway uh yes you know hopefully that works and i will put the notes and everything in the channel as soon as i kind of clean them up so that we did go to this last bit in kind of a whirlwind um maybe one thing that i left out that i will just like to show because i think it's um i mostly kind of lift it out a bit because it was hurrying too much um but the actual laser pulses when you kind of get that like g times sigma x comes from the actual physics of the of the problem so here like the drive around um and the kind of here is actually where you work out the effect of applying a pulse and you get out the sigma x sigma y sigma z and it kind of gives you i mean because we've picked rotation around x or y we you have cos equals one or sine equals one um and so these are just kind of uh this amplitude five of t um as i mentioned the physics in the z case is slightly different uh what creates physically the rotation around the z-axis it's also a function just a function of t but it's what's called the d tuning so you don't so normally you just sort of send it at exactly the difference in frequency between the well the frequency of the photon that exactly matches the energy difference but if you want the rotation around z you give a pulse whose frequency is slightly away from that kind of resonant frequency um so anyway you get a slightly different thing cool okay sorry that my last was yes so like um you have all of these gates that basically perform rotations on like a basis right each of if you would like to equate that to the classical you know bit all of those are valid states for the cubit to be in and they're infinite now you decide what phase you uh yes i mean literally they cover the entire entire sphere so yeah there's a two-dimensional infinite space which has the topology of the skill yeah yes and you're limited only your computational position or like what do you look like or how well you can control the pulses yeah so i mean yeah so yeah there's like a real question here which is like can we really access all of those states i mean how do we specify like theta for example we um we don't know yeah i mean we only so we have like feature specifiers either like a 60 like a double precision floating point number that's still a very finite number um so the question and there's like a really deep question here which is maybe a new answer i guess around maybe like the 80s or something but the question is can you essentially with these gates makes circuits which approximates the kind of infinitely precise circuit well enough and the answer is it's kind of yes you can so you can show that these uh that the space of kind of sort of like of states that you can access within your complete system is um is dense enough that is in it well it's it scales with the it basically it remains equally dense no matter how many kind of cubes you have so you just have to make so the more accurate you make theta the more sort of kind of things you can make but there's no parts of this entire space that i'm sort of not accessible and there's no there's no sort of evolution like there's no kind of like real physical things that you can't access approximately there is a bit of a question whether so this whole things falls under the heading of kind of unitary decomposition so basically given some evolution of the system can i approximate it with these simple operations that we have um and so so okay so given the answer yes you can but the answer is also you might need a lot of gates you might need a very deep circuit and i think in general it scales like four to the n or something so like that's where um if you have a n qubit system arbitrary unitaries might take up to four to the n gates um and the question is how much of a problem is that um and there they just work which which shows that you can approximate them well with fuel gates and stuff um i guess once we get to like a thousand cubits we'll start having an idea of how well this works in practice um but you don't need the whole sphere right like you you can do yeah much like exponentially quicker even with only accessing like a small proportion of the sphere yes yeah so if you only need to do like if you only need like 10 gates then you only need 10 gates to do your problems so yeah there's also a question of like what are the useful algorithms um yeah there's a whole i guess sort of land grab you know like in a way going on to try and make useful algorithms um which is kind of fun i personally sort of stayed out of it myself i sort of i think my personal feeling is that it looks a lot of a lot of fun but i feel that it's maybe the people getting on the ground now what they like make their names like have their names and lights and become super famous one day of having developed like x algorithm um but it also feels like it's work that will only be used like 20 years plus from now so um yeah but it is still like a fun game that lots of people are playing and you if you want money to play this game also people will pay you to play this game thank you so much thank you very yeah and much free to post questions back or just under come find me and yeah and i should be around sprints if anyone is still here to like work on stuff yeah all right thank you very much and what were the thoughts overall like too much i felt that maybe if i knew it was ambitious uh i think like ambitious to pair code together not ambitious to demonstrate so i think that would be the like you know if you if you just if you just do the demo then i think that would be enough time because we were at some points trying to also like do it ourselves um also did we spend too much time like early on just like throwing stuff in detail and then right to the end or or was that the right way to do it well i don't think you can watch this stuff well i mean at least like for me i don't think you could rush the stuff at the beginning because i would not want to try to balance both of those things yeah very audience dependent yeah yeah yeah yeah if it was just but you could just get the whole bus out if we weren't you know there anything that you kind of wanted to i mean we only touched like one tiny corner of you if there's anything that people want to know more about that's absolutely yeah a little bit interesting to have some sort of like environment or noise model so you have we're kind of assuming we have a perfect state right now so it would be interesting to see like okay if i have like zero kelvin right in some environment i kind of wanted to get there and it's not that hard like we we sort of replace like s with m here um and this should this so this uses um the master equation solver which is the first like kind of um you know maybe this even just i mean this should hopefully not that doesn't change anything because we didn't so then for the master equation solver you specify um uh collapse operators um so collapse operators define how the system so okay uh masturbation solver it's solving um something which is um just kind of uh a pathway kind of all that can happen is you can kind of lose you could you can have just some like sort of things kind of well the term collapse operator gets described that you sort of have operations which can kind of cause sort of stuff to kind of like dephase with it itself um or kind of be lost from the system so it's just like a very like simple thing that can just can kind of happen and [Music] uh and so you just specify the operators that you want and so that's just to place up i don't know maybe just on the first cubits and we want uh i don't know maybe just some phase noise so that's like a b so that's just basically this kind of uh so that the zeta operator if you think of in terms of likelihood a and b just kind of introduces like a phase difference and then maybe you want to multiply it by something so it's not happening too fast yeah i don't know i haven't run this yet okay i don't really seem to and i really just need to kind of expect it oh maybe it's maybe it's maybe it's sort of maybe it's something new okay i mean that's him we need to play around with some more and also think about whether that's the right collapse operator for this and whether we can even see it with the control z but um yeah essentially we're adding these collapse operators which then specify a kind of interaction with the system you basically have to derive a collapse operator off the physics yeah what what what you expect the kind of operations to be and then stick them in and that's i guess the simplest sort of like open um like quantum thing you can do in q-tip and then from there it gets more more kind of advanced typically you use things which structure your path more and more so um yeah um so you yeah whatever you want yeah you kind of just described the path in more detail the kind of interactions you get and and you just keep changing which solve you're using and supplying the right options and then also interpreting the results gets more tricky because then you have this like bath sitting off to the side and you may also want to if the boss retains any states you may want to know what the state of the bath is at some time so like with the hierarchical equation of motion you can also get like you can have currents between the system and like of like probability between the system and the path but you can also within the bath you can have like restructuring happening so yeah anyway i put this dynamic detuning imagine if you sort of lose stuff to the path and then somehow get that back out the path has to kind of like keep has to somehow in its internal state keep track of like what's been lost to us yeah yeah okay well hopefully we can definitely look more and we can maybe even find the popular collapse off phrases for these examples at some points
19,Loopy and Unloopy Programming Techniques,https://www.youtube.com/watch?v=Dovyd72eD70,welcome to the session on loopy and unloopy uh programming techniques um so uh just as a general introduction this is kind of um i would i was in on the the intro to numpy session and this is in some sense like an advanced numpy session um so i'll explain more about what i mean by these unloopy programming techniques but generally i mean vectorization the kind of uh thinking that you had to do to do uh numpy uh i'm going to organize this in terms of oh you know what you should first uh this i believe is everybody has access to this link right you all have this and you're looking at it uh i i i posted it on slack earlier today the slack for this session so that should that should help you but you'll find it so we'll get on the same page literally the same web page and then we'll we'll go from there okay yep uh and to do these exercises and to uh to be following along with me um you can uh i encourage everyone to to run this on binder so you can install everything on your on your computer but that's that's up to you uh there is a requirements txt which is what makes the binder work uh and you can get these packages not necessarily these versions but i pinned all of these versions so that this tutorial is repeatable yeah and so the rough outline will be like this i kind of took the material that that i had and uh divided up by the time the uh main part of this the part that we really want to be sure to get to are the exercises and in this actually i'd like you to pair in groups of twos and threes you know whether the twos or threes uh doesn't matter but uh make groups uh except for the helpers who are who are floating around so uh yeah make make groups uh get to know each other um hopefully actually during these exercises this will be a noisy room hopefully um twos and threes to solve the what if we have an odd number of people problem yeah so sit near each other and also uh i will adjust to the level of the room you know if everybody's uh um if everybody is having trouble then i'll go more slowly if everybody's just whizzing through it then i'll then i'll do all the material i prepared more than enough material so if this looks like a lot and gee how are we gonna get through all of this don't worry that that was kind of the point it was in order to have this kind of flex okay so to get started in order to be able to follow along uh click the launch binder button or if you want to be able to get back to github afterward uh uh you know open a new tab and that should open this up rather quickly so uh let's uh so i'm gonna wait until everybody's on the same page here so raise your hand if you're not if if jupiter is still loading and we'll watch the hands go down okay so you're watching the screen and you see all the stuff loading loading loading so everybody's hand is down you all have a jupiter notebook up uh and since this will probably come up if you don't touch jupiter if you don't touch binder for a while it might log you out you might need to reload the page um yeah just reload the page it will remember nothing you'll have to you know run up to sell you know that that's that trick of uh select a cell and under kernel um or here run up to the current cell when all above selected cell that sort of thing to get back back to where you were and and be able to keep going uh it will not remember any of the commands that you've done so uh so when you're working on the exercise you know generally the solution to the exercise is going to be like a one-liner or a couple lines or maybe a little function or something like that but if you don't want to lose what you're working on while while doing the exercise you know be copy copying it into a into a text editor to make sure it doesn't go away all right so let's get started uh yeah uh just on the uh the the part that's not an exercise just shift click along with me and if you want to try out different things just try out different things and if that leads to a question so much the better you know raise your hand and ask um so yeah uh so loopy and unloopy here i'm talking about uh whether your code has explicit loops whether you can see the for loop in your code it's a it's a programming paradigm so i'll be talking about this uh paradigm called array-oriented programming and the way that i'll be using the word uh this is a paradigm in the same sense as paradigms like imperative or object-oriented or functional um has anybody uh not heard of at least one of these imperative object-oriented functional yeah okay good that's that that's all you need just to know that there are these paradigms in programming bigger than than difference between one language than another they're kind of like one group of languages or a way of programming within a language uh in array oriented programming oh uh make it big in array oriented programming the primary data type is an array and most of the functions perform one operation on all elements of the array so to give some examples of what it is not this is not array oriented where here we have some input data and we're filling in output data by writing this explicit for loop and performing the operation one at a time because this explicitly specifies an order of execution and uh and you say what happens to each element so you know it has a lot of control but that's this is not what array of oriented programming is this is imperative programming you're telling the computer exactly what to do imperative you know also this is not array aren't it um does uh does anybody recognize this style you know what paradigm would you call this this one is a shout out yeah yeah uh functional i heard something else but i didn't quite figure out what it was um yeah i would also call this functional uh because you make a function and you give it to another function as something to apply to it and although there's no explicit for loop here is still i guess what you would say uh not coordinate free you know you've explicitly uh you've specified a coordinate this x is one element and the code here is describing what to do to one element so that's that's how it's different and by the way there's nothing good or bad about a paradigm it's not we should be always using this or that except in uh for certain purposes so here this is array oriented and uh if you're long time numpy users uh you probably know this uh this is you know how you do basically all math and numpy is uh uh you write a formula using the arrays as though they were scalars but it applies to whole arrays so here we get the output data as each output datum is the square of each input datum because we take the whole output data array and square it so is this kind of a nebulous distinction yeah because programming paradigms are kind of nebulous and you can have long interesting arguments about whether this is something or other is a an example of a paradigm or not uh so they're they're kind of nebulous they're fuzzy around the edges but they're useful concepts to talk about a general direction so i'll be talking about the general direction of array oriented programming uh and before and as a way to dive into that uh this is one of the little known paradigms you know you'll you'll hear people talking about it uh imperative object-oriented functional you'll hear about that all the time but array aren't it is uh has kind of gone under the radar uh but it has been around for a long time so um i have been making this this diagram of all the languages that i know to be array oriented or have array-oriented features and this is becoming a more and more crowded diagram as i learned about more and more some of them quite obscure languages but it all started with apl and this is way back in the in the 1960s uh it was named after a book called a programming language it was early enough that this is what you say and originally it was really the programming language uh and this is a great brand ancestor of a lot of these and some of them will occur matlab certainly numbered um we'll get back to the general features um atl with this um [Music] and you can spend a very long time [Music] uh since the beginning you know apl was like this that was the whole idea of speakeasy is that you're speaking to a computer um and these are languages for doing statistics and data analysis where one-on-one back-and-forth conversations with a computer is essential um so they were intended for users to tinker with experimentally and in retrospect this makes sense because data analysts are often asking themselves what will this function to the due to the distribution of all data like if you applied a function to one element that might not tell you anything statistically about what you've just done you have to see the whole thing so suppose we take like a million data points uh first question you ask is well what does it look like in this case it's a it's a normal distribution because that's what i generated uh what would happen if we just squared them all you know try to get in your head like what's this distribution going to look like it's a little hard to to to guess but you can just try it see what happens and um okay it gets very it gets a longer tail it's always positive okay that makes sense you know if you just try this on a single positive example you wouldn't necessarily know that you know they can't be negative anymore now if you do this operation take the sine of one over it just imagine what is this distribution gonna look like and i couldn't tell you you know i would have to think uh uh sine of uh i don't know and if you had one one value it's still not really going to tell you what the distribution is going to be distribution is this cool thing you know they pile up at uh one one and minus one because the sign thing but you know there's this this bulge here and it would it would take some serious thinking to to to predict that but this is but it's quick when you're doing it with a computer so this is what array oriented program programming languages are good for do one thing to all your data look at it another thing to all your data look at it so there's two advantages the mathematical expressions are more concise and so it's a convenient way to type interactively not as concise as apl because that was a mistake and the right part of the computation is accelerated the loop over all the data points you've got a million data points you don't want to have to spend wait for it to turn over all those million you want to immediately see the answer so uh you're here uh you are familiar with array oriented programming in python numpy you might be using this uh more often through pandas you might be using this more often through xray you might be using this more often through jacks or pie torch or or any of these things but this is the the basic thing and the the the syntax has been uh it's mostly universal so this quick reference card is uh the kind of thing you want to just sort of have in mind or refer to and that sort of thing so this is just display numpy exists okay got it all right we're all good on numpy uh so and about the interface so numpy makes arrays duct typeable with scalars and mathematical calculations this is the the advantage that it's bringing because python has stuck typing um you know unless you specify the types and type check it uh you you know any types that will work in this expression can go through the expression so let's take the quadratic formula this is um you know the solution to ace a squared plus b x a x squared plus b x plus c equals zero this is that x that solution uh if we put in a b and c as scalar values you can go through the formula give a scalar value if we put in arrays they'll just go through the same formula give you an array and then about performance numpy makes and here's the important thing numpy makes each step in this calculation fast but not necessarily the whole expression and so we have other things for that because that'll be actually another theme that's coming into this is about performance we're doing this for convenience and for performance uh so the quadratic formula above is essentially equivalent to the following we want to compute minus b b squared four times a you know uh and num what numpy is doing behind the scenes is essentially this it does each one of these operations and makes a new array for each one of them and then uh uh and uses you know does each operation makes a whole array does operation make a whole array and that's not necessarily the most efficient way to do it so uh as a way of showing that these are kind of the same this was uh doing just numpy simply and this was doing all of the uh uh steps individually and they come up to be about the same and i'm here i'm gonna wave my hands about the less than two times difference okay so 17 22 milliseconds something on something like that there are other ways to express to to do this like you could take a num expert okay numexper takes this whole expression and evaluates that on each element in a single pass and you'll see uh if you're trying this out that this is faster if you're doing any operations in pandas inside of quoted strings you notice that that's faster it's pandas is using numexper it avoids all of the intermediate arrays in between because it's great if you make an intermediate array and look at it make another intermediate array and look at it but if you're not looking at it it becomes a liability in terms of performance uh another thing that we can do is we can actually compile it with like llvm number does that so you just wanting to be aware of all these tools that you can use when you're when you're thinking about performance number can take a a function like the quadratic formula and actually vectorize it make the thing that that numexpert did but whereas numexper just has a fast virtual machine this will actually compile it down into machine code and it'll be a bit faster as a result 16 versus 2. and jax is has also has a jit compiler number.jit jax.jit it's jit compiler is like quite different so so between number and jax we've got the basis state of basis states of the of the oh my goodness i'm using math terms i shouldn't between numpy and jax we've got two sufficiently different things that it's worth having trying them both so with jax it really must be array oriented you can't go down to an imperative level the way that you can with number but it does more optimizations with what you've got um in this case number is faster but it all depends on the problem okay so generally speaking sort of to put an end parenthesis on that uh uh carefully optimize implementation is out yeah so what what's fastest carefully optimized implementation you get out seed plus plus or rust or something and you really you know you you really do it right and you get all the memory and things that yeah that'll be fastest uh compiled single pass loop um is uh is faster than a numpy expression with all of its intermediate arrays and that in turn is much faster than the python for loops so we've got this hierarchy of speeds so you you pick what's right for the application you're doing so now we're getting to the part that is going into the first exercise i'm getting ready for uh how thinking differently about programming problems uh uh you know thinking on loopy you know um and here here the the focus is on the way that uh this is gonna change your thinking you know the more the more you do this uh you know if programming language doesn't change the way you think then it's not worth learning that's what uh alan perel said um and this really does change the way you think if you are coming from an imperative uh imperative world so uh computing expression in which all the indices line up like the quadratic formula that's one thing but it really becomes powerful so i you know take these numbers and numbers minus numbers you'll get zeros for for every element it really becomes powerful uh when you are doing things that shift indices and then you do some math like cut off the first numbers cut off the last one and subtract them and this now now uh the top aligns with the shifted bottom and you can do differences between members of these numbers so the numbers were 1.1 2.2 3.3 4.4 if you do numbers bracket you know if you slice off the first subtract slice off the last you can find there between number differences 1.1 1.1 1.1 1.1 uh things like that so now let's get creative how about taking these these ones uh and uh add them with a slice okay now um as i could make a triangle by uh by slicing more and more off adding and adding them up you know these kinds of things um and you you'll be using some of these kinds of things to do the exercises uh real world application oh let's say that we've got some noisy data and uh i want to smooth it so i'm going to take the data make a copy and on the smooth thing here i'm going to do that triangle trick and each time you know add it to its shifted self divide by two that's taking a mean of each value with its neighbor let's do it again take a mean of each value with its neighbor and please do like re-evaluate this here the original and just slowly smooth that the data because each element with its neighbor just and just sort of smoothing out now i don't want to leave you with the impression that this is that this is a good way to do smoothing there are uh numpy stride tricks sliding window view which is super efficient and also it has a flat kernel whereas what i did didn't exactly have a flat kernel um but you know i didn't want to just jump to that because it's kind of like do sex machine at the it's like oh there's a function that does the thing for you but just think about it you take all of the elements and they're in their neighbor and just average them and you can slowly smooth out a thing all right so now we go to the exercise so click on the link to jump to the first exercise and what you'll be doing here is uh conway's game of life uh have okay so everybody everybody raise your hand so all the hands go up all right now uh uh lower your hand if you've heard of the game of life okay so game of life now i'm going to introduce it uh game of life is um an automaton it's a simulation um so uh it's it's describing like the cellular automaton like a petri dish kind of thing in which there are some living cells and dead cells and they're all arranged on a grid and the uh the live cells with fewer than two neighbors die as if by overpopulation under sorry under uh the ones with two or three thrive to survive to the next generation if there's more than three then it dies like overpopulation uh and so any dead cell with exactly three live neighbors becomes a lifestyle as if it was reproducing so this petri dish things reproduce die off from from being isolated or die off being from being crowded and so by being sort of in the middle of that you get these interesting patterns uh where things are able to spread but they don't just you know flatten all over everything um it looks like this if you run it for a while so you set up you know you've got this the cell ah this this grid of cells and in each cell is either true or false which will be uh black or white those pixels on or off um and uh and especially if you set up if you set up the initial conditions right uh not only will it spread in interesting ways but it can make these repeating like rocket ships and things and there have been there's a huge you know a lot of uh uh people have gone down a deep path of searching for interesting things in the game of life you know it's just those rules that i that i've read out but uh it's full of of uh interesting patterns so uh let's set up a board and the board is going to be numpy zeros with a given width and height so we're gonna make just a two dimensional array and you have to actually import numpy two-dimensional array and this particular pattern came from here because there are people who study these things all right so when we have this two-dimensional area we'll need a way to visualize it you could use matplotlib and do that if if that's what you're comfortable with and you just actually just pass this two-dimensional array to imshow and you can see it but what i'll be doing uh uh in the prep will be uh ascii art uh because this is very debuggable and you can make variations on this that make it easy even easier to debug which might be needing in the exercise so those rules that i described here's what it looks like in an imperative implementation and when you're working on the array oriented implementation you can look at the imperative as a as a way to you know get in mind uh uh what you're trying to achieve and it can help you know it's easier to read than the four rules i would say um so uh because it's imperative it'll have to be if this has to build up a list of lists uh it does it by two nested for loops uh and for each cell here we are at the the level of determining for one cell uh it has to count the number of living neighbors so in this box i'll scroll back up to the box this is what neighbors are they are eight because we're counting not just north south east and west but also north east south east northwest southwest and you count the neighbors and not the cell itself so eight cells not nine uh and you have to count up how many are living and that's if this uh uh is true if this uh cell value and then these are the rules you know uh if the number is between one and four you the output is a is a is a true um uh and then these are the other rules for which it will end up with true and then else false okay so that's the imperative solution and um in this implementation i made this wrap around uh and i'll leave this up so that you can read it uh while you're doing the exercise in case this point isn't clear but you know you've got a board and you were talking about neighbors so what about if you're at the edge of the board what do you do uh and uh the simplest thing to do is just have it wrap around like an old style video game you know your spaceship goes off the edge here and it comes off onto the edge of the screen here uh and uh uh that's how the imperative solution works so the you can use that as an example um but really uh i chose the initial conditions in a way that you shouldn't that you that you don't care until like the 200th until the 200th application so if it's too difficult dealing with the edges you don't have to so now you can see what your cell is what your implementation is doing by repeatedly evaluating this jupiter cell and you can see it doing the game of life and so i think when you're working on this exercise i'll probably be hearing a lot of okay do that work just to see how it goes okay and yeah so the exercise is to do this in array oriented routines so no for loops allowed uh no for loops over the array if you have other for loops that are not over the over the whole grid cell that's okay it's always just don't do the for loop over the array and also don't have implicit for loops like inside map filter sum or things in editor tools those will just sneak the the python foil up into it in a hidden way uh and one rough way to tell if you're following the the rules is that your result will be much faster than uh um than the imperative python that's a way to know that you're actually doing it all right so there are hints i'm not going to expand them now uh if this is easy and you want to keep on going there's more to do didn't i say there's more to do oh yeah if you think you're really clever do with sci-fi signal convolved 2d okay so this uh an easy and hard so go at it we'll see what time it is now and we'll do uh i think i had like where are we now 150 oh okay 10 minutes late so this will end up at uh uh it's right now two o'clock um let's go for 20 minutes on this so two so 2 20 and then we have a 15 minute break and what i'll do is i'll absorb it in the break so guys this uh we'll end the break at 225. there's three breaks just you know or before so there'll be enough uh and uh yeah so i'll see you at 2 25 and then we'll be all around all of us i'll present so yeah okay so i feel bad about interrupting because uh what i hear sounds like is really productive and you can keep you know looking at what you're doing but i'm going to be going over solutions now so yeah okay of course there is oh okay if this happens to you here's what you do start it up again here comes and if you have let your binder lapse now time to bring it back just like me and i'm going to go into the solutions directory see the answer oh and all of these if you're wondering about these files there's an unevaluated version and an evaluated version of each the evaluated is so you can see it on github without having to start a binder after the fact so we're going to go with the unevaluated ones so yeah so let's look at uh solutions to this problem um and this is uh and if you came across this there's this little warning message don't peek at this unless you tried to solve it and actually in all of this i'm not going to do any kind of policing of you know this is really for you if you decide that it's going to be more fruitful for you to look at the answers and that's what you do uh generally it wouldn't be generally it's better to go through it because i will be going through the answers with you that's scheduled in here i'm going to spend the next 20 minutes just looking at the at these solutions by the way actually i should start with uh any burning questions before uh before i you know sort of give an overview of how you go about thinking about this something came out of the session okay that's all right all right so i'm going to look at the the solution file and the first thing it has to do because a separate notebook we have to build up the infrastructure again so this makes a new world and we've got the show function all right now let's go with the solutions uh before uh talking about this first solution i'm going to introduce a new function because the the first way to do this is do sex machina and what i mean by that is this is oh there's a function that just does it for you after that i'm going to go into uh actually what if you didn't have that function you know how would you do it with slices well let's start with this um and it's i'm introducing the numpy role function uh to show what the role does see this cartoon eyeball okay this is a um we'd say 8-bit but it's actually more like a one bit eyeball you can roll with it with axis equals one to look to the left axis equals one going with a rolling plus one to sort of look to the right axis is zero to look up it's what it's doing is taking that two-dimensional array and it is moving everything over and wrapping around as necessary uh and it wraps all the way around so let's if we go up four then we look up so much that our eyeball comes up from the bottom uh this turns out is is just what we need [Music] because let's say that we have this one dimensional problem of these zeros and ones we want to count the number of neighbors that are one if we roll this this to the left you see all the values they've been shifted over and the zeros has wrapped around roll it to the right which i didn't mean to click twice it goes the other way and then we add them the rolled to the left and the rolled to the right added together is the number of neighbors that each one-dimensional cell has so now extend that to two dimensions you have eight neighbors so that's eight rolls so here the number of neighbors starts off as a zeroed array two dimensional array to do northwest you roll to one way and oh the other way so one axis on the other axis for each of those combinations the only thing that isn't here is rolling zero zero because you know you're not the center point is not its own neighbor if you do that then you'll get the number of neighbors uh and then when you have the number of neighbors um one of the hints was like once you've got the number of neighbors then just computing the number of survivors is doing some uh some logical operations uh you compute the number of births and and uh you the next cell will be one if there's a birth or it's a survivor so just this is applying the rules uh and these are the two ways that you can you can come out with one and so this would do a step in numpy and you can repeatedly evaluate this see the thing flow forward just like it did with the the python for loop case the main difference is that it's much faster now it might seem oh yeah so so this is uh 0.2 milliseconds and the other one is something rather milliseconds which i don't remember um now that might that that feels a little bit cheap if you've been spending this whole time trying to trying to find the right slices uh and oh there's a function that just does it sometimes there is just a function that does it and so it's worth looking through the the numpy documentation oh is there a magic function that does exactly what i need to do that's that's often that's a good first step um but no you didn't have to just guess role there are other ways to do it so um this first way that i'll do does not take into account the boundaries for at all uh and that's why it's useful the problem that i set up doesn't uh need boundaries to wrap around until you get to some really high number of iterations so let's look at a one-dimensional array again what we need to have happen is uh to take this array get some left neighbors get some right neighbors so that we can add them uh i showed the example of slicing where you could take off the first and take off the last and then and then that shifts them so that there's the same so that they line the numbers line up but it shifted i'll get that picture up again come on it's this picture this is what we need in order to do a roll a roll that doesn't take into account the boundary conditions so we can do those shifts add them together and get that uh and the edges are not going to be right but question marks here uh we have to make some choice we could have them be zero um and we can do that by making the left or right neighbors start as start as 0 and then assign in the the shifted so plus equals or the equivalent copy it and add in um so now the game of life if we were to do that these are all of those role shifts and these are the slices so now if we're trying to do this shift and add in two dimensions we use numpy two dimensional slices the ones that have commas so again the eight cases northwest north northeast west east southwest south southeast uh of different things to add up and then all the same logic from that point onward so really this is this is been a project about getting the number of neighbors so there's this step numpy 2 that does not handle the boundary conditions well and here what i'll do is i'll zip forward 200 steps and check that it gives the same answer as the as the first solution up to those 200 steps um this is before that that puffer fish kind of thing gets to the edge uh so it's the same result up to that point but then after that point here we've gotten this far we're going to run to the end and what would actually happen if we got the boundary conditions right is this would get to the left edge and come around on the right edge but instead because we've uh put a new condition on it that the edge uh just zero is full of zeros you know edge kills the cells it the pattern runs into it and goes away and now we've got something that only repeats it's a sort of uh like a an end state for this uh um for this game of life so um that used just slices uh and it was considerably faster it should come out yes considerably faster than numpy roll the numpy roll i think if i remember it was 200 millisecond microseconds and this is 80 microseconds um and that's because we're not doing the whole problem you know numpy roll is more uh is more general can you get the edges right anyway if you went to this trouble of doing slices and also getting the edges uh congratulations to you this is hard and i'm not going to go through it in detail but you you have to do quite a bit of work to get the edges right with slices that's why i you know was not making it a a criteria of this exercise to get the edges right and then then it'll run for a thousand steps and it'll still always be equal to uh to the full solution and then getting the edge conditions in right and doing all of that work it does pay off it's better than the roll i believe yes it is now um raise your hand did anyone try to use the scipy convolve method i would like to see this okay okay cool but did anyone get it uh solution yeah yeah all right cool um yes uh you can also do this with con uh with uh scipy convolve because this problem of counting neighbors is a convolution uh often with these kinds of problems uh they force you know this being a different uh programming paradigm it forces you to uh when i want to say think mathematically that's that's not true you're still thinking mathematically when you're doing things imperatively but it makes you it forces you to think about what other mathematical problems is this like that's not a thing you have to do with imperative solutions with these what other problem is it like it's actually just like a convolution problem if you choose the kernel appropriately uh we want to find out for each of these for each of these cells c how many neighbors it has this is a convolution with a kernel like this and if if those words are not familiar then never mind this is not the way you would go uh it's a convolution kernel like this where you're saying yes i want these direct neighbors no i don't want the thing itself and summing over it because that is the number of neighbors number of neighbors is the sum of all those you know places where it matched um so there is a way to do it to use scipy you get you get math cred granite did i not to somehow i forgot to do that ah interesting that i forgot that somehow you get math credit for doing that but it is not uh not actually faster some things in scipy in order to be fully general are not as fast as if you roll it yourself in numpy and if you really if you really cared about the speed of this you would probably be thinking about um number or jax and if you go number then you're probably writing an imperative solution anyway just in compiled for loops and if it's jax then you think hard about the array-oriented solution and uh and you can often uh uh you actually do win it actually it's it's compiler smart enough to really take advantage of that uh in preparing this tutorial i learned about just how smart jax's compiler is and i think if i'm doing this again following year i'll have a whole exercise on doing things in jacks okay so um any last questions about that before uh i just went over the the solutions to exercise one um last questions about that before we go on to completely new topics just raise your hand if you have a question so this is the thing to know about using number you can use some numpy array oriented code in number and you gain nothing for speed for doing that it becomes just like doing it outside of number you only really gain from number if you write for loops and then l vm knows how to how to optimize that but it's interesting that in jax you do gain from from uh being array oriented and in fact that's the only way to go and it's actually kind of hard to fit a solution into jacks which is why i would make good exercises for next year or something and then jax is also immutable so you can't change anything in place so it gets even more interesting okay so yeah let's uh uh let's talk about the next part and since i have uh i'm going to i'm going to shut down the the kernel associated with this notebook because i found that uh uh giving binder less things to think about um it works better all right now first part was about uh the wonders of ray array oriented programming and with every paradigm there are situations in which you want to use it in situations which you don't want to use it so now i'll be talking about its disadvantages and i'm going to evaluate up to the current cell just in case just in case there's anything in the beginning that i needed i don't know i don't think there is but all right so we start uh let's suppose that uh psi pi doesn't exist and you need to calculate the gamma function and okay this is some some weird function it's one of these special functions that mathematicians in the 19th century got you know excited about but now it's just one of these things you find in libraries you know scipy.special.gamma probably but if uh if it doesn't exist then we'll have to implement it ourselves so there's a book for that does anyone recognize this book just quick hands cool still around yeah uh i use my pascal version of this to work this out ah let's see so you find it on page 214 uh if you want to compute the log of this gamma function there it is there's an algorithm and in that book they have only scalar solutions because they assume that you're going to be writing in some compiled language first version of the book came out before tran then they went you know pascal's c this is a c plus plus version um but you know they only they only did compile languages for this so it's all imperative and their solution uh uh looks like this and by the miracle of duct typing the imperative solution is just like the vectorized solution so they said you're going to step through this function you're going to add you're going to multiply you're going to have a loop over these constants these are coefficients cough and you're going to you know do this thing to each one of them uh and just so happens that if instead of x being a scalar if x were an array all of those steps would still be valid everything would just go right there so we can write a vectorized array-oriented version of log of gamma where did that happen oh thank you thanks do that round restart kernel run it to selected i should have checked that thanks um and it'll eventually get to this uh you know what i'm gonna stop it from wherever it is and it'll be fine to go from here sure i don't have any definitions we need okay that's what the function should look like uh and i evaluated on this lin space this linear space from zero to ten you know ten thousand points uh evaluated at all of them just to see what the shape of that curve is uh excluding zero there that's that's the correct shape as you can see from pulling out scipy and yeah it has an implementation of the same function and they overlap so we're good um all right so score one for array oriented programming we just uh didn't have to change a thing in that function to to get it to work in a oriented way uh what am i saying about the list comprehension i gave it a okay so first yeah i gave it a list comprehension so we do it over scalars and then we're going to run it vectorized so i got ahead of my own notebook yeah first did it with a python for loop and then i did it array oriented and of course it's the same yeah so it automatically worked because every one of these variables could be erased just as easily as they could be scalars as long as the shapes match and or are broadcastable all right now that's one special function here's another special function and uh on page 219 of the book um here's how you can complete compute and incomplete gamma um stepping through this function and what they say you should do is you should iterate no more than a hundred times recompute this value and then when uh some delta gets small enough then the thing has converged and you can return from your function at that time and if you get through all those 100 iterations and it hasn't and delta never got small enough then you know you got into some some place where this function doesn't converge that's uh fine for imperative programming um over these ranges that we're looking at it does converge and you can get you know by doing these list comprehensions and passing each value into incomplete gametp one at a time you can draw the curves this is actually a figure from the book uh and now if you uh run the special function yeah it's the same okay but this is the crux of what is difficult in array oriented programming this iterate until converged thing or iterate until diverged but anyway iteratively apply some solution to some some task until uh a condition is met and at that point stop that's a thing that's very hard to express well if at all in oriented programming so let's say that we just take an array xs plural of x x's and pass that array into the function it complains who all now let's do another round of hands who all has seen the truth value of an array with one more than one elements ambiguous yeah yeah so you're a numpy programmer so you've seen this this will happen like 10 times a day uh and it's because here the result of that comparison and multiplication is an array if array is a thing that uh um numpy authors did not want to be misinterpreted uh um so they just have it fail that's actually good because the bugs you would get if if it didn't just fail it would be doing something would really not be what you intended um yeah and so the if statement can't be evaluated in a ray-oriented way because we can't have some values of the array enter the if block and other values of the array not enter the if block [Music] and actually if you get into writing jack's code and trying to jit compile a jax function this is the the bane of jack's code two it's the same problem it's array oriented you can't uh express if statements this way you have to do numpy.ware or something so um if you have an iterate until conversion problem what do you do uh here are here's here's a branch point suppose we have all the values continue to iterate even if they've already converged just let it keep computing needlessly or keep track of which values can have converged already and keep computing those but not the other ones this one is definitely easier because uh here we just say do the log of gamma and here this is the uh iterate 100 times and even if this thing even if delta has become a very small number keep adding it in it's not going to hurt that you're adding zeros a whole bunch of times and it works and this is a general thing in an array oriented program sometimes you prefer to do to make the computer do extra work now keep track of which ones have converged and don't compute those that means that you have to see okay make an array of is converged or sorry is not converged and at first those are all true and then in this loop uh we compute the delta and the summation for the ones that have not converged and that's a mask if you've not con converged you need to keep computing those so put the mask here the array of booleans inside the brackets is fancy indexing put it here put it here on all of the arrays and then the not converged you and in uh um when something has converged you and it in so that the ones can go to zeros uh and then as you keep doing that you might get to the point where they've all converged and then these three steps are like you know basically just sort of skipping over the whole array each time and you can do that it does work so which one's faster right the bottom line as it turns out the one that was in this case the one that was easier is faster so keep that in mind uh you know doing less work and not trying to keep the bookkeeping you might actually be doing yourself a favor in both the maintainability sense and also the performance sense now in this problem if this had been iterate a thousand or a million times then maybe actually bookkeeping would have helped so it really comes down to a problem-by-problem basis so exercise two we check where we are we are still good yeah we're still good all right the iterate until converged problem is going to be descending a decision tree so first of all decision trees um well what i'll do is again everybody raise your hand so all hands go up uh and put their hand down if you've heard of a decision tree okay um we'll be introducing just enough and so this is good because we'll be working with them and actually what mostly matters here is the tree just walking down a tree so uh to make a decision tree i will just use psychic learn to i'll give it a hard pro hardish problem there's these points to uh to classify this is a um they already have colors and what we're looking the points of colors and we need to find a machine that will for given an xy point to predict whether the the dot there is going to be orange or blue so we asked scipy to do that and it makes us using a decision tree classifier makes a decision tree and this is what the decision tree looks like for every xy point it will say you know yellow or blue and it has optimized this tree uh of of choices that it needs to make which i'll show in a moment it's just a a flow chart it'll optimize that flowchart so that what it's predicting for blue will usually be blue so um and so actually uh for what we'd be doing what what's important is not whether or not it's a good fit actually if you're into machine learning you'll see that this is is way over fit this points here where it's trying to it's making decisions based on individual data points but really i just wanted a tree uh and this will draw the tree yeah this is the tree that we got this is the tree that made the diagram above uh it is a flow chart and i know at some point i have a diagram of a flow chart but each one of these nodes is saying you know uh um is if x is greater than something if y is less than something uh go left or right and it just goes left or right until it gets to uh a decision at the at the leaf of the tree so that's a tree uh in computer science classes uh trees are usually introduced as a data structure like this you have some sort of a class that has you know members of some of its members are the same class so you just have nodes that have nodes under them and the decision tree has a method for walking from one of these so you start at the root of the tree this node and then based on some attributes that are attached to the node like if x is less than five go to the go to the left and if it's not go to the right and it just does that it just recursively walks down this tree that's what this function is and here's an example of a tree just to give an example small tree all right so um and then you can call the predict method at different x y points and i'll say orange or blue because it walks down and that's a flow chart so uh but it was a automatically generated flowchart that's that's what uh that machine learning model is all about and actually if i was ever like teaching a machine learning tutorial i would start with with decision trees because they're just the simplest you know you can see all the moving parts exactly what it does all right but this decision tree because uh scikit-learn is a professional library they want to make things go fast in this numpy world uh all of the attributes of the tree that they have are actually arrays it does not have in it the the class node that i described it has just several arrays and this array is showing all of the nodes of the tree some of these values are are minus two because um because this is a node that doesn't have any children you know it has some of these a leaf nodes some of these are interior nodes but this is how it stores it internally it has you know an array called children left and an array called children right um okay so how this works how it is you can you can express a tree using arrays is that these arrays of integers are indexes of where to go next so between this children left and children right the root is the first element and going left means go go from index 0 to index 1. going right means go from index 0 to index 136. so another cultural reference you guys does anybody know about choose your own adventure books because i've never even played you know with us and they say go to you know because it's a it's a text based adventure game in a book format and it says if you want to open the green door you know go to this page if you want to open the red door go to that page and say you flip to that page and keep your finger in the previous one in case you decide you want to go back and the this is exactly that instead of page numbers these are index positions so it represents the whole tree in just a few arrays okay so these are pointers that act as it as indexes and how you use pointers in numpy is put them in square brackets if you put an index and square brackets that will take you to that point so for instance if you start at index 0 and repeatedly take the children right array and put that index in there and whatever it returns is your next index if you repeatedly evaluate that that walks down the right so 0 136 138 1 148 that is starting here and walking right right right and then reaches a leaf so this is just this is just getting you familiar with what these objects are that you'll be working with you know if you want to do this iteration this thing where you're going to go left right left right left right i just wrote some code to go left right left right left right and that's just walking down that tree in an arbitrary pattern that i chose to to show that you can just walk down the tree by repeatedly uh applying the array and so array application like put those integers in square brackets means tree traversal okay so to check this interpretation let's write a recursive tree walking function and compare it to scikit-learn's output and i and this tree walking function prints the tree so here's my printed tree and psychic learns printed tree and you know just scanning down this you can see that yes this is how you do it just each of these nodes reading whether it's it's going left or right depending on the x value or the y value what the threshold is these are all just arrays that were pulled out of the thing and so that's the tree in ascii so now we can write the predict function using uh using these arrays if we're at some position uh oh sorry this this is an x y point position on the on the two dimensional grid we want to know if that position is orange or blue start the array index at zero and ask does it have children if it has children which feature is it is it x or is it y what's the threshold just pulling these things out of arrays what's the left index and the right index if the position for future x or future y is less than or equal to threshold we're going to go left and by go left i mean recurse this function otherwise recurs right and if you don't have any children then you're at the end and you just grab the score this is going to be just grabbing the score this is you're going to be you you're given this this is uh the imperative function you'll be writing an array oriented function to do this thing uh to see that it works we draw that diagram again the the plot of points and it takes a long time because it is a slow pure python function there we go notice the lag you know that's because it's a slow version uh it is basically every time you do it like walking down the tree one at a time you know one call of the function recursing all the way down to the bottom is going to find out where one point on this xy plane is what we want to do is something that looks like this we've got this prepared data of all the xy points and we want it to look kind of like you're doing this you know sending all of them down and all the points on the plane all the xy points are all traversing the tree together just as this is just a mental image okay uh we end up and this is a quick reminder that you can slice an array by an array of integers so if you're unfamiliar with this as soon as the the the prod the exercise starts so raise your hand and and we'll talk but this is the fancy indexing of integers so you put a big array of integers inside the square brackets okay uh note that the tree has a maximum depth it's 10 that'll help and um yeah go at it you know find a solution there's a suggestion to break it down into two parts one is a predict and the uh to walk separately from predicting um and there are hints so uh yeah good get started and uh raise your hand if you have a question about this point of array indexing okay again just talk with your neighbors groups of two and three and we're gonna go let's see how did i do 306 okay so it's 340. anyone um actually at this point i'll be going over the uh the solutions now um at this point does anyone have like um a question or a comment that you know about this overall something i want to shout out or raise your hand or oh okay so uh uh i'll start into it and um go into the solutions directory again number two unevaluated all right and then first we'll get back up to the point in this notebook where we're ready to go recreate the decision tree predict single and then actually making the plot remember how slow this was with python again a few different solutions and actually if the solutions look you know they look big with lots of stuff in them it's actually because uh there are several ways to do this you know several valid ways to do this and uh and i want to like touch on them all depending on the different divergent ways that you might have uh you might have gone about it so with this there is a a purely immutable way to do it and then a one a way that changes the arrays in place so that's what i mean by immutable the first solution uh leaves the arrays as is the arrays of x and y data and uh um well all of the arrays once created are not changed after they're created that's something that um uh if you are looking into doing some some work in jacks you have to do that that is the way that uh jax works so let's start with the purely immutable uh not allowed to change an array after making it um okay so we start with uh these positions and actually let's let's look at them i think maybe this these positions are xy points this is a set of just three of them because a small number of things is good for debugging um we can pass uh so start with the the positions is a positions are a given set these are all the xy positions in the plane uh here those here we're just going to simulate this with just three of them the array indices we start with all zeros because at first they're all at the the root of the tree and the root is at array index zero and then what we want to do is once you pass them all through the tree and then they all become a different indices uh and let's do the steps one by one before wrapping it up in a into a uh into a function so some one of the things we'll need to know is at each point in the decision tree should we be choosing x or should we be choosing y that's the feature so we can apply them all to that array by putting the array of indexes 0 0 0 into square brackets right after feature and so now we're finding the the feature associated with zero zero zero and it really should be the same number three times what are the thresholds for zero zero zero well it's the same number three times the children if you were to go left the children if you were to go right yeah as long as we're putting all of the array indices that we put into it are the same then the then the array values that come out should be the same this is like actually there's there's a real very real sense in which an array is a function it's a function from index that you put in square brackets to the value at that cell and if you really want to get into it passing a bunch of indices like this is like function composition but i'm not going to go there all right so uh right so first they're all the same because they're all at the root um if we were to just ad hoc in order for for some testing decide that the first should go left and the other two should go right what we would be doing is we'd be changing the array indices from being 0 0 0 to 1 136 136 because left is 1 and right is 136. so let's just ad hoc do that now and if you pass that into the feature you get different things for each value pass that into thresholds you get different things and the lefts and the rights so we're getting different results for different array indices um array wise uh the ones that have children are the ones for which children left is greater than or equal to zero so it has a place to go uh this um this array is set up in a way that there are a lot of different indications that you're at a leaf one of them is that children left is negative you know also children write is negative and the feature is different uh sorry yeah um most of the arrays give you some kind of indication we just choose one um so uh the ones that we'll be choosing left are the ones for which uh are the ones for which when you evaluate the the the positions for all of them and choose the right features uh they are element for element less than thresholds those are the ones those are all the ones that go left okay so so far so good just i see some nodding so okay um so this expression uh this actually was the first hint um this is a two dimensional slice in which we take every member of the first dimension in order so this is a bit like having a colon here except we need it to be a fancy index we take everything along the first dimension that's why this is an a range and we'll either take feature zero or feature one of the second dimension the second dimension is the x and y's of our of our input data um there's a way to do this uh in smaller and smaller steps i i'll try to do that i think that that would probably be a good idea so if these are the input data these positions point one one point one point two two point two point three three point three the a range is just counting numbers let's say the features are x y x y x y zero one zero one zero one um here i want a point so i'll take this with me um where are we where's the output of that output cell i didn't evaluate again oh i see this is the is what is the output this is the output this is the output okay so um in each of these we're taking uh we're just walking through through each one of these these pairs point one one point one point two two point two we're taking every from from the first dimension we take each one of these we'll take one of the pairs in each step and depending on whether the second dimension is 0 1 we'll take either the the left of the pair or the right of the pair and so that's why the output is 0.1 2.2 0.3 4.4 0.5 so making these kind of these little um some these uh test cases can help uh you design the the slices that you want to make um and like really it's it's perfectly good that uh to be putting that you know that much thought into a one-liner this is fine um you know if you are used to imperative programming you just you know would normally be trying to blaze through all this but this is really more this is concentrated code uh i see and then these these cells are another way to do it um take a bunch of truths in the first dimension true true true we're going to take every pair and from each pair we'll either take the left or the right so there are different ways of uh uh setting up this slice this was the hint one by the way um so now we know which indices will be choosing left um we need an updated array indices uh to select uh to know whether to be selecting from the left indices or the right indices so here let's say that we have this choosing left true false true false true false so like again i'm making synthetic examples that are easy to see in the output uh what the slice did so if i have these left indices and these write indices which i've deliberately made very easy to read and different from each other the numpy where function we'll take trues and falses here and based on whether it's true or false either pick the left or pick the right and this this is the immutable solution because it leaves the original array index array indices array as it was and it makes a new array um uh and np.where is good for that so actually if you're doing jax programming you'll be doing a lot of np-wares instead of if statements returning all your statements into numpy where um we're not done because some of the nodes don't have children this is the iterate until converged problem you know on all of these data points going into the tree some of them will have reached parts of the tree that uh that don't have any children so we have to do something different with them we can't send them to the negative indices uh the negative numbers that mean you're at a leaf so um we want something where we can keep computing something for them but not have them go anywhere because they're already where they want to be we want them we want to compute should i go left or should i go right and the answer to that is yes you should stay right here right we want to construct construct code that would do that for them so if they have children something we can determine by seeing if if left is negative we can put the numpy where that we just wrote inside a numpy. where if it has children go to left or right child if it doesn't stay where you are assign x you know the next x is going to be x uh this could have been two lines actually you could have taken this numpy this inner numpy where which again i'm assuming oh you can see the selection you can't see the selection that's good the inner numpy way that i've selected uh you could have put that in a temporary variable that would all be the same so putting all these pieces together here is a function that doesn't traverse the tree it predicts the next step down the tree you're at some node and this function will tell you all of your inputs are all at different nodes at one depth and this function will predict where all those nodes are going to be at the next depth and does all the things it just wraps up in a function all the things i just talked about uh applying it to some synthetic example small three element thing uh this one predicts they should all go left and keep hitting it and they all walk until look more iterations are not changing it they're all doing that x goes to x thing so i'll do it again just for you start off like that they all want to go uh one is maybe yeah one is the left and then they all go in different places down the tree until some of them reach leaves before others but eventually they all reach leaves so we want a function that that iteratively applies this predict many step how many times we don't have to do it more than the the depth of the tree because then you know when you get to 10 everything is converged because there's no more tree left so predict many wraps all this up makes your initial array indices that are zeros iteratively applies this predict next step and then at the end of that it does the same leaf decisions that the uh that the pure python did because when you've gotten to leaves then you just have to read off what the decisions are and so that's the same code so in one step that goes to the solution um [Music] and now we can make a plot with this predict many and that um that plot appeared a lot faster than the python plot um so uh in the the things that you're working on um how many people were trying to write something that did not change the array in place few okay how many people were working on something that did change the array in place that doesn't add up to everybody and really i can't think of any other possibilities um but okay uh so there is a solution for doing the many in place here it has a lot of the same pieces but instead of the where instead of the where clause they would say um uh an array of booleans of whether you're going left or right and then moving left as the second argument and moving right as the third argument instead of that numpy where you can just take your indices and actually change some of them select out the ones that we'll be choosing left uh and set them to choosing left uh that doesn't look very different you know it's uh it's really just that numpy where uh the rest of this is all the indexing to get to get to this part and then it works the same way and we can make the same plot um so from here uh the the rest of this solutions is uh interested in performance comparisons because you know do things in place they're often a bit faster uh um question is uh how were how interest oh and then also went into it also goes into doing this imperatively with number how does it compare to that doing with jax um so uh that is uh i think what i'll do is i'll not get deep into the details i'm just going to run them all and then while they're running because we get bottom line results let that all go and just talk about the different ways of doing different ways of doing this and then we'll see for this particular problem what the results are and in fact the single tree to find a time python function that is so slow i'm just going to stop it and have it only go for one run them all like that okay so uh we have the original python uh iterative solution we have uh numpy changing things in place numpy not changing things in place other possibilities um we can do the single tree traversal you might say okay well the numpy was was fast because it's doing this in compiled for loops what if we take number and do the uh um the imperative walk down a tree and because number will compile this this would be like if we wrote this this function in c um and it also has the advantage over numpy that is not making intermediate arrays what we'll see actually at the end of this is that is that that is in fact slower this is a case where uh even doing something imperative compiled like it's a c program this recursive walk individually for each data input is slower than walking all of the inputs through the tree at the same time so here's something where where are you program is really really paying off for performance as well as uh for conciseness um and there's an array-oriented traversal which we can compiled because we can take our array-oriented solution as an array-oriented solution not turning it into for loops and compile it with jax which um yep it's the same two functions uh just uh doing a jaxa jit um and now this we let jax know we're giving jax more information about this about the fact that they're array oriented by not writing out the for loop and saying you must do this step before that step we give the compiler more freedom to to make optimizations uh and it turns out that that is very fast so the final results the you know drum roll uh in a log scale plot the pure python of course is always going to be on the far end uh recursively walking each tree in compiled numbers so this here this first bar difference that jump of well over a factor of 10 is going from uncompiled code to compiled code yeah we expect it to be a big jump from that but then going from even this compiled iterative solution to uh where it's it's taking you know the single plinko the single thing going down the tree and then another point going down the tree and then another point going down the tree if you just change that to having all of the points going down the tree all at the same time we get another big speed up bigger in fact than the difference between immutably replacing arrays versus changing the arrays in place so changing the arrays in place doesn't actually bias a whole lot here sometimes it does compiling that in jax is able to take that array-oriented solution and make it even a little bit uh trim it down a little bit more um by uh you know compiling up all of the operations into a single pass and then uh psychic learns implementation is faster than that because they spent a lot of time optimizing that actually if i run this on a different machine from binder the jax is pretty close to the scikit-learn it depends on what hardware you run this on well given it's compiled it goes into c code but given this this difference between this compiled iterative uh number with a recursion versus not recursing but doing it array oriented through uh i think that there's that their implementation is some is on arrays after all they represented the thing as a raise so oh that is it often is yes often number is as fast as c actually yeah very next thing yes very next thing i'll be going into that yeah so yeah that's a among the um the tools at your disposal number is a it compiles a subset of python to something that runs as fast as c uh and then the um the key part of that it is a subset of python you know it has to strongly type just like c strongly types 404 yeah all right so um i'll talk about that here on uh run this one on google collab and you can do this too if you have a google co-lab account because this one takes another iterate until converged problem and runs it on all the accelerators so we're going to use number we're going to use jax you know this array oriented solution there's imperative solution cpu gpu that's the reason for google code lab because you can run things on the gpu um and i had decided uh just a moment ago that i will just show this to you but i'm not going to run it um it has the final results at the end um uh but uh here's on a platform that can run gpus so you could do it um mandelbroid it's not like walking down a tree uh it's the uh it is a it's the thing that makes these plots okay it's the thing that makes these pictures that were you know calendars and such and it does that by this is a complex plane the horizontal axis is the real axis the vertical axis is imaginary and if imaginary numbers complex plane if that's you do well it's some calculation that'll be enough um and for each one of those points on the on the plane z it applies the function z gets replaced by z squared plus c let's see is it is uh uh that's the point where it is on the complex plane at first z is equal to c and then and then you iteratively apply this and in doing that uh some of these points the values diverge and some of the points they don't diverge that's the difference between the yellow and the blue um when they diverge uh the iteration number at which they diverge is what we color this so if they diverge right right away uh it's deep blue if uh if it takes two steps it's this if it takes three steps to that that's why actually you can see lines here because this integer number of steps before it diverges so that's the algorithm and this is uh this is how you compute it in python double for loop over all of the pixels and then for each pixel you just keep applying that you know at least 20 times and if it's if the absolute value of it seems like it's going away like it gets bigger than two then uh then you call it divergent and you say that point this is what color we're gonna make it and you get out of that loop onto the next pixel that's very imperative because you only iterate until you see oh this is clearly diverging then you move on to the next pixel that's the thing that's hard to do in array into programming um and you can make that picture this picture uh is low resolution and slow all the other pictures will be high resolution and fast because this is done in pure python so we don't want to do as many pixels it just takes too long um okay uh we'll see a summary of all those numbers at the end the array oriented solution now we have this problem again of uh like what happens when we get to a leaf of a tree what happens when we you know need um what was it the the bessel function or whatever that special function was you know to see if some computation is converged here we want to know what when does this pixel diverge um and you have to keep track of which ones have diverged and only you know apply the calculation to those um actually if you keep applying it and let it diverge very very far it gets to infinity and then down so you know we want to clamp this so this is a reason why we wanted to do some bookkeeping so it has to be uh rearranged to do that but then you could make the same plot uh now we go to town with every possible way of doing this suppose that we write it out as a c plus plus function um compile that as a python module and run it and make the plot so now we do the the imperative solution in c plus you can see why i didn't want to just evaluate this i think we should just skim through cython instead of pi bind 11 okay well this is again the c plus plus number here we get to write our original python for loops but it is as fast as any compiled solution so it's answering your question um if you do something in here that you cannot compile you get a lot of error messages from number that's the price uh number vectorize uh just a different way of using number kupay okay coupe i is a library that has the same interface as numpy but all the arrays live on a gpu so gpus are especially good for vectorized solutions which array oriented is vectorized uh and here oh yes here i'm i'm addressing the the thing in the in the chat on slack uh i use the word array oriented when we're talking about just a programming style and the word vectorized for the hardware speed up that this programming style can take advantage of and people who work in hardware vectorization bristle if you if you don't use the word that way uh so we just take our numpy solution put it on kupay and now it runs on a gpu so gpu made this plot uh coupe i with a custom kernel so now at each pixel we write an imperative solution in c plus actually cuda ah number has a cuda back end you can write python run it on a gpu i hope this isn't getting monotonous we'll get to the plot at the end and then jax uh you take the array oriented solution and you flip a switch you say run this on a cpu run this on a gpu and the final results these results were not these results were run on um uh like a significant computer not on google colab if i re-evaluated this thing the the bars would move a little bit um but uh but not enormously there's python you get some orders of magnitude speed up from just using numpy oh by the way why are we looking at the spot this gives you sort of a sense in the back of your mind you know what should i be reaching for like you know sometimes you'll hear people say oh numpy is fast because it's much faster than python sometimes you'll hear people say numpy is slow because it's so much slower due to the intermediate arrays it's because well on a log plot it's between those two so go from pure python to this array oriented numpy you can gain a lot uh and then just compile the loop in uh pure c plus plus with pi bind 11. the c plus plus python hybrid that that is cython number a compiled subset of of python number vectorized just a different way of using it all of these are all pretty much the same because they were really doing the same thing they were doing this imperative solution that i wrote out and then just written in different languages but they all compiled to the same machine code that's why they're all at same level jax on a cpu is a bit faster and and i never quite found out where they're getting that extra bit of speed actually it was quite a lot of performance tuning to try to figure out how to get to the level where jaxa just was right away and this was the day that i i became excited about jax um kupai just putting it on a gpu is like the difference of going from this numpy solution because it is the same code as this numpy solution just on a gpu it's from there to there and it's just taking it from a cpu and putting it on a gpu and then you're still you're making these intermediate arrays which is a problem so you write a custom kernel in koopa and you gain a lot writing number cuda is writing a customized kernel and then the thing that you get out of the box with with jax is just that you just put the same code you would write uh in numpy into jax and you're there so this kind of uh this helped inform my mental map of of how all these different accelerators can help and i hope that this is useful for you too because you know this is how we should be thinking about these things um uh the speed up versus effort some of these things are easy some of these things are hard um i can tell you that cython was especially hard it was very hard to get uh get to this point with cython i think you can look into that in more detail if you want this this link exists okay uh i think now oops yeah well of course go back now we're on to x beginning of exercise three perfect wow my timing is so good didn't think it would just work out like that okay i do have to restart jupiter though finder i mean okay any questions about that while this is any questions anyway right well some of those in that bar chart some of them were all the same cpu and some of them were all the same gpu so actually so there's only two there were only two uh hardwares on that plot uh it's the single pass thing yes the thing the thing that um uh just applying coupe i just taking the the plain numpy code and and turning all the the np to cp turning numpy to coupe i uh that that first speed up was just the fact that you went from cpu to gpu but then it was making all those intermediate arrays that are you know the the problem for when you really want to optimize when you really want to get fast uh coupe i was making these large intermediate arrays and wasn't able to fuse those operations meaning you know you've got several steps in your mathematical function and you want to do all those steps before moving on to the next right that that would be the most efficient way to walk through it writing the custom kernel applied all those steps before moving on to the next pixel yeah and then jack's compiler did that and that's why jax got there without having to write a custom kernel and that's and that's that's very specifically what jax is for it was the the purpose of it is for fusing the reason that jax has a jit compiler is for fusing operations that's the stated reason for being okay uh now we're on to weird arrays this last section will be [Music] actually i don't think i need to do that i think i just need to import numpy okay up till now all of the arrays that we've been dealing with have been rectangular in fact all the arrays that people usually ever call arrays are rectangular you know they'll be uh maybe you call them tensors if they have a lot of uh many dimensions but they are they're rectangles in all of those dimensions now we're going to be looking at some arrays that aren't rectangles in all dimensions so other data structures what if we had variable length lists what if this was our array you know um here we have some element uh some some record like thing that has two fields x and y the x field uh has a number in it the y field has a list and there's a variable number of these the first array element uh has three of them the second has none and the third has two you know what if you have data like that and i've written the data like json because what we normally do when we have data like this is we you know express it as json it's the most handy format for doing that especially across language but jason is um text is ascii text we want something binary so that we can run machine code on it immediately and without deserializing and we want these to not be objects like python objects you know a variable type we want something of some fixed types we'll be able to run compiled operations on it quickly so uh there are solutions for that now this and this all is very recent we're talking about you know pretty much actually absolutely everything that i'm talking about here uh i don't think there no with lots of ex extensive searching did not exist you know there was nothing like this before before arrow um so let's uh let's see this let's take this array and load it as a pi arrow array or arrow is a project that has many different language back-ends but python is one of them it presents the data in a rather complicated looking way um but you can uh you can convert that back into a python list and see that the list is there inside this object that has child types and is valid all not null and all that um so this this is kind of a pyro is designed arrow actually it's designed as a low-level library used by other things it's an infrastructure and we can do some numpy like operations on it suppose that we take this this array and slice off the first entry and then we'll turn that into a list so that something we can look at now before we had three records in the first zero in the second and two in the in the last now we have zero records here and two records here we've sliced off the first one using this numpy like syntax or if you want to do an array slice you know put an array in square brackets and and pick arbitrarily from these things uh it's not with the square bracket syntax as a take function but you can do that so here's the first list that has three records in it and then the third list that has two records in it and the one with zero records it's not here anymore internally the data struct the data structure is a collection of densely packed buffers so you can ask what they look like but it's not really going to tell you it's going to say yeah some of these places where there ought to be buffers there are buffers and others there don't need to be we can examine each one of these buffers by turning it into an umpire array and see that the that the data are in it and the data are concatenated so these these values 1.1 2.2 3.3 4.4 all belong to the x field of different records but the way it stores it internally is it puts them all together so it makes it array-like this data that didn't look array-like and um some of this some of these are integer arrays in order to make that structure like the variable length lists uh the primary focus of the arrow project is to share memory between languages and between applications so that all these different applications spark pandas drill and paulo you know all you know parquet file format um you wouldn't have to convert between each of these n and the other n minus one of them you would have one in memory representation that they can all talk to uh and they wanted this uh common representation to be something that allows her fast meaning vectorized cpu and gpu cpu cache friendly and gpu friendly algorithms so these are all taken from their website these are these images um it however it's not really a direct user interface uh from what you saw you know it's showing these it's really a low level thing so uh awkward array is a library for looking at that sort of data in a data analysis it uh presents a generalization of the numpy functions so whereas you might do a slice and numpy that would you know cut things a certain way if you do that same slice on an awkward array it has uh it it does the the irregular version of that that slice so i'll be showing examples from uh from a version two that's um uh right and i'll restart this and be talking about it well this is a library that is in production and many of the users are all on uh version one um but we are and oh i'm involved i'm the uh project uh lead for this and so i wanted to find a way to to say that uh uh and we are we've developed a ver a complete version two but it's not going to take the place until people are ready to transition so uh and i'm only going to show new stuff here i'm going to do that uh so version 2 is a sub module within version one so that's on why i'm importing it in a strange way uh-huh get here there and that should get us back okay so pull the awkward array from arrow and the representation is uh not deep internals but showing a little bit of the data but you know this data could be very large and structured so it's it's a compact form but you can expand it out and the data type becomes a very interesting thing normally arrays have a shape and a d type you know a shape for what kind of rectangle is it and how many dimensions and the d type is what kind of data are they floating point are the integers or the booleans now with this more generalized kind of arrays the data type becomes a bit more complex these are it's an array of three variable length lists that could be missing or not missing could be null or not null and they have an x field that could be missing or a floating point and a y field that could be missing or a list of possibly missing integers the many the a lot of the the missing types come from arrow where everything is potentially missing and just as with arrow you can peek at the internals how is it laid out in memory here we can see that the 1.1 2.2 3.3.4.4 5.5 is all contiguous and so when we run numpy like functions on them we can run compiled functions on those these contiguous buffers but then all of this the stuff is orchestrated in python so it can give you a view of it that looks like you know plain old data okay so uh the use of this is like numpy but for data structures so say we have an awkward array it's that one and then we slice the x fields out so just like slicing but now we're going to slice a field instead of some some values so these are all the x's we could have sliced the y's which had lists within lists so there's a generalization of slicing although numpy has structured arrays that slice like these also so in every case when it when numpy can do the same sort of thing we have exactly the same syntax uh slicing multiple dimensions at once like the field and also you know a fancy index of uh you know slicing with a boolean to take this one don't take that one take this other one and then doing some uh mathematical operation like a square root just across everything it's uh you func like um sorry about the figure uh reductions um get interesting because now if you are reducing along this dimension some values might not exist in some of your your lists so um so we had to you know define out what it means to reduce along uh variable length lists but the definition makes sense um we can take this and slice them this way 1.1 plus 4.4 2.2 plus 5.5 which is 3.3 or the other way 1.1 plus 2.2 plus 3.3 is 6.6 there's nothing in there so it's zero 4.4 plus 5.5 slicing the y and then this has a third dimension to to uh to reduce over so some fields can can reduce over different more fields than other fields um broadcasting will be used extensively in exercise so i want to bring it up here say like just like numpy broadcasting if you have a scalar 100 plus everything in this triply nested list uh it will add 100 to everything in the tripoli nested list broadcasting just like numpy here's a question or x is two levels of nesting y is three levels of nesting so that extra level of nesting has to get spread out each individually and actually when you start using these in in like a data analysis it it fits together makes sense uh and then you get into operations you just don't wouldn't even have in numpy like finding various combinations of these and that's you know that's how this is being applied is for combinatorics where you have different numbers of things at different places i won't get into that yeah we won't be needing these for the exercise okay so as a comparison with pure python you can have these kinds of this kind of data in pure python uh but if you wanted to do an operation like this slice the array and then at the innermost index slice just the slice off the first element of every one of those lists uh doing that in python is a lot more verbose you know it's for loops and for loops just like with numpy um and it scales better to big lists because it's using compiled routines so now i have what is this 100 000 um this is the whole memory layout just like before this memory layout looks about as big as the one before and the the bigness is actually in the dot dot dot of that array um so we have the same number of arrays the same number of buffers they're just larger so the uh um yeah so because of that it's uh oops the uh implementations can be uh precompiled and fast uh next is the exercise and maybe i'll wait for this yeah doing this over the the python lists a second doing this over the awkward way is uh 30 milliseconds so it's just like the numpy speed up okay that's the end that will of the main narrative and we'll go to the uh the exercise this exercise is uh exploring a data set so the previous exercise has been really about programming and this one is about data analysis suppose that you're given some data that is awkward you know you're given some data that you know has a difficult to reconcile structure um geographical data has this quite a lot um you know path lengths all have different length you know every like path has a different length so this will be uh an exercise in um uh analyzing some data from a collection of taxi trips and it is a big data set it's all the taxi troops in chicago over a three year period actually the last three years so actually that is interesting as well but we're not studying the the volume change over the past two years um and the data have three levels of nesting there are taxis each of the taxis takes a different number of trips within each of those trips uh each of them has a different length path you know some of these these chips that go around you know some complicated route and that takes a lot of points to describe and some of them are really short so we can read the data set from a parquet file and parquet files are especially good for this kind of thing so this is like json structured data um but it's a binary and it's columnar and being columnar um we can uh pull out just some of the data so here uh i read uh um i think this is about a hundredth of the data the the first hundred taxis no the first hundredth of taxes the first one percent of taxes uh but for all of the fields in all of the records and generally in a data analysis you're not going to need all the fields you know there's some things that uh in this this has payment uh the different how much the trip cost and the analysis won't be overpayment so we can from a parquet file not read that part of the data that's an option we wouldn't have if it was json for json you have to read it at all so these are the columns in it and they have different lengths because you know some of them are paths um and they are called columns we can look at could start by looking at the the type of the data this is more complex than the one we saw before these taxis uh they each taxi is a variable number of trips each trip has it took some time some number of seconds went over some distance kilometers it had a beginning and ending latitude longitude and time had a path which is a nested list of latitude longitude points along it and what we'll be seeing is that the uh for brevity the the for for the file size the latitude longitude points are differences with respect to the starting point so it says start at zero and then increase and then you can make them lower resolution and then the payment stuff that we don't care about um so i believe that we could just jump to um we i think that it's self-explanatory to say that we can pick out some columns you know say we just want this that and the other and only read those and the type of this is now simpler because we read less so this is like this is just to start with a demonstration of what you gain when you have this columnar data and it's not json you know we can read just just the fields that we're interested in that's this point here that's possible to oh yeah what is the total list of columns before you read anything that's there for uh if you want to be fiddling around with this later okay so now um let's start we've read in the data let's explore the data because this is going to be a data analysis you start with just exploring to see what's there so let's take taxi number 75 and what is all the information about it um and this to list we're showing it as python lists so i first selected something that's small so we're not going to kill the browser with lots and lots of output lots of python output just pick one taxi and what is all the information about it well there are multiple trip records they have fields and inside of that the fields have fields there's nested records all the way down to latitude and longitude and here's a path this path is very short this path is even shorter look at that i deliberately spricked a small one oh here's a big path here's one of its paths and it's the fact that these can all differ from each other which is the motivation for all of us okay so yeah the second to last of these has an interesting route so we can select the 75th taxi the second to last trip this just numpy slices are like this and look at just that one and we can extract the numerical data by picking off the field names these are just the longitude differences and these are just the latitude differences and now we've got really numerical arrays and when we have numerical arrays we we can you know perform computations on this add them take square roots that sort of thing so let's plot this one path so just using matplotlib to draw it and that's where the taxi went it's following the streets and chicago is a grid um now this path starts at zero zero uh because these are latitude longitude differences um so now what i'm going to do is plot just not just one trip but a lot of them because remember the thing i said at the beginning is the value of array oriented programming is you get to ask a question about a whole data set about a lot of things and get the answer back for a lot of things so now for one taxi i want to see all of your trips okay and here i'm using a i'm unabashedly using a python for loop because i've already limited the size of the data to only that which you know there's only like 100 things here matplotlib can draw a hundred things if it was drawing the whole data set the whole file that'd be millions and it would crash um thing to notice about this is that some of these look the same but there's a sort of offset it's because there's a road there actually i think this is the road that goes to the airport and starting at different places because these are latitude and longitude differences so we want to find the actual latitude and longitude here's where we're going to use broadcasting because each of these trips so what we want to do is for each one of these paths each of these paths has a single latitude and longitude start we want to take that latitude logic to start and add it to all the points along the path and then a different path has a different latitude longitude start when add over that and if and you're thinking numpy thinking in the way you would do this sort of thing in numpy if this were regular this would be broadcasting like you would have a one-dimensional array of latitude longitude starts and a two-dimensional array of uh difference paths and you would want to take those starts different start for each row and add it to the whole path and all we have here is that the paths have different lengths from each other that's the only difference from numpy um here let's let's go through it uh like in python code just you know one thing at a time we have this uh synthetic example of 0.1.2.3.4.5 and we say we want a begin position of 10. here's broadcasting you take the begin position and you add it to all of the those differences and you can get something like 10.1 10.2 10.3 10.4 uh so to do that to a single trip here's the begin position uh taxi 75 the second to last trip and get its begin longitude taxi 75 second last chip get all of its longitude differences and just looking at them a single begin position and an array of positions along the path and the broadcasted is you just add them often um when people are starting into this and they're thinking about it they they ask what do i need to do in order to and then describe some situation and the situation they're describing is broadcasting so you just you just say plus it's it's often you know the syntax belies it looks simpler than the thing that you're thinking of especially if you're starting from an imperative point of view so now a lot of trips now i'm not picking now i'm picking just one taxi but all the trips so there is an array of begin positions and they're different from each other and there is an array of lists of paths so it's just one level deeper so the syntax is just take the many position begin positions and the many differences and just say plus and that adds the right one to each because they line up and again it's just like numpy but the paths have different lengths from each other now let's look at this in more detail some of them are short and some of them are long but they all have that minus 87 which is you know approximately the longitude of uh uh chicago they're all pretty close to that and the data type of this that we just looked at as uh 500 uh lists of float64 and some of them could in principle be missing so that's that's what option means oh i should help here option or question mark they both mean could be missing okay so now i'm just sort of building this up uh all the longitudes all the longitude differences all the latitudes all latitude differences so we add them um and we uh get rid of the missing values is none there are functions in numpy for for you know asking questions across an array uh this is one that's asking you know is is the data missing uh and so we uh and tilda is not that's by that's uh you know when you're doing logical operations with arrays until it's not not missing so there they are and now we can plot these from where they are so now these are paths that the taxis took all uh in in real coordinates so now it's not a spider all coming out of zero zero it's um something kind of looks like chicago none of them go too far to the east because there's a lake there and uh oh this is the airport whether i keep going there okay so then you want to do this to all taxes at once this is the the broadcasting thing and the the you know the way that you think in array or into programming we just did this not with a scalar but with uh something that is one level of vector less than the whole thing so it's the same syntax so to go from one taxi to all the taxis we remove the brackets one so that's all of them so we've just corrected the uh we've turned all of the relative paths all into absolute paths uh with these lines it's plus and here's the data type it's one level deeper than the previous data type one more var all right i'm going to check time for 448 this is okay i think you'd rather have more introduction than time on the exercises the solution to the exercise is actually a one-liner so this is maybe seems like a lot of buildup for something anticlimactic but uh but i think you you want to have the the introduction because this is new stuff you know literally now so um uh what we'll be doing actually to to motivate where are we going with these calculations to start off just sort of exploratory uh where we'll eventually be going is we'll be um we've got all these paths and the goal of the exercise will be to find the length of each path just computing distance formula on the variable length paths and in order to do that they can't be in latitude and longitude they have to be in a uh distance unit so um convert them convert the latitude longitude into kilometers kilometers relative to some place in the middle um and here i'm doing it approximately by putting a you know a single latitude to kilometers longitude two kilometers this is valid on the globe this is valid in a short you know where we're only looking at a single city so it's you know this approximation is close enough uh so here we've turned the lattices and longitudes into kilometers east and kilometers north now those are our coordinates and then just in the distance units we'll look at this taxi again you know the same trip has been making all this different now is the uh is the axes these are no longer latitude longitude they are distances relative to an arbitrary point which is i think that the mean so that's where zero zero is okay the exercise so long time getting here but yes um the goal is to calculate the length of a path and i'm going to give you you know give you all the formulas you got a path that is defined in terms of points in the lag in the kilometers east kilometers north so those are the xy coordinates the length of a straight section of this path is uh square root of delta x squared plus delta y squared um and uh actually i want to know how much did we labor this uh the is there anyone no um how do i answer this uh everyone hands up again i'm beginning this is the first time i've used this technique and i'm liking it uh uh put your hand down if you are familiar with the distance formula in doing this and okay i thought so i just didn't want to like assume um so yes yes there's a distance formula and you can calculate like that over over one segment and then the length of the whole thing is the sum of all the segments now the the complications that that come here are first you're given the xy positions of these points you want x y differences think about previous exercises um and how you did that there um another complication is that this uh here we want to sum over something but we don't know its length and each one will have a different length that's what's new we can do it with an imperative for loop so example east example north i'm just picking one path 75 minus 2 again our favorite there's that array and here's the imperative formula and yes delta x is a variable name i'm using unicode so uh for i in the range uh length of the list minus one because you're doing differences you can't go over the whole length you have to be looking at length minus one for all the gaps in between uh the next one minus the current one the next one minus the current one and now we've got delta x and delta y so we can compute delta r and the total length will be the sum of those your solution must apply to all trips of all taxes in the data set and so the output type will be the number of taxis variable length and it will have variable length of floating point numbers question mark or option this is missing data um and for the for this problem that's just a nuisance you know there are missing values uh but uh um they won't matter the the calculations go right through the the missing values um with your results if you if you call distances.show it'll look like this so you'll have a way to check that your your result is right and then they're hints and then i've got bonus exercises if you've got lots of time we can go on from there but let me just say this and give you a lot of time to be working on this and if you are you know really excited and you and you go on or actually if you want me to explain the the later parts just call me over and also with all this please do call me over any problems i was expecting to be going around actually i was expecting my two helpers and me to all three helpers and me to all be going around uh answering questions so so please do please do ask and that's it let's go ah the next time is uh at 5 10 it says i'll be going over that's 10 minutes from now now i will give you i won't give you 10 minutes i'll give you 20 minutes and then we'll spend 10 minutes talking about the solutions so this will be 5 20. things in an um array with the metadata besides the i guess pandas overhead is there an advantage to not storing that to something that would call like a tidy data set where you have one row for every single trip and then every feature of that trip in columns is there an advantage to storing it right that's the last one actually the last exercise yeah when i said there's these extra exercises at the end okay there's a comparison to that okay um sometimes yes sometimes no i mean there's always um depends on what kind of problem you're doing but the thing is that trying to fit all problems into tidying data okay um right so i'll be showing how to do this that's why it's so blurry and the first steps will be getting back to where we were in this new notebook i have to load the data again get all the latitudes and longitudes into kilometers okay and the single single path example for comparison because again comparisons is good uh i mean as a way of checking what we're doing all right so um start with the the one line solution one uh one's line solution for doing the whole thing is is this line i'm gonna break it down no i didn't can you believe that i will verbally break it down okay uh the image that we want to have i wonder if i can get to the the picture you remember the picture of the of the spirally path and then a segment on it and uh delta x squared plus delta y squared square root is delta r um so and also i should find my pictures directory so you can keep working while i'm finding uh finding the pictures i want to show you um shifted operation yeah that's one of the pictures i want to show you length by segment that's the other oh good i found them so we want to do this thing and we can get delta x and delta y using this same technique that we did early on where if you have some list some array uh and you take off the first one and then you have the same array with it you take off the last one now they line up you know offset by one you can subtract in that direction and you get the the difference between each element and its neighbor and that's how you can get delta x and delta y and you and the syntax for this now i wish i had like a whiteboard i do have a whiteboard look at that syntax for this is uh in with normal arrays is if it was a one-dimensional array it would be yeah first you take off the first one so this is all the the ladder minus take off the last one so that's what we did in the first exercise um that is here for the kilometers east there it is it goes from this line that line the one colon and up to the uh the colon minus one and then what these are is the first two dimensions the taxi dimension the trip dimension and this is the path dimension so if this had been completely rectangular that's exactly how you would do it with a numpy array you want to do it on the third dimension but these are not rectangular these all all of them have different lengths they're just lengths that happen to line up perfectly because they come from the same arrays and because of that this same syntax just works so you can think about it like like we're done pie and then i'm going to just go away yes it can just go away and then the rest of this is a formula this delta x squared again i'm assuming that you can see my highlights and yeah you can't this delta x squared where my mouse is going around and this is delta y squared they're added there's the the plus that's inside a square root okay last part is you have to sum over that that length aux sum axis is minus one minus one is the deepest axis so like if you had a three-dimensional numpy array axis zero is is the first dimension x is one is this second x is two is the third or you could count from the other end minus one is the third dimension so it's a it's a sum over the third dimension of that uh that square root that's it and i think that this notebook is not working anymore maybe it was just the kernel that died i don't know oh no it didn't die there it is it's working uh and uh and that's the result um so it becomes uh this rather easy one liner not easy i don't want to be saying things like that it becomes this rather concise that's the word this concise one-liner um and what we started with all of this at the beginning uh was apl how they made this super precise language this mathematical notation and it got to the point where it was so concise it was very difficult to build these things and when you have a success at the end of the night and you know your program runs you put it on a t-shirt because and it's just garbled egg well um this concise thing is readable i would i hope and i'm going to i'm going to say that and i'm going to uh hope you are going to hope you agree and invite criticism that's what i'll say uh that you know after you see that maybe it might have been difficult to get to this maybe it's less so if when you've been doing this for a while just like you know when you just started with numpy but when you see that and you are familiar with numpy i don't think there should be anything mysterious about this right because there's your delta x and there's your delta y and pita goji i should have made them separate variables break them out into separate variables um but yeah then then the the further exercises are doing things like not going over all the paths but like finding as the crow flies distances and you know i just had a lot of content there in case you know we zipped past things but actually we are just perfectly on time uh this is exactly where i wanted to end um and so any uh you know you can catch me after um we'll be going to wherever we're going next that's where i'll be and if you want to talk i'm very open to that um questions right here after this too and and if not if we're going several ways thanks and thanks for coming and have a good day have a great conference [Applause] actually i did this problem in solution first so the pedagogy got better as i was writing one i was working backwards
20,Introduction to Causal Modeling,https://www.youtube.com/watch?v=tkJ3xo28-T0,[Music] basic foundational understanding of math and statistics uh what can i show them that i really connect with them versus working really sadly that knows or not knows data um um and then hopefully you'll be able to leave here and start doing your own causal analysis and sort of have that foundational knowledge to get done and learn more because this stuff can be very very happy which is annoying i'm going to try to avoid that it's another one to do a little bit of that but we're going to try to make this magical as possible for you so i think a really good way to kind of take this off [Music] so a long time um a year and a half ago a year ago but people were wondering whether you supplementation taking the supplements up to you this morning had any effect on your development of severe healthy symptoms because vitamin d is kind of linked to the immune system it was kind of an actual question you know um [Music] very natural questions and uh when you think about that from like a causal lens uh maybe you know if you had everything at your disposal and you had a time machine and you should go into alternative realities you could just you really want to answer that question [Music] started um and being really helpful on a daily basis in early 2020 started picking up right every day they were really diligent about it um they lived their life you know they mostly say the noise [Music] [Music] [Music] it's would be to take that same person an alternative reality uh also called a counter-factual reality [Music] and we take that person and we somehow get in [Music] demographic same lifestyle they make the same mistakes occasionally they get viewers with friends who haven't asked that um [Music] the same way except this time we'll observe what happens and you can essentially control for every single thing except for that one factor which is vitamin d supplementation and now you have a really interesting contrast here you can see the vitamin d make a difference at all for this person of course you can't do that right like we don't have access to alternative reality so if you really make these classes so the closest thing we have are things called experiments or uh av tests or randomized controlled products have any of you been part of experiment like whether it's in physics or uh social sciences or anything okay it's pretty intuitive right like you take a treatment that has that's kind of the vitamin d supplementation and you randomize people you follow them forward in time and we see what happens in a few months you know six months uh i don't know maybe thirty percent of the three members get severe children but only twenty percent of the control group does now we have that sort of [Music] or something like difference so same sort of deal we'll give them sugar pills versus vitamin d and we follow them forward um the problem is for blind you can't always do experiments to see where medication works or whether lead exposure causes during damage or whether um neighborhoods that have more equal income distributions have less violence or something like that all these interesting social experiments that we want to do is they want to save us um sometimes experiments are not ethical right uh you know i can't take a thousand pregnant uh people and give like half of them uh severe lead exposure and the other helps like nothing at all and see like oh you know what the kids do develop kind of differently and it seems to be a problem i mean you can do interesting things like assign half the group to get some sort of abatement program or something if they went out of their house to see if that induces some sort of benefit but when you do tricky things like that you sort of there's a whole set of problems um also experiments are just not feasible i can't if i really want to see whether income inequality is related to violence in a neighborhood i can't take uh a thousand families and ask them to move into other neighborhoods where there's like more equal income distribution that doesn't happen it's very logistically hard it's not impossible um so the whole point of this tutorial session basically is what to do with these situations uh you have observational data and you really want to make a combo claim whether it's a client that your company is launching is going to work or a new feature is going to work for that product or whether you know in your social science research um you know some question is you want to answer that question and uh i keep asking this question how many of you have prior experience at all with classic machine learning like nothing fancy just sort of tiny machine learning oh okay okay so that's more than that um [Music] one thing that was kind of helpful for me when i started this out was understanding that there's this difference between um prediction which is like classic machine learning i want to predict something i'm classifying something you know like this um this new user is going to be a power user this is a you know moderate long or uh i don't know like this car is going to have an issue versus not versus an inference type question which is more classic statistics and i think this is a good frame and it kind of kind of gets you your mind in the right space to think about possible modeling so i highly recommend this paper by the way this paper is incredible and it's a little old but um the first of it is inference questions and modeling answers questions like what's really events um is x associated with uh what is the strength of that association is it really correlated is it legally or the causal difference question which is that z cos y in real life versus prediction which we will use a little bit in this talk but i recommend you kind of beat it by the door for now what we're going to be doing is very different from position which is how can i best predict something um between the three predictors a b and c what's the best predictor of the outcome uh or why did the model make this particular prediction which is interpretable um so highly recommend if you have an interest in this topic check out the speaker really good but yeah so um a way to think another way to think about experiments versus causal inferences sort of hierarchy of you know on the far right there you have randomized experiments which are the best way we have right now is the old saturday to figure out if something works or causes something you can make really strong puzzle planes with it if i work for a company we've got a huge ap text and it shows a product that's working that you know results in this much lift is they're not easy like i mentioned before like it's like not always feasible uh it's not always technical [Music] if you move a little bit to the left there is this group of methods called causal entrance which is why you're here today which are really powerful awesome methods they you can make causal claims with them they're just a little bit less strong than experiments but they're still gold you know when you can't run an experiment causal modeling exercise is fantastic it's a little bit easier because you just have to work with the data you have funding you don't have to randomize people and follow them forward in time and then finally on the far left which is is something that i think a lot of machine learning practitioners are familiar with which is uh just looking at pure correlations you know i have a data set it's a titanic data set or something and i'm noticing there's a correlation between uh what class you want with titanic and your likelihood of passing away or something like that that's that is a pure association um and there's nothing causal associated with it it's just you know it's like that's when i saw all the data i don't know if it's real or not but there's an association it's the easiest thing to do all these other things but it's also it provides the weakest costs uh someone already even useless in terms of making possible so this is a really good hierarchy to think about as you explore possible modeling um a lot of people will kind of barked at me for not including uh there's a mathematician and computer scientist named judea charles who puts out this causal hierarchy that i don't want to show you this for very long because it gets more theoretical and it's kind of complicated but basically this is a simple location of this so if you know you carry on with this stuff you'll find at some point oh this guy you say a pearl he writes a lot about the possible hierarchy um take a stand up looking at it but understand what it means but basically if you still like that that's all um cool so uh we're going to talk on a little group exercise in a second but um is anyone familiar with causal graphs or just graphs in general sort of like you know the edges uh yeah okay half the people for the folks that aren't familiar um this is going to be related to our first notebook exercise graphs are a really really uh fantastic way to understand causal relationships [Music] and sort of take your domain knowledge about a topic whether i think how many quality some sort of public policy outcome or health outcome or some you know whatever some project you have at work and put it on paper show the relationships between things kind of understand what's really going on what's the process that's resulting in this data what's really going on here so uh it's really simple actually i mean you'll get it within 10 seconds if you haven't done this before but essentially each of these circles the nodes represent some concepts or some uh entity or something like that and the arrows represent a causal relationship a is causing g uh e is causing e but b also causes t uh a little bi-directional area at the bottom between c and d we're not going to touch on that a lot but sometimes we'll see that that represents a pure correlation there's no causality is not called c and z does not cause d um so look at this i'd say okay cool so i have some causes and effects uh it looks like e is sort of the final effect so coffee primary says [Music] i guess and yeah that's how you interpret it it's kind of simple um but very helpful for this work and i thought we could do a little group exercise to kind of lift everyone up because i'm still really sleepy so let's say um [Music] and our primary task of this company is to kind of assess whether incidents are very costly in terms of medical costs versus plus costly and so let's together kind of draw out a causal graph relating all these concepts on the left to the medical cost of an accident that happened yesterday so you know imagine like put on your sort of like insurance car insurance policy and let's try to relate some of these things to medical costs that are accidentally and some of these things on the left can cause each other it doesn't have to be just like sort of long-lasting um so i'll get started to kind of get everyone together i think that [Music] i think that the presence of an advanced airbag is going to be causally related to the medical cost of that accident so i think if you have a good airbag that functions well and it's very advanced you're probably going to have less injuries to me um so if you have any other ideas here to shout out i'll add them on the screen and work together what do you think maybe more than would watch everybody [Music] sorry what's up megan morton uh [Music] uh i think the cause of the relationship is from medium order to advancement that makes so much sense yeah right rating power security i think it probably it'll the advanced airbag will cause a higher uh yeah at least just so much sense yeah okay all right so first the direction of things thank you so much um so like this thing exactly yes that makes sense cool what else do you think i mean i don't know how uh an insurance company would get uh a person's risk aversion but look i know maybe there's some crazy advanced car insurance company um i don't know what else do you think i'm free to shout them out if they took a driving course that probably affects an estimator that's where the worst conversion okay cool i like that uh how should these things be around your horse to your worst exhibit driving closer version [Music] so you think uh taking a driving force will make you less for slippers but the idea or more a risk of being risk-averse will make you more likely to take a driving force oh yeah i don't know interesting um yeah i mean we could make this bi-directional i guess [Music] right what's that causality can that be bi-directional it can um so for instance uh physical activity and depression have a bi-directional um causal relationship you know do you do less exercise you're more likely to be depressed if you are addressed you're less likely to exercise but we won't be talking about too much about that it's kind of complex um but yeah that's a good question very good question um [Music] how does this relate to the medical costs do you think does this [Music] yeah i think so if you're risk-averse you're not gonna drive super fast [Music] the point is [Music] there's no data involved and that's that's something that is really important in causal modeling is you have to sort of know the domain you have to know what generates the data and what's going on in real life um a lot of people say causal modeling is sort of machine learning plus or deeper domain knowledge essentially a good question or not question [Music] cool okay so um we're getting into an exercise it'll happen a little bit but um we're going to start talking now about the four types of causal relationships that are going to come up by getting on the [Music] um because you know if we can draw a graph like this you can kind of talk through like uh this is an interesting little piece here we gotta watch out for this this is the problem there are things to look out for with causal entrance you have to sort of avoid traps so uh let's say you are the mayor of a big city uh a million people live in that and you have a big crime problem particularly and so you're a responsible mayor you're 21st century mayor so you say i want to see the data i want to see what's going on here so uh you have some analysts that brings you data and it's like a really strong television between ice cream um does that make sense yes yeah yeah it does not make sense um but it's such a strong correlation and you can't ignore it right there's going to be something there uh well probably this is kind of a canonical confounding example here and we're going to hear about confounding a lot the problem is there's actually a causal relationship between hot weather this third variable which causes ice cream sales that causes violent crime uh both of these things are in furniture if usually um you know if you have more opportunities for violent crime when the weather is good and also um a crazy epidemiology finding other weather induces more like range i think in baseball games or something like that when they're hotter um [Music] so with the confounding um what's happening here is the fact that hot weather is possibly related to ice cream sales and violent crime so that sort of splitting there induces a fake relationship [Music] [Music] so how do you how do you fix this i mean you might say hey my status actually brought me a nice correlation but that's you know that's bs that's not what's going on here i want you to update your analysis you sort of get back to me and building thank much so what we do is you control for this hot weather founder this first variable and once you do that the association between ice cream sales and violent crime goes away [Music] and what's the most appropriate way to control for that encounter so i can really understand what the except what the deal is between ice cream sales focus association and so one of the four big uh [Music] data set where people provide information on their coffee consumption and you have information on their medical records as to whether they have lung cancer you will find a moderate association between coffee consumption of lung cancer every single time which turns out is completely bogus there is no biological mechanism that can relate healthy consumption to the risk of getting cancer if they study that extensively if anything it's the opposite it's coffee it has some antioxidant properties so what's going on the problem is smoking um i killed checkers the other day is actually possibly related to coffee consumption but you are more likely to consume coffee if you smoke uh which i find in the middle check but it is a real thing actually turns out um it's not a correlation causes lung cancer so you see this bogus association in your data set your observational data between the amino acids shouldn't be there um so what you need to do is control for smoking in any sort of study where you want to see what's the relationship between epoxy confounding uh is uh can be very damaging to analysis it can change the direction of the association between two things you know you know there's truly no relationship between pogba and my cancer but you might find in your observational data there's a strong direct association you know [Music] um coffee has a strong protective effect like super protective like we should all be drinking coffee constantly we'll never get cancer ever which doesn't make sense um and so it's a big problem uh and a model will ideally control for encounters like this but if it doesn't uh this is a little bit of lego here um any leftover confounded like if there are other confounders kind of messing up the association between coffee and not texture we call that residual confounding like leftover confounding after you've tried control improvement question yeah so how do you come up with that you did it a few minutes ago so you have some domain knowledge and that's really cool with the graph yeah yeah you have to have some domain knowledge you have to kind of this is what makes this is what makes causal modeling and difference i think more challenging than machine learning frankly you have to have really good domain knowledge you sort of observe like there is this association it seems between coffee and smoking i wonder if it's causal we have data on that that's awesome i don't think it's you know anything but it can found let's try to control for it and see what happens and we do that um it's a lot of domain knowledge yeah but i mean i think the main knowledges should be really important machine learning too right just be able to know what questions to ask and what questions not to ask that's that's very true that's an interesting question yeah that's a good point i guess the difference is with machine learning domain knowledge is really key it helps you sort of identify the projectors like based on this i think we're going to make this feature in that feature or something like that with causal inferences we're going to find out later on you don't ever want to put all your variables into the model to like sort of control that controlling for some things is actually harmful but they're going to be the next enemy to look at actually so you have to sort of be a bit discriminating um yeah and more lego um i mean i don't know this is necessary early but we call a situation where a confounder pushes the association from zero outward a positive confounding versus negative compounding where it kind of shrinks the real yeah so uh go back to the previous slide sure so uh we talked about kind of like the leftovering boundaries calling the residual compound that sounds kind of like it's analogous to like aesthetic uncertainty in machine learning like you really don't really know like your whole model i guess i'm not too familiar with that you just thought it was kind of like a bias right so you would have like things related to the stochastic statistics of something versus like missing information missing domain knowledge in your model so how do you quantify that i guess so um so they can come up in a few ways so one is let's say copy is a simple one or zero interview it's a binary variable and one cancer is a simple binary one or zero and you have a choice between controlling for smoking which is either a one or zero or a continuous variable which you can sort of handle non-linearly working interesting ways oftentimes if you control over smoking as a binary variable you'll have residual confounding because you haven't done if you sort of lose information of smoking whereas if you were to use a causal model that we'll get into later on and it's sort of continuous maybe you can account for more nonlinear and interesting sort of confounding effects um that's one residual compounding or you know if um you identify smoking and some other confounders of this association but you leave a few out that's also the residual compounding i'm not sure maybe that is sort of what you're talking about right so it's kind of like how do you detect that you have like an unacceptable amount of residual compounding [Music] so that's what makes cosmetics really hard there is no ground truth i i you know if i'm really if i want to understand the difference the association between coffee and lung cancer i'm not really going to find any ground truth um data or like thing in the real world that says no there really should be an association and there should be 0.5 or something like that with with machine learning um you know i can make a model predict something and i can tell you repeatedly it predicts 80 accuracy and many different test sets it hasn't seen and so i can be reasonably sure that it's going to perform that well and uh i know how it performs on unseen data with console inference you know it just is your generally knowledge good because your data good you have to the best swing that's i think that's what makes this a lot harder than machine learning honestly yeah um okay so another quantity number two or like um console relationship number two is called the collider it looks similar so in this case here the arrows are going from the two to one you have a common effect a little different than this you never want to control for a flight people will induce bias so in this case here if you want to understand the association between smoking and lung cancer and you happen to have a data set that has those quantities and you also have numbers of sick days taken you might think cool i'll just control for that you know [Music] [Music] um [Music] they should be causally related and lung cancer will cause you to have more security for that you just have to follow your domain knowledge that's really another way does that make sense the difference between this symbols the third one is called mediation which i think is really intuitive which is when you have a variable just sort of in the middle of a pathway between causal pathway between two things if i want to understand the relationship between smoking and lung cancer and i have some data on clinical signs of lung damage which is sort of between the two if i control that the association between smoking lung cancer goes completely away because i block the pathway that makes sense you know if i if i shoot out that little variable there there's no connection between smoking and lung cancer i don't think there's another uh can you explain what you practically mean by control for like how does that manifest in an experiment yeah sorry um [Music] so there are a few ways to control for one is um you can control for things with a model which will do some modeling a little bit and uh exercises two three four five um so these are actually the models we can usually throw in and it will do the magic for us you can also do really basic things like um if you have a data set and uh the variable you want to control is like a one or a zero like a binary if you force if you sort of filter out all the zeros so it's already one there's no variability in that variable that's controlled before that's called stratified comment so you know in the absence of any modeling techniques to just force that very well you want controls [Music] does this make sense sort of mediation relationship [Music] okay um and then the last one which i guess is less of a concern is just unrelated so if i want to understand [Music] which um causes lung cancer there's and that's it it's just sort of a single pathway there's no problem with um leaving your own controller [Music] pathway between that to a variable and not something it's not a problem it's all good yeah so wouldn't this still kind of affect the effect size for small paint oh sorry what's that wouldn't that living out the other variable affect the effect size of smoking it would if there was a causal arrow from here to here then it would be a confounder that first relationship we talked about but because there's really only one pathway here and this is the only relationship then it's you know leave it in control for it doesn't really matter [Music] this can be really confusing actually because does this make sense these four relationships more closely that would be helpful it took me a while to understand these four things so it's cool makes sense [Music] now of course um the real world is really complicated um and so if you do embark on these exercises you're gonna find you're on a whiteboard somewhere drawing out things like this it gets really complicated um but it's important to do these things because if i were to just sort of say i want to see if smoking has an effect on lung cancer and you toss all these things into the model if you're going to be controlling for mediators you're going to be controlling for containers which is good you'll be controlling for colliders that's something that's bad so you have to sort of think through what's going on here to the best of your ability um you know work with zoning experts learn it yourself and start to kind of piece it apart um because this is probably closer to reality now if you have like the uh variables or like you know entities then you'll have all these different arrows going between them it can get complicated quick but it's important to do this stuff and i think the best advice i can give honestly is um sit down fixed up at what variables you have like what data you have and then talk with your domain expert and try to figure out what are like what do you think are really key variables like let's just focus on like four of these and sort of think through what they could be causing what their relationship is as opposed to like let's bring a data set into like a hundred programs and like sit down for a few days like that's kind of a losing proposition i think so yeah so we're about to take a break for that first exercise one sort of final summary is experiments or a b test are really awesome because the active randomization uh between some treatment like treatment and control uh breaks all the balance that's why they're so valuable they're awesome you know if i'm randomizing half the room over here to um take some real pill and just have to take a sugar pill all of your demographics and all of your attributes are uh no longer causally related to the treatments that you're randomized so you've broken all that confounding and it's you can make an awesome easy causal claim from that with console entrance the task is to take observational data and think through the domain knowledge and try to construct something like this and sort of think through like control for that why not and that's i mean if you can do that that's when the magic happens and you can make a really great possible plans for things so um the first notebook exercise uh requires that which unfortunately i heard some folks were not able to install um if you're not able to install it don't worry i think you can still use a lot of the notebook but i would i would look at the teacher notebook which has some of those thoughts in it you know basically we're going to be uh generating synthetic data simulation data that covers the four relationships confounding collider violence deviation and unrelated predictors and just sort of verifying for ourselves that okay i have a compound either if i leave it in what's the association if i control force is that change so that's what you'll be doing it's in um one student puzzle graphs that um yeah i'll walk around let me know if you have any questions or anything and um you know probably spending like 15 20 minutes see where we're at if you'll be working groups if you want to work individually yeah and one more question for the last night when you said in your advice that um you would like to only you know care about four of these things and then really figure those out aren't you prone to accidentally you know control over letters for instance when or do you have to work out all the relationships i guess that was foreign [Music] i guess like the best you can really do is sort of take stock of the variables you do have and just try to make an honest effort at capturing the really important length of your mind for the biggest confounders and i mean honestly that is the best you can do you're doing it vigorously with domain knowledge and you know take a honest stat at it um honestly that's the truth uh i think a lot of the way like people treat this stuff is a bunch of a few causal different smaller exercises are as valuable as one experiment so you know if you do a bunch of this work over time you might see the preponderance of evidence suggest there is this causal relationship and i really believe it now after a few different exercises you sort of get their pictures that way um yeah so there's no access to the truth with a capital t that i can look up and see like that and so yes you're correct experiment is the only verification right yeah it's the best way we have should probably be the best way we ever have unless we can access like remote first or something so um so yeah um i guess everyone should have access to the repo right but this is this is essentially what yeah you'll be working on so give it a shot let me know what you think i'll be walking around [Music] so other times time is there any cost to [Music] doing different you know experiments with your data sort of [Music] um [Music] is [Music] pretty frustrating [Music] just [Music] um [Music] um [Music] um um [Music] um [Music] um [Music] um [Music] [Music] um [Music] [Music] [Music] [Music] [Music] [Music] yes [Music] [Music] if [Music] i'm not sure [Music] [Music] [Music] um [Music] [Music] [Music] [Music] [Music] [Music] um [Music] [Music] [Music] um [Music] [Music] [Music] medications [Music] [Music] yes [Music] um [Music] um [Music] [Music] [Music] yes [Music] [Music] [Music] [Music] um [Music] [Music] [Music] yes [Music] very cool [Music] [Music] [Music] um [Music] [Music] [Music] [Music] [Music] data i think to be honest with you i think it's like useful but people tend to like to have formal faith in it i think if you're not using [Music] [Music] [Music] hey [Music] i guess [Music] [Music] okay so what you're doing here is um we're looking closer at those four possible [Music] that you're able to sort of like direct like what the relationships are certain things and simulate data it's really eventually sort of tested in this case here what we're doing is looking at smoking coffee and lung cancer a classic confounding example we looked at in the slide and i'm saying you know in this case here's a true causal relationship that smoking is causing coffee consumption and smoking is one cancer so it's sort of [Music] like this i'm basically encoding that in my package and then once i have that uh um and then you know once you have that panda seated frame you can start looking at correlations between some of these things so this is a spearmint uh pearson correlation uh and i want to see what it is between coffee and lunch cancer risk and i can draw it out i can plot it and then what i'm doing here is i'm saying um we talked about earlier like there's two ways to control for a variable you can throw it into a model which we will do in the next exercises but for this one the really basic way you can control for something is just force it to take down one's out or filter it if there's no variability in that it doesn't deserve any effect of your and your data so i'm saying smoking should be just zero in this data set like look at that substrate and look at that correlation again and you will find that um it changes the probability you control the confounding and you will find that there is no longer any association between coffee generate data control for that third variable and see what you can change that it doesn't have to be 1.5 or six because you have to specify something so okay yeah that's a good point it doesn't mean anything just like that um [Music] i guess my question is like if the coefficient is larger does that mean a strong correlation or it can be yeah yeah if you if you change those coefficients you can have stronger weaker confounding you'll still see some confounding if the basic relationships are like this you know but it still changes the magnitude [Music] [Music] i want to see is coffee consumption correlated with lung cancers there should be much because the coffee does not cause lung cancer it will protect strongly against them and i find that they're directly but if i take this data set i force everyone there's this is basically the zero correlation [Music] so you'll explore here like if i control for a collider you'll see that it induces bias and if you control for a mediator you'll see that it completely nullifies the effect between your variable of interest and outcome and you'll see you know if this is like not enough information to work out but look at the teacher notebook and you'll find that you create an unrelated predictor like in here controlling for it does nothing at all it's just you can do it if you want you don't have to do it so i guess i guess i would say like refer to these slides here back and forth to understand what's going on with these four sections here can you briefly say how the weight argument works in like follow-up creation yeah sure i think i think it just determines sort of the strength of the sort of encoded relationship between these things so like in this case here if i change this to 6 or like 10 or 100 i mean all it's going to do is sort of change the strength of the contact the compounding will still exist so i don't know you'll see slightly different correlations at the end but you'll still basically see this thing of like my control perfect pounding coffee is in no way associated with cancer as it should be [Music] um i probably should have made that clear yeah yeah there's something arbitrary when you put two variables is the first argument into the logistic model things are going to be correlated yeah like in the examples uh generate a smoking variable which is one or zero where one happens thirty percent of the time um and generate like some number of samples of it for a coffee i'm saying i want you to force smoking to cause coffee consumption and i want that magnitude of that that forcing to be 1.5 whatever that means you can make it 350 22 whatever and i'm saying here force this new variable lung cancer raised to be arise from smoking it's kind of cool in a sense i mean you can you can construct like you can use that to construct plastics if you're interested represent um there is no correlation between household income and your genetic risk which makes sense um but if i control for that collider [Music] if i control my collider i see uh oh play with these coefficients here um [Music] [Music] oh is [Music] real quick i had this conversation with you yes okay um uh there's a sort of impression out there that um in machine learning variable importance plots uh tell you about what's going on with the data [Music] i think something really important is going on here taking class two like that too these variable important spots that you see machine learning tells you nothing at all about causal relationships so think about these i think about sort of helping you understand what variables strongly predict versus how strongly but it doesn't tell you about what's causing what at all so be aware that there's sort of a misunderstanding about the data generating [Music] um you know i guess my high level recommendation would be when you want to do a causal impossible entrance exercise i see a two things um i would first sit down and think through domain knowledge bring in domain experts talk through them maybe you do some whiteboarding with them sort of like drawing the nodes and causal relationships but understand what's going on perhaps um and i would pick a small subset of variables that you know you have data on and you sort of think will observe a big effect like for your words and uh before you do any modeling work which we're going to do in the next exercise we're not going to do the sort of simplistic stuff we did in the first uh look at exercise relationships try to understand for yourself this variable in this variable are they is there any correlation there positive negative or not what about these two or not these two that sort of gets you it gives you a sense for like what could be going on what traps you could be volunteering um and then you know of course identify as founders if you can and avoid controlling for things that you shouldn't control for ie colliders and mediators someone else asked a really interesting question actually someone asked this seems like i could possibly automate with stuff like if i eat in a data set with a hundred columns in it um it seems like i could generate some rules or some algorithm to sort of say i think these are possibly related based on i control whether or not the spanish is i control for that this one vanishes so um it is they they do exist um i would say just be very cautious um i i use them sometimes but i always um domain knowledge has veto power over them you know so i'll run it there's a lot of interesting research around it but always rely on don't acknowledge persons the empirical stuff i think they could be a bit about what you want to do if you don't really think about stuff you're just running through it um but um this is an interesting fact uh when you use those approaches to sort of they call it a causal structure like to learn like what were the nodes it could be really tricky so for instance these three graphs they are completely indistinguishable if i only have three columns of data i have no doubt they have just what could be causing a lot i throw one of these out of our legs completely in distinction i for this stuff up here i had to go i strongly suspect that smoking is you know clinical signs and lung damage is not an encounter i think it's causally related to these this way i rely on my domain knowledge um i think that this is a collider i think this is a compound that's the domain alpha keys if i just throw these into an approachable different structure uh or a lot more variables it actually becomes easier to determine the structure because you can rely on sort of more variables to bounce each other off yeah you just want the system to learn like the amount of influence would you still say that's not a good idea when you if you if you gave it the structure just say give that data can you learn like how big you still say that the relationships and the strength of cause um which is you know nowadays we know that black box models are awful for modeling like crying or against credit card or you know what job applications should be looked at more closely modeling offers you a really interesting way to sort of describe the relationships and data and things through where bias could be coming in and causing damage in a nice like visual framework so there's this like well-known base from the 70s uh um sex violence that occurred at berkeley um sort of a debate whether or not it actually happened or not it's a really interesting case actually if you look into it a little deeper but um causal perhaps like the kind we just worked on offered you a really nice way to sort of understand what could be going on there's a really cool um the google lead mine post by uh sylvia where she explores the gender and how it influences uh department edition a really really cool article i highly recommend if you're interested in stuff cool okay another improved exercise um so we talked through the four causal relationships we're going to be getting very very soon to actually model and work something kind of closer to like sk with causal inputs which is cool much easier than the first couple of people okay um we have to talk through some of the assumptions of causal interest turns out you can't just like you know sort of that data really really you have to also beyond domain knowledge you have to sort of think throughout my assumptions met or not because there are close functions so bridge consumption is you know confusing doing puzzle records work is temporality causes always occur before and so if your data set you want to see whether some variable has an effect on another variable and the outcome occurred before the thing you want to see if it had an effect causalities does not work for you it will provide focus results i would not trust it at all you have to ensure that you know as you drum this out our causes actually from the core effects in the data generating process um you have to there's something called uh you know so in the case of like smoking coffee and lung cancer if for some reason patients cluster a certain way and like influence the coffee consumption of behavior of other people if they throw those causality out the way you can't trust results every individual is completely unique and they're often related to each other which can be challenging like especially if you do personal interests work with like social media stuff people talk to each other there's something called positivity which means um you have to make sure you have enough data so that within each of your code areas you're controlling for are there enough outcome values one way or are there enough treatments to have useful yeah um let you finish oh okay um yeah you don't have positivity for instance um you know we want to control for event boundary but in your small data set you only have folks that like receive your treatment and um and then finally the big one which is you know this happens quite often honestly um ignorability which is you have to account for all major confounding variables if your data doesn't have critical and valid variables [Music] i don't know maybe consider what it's feasible to write an experiment and sort of control for compounding that way and probably better estimates but you have to have the right kind of data otherwise this doesn't work yeah so so for bi-directionality then your data set needs some kind of high measurement there right to be able to say follow the here before effects and then you have a bi-directional oh yeah that can be really tricky yeah yeah exactly yeah i can throw things off here so i would say if you have this much of them keep them on this one this page on your screen for the next few exercises and here's some example studies i'll actually sort of shout out when you think assumptions being private uh okay i want to understand whether frequent emails to customers could impact customer satisfaction um i have survey data from uh customers um and uh self-reported satisfaction from a year ago and i'm going to use this month's number of emails right into that customer sort of a proxy for how much we generally have an outcome from a year ago that i'm going to sort of use as a proxy for satisfaction i'm going to use this month's number of emails we wrote to the customers in the cents for like how often do we fight them generally what do you think was vital here [Music] and so i want to control for something like 20 confounders the problem is like 10 neighborhoods of analysis is a neighborhood i'm not looking at people i'm looking at each row corresponds to a unique paper so my sample size is effective 40. can i do this multiple what's that positivity is positivity yeah there is across 20 strata of hilarious to account for like you know clean neighborhood versus nothing high congress is not just there's no way there's no direct building in each column essentially so this is bogus um what about this one i i'm working on a company we're gonna release a new multiplayer game through uh some social media app um and i wanna see if like this um this game this app impacts user engagement i want to see if like the presence of the game makes people more engaged so the problem here though is when i ran this this analysis users some users have played the game some have not and because of the social media app users can sort of talk to each other and say like hey this is a cool game let me check it out and sort of influence each other's exposure to the game what's wrong with that we have a job training program we want to see how you can impact someone's future income um unfortunately we don't have a ton of data on participants so essentially we're just going to control privilege to see if there's a possible relationship between doing the program [Music] [Music] sort of pattern as that earlier hierarchy is showing you where you have like trade-offs you can make between approaches um um is probably the most interpretable and pretty commonly used approach uh you see it everywhere like in the literature and companies uh it's kind of intensity score matching or psn uh it is it provides very interpretable and results for studies but it is a little bit less complex in other methods and it's there's some discussion as to whether it does all that candida control um bit over here you have something a very pretentious name computation um it uses machine learning at the heart of it and there are some studies that suggest cited inside of the us controlled and frequent county um and then at the end of the month which we might have time to get into but um um but the journey's still out people are running simulation studies and trying to understand what's the best way to do it um i think for now i would just trust this hierarchy okay so we're going to be doing psn on the first exercise but i i wanted to sort of return to this idea of in our factuals um so we have an observable um let's say you're working with a binary treatment but i'm sorry treatment smokers is not smoke and the outcome of interest is spell like blood pressure there's a relationship between two smoking messes up blood pressure um i really understand that a causal model approach like with the impact of sodium is essentially i'm going to model the smoking status that we observe and the kind of factual not smoking and hold everything else and see what that build is essentially um among [Music] [Music] um uh there's some great for foreign we're going to use a machine learning model doesn't matter whether it can be something really difficult it's not very simple um [Music] response to get a treatment for um you want to receive the treatment and i'm just getting them on the population of id number five here given all the attributes in this population we'll just barely likely receive the treatment from what we see so that's [Music] [Music] so i'm very similar um like very similar attributes except architecture so eventually you get something like this where i just pulled together the people that are not and i can say that they're very similar in their attributes except for the treatment and now i've controlled for these next in essence and i calculate the average treatment you know which is just you can see the formula down here so essentially like the outcome the average of the outcome [Music] boundaries is uh so the difference here is sort um and shot values are just they're more sort of like correlated you know it's um a strong predictor of the outcome but it's not yeah there's no distinction between this um you know i think it's a different set of applications [Music] training program on eventual learning [Music] [Music] um [Music] training programs um [Music] um [Laughter] [Music] um um [Music]
